{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2096458"
                        ],
                        "name": "Luke Metz",
                        "slug": "Luke-Metz",
                        "structuredName": {
                            "firstName": "Luke",
                            "lastName": "Metz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luke Metz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "16443937"
                        ],
                        "name": "Ben Poole",
                        "slug": "Ben-Poole",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Poole",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ben Poole"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144846367"
                        ],
                        "name": "D. Pfau",
                        "slug": "D.-Pfau",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Pfau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Pfau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1407546424"
                        ],
                        "name": "J. Sohl-Dickstein",
                        "slug": "J.-Sohl-Dickstein",
                        "structuredName": {
                            "firstName": "Jascha",
                            "lastName": "Sohl-Dickstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Sohl-Dickstein"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 273,
                                "start": 256
                            }
                        ],
                        "text": "This is due to the fact that mode collapse comes from the fact that the optimal generator for a fixed discriminator is a sum of deltas on the points the discriminator assigns the highest values, as observed by (Goodfellow et al., 2014) and highlighted in (Metz et al., 2016)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 103
                            }
                        ],
                        "text": "This last phenomenon has been theoretically explained in (Arjovsky & Bottou, 2017) and highlighted in (Metz et al., 2016)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 4
                            }
                        ],
                        "text": "In (Metz et al., 2016) the authors presented a simple mixture of Gaussians experiments that served a very specific purpose."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6610705,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "488bb25e0b1777847f04c943e6dbc4f84415b712",
            "isKey": false,
            "numCitedBy": 741,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a method to stabilize Generative Adversarial Networks (GANs) by defining the generator objective with respect to an unrolled optimization of the discriminator. This allows training to be adjusted between using the optimal discriminator in the generator's objective, which is ideal but infeasible in practice, and using the current value of the discriminator, which is often unstable and leads to poor solutions. We show how this technique solves the common problem of mode collapse, stabilizes training of GANs with complex recurrent generators, and increases diversity and coverage of the data distribution by the generator."
            },
            "slug": "Unrolled-Generative-Adversarial-Networks-Metz-Poole",
            "title": {
                "fragments": [],
                "text": "Unrolled Generative Adversarial Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "This work introduces a method to stabilize Generative Adversarial Networks by defining the generator objective with respect to an unrolled optimization of the discriminator, and shows how this technique solves the common problem of mode collapse, stabilizes training of GANs with complex recurrent generators, and increases diversity and coverage of the data distribution by the generator."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2877311"
                        ],
                        "name": "Mart\u00edn Arjovsky",
                        "slug": "Mart\u00edn-Arjovsky",
                        "structuredName": {
                            "firstName": "Mart\u00edn",
                            "lastName": "Arjovsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mart\u00edn Arjovsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18828233,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9a700c7a7e7468e436f00c34551fbe3e0f70e42f",
            "isKey": false,
            "numCitedBy": 1436,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of generative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first section introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a practical and theoretically grounded direction towards solving these problems, while introducing new tools to study them."
            },
            "slug": "Towards-Principled-Methods-for-Training-Generative-Arjovsky-Bottou",
            "title": {
                "fragments": [],
                "text": "Towards Principled Methods for Training Generative Adversarial Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "The goal of this paper is to make theoretical steps towards fully understanding the training dynamics of generative adversarial networks, and performs targeted experiments to substantiate the theoretical analysis and verify assumptions, illustrate claims, and quantify the phenomena."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47002813"
                        ],
                        "name": "Yujia Li",
                        "slug": "Yujia-Li",
                        "structuredName": {
                            "firstName": "Yujia",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yujia Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1754860"
                        ],
                        "name": "Kevin Swersky",
                        "slug": "Kevin-Swersky",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Swersky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin Swersky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804104"
                        ],
                        "name": "R. Zemel",
                        "slug": "R.-Zemel",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Zemel",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Zemel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 536962,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c8b509be29721ee6b12c880b4d97ed6b60bad217",
            "isKey": false,
            "numCitedBy": 657,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of learning deep generative models from data. We formulate a method that generates an independent sample via a single feedforward pass through a multilayer perceptron, as in the recently proposed generative adversarial networks (Goodfellow et al., 2014). Training a generative adversarial network, however, requires careful optimization of a difficult minimax program. Instead, we utilize a technique from statistical hypothesis testing known as maximum mean discrepancy (MMD), which leads to a simple objective that can be interpreted as matching all orders of statistics between a dataset and samples from the model, and can be trained by backpropagation. We further boost the performance of this approach by combining our generative network with an auto-encoder network, using MMD to learn to generate codes that can then be decoded to produce samples. We show that the combination of these techniques yields excellent generative models compared to baseline approaches as measured on MNIST and the Toronto Face Database."
            },
            "slug": "Generative-Moment-Matching-Networks-Li-Swersky",
            "title": {
                "fragments": [],
                "text": "Generative Moment Matching Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "This work forms a method that generates an independent sample via a single feedforward pass through a multilayer perceptron, as in the recently proposed generative adversarial networks, using MMD to learn to generate codes that can then be decoded to produce samples."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3108066"
                        ],
                        "name": "Ferenc Husz\u00e1r",
                        "slug": "Ferenc-Husz\u00e1r",
                        "structuredName": {
                            "firstName": "Ferenc",
                            "lastName": "Husz\u00e1r",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ferenc Husz\u00e1r"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17206794,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0e4a97a0ccbf699272e3d6dc25b6fe16eb35382d",
            "isKey": false,
            "numCitedBy": 222,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Modern applications and progress in deep learning research have created renewed interest for generative models of text and of images. However, even today it is unclear what objective functions one should use to train and evaluate these models. In this paper we present two contributions. \nFirstly, we present a critique of scheduled sampling, a state-of-the-art training method that contributed to the winning entry to the MSCOCO image captioning benchmark in 2015. Here we show that despite this impressive empirical performance, the objective function underlying scheduled sampling is improper and leads to an inconsistent learning algorithm. \nSecondly, we revisit the problems that scheduled sampling was meant to address, and present an alternative interpretation. We argue that maximum likelihood is an inappropriate training objective when the end-goal is to generate natural-looking samples. We go on to derive an ideal objective function to use in this situation instead. We introduce a generalisation of adversarial training, and show how such method can interpolate between maximum likelihood training and our ideal training objective. To our knowledge this is the first theoretical analysis that explains why adversarial training tends to produce samples with higher perceived quality."
            },
            "slug": "How-(not)-to-Train-your-Generative-Model:-Scheduled-Husz\u00e1r",
            "title": {
                "fragments": [],
                "text": "How (not) to Train your Generative Model: Scheduled Sampling, Likelihood, Adversary?"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper presents a critique of scheduled sampling, a state-of-the-art training method that contributed to the winning entry to the MSCOCO image captioning benchmark in 2015, and presents the first theoretical analysis that explains why adversarial training tends to produce samples with higher perceived quality."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2533850"
                        ],
                        "name": "G. Dziugaite",
                        "slug": "G.-Dziugaite",
                        "structuredName": {
                            "firstName": "Gintare",
                            "lastName": "Dziugaite",
                            "middleNames": [
                                "Karolina"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Dziugaite"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39331522"
                        ],
                        "name": "Daniel M. Roy",
                        "slug": "Daniel-M.-Roy",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Roy",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel M. Roy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744700"
                        ],
                        "name": "Zoubin Ghahramani",
                        "slug": "Zoubin-Ghahramani",
                        "structuredName": {
                            "firstName": "Zoubin",
                            "lastName": "Ghahramani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zoubin Ghahramani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9127770,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "4e2f6b4bc889eed1afe5833d5190f6f02e501061",
            "isKey": false,
            "numCitedBy": 396,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider training a deep neural network to generate samples from an unknown distribution given i.i.d. data. We frame learning as an optimization minimizing a two-sample test statistic\u2014informally speaking, a good generator network produces samples that cause a two-sample test to fail to reject the null hypothesis. As our two-sample test statistic, we use an unbiased estimate of the maximum mean discrepancy, which is the centerpiece of the nonparametric kernel two-sample test proposed by Gretton et al. [2]. We compare to the adversarial nets framework introduced by Goodfellow et al. [1], in which learning is a two-player game between a generator network and an adversarial discriminator network, both trained to outwit the other. From this perspective, the MMD statistic plays the role of the discriminator. In addition to empirical comparisons, we prove bounds on the generalization error incurred by optimizing the empirical MMD."
            },
            "slug": "Training-generative-neural-networks-via-Maximum-Dziugaite-Roy",
            "title": {
                "fragments": [],
                "text": "Training generative neural networks via Maximum Mean Discrepancy optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "This work considers training a deep neural network to generate samples from an unknown distribution given i.i.d. data to frame learning as an optimization minimizing a two-sample test statistic, and proves bounds on the generalization error incurred by optimizing the empirical MMD."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153440022"
                        ],
                        "name": "Ian J. Goodfellow",
                        "slug": "Ian-J.-Goodfellow",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Goodfellow",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ian J. Goodfellow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403025868"
                        ],
                        "name": "Jean Pouget-Abadie",
                        "slug": "Jean-Pouget-Abadie",
                        "structuredName": {
                            "firstName": "Jean",
                            "lastName": "Pouget-Abadie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jean Pouget-Abadie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153583218"
                        ],
                        "name": "Mehdi Mirza",
                        "slug": "Mehdi-Mirza",
                        "structuredName": {
                            "firstName": "Mehdi",
                            "lastName": "Mirza",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mehdi Mirza"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113742925"
                        ],
                        "name": "Bing Xu",
                        "slug": "Bing-Xu",
                        "structuredName": {
                            "firstName": "Bing",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bing Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1393680089"
                        ],
                        "name": "David Warde-Farley",
                        "slug": "David-Warde-Farley",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Warde-Farley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Warde-Farley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1955694"
                        ],
                        "name": "Sherjil Ozair",
                        "slug": "Sherjil-Ozair",
                        "structuredName": {
                            "firstName": "Sherjil",
                            "lastName": "Ozair",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sherjil Ozair"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760871"
                        ],
                        "name": "Aaron C. Courville",
                        "slug": "Aaron-C.-Courville",
                        "structuredName": {
                            "firstName": "Aaron",
                            "lastName": "Courville",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aaron C. Courville"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 104
                            }
                        ],
                        "text": "GANs offer much more flexibility in the definition of the objective function, including Jensen-Shannon (Goodfellow et al., 2014), and all f -divergences (Nowozin et al., 2016) as well as some exotic combinations (Huszar, 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 234,
                                "start": 211
                            }
                        ],
                        "text": "This is due to the fact that mode collapse comes from the fact that the optimal generator for a fixed discriminator is a sum of deltas on the points the discriminator assigns the highest values, as observed by (Goodfellow et al., 2014) and highlighted in (Metz et al., 2016)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 102
                            }
                        ],
                        "text": "Variational Auto-Encoders (VAEs) (Kingma & Welling, 2013) and Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) are well known examples of this approach."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 24
                            }
                        ],
                        "text": "\u2022 JS(Pn,P)\u2192 0 with JS the Jensen-Shannon di-\nvergence."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 159
                            }
                        ],
                        "text": "Our baseline comparison is DCGAN (Radford et al., 2015), a GAN with a convolutional architecture trained with the standard GAN procedure using the\u2212 logD trick (Goodfellow et al., 2014)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 247,
                                "start": 224
                            }
                        ],
                        "text": "In other words, the JS distance saturates, the discriminator has zero loss, and the generated samples are in some cases meaningful (DCGAN generator, top right plot) and in other cases collapse to a single nonsensical image (Goodfellow et al., 2014)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 27
                            }
                        ],
                        "text": "When using the \u2212 logD trick (Goodfellow et al., 2014), the discriminator loss and the generator loss are different."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 5
                            }
                        ],
                        "text": "\u2022 The Jensen-Shannon (JS) divergence\nJS(Pr,Pg) = KL(Pr\u2016Pm) +KL(Pg\u2016Pm) ,\nwhere Pm is the mixture (Pr + Pg)/2."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 33
                            }
                        ],
                        "text": "Statements 1-2 are false for the Jensen-Shannon divergence JS(Pr,P\u03b8) and all the KLs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 99
                            }
                        ],
                        "text": "All this indicates that EM is a much more sensible cost function for our problem than at least the Jensen-Shannon divergence."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1033682,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "54e325aee6b2d476bbbb88615ac15e251c6e8214",
            "isKey": true,
            "numCitedBy": 29656,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to \u00bd everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples."
            },
            "slug": "Generative-Adversarial-Nets-Goodfellow-Pouget-Abadie",
            "title": {
                "fragments": [],
                "text": "Generative Adversarial Nets"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A new framework for estimating generative models via an adversarial process, in which two models are simultaneously train: a generative model G that captures the data distribution and a discriminative model D that estimates the probability that a sample came from the training data rather than G."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2388416"
                        ],
                        "name": "S. Nowozin",
                        "slug": "S.-Nowozin",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Nowozin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Nowozin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2084925"
                        ],
                        "name": "Botond Cseke",
                        "slug": "Botond-Cseke",
                        "structuredName": {
                            "firstName": "Botond",
                            "lastName": "Cseke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Botond Cseke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2870603"
                        ],
                        "name": "Ryota Tomioka",
                        "slug": "Ryota-Tomioka",
                        "structuredName": {
                            "firstName": "Ryota",
                            "lastName": "Tomioka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ryota Tomioka"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 32
                            }
                        ],
                        "text": ", 2014), and all f -divergences (Nowozin et al., 2016) as well as some exotic combinations (Huszar, 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 154
                            }
                        ],
                        "text": "GANs offer much more flexibility in the definition of the objective function, including Jensen-Shannon (Goodfellow et al., 2014), and all f -divergences (Nowozin et al., 2016) as well as some exotic combinations (Huszar, 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 107645,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ffdcad14d2f6a12f607b59f88da4a939f4821691",
            "isKey": false,
            "numCitedBy": 1182,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "Generative neural samplers are probabilistic models that implement sampling using feedforward neural networks: they take a random input vector and produce a sample from a probability distribution defined by the network weights. These models are expressive and allow efficient computation of samples and derivatives, but cannot be used for computing likelihoods or for marginalization. The generative-adversarial training method allows to train such models through the use of an auxiliary discriminative neural network. We show that the generative-adversarial approach is a special case of an existing more general variational divergence estimation approach. We show that any f-divergence can be used for training generative neural samplers. We discuss the benefits of various choices of divergence functions on training complexity and the quality of the obtained generative models."
            },
            "slug": "f-GAN:-Training-Generative-Neural-Samplers-using-Nowozin-Cseke",
            "title": {
                "fragments": [],
                "text": "f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that any f-divergence can be used for training generative neural samplers and the benefits of various choices of divergence functions on training complexity and the quality of the obtained generative models are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7818229"
                        ],
                        "name": "J. Zhao",
                        "slug": "J.-Zhao",
                        "structuredName": {
                            "firstName": "Junbo",
                            "lastName": "Zhao",
                            "middleNames": [
                                "Jake"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Zhao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143949035"
                        ],
                        "name": "Micha\u00ebl Mathieu",
                        "slug": "Micha\u00ebl-Mathieu",
                        "structuredName": {
                            "firstName": "Micha\u00ebl",
                            "lastName": "Mathieu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Micha\u00ebl Mathieu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15876696,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2ba23d9b46027e47b4483243871760e315213ffe",
            "isKey": false,
            "numCitedBy": 874,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce the \"Energy-based Generative Adversarial Network\" model (EBGAN) which views the discriminator as an energy function that attributes low energies to the regions near the data manifold and higher energies to other regions. Similar to the probabilistic GANs, a generator is seen as being trained to produce contrastive samples with minimal energies, while the discriminator is trained to assign high energies to these generated samples. Viewing the discriminator as an energy function allows to use a wide variety of architectures and loss functionals in addition to the usual binary classifier with logistic output. Among them, we show one instantiation of EBGAN framework as using an auto-encoder architecture, with the energy being the reconstruction error, in place of the discriminator. We show that this form of EBGAN exhibits more stable behavior than regular GANs during training. We also show that a single-scale architecture can be trained to generate high-resolution images."
            },
            "slug": "Energy-based-Generative-Adversarial-Network-Zhao-Mathieu",
            "title": {
                "fragments": [],
                "text": "Energy-based Generative Adversarial Network"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "The \"Energy-based Generative Adversarial Network\" model (EBGAN) which views the discriminator as an energy function that attributes low energies to the regions near the data manifold and higher energies to other regions is introduced."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3374063"
                        ],
                        "name": "Yuhuai Wu",
                        "slug": "Yuhuai-Wu",
                        "structuredName": {
                            "firstName": "Yuhuai",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuhuai Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3080409"
                        ],
                        "name": "Yuri Burda",
                        "slug": "Yuri-Burda",
                        "structuredName": {
                            "firstName": "Yuri",
                            "lastName": "Burda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuri Burda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1785346"
                        ],
                        "name": "Roger B. Grosse",
                        "slug": "Roger-B.-Grosse",
                        "structuredName": {
                            "firstName": "Roger",
                            "lastName": "Grosse",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Roger B. Grosse"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 45
                            }
                        ],
                        "text": "For example, we can see in the recent paper (Wu et al., 2016) that the optimal standard deviation of the noise added to the model when maximizing likelihood is around 0.1 to each pixel in a generated image, when the pixels were already normalized to be in the range [0, 1]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 44
                            }
                        ],
                        "text": "For example, we can see in the recent paper (Wu et al., 2016) that the optimal standard deviation of the noise added to the model when maximizing likelihood is around 0."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13583585,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "858dc7408c27702ec42778599fc8d11f73ef3f76",
            "isKey": true,
            "numCitedBy": 195,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "The past several years have seen remarkable progress in generative models which produce convincing samples of images and other modalities. A shared component of many powerful generative models is a decoder network, a parametric deep neural net that defines a generative distribution. Examples include variational autoencoders, generative adversarial networks, and generative moment matching networks. Unfortunately, it can be difficult to quantify the performance of these models because of the intractability of log-likelihood estimation, and inspecting samples can be misleading. We propose to use Annealed Importance Sampling for evaluating log-likelihoods for decoder-based models and validate its accuracy using bidirectional Monte Carlo. The evaluation code is provided at this https URL. Using this technique, we analyze the performance of decoder-based models, the effectiveness of existing log-likelihood estimators, the degree of overfitting, and the degree to which these models miss important modes of the data distribution."
            },
            "slug": "On-the-Quantitative-Analysis-of-Decoder-Based-Wu-Burda",
            "title": {
                "fragments": [],
                "text": "On the Quantitative Analysis of Decoder-Based Generative Models"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work proposes to use Annealed Importance Sampling for evaluating log-likelihoods for decoder-based models and validate its accuracy using bidirectional Monte Carlo, and analyzes the performance of decoded models, the effectiveness of existing log- likelihood estimators, the degree of overfitting, and the degree to which these models miss important modes of the data distribution."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36326783"
                        ],
                        "name": "Danica J. Sutherland",
                        "slug": "Danica-J.-Sutherland",
                        "structuredName": {
                            "firstName": "Danica",
                            "lastName": "Sutherland",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Danica J. Sutherland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693704"
                        ],
                        "name": "H. Tung",
                        "slug": "H.-Tung",
                        "structuredName": {
                            "firstName": "Hsiao-Yu",
                            "lastName": "Tung",
                            "middleNames": [
                                "Fish"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Tung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679743"
                        ],
                        "name": "Heiko Strathmann",
                        "slug": "Heiko-Strathmann",
                        "structuredName": {
                            "firstName": "Heiko",
                            "lastName": "Strathmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Heiko Strathmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "25740168"
                        ],
                        "name": "Soumyajit De",
                        "slug": "Soumyajit-De",
                        "structuredName": {
                            "firstName": "Soumyajit",
                            "lastName": "De",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Soumyajit De"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2556942"
                        ],
                        "name": "Aaditya Ramdas",
                        "slug": "Aaditya-Ramdas",
                        "structuredName": {
                            "firstName": "Aaditya",
                            "lastName": "Ramdas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aaditya Ramdas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708497"
                        ],
                        "name": "A. Gretton",
                        "slug": "A.-Gretton",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "Gretton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gretton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2263947,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d772a21f2f687985b41bec2dc47bc2760c57ed2f",
            "isKey": false,
            "numCitedBy": 182,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a method to optimize the representation and distinguishability of samples from two probability distributions, by maximizing the estimated power of a statistical test based on the maximum mean discrepancy (MMD). This optimized MMD is applied to the setting of unsupervised learning by generative adversarial networks (GAN), in which a model attempts to generate realistic samples, and a discriminator attempts to tell these apart from data samples. In this context, the MMD may be used in two roles: first, as a discriminator, either directly on the samples, or on features of the samples. Second, the MMD can be used to evaluate the performance of a generative model, by testing the model's samples against a reference data set. In the latter role, the optimized MMD is particularly helpful, as it gives an interpretable indication of how the model and data distributions differ, even in cases where individual model samples are not easily distinguished either by eye or by classifier."
            },
            "slug": "Generative-Models-and-Model-Criticism-via-Optimized-Sutherland-Tung",
            "title": {
                "fragments": [],
                "text": "Generative Models and Model Criticism via Optimized Maximum Mean Discrepancy"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This optimized MMD is applied to the setting of unsupervised learning by generative adversarial networks (GAN), in which a model attempts to generate realistic samples, and a discriminator attempts to tell these apart from data samples."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1807197"
                        ],
                        "name": "F. Yu",
                        "slug": "F.-Yu",
                        "structuredName": {
                            "firstName": "Fisher",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1591143181"
                        ],
                        "name": "Yinda Zhang",
                        "slug": "Yinda-Zhang",
                        "structuredName": {
                            "firstName": "Yinda",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yinda Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3340170"
                        ],
                        "name": "S. Song",
                        "slug": "S.-Song",
                        "structuredName": {
                            "firstName": "Shuran",
                            "lastName": "Song",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Song"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2233674"
                        ],
                        "name": "Ari Seff",
                        "slug": "Ari-Seff",
                        "structuredName": {
                            "firstName": "Ari",
                            "lastName": "Seff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ari Seff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40599257"
                        ],
                        "name": "Jianxiong Xiao",
                        "slug": "Jianxiong-Xiao",
                        "structuredName": {
                            "firstName": "Jianxiong",
                            "lastName": "Xiao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianxiong Xiao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8317437,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4dcdae25a5e33682953f0853ee4cf7ca93be58a9",
            "isKey": false,
            "numCitedBy": 1072,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "While there has been remarkable progress in the performance of visual recognition algorithms, the state-of-the-art models tend to be exceptionally data-hungry. Large labeled training datasets, expensive and tedious to produce, are required to optimize millions of parameters in deep network models. Lagging behind the growth in model capacity, the available datasets are quickly becoming outdated in terms of size and density. To circumvent this bottleneck, we propose to amplify human effort through a partially automated labeling scheme, leveraging deep learning with humans in the loop. Starting from a large set of candidate images for each category, we iteratively sample a subset, ask people to label them, classify the others with a trained model, split the set into positives, negatives, and unlabeled based on the classification confidence, and then iterate with the unlabeled set. To assess the effectiveness of this cascading procedure and enable further progress in visual recognition research, we construct a new image dataset, LSUN. It contains around one million labeled images for each of 10 scene categories and 20 object categories. We experiment with training popular convolutional networks and find that they achieve substantial performance gains when trained on this dataset."
            },
            "slug": "LSUN:-Construction-of-a-Large-scale-Image-Dataset-Yu-Zhang",
            "title": {
                "fragments": [],
                "text": "LSUN: Construction of a Large-scale Image Dataset using Deep Learning with Humans in the Loop"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work proposes to amplify human effort through a partially automated labeling scheme, leveraging deep learning with humans in the loop, and constructs a new image dataset, LSUN, which contains around one million labeled images for each of 10 scene categories and 20 object categories."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38909097"
                        ],
                        "name": "Alec Radford",
                        "slug": "Alec-Radford",
                        "structuredName": {
                            "firstName": "Alec",
                            "lastName": "Radford",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alec Radford"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2096458"
                        ],
                        "name": "Luke Metz",
                        "slug": "Luke-Metz",
                        "structuredName": {
                            "firstName": "Luke",
                            "lastName": "Metz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luke Metz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2127604"
                        ],
                        "name": "Soumith Chintala",
                        "slug": "Soumith-Chintala",
                        "structuredName": {
                            "firstName": "Soumith",
                            "lastName": "Chintala",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Soumith Chintala"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 26
                            }
                        ],
                        "text": "We keep the convolutional DCGAN architecture for the WGAN critic or the GAN discriminator."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 34
                            }
                        ],
                        "text": "Our baseline comparison is DCGAN (Radford et al., 2015), a GAN with a convolutional architecture trained with the standard GAN procedure using the\u2212 logD trick (Goodfellow et al., 2014)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 132
                            }
                        ],
                        "text": "In other words, the JS distance saturates, the discriminator has zero loss, and the generated samples are in some cases meaningful (DCGAN generator, top right plot) and in other cases collapse to a single nonsensical image (Goodfellow et al., 2014)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 26
                            }
                        ],
                        "text": "Besides the convolutional DCGAN architecture, we also ran experiments where we replace the generator or both the generator and the critic by 4-layer ReLU-MLP with 512 hidden units."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 96
                            }
                        ],
                        "text": "We illustrate this by running experiments on three generator architectures: (1) a convolutional DCGAN generator, (2) a convolutional DCGAN generator without batch normalization and with a constant number of filters (the capacity of the generator is drastically smaller than that of the discriminator), and (3) a 4-layer ReLU-MLP with 512 hidden units."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 33
                            }
                        ],
                        "text": "Our baseline comparison is DCGAN (Radford et al., 2015), a GAN with a convolutional architecture trained with the standard GAN procedure using the\u2212 logD trick (Goodfellow et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 191,
                                "start": 169
                            }
                        ],
                        "text": "Figure 7: Algorithms trained with a generator without batch normalization and constant number of filters at every layer (as opposed to duplicating them every time as in (Radford et al., 2015))."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11758569,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8388f1be26329fa45e5807e968a641ce170ea078",
            "isKey": false,
            "numCitedBy": 9853,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations."
            },
            "slug": "Unsupervised-Representation-Learning-with-Deep-Radford-Metz",
            "title": {
                "fragments": [],
                "text": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work introduces a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrates that they are a strong candidate for unsupervised learning."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144535526"
                        ],
                        "name": "G. Montavon",
                        "slug": "G.-Montavon",
                        "structuredName": {
                            "firstName": "Gr\u00e9goire",
                            "lastName": "Montavon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Montavon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145034054"
                        ],
                        "name": "K. M\u00fcller",
                        "slug": "K.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Klaus-Robert",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. M\u00fcller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711979"
                        ],
                        "name": "Marco Cuturi",
                        "slug": "Marco-Cuturi",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Cuturi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marco Cuturi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 20
                            }
                        ],
                        "text": "The recent work of (Montavon et al., 2016) has explored the use of Wasserstein distances in the context of learning for Restricted Boltzmann Machines for discrete spaces."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14390733,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e5abcd3dcc1dad7354c2fa85d7142294860bd83e",
            "isKey": false,
            "numCitedBy": 84,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Boltzmann machines are able to learn highly complex, multimodal, structured and multiscale real-world data distributions. Parameters of the model are usually learned by minimizing the Kullback-Leibler (KL) divergence from training samples to the learned model. We propose in this work a novel approach for Boltzmann machine training which assumes that a meaningful metric between observations is known. This metric between observations can then be used to define the Wasserstein distance between the distribution induced by the Boltzmann machine on the one hand, and that given by the training sample on the other hand. We derive a gradient of that distance with respect to the model parameters. Minimization of this new objective leads to generative models with different statistical properties. We demonstrate their practical potential on data completion and denoising, for which the metric between observations plays a crucial role."
            },
            "slug": "Wasserstein-Training-of-Restricted-Boltzmann-Montavon-M\u00fcller",
            "title": {
                "fragments": [],
                "text": "Wasserstein Training of Restricted Boltzmann Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This work proposes a novel approach for Boltzmann machine training which assumes that a meaningful metric between observations is known, and derives a gradient of that distance with respect to the model parameters from the Kullback-Leibler divergence."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726807"
                        ],
                        "name": "Diederik P. Kingma",
                        "slug": "Diederik-P.-Kingma",
                        "structuredName": {
                            "firstName": "Diederik",
                            "lastName": "Kingma",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Diederik P. Kingma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678311"
                        ],
                        "name": "M. Welling",
                        "slug": "M.-Welling",
                        "structuredName": {
                            "firstName": "Max",
                            "lastName": "Welling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Welling"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 216078090,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5f5dc5b9a2ba710937e2c413b37b053cd673df02",
            "isKey": false,
            "numCitedBy": 16776,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract: How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results."
            },
            "slug": "Auto-Encoding-Variational-Bayes-Kingma-Welling",
            "title": {
                "fragments": [],
                "text": "Auto-Encoding Variational Bayes"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "A stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case is introduced."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3403009"
                        ],
                        "name": "Aude Genevay",
                        "slug": "Aude-Genevay",
                        "structuredName": {
                            "firstName": "Aude",
                            "lastName": "Genevay",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aude Genevay"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711979"
                        ],
                        "name": "Marco Cuturi",
                        "slug": "Marco-Cuturi",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Cuturi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marco Cuturi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145481009"
                        ],
                        "name": "G. Peyr\u00e9",
                        "slug": "G.-Peyr\u00e9",
                        "structuredName": {
                            "firstName": "Gabriel",
                            "lastName": "Peyr\u00e9",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Peyr\u00e9"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144570279"
                        ],
                        "name": "F. Bach",
                        "slug": "F.-Bach",
                        "structuredName": {
                            "firstName": "Francis",
                            "lastName": "Bach",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Bach"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 22
                            }
                        ],
                        "text": "Finally, the work of (Genevay et al., 2016) shows new algorithms for calculating Wasserstein distances between different distributions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6849824,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6f840ce7c8373c11f89ea1693d0cab170627d31f",
            "isKey": false,
            "numCitedBy": 300,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Optimal transport (OT) defines a powerful framework to compare probability distributions in a geometrically faithful way. However, the practical impact of OT is still limited because of its computational burden. We propose a new class of stochastic optimization algorithms to cope with large-scale problems routinely encountered in machine learning applications. These methods are able to manipulate arbitrary distributions (either discrete or continuous) by simply requiring to be able to draw samples from them, which is the typical setup in high-dimensional learning problems. This alleviates the need to discretize these densities, while giving access to provably convergent methods that output the correct distance without discretization error. These algorithms rely on two main ideas: (a) the dual OT problem can be re-cast as the maximization of an expectation ; (b) entropic regularization of the primal OT problem results in a smooth dual optimization optimization which can be addressed with algorithms that have a provably faster convergence. We instantiate these ideas in three different setups: (i) when comparing a discrete distribution to another, we show that incremental stochastic optimization schemes can beat Sinkhorn's algorithm, the current state-of-the-art finite dimensional OT solver; (ii) when comparing a discrete distribution to a continuous density, a semi-discrete reformulation of the dual program is amenable to averaged stochastic gradient descent, leading to better performance than approximately solving the problem by discretization ; (iii) when dealing with two continuous densities, we propose a stochastic gradient descent over a reproducing kernel Hilbert space (RKHS). This is currently the only known method to solve this problem, apart from computing OT on finite samples. We backup these claims on a set of discrete, semi-discrete and continuous benchmark problems."
            },
            "slug": "Stochastic-Optimization-for-Large-scale-Optimal-Genevay-Cuturi",
            "title": {
                "fragments": [],
                "text": "Stochastic Optimization for Large-scale Optimal Transport"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A new class of stochastic optimization algorithms to cope with large-scale problems routinely encountered in machine learning applications, based on entropic regularization of the primal OT problem, which results in a smooth dual optimization optimization which can be addressed with algorithms that have a provably faster convergence."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726807"
                        ],
                        "name": "Diederik P. Kingma",
                        "slug": "Diederik-P.-Kingma",
                        "structuredName": {
                            "firstName": "Diederik",
                            "lastName": "Kingma",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Diederik P. Kingma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2503659"
                        ],
                        "name": "Jimmy Ba",
                        "slug": "Jimmy-Ba",
                        "structuredName": {
                            "firstName": "Jimmy",
                            "lastName": "Ba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jimmy Ba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6628106,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
            "isKey": false,
            "numCitedBy": 90063,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm."
            },
            "slug": "Adam:-A-Method-for-Stochastic-Optimization-Kingma-Ba",
            "title": {
                "fragments": [],
                "text": "Adam: A Method for Stochastic Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "This work introduces Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments, and provides a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3255983"
                        ],
                        "name": "Volodymyr Mnih",
                        "slug": "Volodymyr-Mnih",
                        "structuredName": {
                            "firstName": "Volodymyr",
                            "lastName": "Mnih",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Volodymyr Mnih"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36045539"
                        ],
                        "name": "Adri\u00e0 Puigdom\u00e8nech Badia",
                        "slug": "Adri\u00e0-Puigdom\u00e8nech-Badia",
                        "structuredName": {
                            "firstName": "Adri\u00e0",
                            "lastName": "Badia",
                            "middleNames": [
                                "Puigdom\u00e8nech"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adri\u00e0 Puigdom\u00e8nech Badia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153583218"
                        ],
                        "name": "Mehdi Mirza",
                        "slug": "Mehdi-Mirza",
                        "structuredName": {
                            "firstName": "Mehdi",
                            "lastName": "Mirza",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mehdi Mirza"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753223"
                        ],
                        "name": "A. Graves",
                        "slug": "A.-Graves",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Graves",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Graves"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2542999"
                        ],
                        "name": "T. Lillicrap",
                        "slug": "T.-Lillicrap",
                        "structuredName": {
                            "firstName": "Timothy",
                            "lastName": "Lillicrap",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Lillicrap"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3367786"
                        ],
                        "name": "Tim Harley",
                        "slug": "Tim-Harley",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Harley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tim Harley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145824029"
                        ],
                        "name": "David Silver",
                        "slug": "David-Silver",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Silver",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Silver"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2645384"
                        ],
                        "name": "K. Kavukcuoglu",
                        "slug": "K.-Kavukcuoglu",
                        "structuredName": {
                            "firstName": "Koray",
                            "lastName": "Kavukcuoglu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kavukcuoglu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6875312,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "69e76e16740ed69f4dc55361a3d319ac2f1293dd",
            "isKey": false,
            "numCitedBy": 5309,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input."
            },
            "slug": "Asynchronous-Methods-for-Deep-Reinforcement-Mnih-Badia",
            "title": {
                "fragments": [],
                "text": "Asynchronous Methods for Deep Reinforcement Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers and shows that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11112994,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "2f59406cce55c7bb9a78521bd14755a0db0aee7d",
            "isKey": false,
            "numCitedBy": 1211,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Simulated annealing\u2014moving from a tractable distribution to a distribution of interest via a sequence of intermediate distributions\u2014has traditionally been used as an inexact method of handling isolated modes in Markov chain samplers. Here, it is shown how one can use the Markov chain transitions for such an annealing sequence to define an importance sampler. The Markov chain aspect allows this method to perform acceptably even for high-dimensional problems, where finding good importance sampling distributions would otherwise be very difficult, while the use of importance weights ensures that the estimates found converge to the correct values as the number of annealing runs increases. This annealed importance sampling procedure resembles the second half of the previously-studied tempered transitions, and can be seen as a generalization of a recently-proposed variant of sequential importance sampling. It is also related to thermodynamic integration methods for estimating ratios of normalizing constants. Annealed importance sampling is most attractive when isolated modes are present, or when estimates of normalizing constants are required, but it may also be more generally useful, since its independent sampling allows one to bypass some of the problems of assessing convergence and autocorrelation in Markov chain samplers."
            },
            "slug": "Annealed-importance-sampling-Neal",
            "title": {
                "fragments": [],
                "text": "Annealed importance sampling"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is shown how one can use the Markov chain transitions for such an annealing sequence to define an importance sampler, which can be seen as a generalization of a recently-proposed variant of sequential importance sampling."
            },
            "venue": {
                "fragments": [],
                "text": "Stat. Comput."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708497"
                        ],
                        "name": "A. Gretton",
                        "slug": "A.-Gretton",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "Gretton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gretton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704422"
                        ],
                        "name": "K. Borgwardt",
                        "slug": "K.-Borgwardt",
                        "structuredName": {
                            "firstName": "Karsten",
                            "lastName": "Borgwardt",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Borgwardt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733256"
                        ],
                        "name": "M. Rasch",
                        "slug": "M.-Rasch",
                        "structuredName": {
                            "firstName": "Malte",
                            "lastName": "Rasch",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Rasch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10742222,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "225f78ae8a44723c136646044fd5c5d7f1d3d15a",
            "isKey": false,
            "numCitedBy": 2807,
            "numCiting": 106,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a framework for analyzing and comparing distributions, which we use to construct statistical tests to determine if two samples are drawn from different distributions. Our test statistic is the largest difference in expectations over functions in the unit ball of a reproducing kernel Hilbert space (RKHS), and is called the maximum mean discrepancy (MMD).We present two distribution free tests based on large deviation bounds for the MMD, and a third test based on the asymptotic distribution of this statistic. The MMD can be computed in quadratic time, although efficient linear time approximations are available. Our statistic is an instance of an integral probability metric, and various classical metrics on distributions are obtained when alternative function classes are used in place of an RKHS. We apply our two-sample tests to a variety of problems, including attribute matching for databases using the Hungarian marriage method, where they perform strongly. Excellent performance is also obtained when comparing distributions over graphs, for which these are the first such tests."
            },
            "slug": "A-Kernel-Two-Sample-Test-Gretton-Borgwardt",
            "title": {
                "fragments": [],
                "text": "A Kernel Two-Sample Test"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "This work proposes a framework for analyzing and comparing distributions, which is used to construct statistical tests to determine if two samples are drawn from different distributions, and presents two distribution free tests based on large deviation bounds for the maximum mean discrepancy (MMD)."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143965435"
                        ],
                        "name": "A. M\u00fcller",
                        "slug": "A.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Alfred",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. M\u00fcller"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 124648603,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "f3d4c52a3cfcc6c550fa30cc20c516e285eabf00",
            "isKey": false,
            "numCitedBy": 497,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider probability metrics of the following type: for a class of functions and probability measures P, Q we define A unified study of such integral probability metrics is given. We characterize the maximal class of functions that generates such a metric. Further, we show how some interesting properties of these probability metrics arise directly from conditions on the generating class of functions. The results are illustrated by several examples, including the Kolmogorov metric, the Dudley metric and the stop-loss metric."
            },
            "slug": "Integral-Probability-Metrics-and-Their-Generating-M\u00fcller",
            "title": {
                "fragments": [],
                "text": "Integral Probability Metrics and Their Generating Classes of Functions"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "A unified study of integral probability metrics of the following type are given and how some interesting properties of these probability metrics arise directly from conditions on the generating class of functions is shown."
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Applied Probability"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2556942"
                        ],
                        "name": "Aaditya Ramdas",
                        "slug": "Aaditya-Ramdas",
                        "structuredName": {
                            "firstName": "Aaditya",
                            "lastName": "Ramdas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aaditya Ramdas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1981186"
                        ],
                        "name": "Sashank J. Reddi",
                        "slug": "Sashank-J.-Reddi",
                        "structuredName": {
                            "firstName": "Sashank",
                            "lastName": "Reddi",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sashank J. Reddi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719347"
                        ],
                        "name": "B. P\u00f3czos",
                        "slug": "B.-P\u00f3czos",
                        "structuredName": {
                            "firstName": "Barnab\u00e1s",
                            "lastName": "P\u00f3czos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. P\u00f3czos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109423866"
                        ],
                        "name": "Aarti Singh",
                        "slug": "Aarti-Singh",
                        "structuredName": {
                            "firstName": "Aarti",
                            "lastName": "Singh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aarti Singh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733999"
                        ],
                        "name": "L. Wasserman",
                        "slug": "L.-Wasserman",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Wasserman",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Wasserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2207512,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "30746a4a8d445ecd861d95078b3f5a7770de791b",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Nonparametric two sample testing deals with the question of consistently deciding if two distributions are different, given samples from both, without making any parametric assumptions about the form of the distributions. The current literature is split into two kinds of tests - those which are consistent without any assumptions about how the distributions may differ (\\textit{general} alternatives), and those which are designed to specifically test easier alternatives, like a difference in means (\\textit{mean-shift} alternatives). \nThe main contribution of this paper is to explicitly characterize the power of a popular nonparametric two sample test, designed for general alternatives, under a mean-shift alternative in the high-dimensional setting. Specifically, we explicitly derive the power of the linear-time Maximum Mean Discrepancy statistic using the Gaussian kernel, where the dimension and sample size can both tend to infinity at any rate, and the two distributions differ in their means. As a corollary, we find that if the signal-to-noise ratio is held constant, then the test's power goes to one if the number of samples increases faster than the dimension increases. This is the first explicit power derivation for a general nonparametric test in the high-dimensional setting, and also the first analysis of how tests designed for general alternatives perform when faced with easier ones."
            },
            "slug": "On-the-High-dimensional-Power-of-Linear-time-Kernel-Ramdas-Reddi",
            "title": {
                "fragments": [],
                "text": "On the High-dimensional Power of Linear-time Kernel Two-Sample Testing under Mean-difference Alternatives"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper explicitly derive the power of the linear-time Maximum Mean Discrepancy statistic using the Gaussian kernel, where the dimension and sample size can both tend to infinity at any rate, and the two distributions differ in their means."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48548583"
                        ],
                        "name": "P. Milgrom",
                        "slug": "P.-Milgrom",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Milgrom",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Milgrom"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "11384921"
                        ],
                        "name": "I. Segal",
                        "slug": "I.-Segal",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Segal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Segal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7563628,
            "fieldsOfStudy": [
                "Mathematics",
                "Economics"
            ],
            "id": "d6f33b6a10b890999a28d5a59e991ec7a697dd45",
            "isKey": false,
            "numCitedBy": 1170,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "The standard envelope theorems apply to choice sets with convex and topological structure, providing sufficient conditions for the value function to be differentiable in a parameter and characterizing its derivative. This paper studies optimization with arbitrary choice sets and shows that the traditional envelope formula holds at any differentiability point of the value function. We also provide conditions for the value function to be, variously, absolutely continuous, left- and right-differentiable, or fully differentiable. These results are applied to mechanism design, convex programming, continuous optimization problems, saddle-point problems, problems with parameterized constraints, and optimal stopping problems."
            },
            "slug": "Envelope-Theorems-for-Arbitrary-Choice-Sets-Milgrom-Segal",
            "title": {
                "fragments": [],
                "text": "Envelope Theorems for Arbitrary Choice Sets"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "16614521"
                        ],
                        "name": "C. Villani",
                        "slug": "C.-Villani",
                        "structuredName": {
                            "firstName": "C\u00e9dric",
                            "lastName": "Villani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Villani"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 118347220,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "ca73ce2623aa93bf32b88f7fb998af9576aed20f",
            "isKey": false,
            "numCitedBy": 4591,
            "numCiting": 250,
            "paperAbstract": {
                "fragments": [],
                "text": "Couplings and changes of variables.- Three examples of coupling techniques.- The founding fathers of optimal transport.- Qualitative description of optimal transport.- Basic properties.- Cyclical monotonicity and Kantorovich duality.- The Wasserstein distances.- Displacement interpolation.- The Monge-Mather shortening principle.- Solution of the Monge problem I: global approach.- Solution of the Monge problem II: Local approach.- The Jacobian equation.- Smoothness.- Qualitative picture.- Optimal transport and Riemannian geometry.- Ricci curvature.- Otto calculus.- Displacement convexity I.- Displacement convexity II.- Volume control.- Density control and local regularity.- Infinitesimal displacement convexity.- Isoperimetric-type inequalities.- Concentration inequalities.- Gradient flows I.- Gradient flows II: Qualitative properties.- Gradient flows III: Functional inequalities.- Synthetic treatment of Ricci curvature.- Analytic and synthetic points of view.- Convergence of metric-measure spaces.- Stability of optimal transport.- Weak Ricci curvature bounds I: Definition and Stability.- Weak Ricci curvature bounds II: Geometric and analytic properties."
            },
            "slug": "Optimal-Transport:-Old-and-New-Villani",
            "title": {
                "fragments": [],
                "text": "Optimal Transport: Old and New"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 32
                            }
                        ],
                        "text": "10: g\u03b8 \u2190 \u2212\u2207\u03b8 1m \u2211m i=1 fw(g\u03b8(z\n(i))) 11: \u03b8 \u2190 \u03b8 \u2212 \u03b1 \u00b7 RMSProp(\u03b8, g\u03b8) 12: end while\nTheorem 3."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 60
                            }
                        ],
                        "text": "5: gw \u2190 \u2207w[ 1m \u2211m i=1 fw(x (i))\n\u2212 1m \u2211m i=1 fw(g\u03b8(z\n(i)))] 6: w \u2190 w + \u03b1 \u00b7 RMSProp(w, gw) 7: w \u2190 clip(w,\u2212c, c) 8: end for 9: Sample {z(i)}mi=1 \u223c p(z) a batch of prior samples."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 34
                            }
                        ],
                        "text": "We therefore switched to RMSProp (Tieleman & Hinton, 2012) which is known to perform well even on very nonstationary problems (Mnih et al., 2016)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Lecture 6.5\u2014RmsProp: Divide the gradient by a running average of its recent magnitude"
            },
            "venue": {
                "fragments": [],
                "text": "COURSERA: Neural Networks for Machine Learning,"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50811541"
                        ],
                        "name": "S. Kakutani",
                        "slug": "S.-Kakutani",
                        "structuredName": {
                            "firstName": "Shizuo",
                            "lastName": "Kakutani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kakutani"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 124269910,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "555ee068d7826768429ec87114f2f2cddeb52460",
            "isKey": false,
            "numCitedBy": 263,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Concrete-Representation-of-Abstract-(M)-Spaces-(A-Kakutani",
            "title": {
                "fragments": [],
                "text": "Concrete Representation of Abstract (M)-Spaces (A characterization of the Space of Continuous Functions)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1941
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Energy-based generative adversarial"
            },
            "venue": {
                "fragments": [],
                "text": "network. Corr,"
            },
            "year": 2016
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Envelope theorems for arbitrary choice"
            },
            "venue": {
                "fragments": [],
                "text": "sets. Econometrica,"
            },
            "year": 2002
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 4,
            "methodology": 5
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 27,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Wasserstein-Generative-Adversarial-Networks-Arjovsky-Chintala/acd87843a451d18b4dc6474ddce1ae946429eaf1?sort=total-citations"
}