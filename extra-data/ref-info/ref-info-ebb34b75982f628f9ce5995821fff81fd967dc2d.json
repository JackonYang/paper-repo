{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2275441"
                        ],
                        "name": "K. Yow",
                        "slug": "K.-Yow",
                        "structuredName": {
                            "firstName": "Kin",
                            "lastName": "Yow",
                            "middleNames": [
                                "Choong"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Yow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745672"
                        ],
                        "name": "R. Cipolla",
                        "slug": "R.-Cipolla",
                        "structuredName": {
                            "firstName": "Roberto",
                            "lastName": "Cipolla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Cipolla"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 16932868,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "30e2f29d26f31846d6e0294cfa3733adfc618bbb",
            "isKey": false,
            "numCitedBy": 432,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Feature-based-human-face-detection-Yow-Cipolla",
            "title": {
                "fragments": [],
                "text": "Feature-based human face detection"
            },
            "venue": {
                "fragments": [],
                "text": "Image Vis. Comput."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736143"
                        ],
                        "name": "Constantine Kotropoulos",
                        "slug": "Constantine-Kotropoulos",
                        "structuredName": {
                            "firstName": "Constantine",
                            "lastName": "Kotropoulos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Constantine Kotropoulos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144064571"
                        ],
                        "name": "I. Pitas",
                        "slug": "I.-Pitas",
                        "structuredName": {
                            "firstName": "Ioannis",
                            "lastName": "Pitas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Pitas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 31
                            }
                        ],
                        "text": "In contrast to template matching, the models (or templates) are learned from a set of training images which should capture the representative variability of facial appearance."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 135301,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a1d194fd28c58075efed3425483fe8cfc6df0ae6",
            "isKey": false,
            "numCitedBy": 248,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Face detection is a key problem in building automated systems that perform face recognition. A very attractive approach for face detection is based on multiresolution images (also known as mosaic images). Motivated by the simplicity of this approach, a rule-based face detection algorithm in frontal views is developed that extends the work of G. Yang and T.S. Huang (see Pattern Recognition, vol.27, no.1, p.53-63, 1994). The proposed algorithm has been applied to frontal views extracted from the European ACTS M2VTS database that contains the videosequences of 37 different persons. It has been found that the algorithm provides a correct facial candidate in all cases. However, the success rate of the detected facial features (e.g. eyebrows/eyes, nostrils/nose, and mouth) that validate the choice of a facial candidate is found to be 86.5% under the most strict evaluation conditions."
            },
            "slug": "Rule-based-face-detection-in-frontal-views-Kotropoulos-Pitas",
            "title": {
                "fragments": [],
                "text": "Rule-based face detection in frontal views"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A rule-based face detection algorithm in frontal views is developed that is applied to frontal views extracted from the European ACTS M2VTS database that contains the videosequences of 37 different persons and found that the algorithm provides a correct facial candidate in all cases."
            },
            "venue": {
                "fragments": [],
                "text": "1997 IEEE International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144986880"
                        ],
                        "name": "I. Craw",
                        "slug": "I.-Craw",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Craw",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Craw"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055741802"
                        ],
                        "name": "H. Ellis",
                        "slug": "H.-Ellis",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Ellis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ellis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144579828"
                        ],
                        "name": "J. Lishman",
                        "slug": "J.-Lishman",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Lishman",
                            "middleNames": [
                                "Rowland"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lishman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 115
                            }
                        ],
                        "text": "For example, template matching methods usually use a face model and subtemplates to extract facial features [132], [27], [180], [143], [51], and then use these features to locate or detect faces."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1300797,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3d9a1b66ddac96b4bbb3592cfe40db4bb28b24e8",
            "isKey": false,
            "numCitedBy": 232,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Automatic-extraction-of-face-features-Craw-Ellis",
            "title": {
                "fragments": [],
                "text": "Automatic extraction of face-features"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit. Lett."
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115746464"
                        ],
                        "name": "Y. Kwon",
                        "slug": "Y.-Kwon",
                        "structuredName": {
                            "firstName": "Young",
                            "lastName": "Kwon",
                            "middleNames": [
                                "Ho"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Kwon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700665"
                        ],
                        "name": "N. Lobo",
                        "slug": "N.-Lobo",
                        "structuredName": {
                            "firstName": "Niels",
                            "lastName": "Lobo",
                            "middleNames": [
                                "da",
                                "Vitoria"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Lobo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "In [84], a detection method based on snakes [73], [90] and templates was developed."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 46942178,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c13818310b9019bb10e58177abc8dc0554cd7836",
            "isKey": false,
            "numCitedBy": 56,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "In the problem area of human facial image processing, the first computational task that needs to be solved is that of detecting a face under arbitrary scene conditions. Although some progress towards this has been reported in the literature, face detection remains a difficult problem. In this paper the authors report on a novel face-finding method that appears quite robust. First, \"snakelets\" are used to find candidate edges. Candidate ovals (face-locations) are then found from these snakelets using a voting method. For each of these candidate face-locations, the authors use a method introduced previously to find detailed facial features. If a substantial number of the facial features are found successfully, and their positions satisfy ratio-tests for being standard, the procedure positively reports the existence of a face at this location in the image."
            },
            "slug": "Face-detection-using-templates-Kwon-Lobo",
            "title": {
                "fragments": [],
                "text": "Face detection using templates"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A novel face-finding method that appears quite robust is reported on, using \"snakelets\" to find candidate edges and a voting method to find face-locations."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 12th International Conference on Pattern Recognition"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144013393"
                        ],
                        "name": "A. Samal",
                        "slug": "A.-Samal",
                        "structuredName": {
                            "firstName": "Ashok",
                            "lastName": "Samal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Samal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32130323"
                        ],
                        "name": "Prasana A. Iyengar",
                        "slug": "Prasana-A.-Iyengar",
                        "structuredName": {
                            "firstName": "Prasana",
                            "lastName": "Iyengar",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Prasana A. Iyengar"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Index Terms\u2014Face detection, face recognition, object recognition, view-based recognition, statistical pattern recognition, machine learning."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 205013679,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0be360a2964c4bb91aaad0cc6d1baa6639746028",
            "isKey": false,
            "numCitedBy": 1067,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Automatic-recognition-and-analysis-of-human-faces-a-Samal-Iyengar",
            "title": {
                "fragments": [],
                "text": "Automatic recognition and analysis of human faces and facial expressions: a survey"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144013393"
                        ],
                        "name": "A. Samal",
                        "slug": "A.-Samal",
                        "structuredName": {
                            "firstName": "Ashok",
                            "lastName": "Samal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Samal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32130323"
                        ],
                        "name": "Prasana A. Iyengar",
                        "slug": "Prasana-A.-Iyengar",
                        "structuredName": {
                            "firstName": "Prasana",
                            "lastName": "Iyengar",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Prasana A. Iyengar"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 67
                            }
                        ],
                        "text": "Silhouettes have also been used as templates for face localization [134]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 26186431,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3c3592f4cb1c182ef0536f87a6695ed9aa960e2e",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Face detection is integral to any automatic face recognition system. The goal of this research is to develop a system that performs the task of human face detection automatically in a scene. A system to correctly locate and identify human faces will find several applications, some examples are criminal identification and authentication in secure systems. This work presents a new approach based on principal component analysis. Face silhouettes instead of intensity images are used for this research. It results in reduction in both space and processing time. A set of basis face silhouettes are obtained using principal component analysis. These are then used with a Hough-like technique to detect faces. The results show that the approach is robust, accurate and reasonably fast."
            },
            "slug": "Human-Face-Detection-Using-Silhouettes-Samal-Iyengar",
            "title": {
                "fragments": [],
                "text": "Human Face Detection Using Silhouettes"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Face silhouettes instead of intensity images are used for this research, which results in reduction in both space and processing time and shows that the approach is robust, accurate and reasonably fast."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Pattern Recognit. Artif. Intell."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152292143"
                        ],
                        "name": "Jeffrey R. Huang",
                        "slug": "Jeffrey-R.-Huang",
                        "structuredName": {
                            "firstName": "Jeffrey R.",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeffrey R. Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145699982"
                        ],
                        "name": "S. Gutta",
                        "slug": "S.-Gutta",
                        "structuredName": {
                            "firstName": "Srinivas",
                            "lastName": "Gutta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Gutta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143979395"
                        ],
                        "name": "H. Wechsler",
                        "slug": "H.-Wechsler",
                        "structuredName": {
                            "firstName": "Harry",
                            "lastName": "Wechsler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Wechsler"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 17638560,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8e3ccd490466445790e246cf7e993d450ee354ba",
            "isKey": false,
            "numCitedBy": 59,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper proposes a novel algorithm for face detection using decision trees (DT) and shows its generality and feasibility using a database consisting of 2340 face images from the FERET database (corresponding to 817 subjects and including 190 sets of duplicates) over a semi-uniform background. The approach used for face detection involves three main stages, those of location, cropping, and post-processing. The first stage finds a rough approximation for the possible location of the face box, the second stage will refine it, and the last stage decider whether a face is present in the image and if the answer is positive would normalize the face image. The algorithm does not require multiple (scale) templates and the accuracy achieved is 96%. Accuracy is based on the visual observation that the face box includes both eyes, nose, and mouth, and that the top side of the box is below the hairline. Experiments were also performed to assess the accuracy of the algorithm in rejecting images where no face is present. Using a small database of 25 images of various but complex backgrounds the algorithm failed on two images for an overall accuracy rate of 92%."
            },
            "slug": "Detection-of-human-faces-using-decision-trees-Huang-Gutta",
            "title": {
                "fragments": [],
                "text": "Detection of human faces using decision trees"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "A novel algorithm for face detection using decision trees (DT) is proposed and its generality and feasibility is shown using a database consisting of 2340 face images from the FERET database over a semi-uniform background."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Second International Conference on Automatic Face and Gesture Recognition"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723877"
                        ],
                        "name": "V. Govindaraju",
                        "slug": "V.-Govindaraju",
                        "structuredName": {
                            "firstName": "Venu",
                            "lastName": "Govindaraju",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Govindaraju"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 135
                            }
                        ],
                        "text": "For example, template matching methods usually use a face model and subtemplates to extract facial features [132], [27], [180], [143], [51], and then use these features to locate or detect faces."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 106
                            }
                        ],
                        "text": "presented a two stage face detection method in which face hypotheses are generated and tested [52], [53], [51]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 29332361,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6275f9f4208e91bece3e31312717fb5ffdbdf08c",
            "isKey": false,
            "numCitedBy": 98,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "The human face is an object that is easily located in complex scenes by infants and adults alike. Yet the development of an automated system to perform this task is extremely challenging. An attempt to solve this problem raises two important issues in object location. First, natural objects such as human faces tend to have boundaries which are not exactly described by analytical functions. Second, the object of interest (face) could occur in a scene in various sizes, thus requiring the use of scale independent techniques which can detect instances of the object at all scales.Although, the task of identifying a well-framed face (as one of a set of labeled faces) has been well researched, the task of locating a face in a natural scene is relatively unexplored. We present a computational theory for locating human faces in scenes with certain constraints. The theory will be validated by experiments confined to instances where people's faces are the primary subject of the scene, occlusion is minimal, and the faces contrast well against the background."
            },
            "slug": "Locating-human-faces-in-photographs-Govindaraju",
            "title": {
                "fragments": [],
                "text": "Locating human faces in photographs"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A computational theory for locating human faces in scenes with certain constraints is presented and will be validated by experiments confined to instances where people's faces are the primary subject of the scene, occlusion is minimal, and the faces contrast well against the background."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144677796"
                        ],
                        "name": "A. Loui",
                        "slug": "A.-Loui",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Loui",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Loui"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40039352"
                        ],
                        "name": "C. Judice",
                        "slug": "C.-Judice",
                        "structuredName": {
                            "firstName": "Charlie",
                            "lastName": "Judice",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Judice"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152939314"
                        ],
                        "name": "Sheng Liu",
                        "slug": "Sheng-Liu",
                        "structuredName": {
                            "firstName": "Sheng",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sheng Liu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 36830,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3585e6197255849a6739f280d387c10067e28453",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a face image database which has been created and developed at Kodak as a common database for direct benchmarking of automatic face detection and recognition algorithms. This consumer application-oriented face image database is composed of two main sub-databases, one for face detection, and one for face recognition. The database is intended to be distributed to researchers both inside and outside of Kodak working in face detection and recognition research. The database contains pictures taken using consumer digital cameras, scanned in from a photo scanner, as well as pictures from Kodak Image Magic picture disks. The details of the face database such as the development process and the image file formats, are described, together with a discussion on some application scenarios, as well as current benchmarking activities."
            },
            "slug": "An-image-database-for-benchmarking-of-automatic-and-Loui-Judice",
            "title": {
                "fragments": [],
                "text": "An image database for benchmarking of automatic face detection and recognition algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The details of the face database such as the development process and the image file formats are described, together with a discussion on some application scenarios, as well as current benchmarking activities."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 1998 International Conference on Image Processing. ICIP98 (Cat. No.98CB36269)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "122439944"
                        ],
                        "name": "Qian Chen",
                        "slug": "Qian-Chen",
                        "structuredName": {
                            "firstName": "Qian",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qian Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794240"
                        ],
                        "name": "Haiyuan Wu",
                        "slug": "Haiyuan-Wu",
                        "structuredName": {
                            "firstName": "Haiyuan",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Haiyuan Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735941"
                        ],
                        "name": "M. Yachida",
                        "slug": "M.-Yachida",
                        "structuredName": {
                            "firstName": "Masahiko",
                            "lastName": "Yachida",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Yachida"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 31690449,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "857b32ba2a3b10855db4051adc379af7c74cb795",
            "isKey": false,
            "numCitedBy": 151,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper describes an approach to detect faces whose size and position are unknown in an image with a complex background. The candidates of faces are detected by finding out \"face like\" regions in the input image using the fuzzy pattern matching method. The perceptually uniform color space is used in our research in order to obtain reliable results. The skin color that is used to detect face like regions, is represented by a model developed by us called skin color distribution function. The skin color regions are then extracted by estimating a measure that describes how well the color of a pixel looks like the skin color for each pixel in the input image. The faces which appear in images are modeled as several 2 dimensional patterns. The face like regions are extracted by a fuzzy pattern matching approach using these face models. The face candidates are then verified by estimating how well the extracted facial features fit a face model which describes the geometrical relations among facial features.<<ETX>>"
            },
            "slug": "Face-detection-by-fuzzy-pattern-matching-Chen-Wu",
            "title": {
                "fragments": [],
                "text": "Face detection by fuzzy pattern matching"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "The paper describes an approach to detect faces whose size and position are unknown in an image with a complex background by finding out \"face like\" regions in the input image using the fuzzy pattern matching method."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE International Conference on Computer Vision"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4801654"
                        ],
                        "name": "Y. Amit",
                        "slug": "Y.-Amit",
                        "structuredName": {
                            "firstName": "Yali",
                            "lastName": "Amit",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Amit"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707642"
                        ],
                        "name": "D. Geman",
                        "slug": "D.-Geman",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Geman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Geman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2521020"
                        ],
                        "name": "B. Jedynak",
                        "slug": "B.-Jedynak",
                        "structuredName": {
                            "firstName": "Bruno",
                            "lastName": "Jedynak",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Jedynak"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "intensity images [3]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 118052090,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b56ab69d6d2ed840f3850df3c60ccdf1d5f284d9",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an algorithm for shape detection and apply it to frontal views of faces in still grey level images with arbitrary backgrounds. Detection is done in two stages: (i) \u201cfocusing,\u201d during which a relatively small number of regions-of-interest are identified, minimizing computation and false negatives at the (temporary) expense of false positives; and (ii) \u201cintensive classification,\u201d during which a selected region-of-interest is labeled face or background based on multiple decision trees and normalized data. In contrast to most detection algorithms, the processing is then very highly concentrated in the regions near faces and near false positives."
            },
            "slug": "Efficient-Focusing-and-Face-Detection-Amit-Geman",
            "title": {
                "fragments": [],
                "text": "Efficient Focusing and Face Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "An algorithm for shape detection is presented and applied to frontal views of faces in still grey level images with arbitrary backgrounds and a selected region-of-interest is labeled face or background based on multiple decision trees and normalized data."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699756"
                        ],
                        "name": "K. Sobottka",
                        "slug": "K.-Sobottka",
                        "structuredName": {
                            "firstName": "Karin",
                            "lastName": "Sobottka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Sobottka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144064571"
                        ],
                        "name": "I. Pitas",
                        "slug": "I.-Pitas",
                        "structuredName": {
                            "firstName": "Ioannis",
                            "lastName": "Pitas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Pitas"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 31005947,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5a26636fd98774b15d510ab3edee048fc8947702",
            "isKey": false,
            "numCitedBy": 165,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Recognition of human faces out of still images or image sequences is a research field of fast increasing interest. At first, facial regions and facial features like eyes and mouth have to be extracted. In the present paper we propose an approach that copes with problems of these first two steps. We perform face localization based on the observation that human faces are characterized by their oval shape and skin-color, also in the case of varying light conditions. For that we segment faces by evaluating shape and color (HSV) information. Then face hypotheses are verified by searching for facial features inside of the face-like regions. This is done by applying morphological operations and minima localization to intensity images."
            },
            "slug": "Face-localization-and-facial-feature-extraction-on-Sobottka-Pitas",
            "title": {
                "fragments": [],
                "text": "Face localization and facial feature extraction based on shape and color information"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper performs face localization based on the observation that human faces are characterized by their oval shape and skin-color, also in the case of varying light conditions, and segment faces by evaluating shape and color information."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 3rd IEEE International Conference on Image Processing"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37495435"
                        ],
                        "name": "Qibin Sun",
                        "slug": "Qibin-Sun",
                        "structuredName": {
                            "firstName": "Qibin",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qibin Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2145032549"
                        ],
                        "name": "Weimin Huang",
                        "slug": "Weimin-Huang",
                        "structuredName": {
                            "firstName": "Weimin",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Weimin Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109178668"
                        ],
                        "name": "Jian-Kang Wu",
                        "slug": "Jian-Kang-Wu",
                        "structuredName": {
                            "firstName": "Jian-Kang",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian-Kang Wu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 165
                            }
                        ],
                        "text": "Several color spaces have been utilized to label pixels as skin including RGB [66], [67], [137], normalized RGB [102], [29], [149], [172], [30], [105], [171], [77], [151], [120], HSV (or HSI) [138], [79], [147], [146], YCrCb [167], [17], YIQ [31], [32], YES [131], CIE XYZ [19], and CIE LUV [173]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5905541,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b772b99e410333e5edbbe259ad53324671a519c0",
            "isKey": false,
            "numCitedBy": 70,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "As we know, a robust approach to face and facial features detection must be able to handle the variation issues such as changes in imaging conditions, face appearances and image contents. Here we present a method which utilizes color, local symmetry and geometry information of human face based on various models. The algorithm first detects most likely face regions or ROIs (Region-Of-Interest) from the image using face color model and face outline model, produces a face color similarity map. Then it performs local symmetry detection within these ROIs to obtain a local symmetry similarity map. These two maps are fused to obtain potential facial feature points. Finally similarity matching is performed to identify faces between the fusion map and face geometry model under affine transformation. The output results are the detected faces with confidence values. Experimental results have demonstrated its validity and robustness to identify faces under certain variations."
            },
            "slug": "Face-detection-based-on-color-and-local-symmetry-Sun-Huang",
            "title": {
                "fragments": [],
                "text": "Face detection based on color and local symmetry information"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A method which utilizes color, local symmetry and geometry information of human face based on various models to identify faces under certain variations and its validity and robustness have been demonstrated."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Third IEEE International Conference on Automatic Face and Gesture Recognition"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793040"
                        ],
                        "name": "N. Duta",
                        "slug": "N.-Duta",
                        "structuredName": {
                            "firstName": "Nicolae",
                            "lastName": "Duta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Duta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 1664488,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "371822469e4a3b12fd78daeb130547c4242a69b4",
            "isKey": false,
            "numCitedBy": 16,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Presents a learning approach for the face detection problem. The problem can be stated as follows: given an arbitrary black and white, still image, find the location and size of every human face it contains. Numerous applications of automatic face detection have attracted considerable interest in this problem, but no present face detection system is completely satisfactory from the point of view of detection rate, false alarm rate and detection time. We describe an inductive learning-based detection method that produces a maximally specific hypothesis consistent with the training data. Three different sets of features were considered for defining the concept of a human face. The performance achieved is as follows: 85% detection rate, a false alarm rate of 0.04% of the number of windows analyzed and 1 minute detection table for a 320/spl times/240 image on a Sun Ultrasparc 1."
            },
            "slug": "Learning-the-human-face-concept-in-black-and-white-Duta-Jain",
            "title": {
                "fragments": [],
                "text": "Learning the human face concept in black and white images"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "An inductive learning-based detection method is described that produces a maximally specific hypothesis consistent with the training data and achieves 85% detection rate, a false alarm rate of 0.04% and 1 minute detection table for a 320/spl times/240 image on a Sun Ultrasparc 1."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. Fourteenth International Conference on Pattern Recognition (Cat. No.98EX170)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2275441"
                        ],
                        "name": "K. Yow",
                        "slug": "K.-Yow",
                        "structuredName": {
                            "firstName": "Kin",
                            "lastName": "Yow",
                            "middleNames": [
                                "Choong"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Yow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745672"
                        ],
                        "name": "R. Cipolla",
                        "slug": "R.-Cipolla",
                        "structuredName": {
                            "firstName": "Roberto",
                            "lastName": "Cipolla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Cipolla"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 101
                            }
                        ],
                        "text": "An image region becomes a valid facial feature candidate if the Mahalanobis distance between the corresponding feature vectors is below a threshold."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13468462,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eb2b26b056b4535cc84f395c524752ef32659e3a",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent advances in human face detection algorithms have seen varying degrees of success using numerous approaches. We identify that a feature-based approach is able to detect faces efficiently over large viewpoint and illumination variations. In this paper, we will enhance the approach by proposing the use of active contour models to detect the face boundary, and subsequently use it to verify face candidates. We present a method to initialize the active contour model, and show how the resulting information can be used to verify true face candidates. Further verification of the face hypothesis is achieved by checking for consistency with motion. We present results of testing the system under a wide range of imaging conditions, demonstrating its capability and robustness."
            },
            "slug": "Enhancing-Human-Face-Detection-Using-Motion-and-Yow-Cipolla",
            "title": {
                "fragments": [],
                "text": "Enhancing Human Face Detection Using Motion and Active Contours"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper proposes the use of active contour models to detect the face boundary, and subsequently uses it to verify face candidates, and presents a method to initialize the active contours model, and shows how the resulting information can be used to verify true face candidates."
            },
            "venue": {
                "fragments": [],
                "text": "ACCV"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "117481816"
                        ],
                        "name": "D. Mumford",
                        "slug": "D.-Mumford",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mumford",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mumford"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36427051"
                        ],
                        "name": "Peter W. Hallinan",
                        "slug": "Peter-W.-Hallinan",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Hallinan",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter W. Hallinan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 118557991,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "07df055035fb3e877c28efe4d2300b697c3b225a",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Successful recognition systems must be able to handle not only variations in the geometry of the objects they model, but also arbitrary variations in lighting. We propose a deformable template for unoccluded, frontally-viewed human faces that handles both kinds of variation. Lighting is modeled by finding a basis for face space that can be used to synthesize a face image given lighting conditions, or to determine lighting conditions given a face image. Geometric distortions are captured by automatically \"morphing\" the input face to the synthesized face. Each of several different face models representing both individuals and average faces was tested on two tasks: discriminating the individual represented by the model from all other faces and nonfaces (about 63 positive examples and 1100 negative examples) and discriminating faces from nonfaces (about 755 positive examples and 450 negative examples). Nonfaces here means patches from random natural and artificial scenes. Each model performed extremely well; for false alarm rates of about 0-3 percent miss rates typically fell in 0-5 percent range indicating that distributions of goodness of fit criteria for negative and positive exemplars are actually very well separated. Nothing about the recognition strategy advocated here is particular to faces; in principle, the model is easily extendible to any other viewpoint or to any other object."
            },
            "slug": "A-deformable-model-for-the-recognition-of-human-Mumford-Hallinan",
            "title": {
                "fragments": [],
                "text": "A deformable model for the recognition of human faces under arbitrary illumination"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work proposes a deformable template for unoccluded, frontally-viewed human faces that handles both kinds of variation in lighting and geometry, and in principle is easily extendible to any other viewpoint or toAny other object."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3430625"
                        ],
                        "name": "R. F\u00e9raud",
                        "slug": "R.-F\u00e9raud",
                        "structuredName": {
                            "firstName": "Rapha\u00ebl",
                            "lastName": "F\u00e9raud",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. F\u00e9raud"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3118328"
                        ],
                        "name": "O. Bernier",
                        "slug": "O.-Bernier",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Bernier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Bernier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35094984"
                        ],
                        "name": "J. Viallet",
                        "slug": "J.-Viallet",
                        "structuredName": {
                            "firstName": "Jean-Emmanuel",
                            "lastName": "Viallet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Viallet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2586567"
                        ],
                        "name": "M. Collobert",
                        "slug": "M.-Collobert",
                        "structuredName": {
                            "firstName": "Michel",
                            "lastName": "Collobert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collobert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 33451369,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ebfab3c7d0fdffc3d2f570c105e7a4e43997d04c",
            "isKey": false,
            "numCitedBy": 355,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "Detecting faces in images with complex backgrounds is a difficult task. Our approach, which obtains state of the art results, is based on a neural network model: the constrained generative model (CGM). Generative, since the goal of the learning process is to evaluate the probability that the model has generated the input data, and constrained since some counter-examples are used to increase the quality of the estimation performed by the model. To detect side view faces and to decrease the number of false alarms, a conditional mixture of networks is used. To decrease the computational time cost, a fast search algorithm is proposed. The level of performance reached, in terms of detection accuracy and processing time, allows us to apply this detector to a real world application: the indexing of images and videos."
            },
            "slug": "A-Fast-and-Accurate-Face-Detector-Based-on-Neural-F\u00e9raud-Bernier",
            "title": {
                "fragments": [],
                "text": "A Fast and Accurate Face Detector Based on Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The level of performance reached, in terms of detection accuracy and processing time, allows us to apply this detector to a real world application: the indexing of images and videos."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1830709"
                        ],
                        "name": "A. Lanitis",
                        "slug": "A.-Lanitis",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Lanitis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Lanitis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144482985"
                        ],
                        "name": "C. Taylor",
                        "slug": "C.-Taylor",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Taylor",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7205190"
                        ],
                        "name": "Tim Cootes",
                        "slug": "Tim-Cootes",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Cootes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tim Cootes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10293011,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6bc436d2892be45fd16ba2620ca0a620bf9f52d7",
            "isKey": false,
            "numCitedBy": 299,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe the use of flexible models for representing the shape and grey-level appearance of human faces. These models are controlled by a small number of parameters which can be used to code the overall appearance of a face for image compression and classification purposes. The model parameters control both inter-class and within-class variation. Discriminant analysis techniques are employed to enhance the effect of those parameters affecting inter-class variation, which are useful for classification. We have performed experiments on face coding and reconstruction and automatic face identification. Good recognition rates are obtained even when significant variation in lighting, expression and 3D viewpoint, is allowed. Human faces display significant variation in appearance due to changes in expression, 3D orientation, lighting conditions, hairstyles and so on. A successful automatic face identification system should be capable of suppressing the effect of these factors allowing any face image to be rendered expression-free with standardised 3D orientation and lighting. We describe how the variations in shape and grey-level appearance in face images can be modelled, and present results for a fully automatic face identification system which tolerates changes in expression, viewpoint and lighting."
            },
            "slug": "An-Automatic-Face-Identification-System-Using-Lanitis-Taylor",
            "title": {
                "fragments": [],
                "text": "An Automatic Face Identification System Using Flexible Appearance Models"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "How the variations in shape and grey-level appearance in face images can be modelled are described, and results for a fully automatic face identification system which tolerates changes in expression, viewpoint and lighting are presented."
            },
            "venue": {
                "fragments": [],
                "text": "BMVC"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9215658"
                        ],
                        "name": "R. Chellappa",
                        "slug": "R.-Chellappa",
                        "structuredName": {
                            "firstName": "Rama",
                            "lastName": "Chellappa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Chellappa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144286385"
                        ],
                        "name": "Charles L. Wilson",
                        "slug": "Charles-L.-Wilson",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Wilson",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charles L. Wilson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2503366"
                        ],
                        "name": "S. Sirohey",
                        "slug": "S.-Sirohey",
                        "structuredName": {
                            "firstName": "Saad",
                            "lastName": "Sirohey",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sirohey"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Index Terms\u2014Face detection, face recognition, object recognition, view-based recognition, statistical pattern recognition, machine learning."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62185766,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aa1755e87301af36485ca01e3454bf8888dde8d1",
            "isKey": false,
            "numCitedBy": 3007,
            "numCiting": 177,
            "paperAbstract": {
                "fragments": [],
                "text": "The goal of this paper is to present a critical survey of existing literature on human and machine recognition of faces. Machine recognition of faces has several applications, ranging from static matching of controlled photographs as in mug shots matching and credit card verification to surveillance video images. Such applications have different constraints in terms of complexity of processing requirements and thus present a wide range of different technical challenges. Over the last 20 years researchers in psychophysics, neural sciences and engineering, image processing analysis and computer vision have investigated a number of issues related to face recognition by humans and machines. Ongoing research activities have been given a renewed emphasis over the last five years. Existing techniques and systems have been tested on different sets of images of varying complexities. But very little synergism exists between studies in psychophysics and the engineering literature. Most importantly, there exists no evaluation or benchmarking studies using large databases with the image quality that arises in commercial and law enforcement applications In this paper, we first present different applications of face recognition in commercial and law enforcement sectors. This is followed by a brief overview of the literature on face recognition in the psychophysics community. We then present a detailed overview of move than 20 years of research done in the engineering community. Techniques for segmentation/location of the face, feature extraction and recognition are reviewed. Global transform and feature based methods using statistical, structural and neural classifiers are summarized. >"
            },
            "slug": "Human-and-machine-recognition-of-faces:-a-survey-Chellappa-Wilson",
            "title": {
                "fragments": [],
                "text": "Human and machine recognition of faces: a survey"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "A critical survey of existing literature on human and machine recognition of faces is presented, followed by a brief overview of the literature on face recognition in the psychophysics community and a detailed overview of move than 20 years of research done in the engineering community."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48636477"
                        ],
                        "name": "G. Yang",
                        "slug": "G.-Yang",
                        "structuredName": {
                            "firstName": "Guangzheng",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153652752"
                        ],
                        "name": "Thomas S. Huang",
                        "slug": "Thomas-S.-Huang",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Huang",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas S. Huang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 38060615,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2bb1ba70d48561ce8c3fbf59739fabc95e7b3d50",
            "isKey": false,
            "numCitedBy": 661,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Human-face-detection-in-a-complex-background-Yang-Huang",
            "title": {
                "fragments": [],
                "text": "Human face detection in a complex background"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2228528"
                        ],
                        "name": "J. Terrillon",
                        "slug": "J.-Terrillon",
                        "structuredName": {
                            "firstName": "Jean-Christophe",
                            "lastName": "Terrillon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Terrillon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067641238"
                        ],
                        "name": "Martin David",
                        "slug": "Martin-David",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "David",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Martin David"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49052113"
                        ],
                        "name": "S. Akamatsu",
                        "slug": "S.-Akamatsu",
                        "structuredName": {
                            "firstName": "Shigeru",
                            "lastName": "Akamatsu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Akamatsu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2586252,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c724cea5d76cb8611adfcc9bca8d02ce4158d6cf",
            "isKey": false,
            "numCitedBy": 184,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "We use a skin color model based on the Muhulanobis metric and a shape analysis based on invariant moments to automatically detect and locate human faces in two-dimensional natural scene images. First, color segmentation of an input image is performed by thresholding in a perceptually plausible hue-saturation color space where the effects of the variability of human skin color and the dependency of chrominance on changes in illumination are reduced. We then group regions of the resulting binary image which have been classified as candidates into clusters of connected pixels. Performing median filtering on the image and discarding the smallest remaining clusters ensures that only a small number of clusters will be used for further analysis. Fully translation-, scale- anti in-plane rotation invariant moments are calculated for each remaining cluster. Finally, in order to distinguish faces from distractors, a multilayer perceptron neural network is used with the invariant moments as the input vector. Supervised learning of the network is implemented with the backpropagation algorithm, at first for frontal views of faces. Preliminary results show the efficiency of the combination of color segmentation and of invariant moments in detecting faces with a large variety of poses and against relatively complex backgrounds."
            },
            "slug": "Automatic-detection-of-human-faces-in-natural-scene-Terrillon-David",
            "title": {
                "fragments": [],
                "text": "Automatic detection of human faces in natural scene images by use of a skin color model and of invariant moments"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Preliminary results show the efficiency of the combination of color segmentation and of invariant moments in detecting faces with a large variety of poses and against relatively complex backgrounds."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Third IEEE International Conference on Automatic Face and Gesture Recognition"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699756"
                        ],
                        "name": "K. Sobottka",
                        "slug": "K.-Sobottka",
                        "structuredName": {
                            "firstName": "Karin",
                            "lastName": "Sobottka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Sobottka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144064571"
                        ],
                        "name": "I. Pitas",
                        "slug": "I.-Pitas",
                        "structuredName": {
                            "firstName": "Ioannis",
                            "lastName": "Pitas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Pitas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 217,
                                "start": 212
                            }
                        ],
                        "text": "Several color spaces have been utilized to label pixels as skin including RGB [66], [67], [137], normalized RGB [102], [29], [149], [172], [30], [105], [171], [77], [151], [120], HSV (or HSI) [138], [79], [147], [146], YCrCb [167], [17], YIQ [31], [32], YES [131], CIE XYZ [19], and CIE LUV [173]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10318941,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "037cb2acf08d1f32434d7740385b5fecb5e56850",
            "isKey": false,
            "numCitedBy": 217,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors present a new approach for automatically segmentation and tracking of faces in color images. Segmentation of faces is performed by evaluating color and shape information. First, skin-like regions are determined based on the color attributes hue and saturation. Then regions with elliptical shape are selected as face hypotheses. They are verified by searching for facial features in their interior. After a face is reliably detected it is tracked over time. Tracking is realized by using an active contour model. The exterior forces of the snake are defined based on color features. They push or pull snaxels perpendicular to the snake. Results for tracking are shown for an image sequence consisting of 150 frames."
            },
            "slug": "Segmentation-and-tracking-of-faces-in-color-images-Sobottka-Pitas",
            "title": {
                "fragments": [],
                "text": "Segmentation and tracking of faces in color images"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "A new approach for automatically segmentation and tracking of faces in color images by evaluating color and shape information and finding regions with elliptical shape selected as face hypotheses is presented."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Second International Conference on Automatic Face and Gesture Recognition"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2808897"
                        ],
                        "name": "A. Tsukamoto",
                        "slug": "A.-Tsukamoto",
                        "structuredName": {
                            "firstName": "Akitoshi",
                            "lastName": "Tsukamoto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Tsukamoto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711788"
                        ],
                        "name": "Chil-Woo Lee",
                        "slug": "Chil-Woo-Lee",
                        "structuredName": {
                            "firstName": "Chil-Woo",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chil-Woo Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143873014"
                        ],
                        "name": "S. Tsuji",
                        "slug": "S.-Tsuji",
                        "structuredName": {
                            "firstName": "Saburo",
                            "lastName": "Tsuji",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Tsuji"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 46947149,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "53857aa73f7c2ed9d6eb05070855e00846ddd410",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a new method for detecting a human face, and estimating its pose while tracking it in real image sequences. The virtue of the method is that parameterized qualitative features derived from a lot of sampled facial images are introduced in the detection process, and in the face tracking process, some temporary model images of the face with various poses are synthesized by a texture mapping technique and utilized. While tracking the detected face, many model images are accumulated and the pose of the human face is estimated as a linear combination of correlations between the models."
            },
            "slug": "Detection-and-pose-estimation-of-human-face-with-Tsukamoto-Lee",
            "title": {
                "fragments": [],
                "text": "Detection and pose estimation of human face with synthesized image models"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "A new method for detecting a human face, and estimating its pose while tracking it in real image sequences, using parameterized qualitative features derived from a lot of sampled facial images."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 12th International Conference on Pattern Recognition"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733172"
                        ],
                        "name": "E. Saber",
                        "slug": "E.-Saber",
                        "structuredName": {
                            "firstName": "Eli",
                            "lastName": "Saber",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Saber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747853"
                        ],
                        "name": "A. Tekalp",
                        "slug": "A.-Tekalp",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Tekalp",
                            "middleNames": [
                                "Murat"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Tekalp"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 263,
                                "start": 258
                            }
                        ],
                        "text": "Several color spaces have been utilized to label pixels as skin including RGB [66], [67], [137], normalized RGB [102], [29], [149], [172], [30], [105], [171], [77], [151], [120], HSV (or HSI) [138], [79], [147], [146], YCrCb [167], [17], YIQ [31], [32], YES [131], CIE XYZ [19], and CIE LUV [173]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 91
                            }
                        ],
                        "text": "Skin/nonskin classification is carried out using the class-conditional density function in YES color space followed by smoothing in order to yield contiguous regions."
                    },
                    "intents": []
                }
            ],
            "corpusId": 206037537,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3bc9273d93d8b2ccb23564b4843be118cbc4f254",
            "isKey": false,
            "numCitedBy": 279,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Frontal-view-face-detection-and-facial-feature-and-Saber-Tekalp",
            "title": {
                "fragments": [],
                "text": "Frontal-view face detection and facial feature extraction using color, shape and symmetry based cost functions"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit. Lett."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2228528"
                        ],
                        "name": "J. Terrillon",
                        "slug": "J.-Terrillon",
                        "structuredName": {
                            "firstName": "Jean-Christophe",
                            "lastName": "Terrillon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Terrillon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067641238"
                        ],
                        "name": "Martin David",
                        "slug": "Martin-David",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "David",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Martin David"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49052113"
                        ],
                        "name": "S. Akamatsu",
                        "slug": "S.-Akamatsu",
                        "structuredName": {
                            "firstName": "Shigeru",
                            "lastName": "Akamatsu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Akamatsu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 27153202,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "abc3e3810462c664eb91456cdd383d029824c360",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We use a skin color model based on the Mahalanobis metric and a shape analysis based on invariant Fourier-Mellin moments to automatically detect and locate human faces in two-dimensional complex scene images. First, color segmentation of an input image is performed by thresholding in a normalized hue-saturation. Color space where the effects of the variability of human skin color and the dependency of chrominance on changes in illumination are reduced. We then group regions of the resulting binary image that have been classified as face candidates into clusters of connected pixels. Discarding the smallest clusters in the image ensures that only a small number of clusters will be used for further analysis. Fully translation-, scale- and in-plane rotation-invariant moments are calculated for each remaining cluster Finally, in order to distinguish faces from distractors, a multilayer perceptron neural network is used with the invariant moments as the input vector supervised learning of the network is implemented with the backpropagation algorithm, at first for frontal views of faces. Preliminary results show the efficiency of the combination of color segmentation and of invariant moments in detecting faces with a large variety of poses and against relatively complex backgrounds."
            },
            "slug": "Detection-of-human-faces-in-complex-scene-images-by-Terrillon-David",
            "title": {
                "fragments": [],
                "text": "Detection of human faces in complex scene images by use of a skin color model and of invariant Fourier-Mellin moments"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Preliminary results show the efficiency of the combination of color segmentation and of invariant moments in detecting faces with a large variety of poses and against relatively complex backgrounds."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. Fourteenth International Conference on Pattern Recognition (Cat. No.98EX170)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3054345"
                        ],
                        "name": "C. Kervrann",
                        "slug": "C.-Kervrann",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Kervrann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Kervrann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742818"
                        ],
                        "name": "F. Davoine",
                        "slug": "F.-Davoine",
                        "structuredName": {
                            "firstName": "Franck",
                            "lastName": "Davoine",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Davoine"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144565371"
                        ],
                        "name": "P. P\u00e9rez",
                        "slug": "P.-P\u00e9rez",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "P\u00e9rez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. P\u00e9rez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1767736"
                        ],
                        "name": "R. Forchheimer",
                        "slug": "R.-Forchheimer",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Forchheimer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Forchheimer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3339849"
                        ],
                        "name": "C. Labit",
                        "slug": "C.-Labit",
                        "structuredName": {
                            "firstName": "Claude",
                            "lastName": "Labit",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Labit"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13389342,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4b9bc8cf98f915e0ffd7ea781633d8ed20f2f897",
            "isKey": false,
            "numCitedBy": 21,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Generalized-likelihood-ratio-based-face-detection-Kervrann-Davoine",
            "title": {
                "fragments": [],
                "text": "Generalized Likelihood Ratio-based Face Detection and Extraction of Mouth Features"
            },
            "venue": {
                "fragments": [],
                "text": "AVBPA"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144097660"
                        ],
                        "name": "M. Turk",
                        "slug": "M.-Turk",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Turk",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Turk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144994682"
                        ],
                        "name": "A. Pentland",
                        "slug": "A.-Pentland",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Pentland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pentland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Index Terms\u2014Face detection, face recognition, object recognition, view-based recognition, statistical pattern recognition, machine learning."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 26127529,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a6f1dfcc44277d4cfd8507284d994c9283dc3a2f",
            "isKey": false,
            "numCitedBy": 14954,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We have developed a near-real-time computer system that can locate and track a subject's head, and then recognize the person by comparing characteristics of the face to those of known individuals. The computational approach taken in this system is motivated by both physiology and information theory, as well as by the practical requirements of near-real-time performance and accuracy. Our approach treats the face recognition problem as an intrinsically two-dimensional (2-D) recognition problem rather than requiring recovery of three-dimensional geometry, taking advantage of the fact that faces are normally upright and thus may be described by a small set of 2-D characteristic views. The system functions by projecting face images onto a feature space that spans the significant variations among known face images. The significant features are known as \"eigenfaces,\" because they are the eigenvectors (principal components) of the set of faces; they do not necessarily correspond to features such as eyes, ears, and noses. The projection operation characterizes an individual face by a weighted sum of the eigenface features, and so to recognize a particular face it is necessary only to compare these weights to those of known individuals. Some particular advantages of our approach are that it provides for the ability to learn and later recognize new faces in an unsupervised manner, and that it is easy to implement using a neural network architecture."
            },
            "slug": "Eigenfaces-for-Recognition-Turk-Pentland",
            "title": {
                "fragments": [],
                "text": "Eigenfaces for Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "A near-real-time computer system that can locate and track a subject's head, and then recognize the person by comparing characteristics of the face to those of known individuals, and that is easy to implement using a neural network architecture."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Cognitive Neuroscience"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794240"
                        ],
                        "name": "Haiyuan Wu",
                        "slug": "Haiyuan-Wu",
                        "structuredName": {
                            "firstName": "Haiyuan",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Haiyuan Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055712370"
                        ],
                        "name": "T. Yokoyama",
                        "slug": "T.-Yokoyama",
                        "structuredName": {
                            "firstName": "Taro",
                            "lastName": "Yokoyama",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Yokoyama"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2127304"
                        ],
                        "name": "D. Pramadihanto",
                        "slug": "D.-Pramadihanto",
                        "structuredName": {
                            "firstName": "Dadet",
                            "lastName": "Pramadihanto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Pramadihanto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735941"
                        ],
                        "name": "M. Yachida",
                        "slug": "M.-Yachida",
                        "structuredName": {
                            "firstName": "Masahiko",
                            "lastName": "Yachida",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Yachida"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 46083950,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "20bc70459a93f229cc89a1252e6220261d70ec44",
            "isKey": false,
            "numCitedBy": 65,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents an automatic processing of human face from color images. The system works hierarchically from detecting the position of human face and its features (such as eyes, nose, mouth, etc.) to contours and feature points extraction. The position of human face and its parts are detected from the image by applying the integral projection method, which synthesize the color information (skin and hair color) and the edge information (intensity and sign). In order to extract the contour-line of face features we used a multiple active contour model with color information based energy terms. Facial feature points are decided based on the optimized contours. The proposed system is confirmed to be very effective and robust to deal with the image of faces in the complex background."
            },
            "slug": "Face-and-facial-feature-extraction-from-color-image-Wu-Yokoyama",
            "title": {
                "fragments": [],
                "text": "Face and facial feature extraction from color image"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "The proposed system works hierarchically from detecting the position of human face and its features to contours and feature points extraction, and is confirmed to be very effective and robust to deal with the image of faces in the complex background."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Second International Conference on Automatic Face and Gesture Recognition"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50564384"
                        ],
                        "name": "G. Edwards",
                        "slug": "G.-Edwards",
                        "structuredName": {
                            "firstName": "Gareth",
                            "lastName": "Edwards",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Edwards"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144482985"
                        ],
                        "name": "C. Taylor",
                        "slug": "C.-Taylor",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Taylor",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7205190"
                        ],
                        "name": "Tim Cootes",
                        "slug": "Tim-Cootes",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Cootes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tim Cootes"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Index Terms\u2014Face detection, face recognition, object recognition, view-based recognition, statistical pattern recognition, machine learning."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5497564,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "98f4a96d0bb9112fffceb108cbbab5bf80407f84",
            "isKey": false,
            "numCitedBy": 82,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the problem of robust face identification in the presence of pose, lighting, and expression variation. Previous approaches to the problem have assumed similar models of variation for each individual, estimated from pooled training data. We describe a method of updating a first order global estimate of identity by learning the class-specific correlation between the estimate and the residual variation during a sequence. This is integrated with an optimal tracking scheme, in which identity variation is decoupled from pose, lighting and expression variation. The method results in robust tracking and a more stable estimate of facial identity under changing conditions."
            },
            "slug": "Learning-to-identify-and-track-faces-in-image-Edwards-Taylor",
            "title": {
                "fragments": [],
                "text": "Learning to identify and track faces in image sequences"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A method of updating a first order global estimate of identity by learning the class-specific correlation between the estimate and the residual variation during a sequence is described, integrated with an optimal tracking scheme, in which identity variation is decoupled from pose, lighting and expression variation."
            },
            "venue": {
                "fragments": [],
                "text": "Sixth International Conference on Computer Vision (IEEE Cat. No.98CH36271)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792053"
                        ],
                        "name": "B. Scassellati",
                        "slug": "B.-Scassellati",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Scassellati",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Scassellati"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7383841,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2699c31109e6ac262a4bf2f4d832646ed3834656",
            "isKey": false,
            "numCitedBy": 98,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Eye finding is the first step toward building a machine that can recognize social cues, like eye contact and gaze direction, in a natural context. In this paper, we present a real-time implementation of an eye finding algorithm for a foveated active vision system. The system uses a motion-based prefilter to identify potential face locations. These locations are analyzed for faces with a template-based algorithm developed by Sinha (1996). Detected faces are tracked in real time, and the active vision system saccades to the face using a learned sensorimotor mapping. Once gaze has been centered on the face, a high-resolution image of the eye can be captured from the foveal camera using a self-calibrated peripheral-ta-foveal mapping.We also present a performance analysis of Sinha's ratio template algorithm on a standard set of static face images. Although this algorithm performs relatively poorly on static images, this result is a poor indicator of real-time performance of the behaving system. We find that our system finds eyes in 94% of a set of behavioral trials. We suggest that alternate means of evaluating behavioral systems are necessary."
            },
            "slug": "Eye-Finding-via-Face-Detection-for-a-Foveated-Scassellati",
            "title": {
                "fragments": [],
                "text": "Eye Finding via Face Detection for a Foveated Active Vision System"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper presents a real-time implementation of an eye finding algorithm for a foveated active vision system, and finds that the system finds eyes in 94% of a set of behavioral trials, suggesting that alternate means of evaluating behavioral systems are necessary."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI/IAAI"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47128406"
                        ],
                        "name": "Shang-Hung Lin",
                        "slug": "Shang-Hung-Lin",
                        "structuredName": {
                            "firstName": "Shang-Hung",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shang-Hung Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144410963"
                        ],
                        "name": "S. Kung",
                        "slug": "S.-Kung",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Kung",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111062938"
                        ],
                        "name": "Long-Ji Lin",
                        "slug": "Long-Ji-Lin",
                        "structuredName": {
                            "firstName": "Long-Ji",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Long-Ji Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 24636915,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fa2603efaf717974c77162c93d800defae61a129",
            "isKey": false,
            "numCitedBy": 669,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a face recognition system, based on probabilistic decision-based neural networks (PDBNN). With technological advance on microelectronic and vision system, high performance automatic techniques on biometric recognition are now becoming economically feasible. Among all the biometric identification methods, face recognition has attracted much attention in recent years because it has potential to be most nonintrusive and user-friendly. The PDBNN face recognition system consists of three modules: First, a face detector finds the location of a human face in an image. Then an eye localizer determines the positions of both eyes in order to generate meaningful feature vectors. The facial region proposed contains eyebrows, eyes, and nose, but excluding mouth (eye-glasses will be allowed). Lastly, the third module is a face recognizer. The PDBNN can be effectively applied to all the three modules. It adopts a hierarchical network structures with nonlinear basis functions and a competitive credit-assignment scheme. The paper demonstrates a successful application of PDBNN to face recognition applications on two public (FERET and ORL) and one in-house (SCR) databases. Regarding the performance, experimental results on three different databases such as recognition accuracies as well as false rejection and false acceptance rates are elaborated. As to the processing speed, the whole recognition process (including PDBNN processing for eye localization, feature extraction, and classification) consumes approximately one second on Sparc10, without using hardware accelerator or co-processor."
            },
            "slug": "Face-recognition/detection-by-probabilistic-neural-Lin-Kung",
            "title": {
                "fragments": [],
                "text": "Face recognition/detection by probabilistic decision-based neural network"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The paper demonstrates a successful application of PDBNN to face recognition applications on two public (FERET and ORL) and one in-house (SCR) databases and experimental results on three different databases such as recognition accuracies as well as false rejection and false acceptance rates are elaborated."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "117208225"
                        ],
                        "name": "Venu Govindaraju",
                        "slug": "Venu-Govindaraju",
                        "structuredName": {
                            "firstName": "Venu",
                            "lastName": "Govindaraju",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Venu Govindaraju"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35100689"
                        ],
                        "name": "D. Sher",
                        "slug": "D.-Sher",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Sher",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Sher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748081"
                        ],
                        "name": "R. Srihari",
                        "slug": "R.-Srihari",
                        "structuredName": {
                            "firstName": "Rohini",
                            "lastName": "Srihari",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Srihari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696384"
                        ],
                        "name": "S. Srihari",
                        "slug": "S.-Srihari",
                        "structuredName": {
                            "firstName": "Sargur",
                            "lastName": "Srihari",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Srihari"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 94
                            }
                        ],
                        "text": "presented a two stage face detection method in which face hypotheses are generated and tested [52], [53], [51]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 202,
                                "start": 198
                            }
                        ],
                        "text": "When detecting faces in newspaper articles, collateral information, which indicates the number of persons in the image, is obtained from the caption of the input image to select the best hypotheses [52]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 31424544,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "c9b1f3bfa3301c68cdb9382fa79512e80c5a92a1",
            "isKey": false,
            "numCitedBy": 100,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "A computational approach to locating human faces in newspaper photographs is described. While the computational recognition of a well-framed face as one of a known set of faces has received some attention, the computational location of faces in varying contexts is relatively unexplored. Candidates for the locations of faces are hypothesized by extracting features in the edge-image of the photograph and matching with a model of a face profile. A face is defined by a parametric representation of each of the component parts. Knowledge contained in the caption is represented using semantic networks and is used to reason about the locations of faces of individuals in the photograph.<<ETX>>"
            },
            "slug": "Locating-human-faces-in-newspaper-photographs-Govindaraju-Sher",
            "title": {
                "fragments": [],
                "text": "Locating human faces in newspaper photographs"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "A computational approach to locating human faces in newspaper photographs is described, and candidates for the locations of faces are hypothesized by extracting features in the edge-image of the photograph and matching with a model of a face profile."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings CVPR '89: IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118808101"
                        ],
                        "name": "Sang-Hoon Kim",
                        "slug": "Sang-Hoon-Kim",
                        "structuredName": {
                            "firstName": "Sang-Hoon",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sang-Hoon Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2119401931"
                        ],
                        "name": "Nam-Kyu Kim",
                        "slug": "Nam-Kyu-Kim",
                        "structuredName": {
                            "firstName": "Nam-Kyu",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nam-Kyu Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1853776"
                        ],
                        "name": "S. Ahn",
                        "slug": "S.-Ahn",
                        "structuredName": {
                            "firstName": "Sang",
                            "lastName": "Ahn",
                            "middleNames": [
                                "Chul"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ahn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1931368"
                        ],
                        "name": "Hyoung-Gon Kim",
                        "slug": "Hyoung-Gon-Kim",
                        "structuredName": {
                            "firstName": "Hyoung-Gon",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hyoung-Gon Kim"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 159
                            }
                        ],
                        "text": "Several color spaces have been utilized to label pixels as skin including RGB [66], [67], [137], normalized RGB [102], [29], [149], [172], [30], [105], [171], [77], [151], [120], HSV (or HSI) [138], [79], [147], [146], YCrCb [167], [17], YIQ [31], [32], YES [131], CIE XYZ [19], and CIE LUV [173]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7394271,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b3cba0f5a0c3aaac180a8abe0fb27de27fdb6618",
            "isKey": false,
            "numCitedBy": 68,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes an object oriented face detection method using range and color information. Objects are segmented from the background using the stereo disparity histogram that represents the range information of the objects. A matching pixel count (MPC) disparity measure is introduced to enhance the matching accuracy, and removes the effect of unexpected noise in the boundary region. For the high-performance implementation of the MPC disparity histogram, redundancy operations inherent to the area-based search operation are removed. To detect facial regions among segmented objects, a skin-color transform technique is used with the generalized face color distribution (GFCD) modeled by a 2D Gaussian function in a normalized color space. Using GFCD, the input color image can be transformed into a gray-level image enhancing only facial color components. To detect facial information only in the defined range, both results from range segmentation and color transforms are combined effectively. The experimental results show that the proposed algorithm works well in various environments with multiple human objects. Moreover, the processing time for a test image is not exceeding 2 seconds in general purpose workstations. The range information of the objects can be useful in MPEG-4 where natural and synthetic images can be mixed and synthesized."
            },
            "slug": "Object-oriented-face-detection-using-range-and-Kim-Kim",
            "title": {
                "fragments": [],
                "text": "Object oriented face detection using range and color information"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "An object oriented face detection method using range and color information that works well in various environments with multiple human objects and can be useful in MPEG-4 where natural and synthetic images can be mixed and synthesized."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Third IEEE International Conference on Automatic Face and Gesture Recognition"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39682833"
                        ],
                        "name": "H. Rowley",
                        "slug": "H.-Rowley",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Rowley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Rowley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1767244"
                        ],
                        "name": "S. Baluja",
                        "slug": "S.-Baluja",
                        "structuredName": {
                            "firstName": "Shumeet",
                            "lastName": "Baluja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Baluja"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733113"
                        ],
                        "name": "T. Kanade",
                        "slug": "T.-Kanade",
                        "structuredName": {
                            "firstName": "Takeo",
                            "lastName": "Kanade",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanade"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 74
                            }
                        ],
                        "text": "On a small test set of 42 images, they report a detection rate similar to [126]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 676887,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f6af749b2b813af20c2f26962249fafdccdc6a1e",
            "isKey": false,
            "numCitedBy": 477,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a neural network-based face detection system. A retinally connected neural network examines small windows of an image, and decides whether each window contains a face. The system arbitrates between multiple networks to improve performance over a single network. We use a bootstrap algorithm for training, which adds false detections into the training set as training progresses. This eliminates the difficult task of manually selecting non-face training examples, which must be chosen to span the entire space of non-face images. Comparisons with another state-of-the-art face detection system are presented; our system has better performance in terms of detection and false-positive rates."
            },
            "slug": "Human-Face-Detection-in-Visual-Scenes-Rowley-Baluja",
            "title": {
                "fragments": [],
                "text": "Human Face Detection in Visual Scenes"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "A neural network-based face detection system that uses a bootstrap algorithm for training, which adds false detections into the training set as training progresses, and has better performance in terms of detection and false-positive rates than other state-of-the-art face detection systems."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717217"
                        ],
                        "name": "R. J. Qian",
                        "slug": "R.-J.-Qian",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Qian",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. J. Qian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5208597"
                        ],
                        "name": "M. Sezan",
                        "slug": "M.-Sezan",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Sezan",
                            "middleNames": [
                                "Ibrahim"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Sezan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2930168"
                        ],
                        "name": "Kristine E. Matthews",
                        "slug": "Kristine-E.-Matthews",
                        "structuredName": {
                            "firstName": "Kristine",
                            "lastName": "Matthews",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kristine E. Matthews"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 172
                            }
                        ],
                        "text": "Several color spaces have been utilized to label pixels as skin including RGB [66], [67], [137], normalized RGB [102], [29], [149], [172], [30], [105], [171], [77], [151], [120], HSV (or HSI) [138], [79], [147], [146], YCrCb [167], [17], YIQ [31], [32], YES [131], CIE XYZ [19], and CIE LUV [173]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 37790367,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3310fbf844fae86ec5c40a573bd5be33aab695a3",
            "isKey": false,
            "numCitedBy": 72,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a statistics-based method for estimating the position and size of a face in a complex background. Face position and size are estimated based on robust statistical measurements which are derived from two one-dimensional histograms obtained by projecting the result of skin color filtering. The proposed algorithm also utilizes a linear Kalman filter and a simple nonlinear filter to perform smooth tracking and remove jitter. The algorithm has been implemented and tested under a wide range of real-world conditions. It has consistently provided performance which satisfies the following requirements: (1) the ability to automatically determine the initial position and size of a face and track it in a complex background; (2) is insensitive to partial occlusions and shadows; (3) is insensitive to face orientation and scale changes; and (4) is also insensitive to lighting condition changes. In addition, the algorithm is computationally simple so that it can be executed in real-time."
            },
            "slug": "A-robust-real-time-face-tracking-algorithm-Qian-Sezan",
            "title": {
                "fragments": [],
                "text": "A robust real-time face tracking algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "This paper presents a statistics-based method for estimating the position and size of a face in a complex background based on robust statistical measurements derived from two one-dimensional histograms obtained by projecting the result of skin color filtering."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 1998 International Conference on Image Processing. ICIP98 (Cat. No.98CB36269)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38817267"
                        ],
                        "name": "K. Sung",
                        "slug": "K.-Sung",
                        "structuredName": {
                            "firstName": "Kah",
                            "lastName": "Sung",
                            "middleNames": [
                                "Kay"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Sung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7164794,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "088eb2d102c6bb486f5270d0b2adff76961994cf",
            "isKey": false,
            "numCitedBy": 2061,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an example-based learning approach for locating vertical frontal views of human faces in complex scenes. The technique models the distribution of human face patterns by means of a few view-based \"face\" and \"nonface\" model clusters. At each image location, a difference feature vector is computed between the local image pattern and the distribution-based model. A trained classifier determines, based on the difference feature vector measurements, whether or not a human face exists at the current image location. We show empirically that the distance metric we adopt for computing difference feature vectors, and the \"nonface\" clusters we include in our distribution-based model, are both critical for the success of our system."
            },
            "slug": "Example-Based-Learning-for-View-Based-Human-Face-Sung-Poggio",
            "title": {
                "fragments": [],
                "text": "Example-Based Learning for View-Based Human Face Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "An example-based learning approach for locating vertical frontal views of human faces in complex scenes and shows empirically that the distance metric adopted for computing difference feature vectors, and the \"nonface\" clusters included in the distribution-based model, are both critical for the success of the system."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144470775"
                        ],
                        "name": "D. Chai",
                        "slug": "D.-Chai",
                        "structuredName": {
                            "firstName": "Douglas",
                            "lastName": "Chai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Chai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1684869"
                        ],
                        "name": "K. Ngan",
                        "slug": "K.-Ngan",
                        "structuredName": {
                            "firstName": "King",
                            "lastName": "Ngan",
                            "middleNames": [
                                "Ngi"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Ngan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 232
                            }
                        ],
                        "text": "Several color spaces have been utilized to label pixels as skin including RGB [66], [67], [137], normalized RGB [102], [29], [149], [172], [30], [105], [171], [77], [151], [120], HSV (or HSI) [138], [79], [147], [146], YCrCb [167], [17], YIQ [31], [32], YES [131], CIE XYZ [19], and CIE LUV [173]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 81
                            }
                        ],
                        "text": "The simplest model is to define a region of skin tone pixels using Cr;Cb values [17], i.e., R\u00f0Cr;Cb\u00de, from samples of skin color pixels."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 35628066,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "9c1456151a4e35fafe0efded8c151a9824cc7645",
            "isKey": false,
            "numCitedBy": 221,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper addresses our proposed method to automatically locate the person's face from a given image that consists of a head-and-shoulders view of the person and a complex background scene. The method involves a fast, simple and yet robust algorithm that exploits the spatial distribution characteristics of human skin color. It first uses the chrominance component of the input image to detect pixels with skin color appearance. Then, bused on the spatial distribution of the detected skin-color pixels and their corresponding luminance values, the algorithm employs some regularization processes to reinforce regions of skin-color pixels that are more likely to belong to the facial regions and eliminate those that are not. The performance of the face localization algorithm is illustrated by some simulation results carried out on various head-and-shoulders test images."
            },
            "slug": "Locating-facial-region-of-a-head-and-shoulders-Chai-Ngan",
            "title": {
                "fragments": [],
                "text": "Locating facial region of a head-and-shoulders color image"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "This paper addresses the proposed method to automatically locate the person's face from a given image that consists of a head-and-shoulders view of the person and a complex background scene and involves a fast, simple and yet robust algorithm that exploits the spatial distribution characteristics of human skin color."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Third IEEE International Conference on Automatic Face and Gesture Recognition"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46599154"
                        ],
                        "name": "T. Sakai",
                        "slug": "T.-Sakai",
                        "structuredName": {
                            "firstName": "Toshiyuki",
                            "lastName": "Sakai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sakai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2162080"
                        ],
                        "name": "M. Nagao",
                        "slug": "M.-Nagao",
                        "structuredName": {
                            "firstName": "Makoto",
                            "lastName": "Nagao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Nagao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153311318"
                        ],
                        "name": "Shinya Fujibayashi",
                        "slug": "Shinya-Fujibayashi",
                        "structuredName": {
                            "firstName": "Shinya",
                            "lastName": "Fujibayashi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shinya Fujibayashi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 29357850,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "99fde1986e9ea3dea7af76b86f39dfefcdec9db0",
            "isKey": false,
            "numCitedBy": 112,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Line-extraction-and-pattern-detection-in-a-Sakai-Nagao",
            "title": {
                "fragments": [],
                "text": "Line extraction and pattern detection in a photograph"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1969
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2121240618"
                        ],
                        "name": "J. Cai",
                        "slug": "J.-Cai",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Cai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2312891"
                        ],
                        "name": "A. Goshtasby",
                        "slug": "A.-Goshtasby",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Goshtasby",
                            "middleNames": [
                                "Ardeshir"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Goshtasby"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11775703,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0a406e97cf090b697575d1be813b3106cf2ef49d",
            "isKey": false,
            "numCitedBy": 277,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "A method is introduced that detects human faces in color images by first separating skin regions from non-skin regions and then locating faces within the skin regions. A chroma chart is prepared via a training process that shows likelihoods of different colors representing the skin. Using the chroma chart, a color image is transformed into a gray-scale image, with the gray value at a pixel showing the likelihood of the pixel representing the skin. By segmenting the gray-scale image, skin regions are separated from non-skin regions. Then, using the luminance component of the color image and by template matching, faces are located within skin regions."
            },
            "slug": "Detecting-human-faces-in-color-images-Cai-Goshtasby",
            "title": {
                "fragments": [],
                "text": "Detecting human faces in color images"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "A method is introduced that detects human faces in color images by first separating skin regions from non-skin regions and then locating faces within the skin regions by using the luminance component of the color image and by template matching."
            },
            "venue": {
                "fragments": [],
                "text": "Image Vis. Comput."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39682833"
                        ],
                        "name": "H. Rowley",
                        "slug": "H.-Rowley",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Rowley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Rowley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1767244"
                        ],
                        "name": "S. Baluja",
                        "slug": "S.-Baluja",
                        "structuredName": {
                            "firstName": "Shumeet",
                            "lastName": "Baluja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Baluja"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733113"
                        ],
                        "name": "T. Kanade",
                        "slug": "T.-Kanade",
                        "structuredName": {
                            "firstName": "Takeo",
                            "lastName": "Kanade",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanade"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1619589,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2fd1c99edbb3d22cec4adc9ba9319cfc2360e903",
            "isKey": false,
            "numCitedBy": 348,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present a neural network-based face detection system. Unlike similar systems which are limited to detecting upright, frontal faces, this system detects faces at any degree of rotation in the image plane. The system employs multiple networks; a \"router\" network first processes each input window to determine its orientation and then uses this information to prepare the window for one or more \"detector\" networks. We present the training methods for both types of networks. We also perform sensitivity analysis on the networks, and present empirical results on a large test set. Finally, we present preliminary results for detecting faces rotated out of the image plane, such as profiles and semi-profiles."
            },
            "slug": "Rotation-invariant-neural-network-based-face-Rowley-Baluja",
            "title": {
                "fragments": [],
                "text": "Rotation invariant neural network-based face detection"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "This paper presents a neural network-based face detection system, which is limited to detecting upright, frontal faces, and presents preliminary results for detecting faces rotated out of the image plane, such as profiles and semi-profiles."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 1998 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.98CB36231)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143891066"
                        ],
                        "name": "A. Rajagopalan",
                        "slug": "A.-Rajagopalan",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Rajagopalan",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Rajagopalan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39219932"
                        ],
                        "name": "K. S. Kumar",
                        "slug": "K.-S.-Kumar",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Kumar",
                            "middleNames": [
                                "Sunil"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. S. Kumar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1785111"
                        ],
                        "name": "J. Karlekar",
                        "slug": "J.-Karlekar",
                        "structuredName": {
                            "firstName": "Jayashree",
                            "lastName": "Karlekar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Karlekar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2269939"
                        ],
                        "name": "R. Manivasakan",
                        "slug": "R.-Manivasakan",
                        "structuredName": {
                            "firstName": "Rathinam",
                            "lastName": "Manivasakan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Manivasakan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052261268"
                        ],
                        "name": "M. Patil",
                        "slug": "M.-Patil",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Patil",
                            "middleNames": [
                                "Milind"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Patil"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144907915"
                        ],
                        "name": "U. Desai",
                        "slug": "U.-Desai",
                        "structuredName": {
                            "firstName": "Uday",
                            "lastName": "Desai",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Desai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2772733"
                        ],
                        "name": "P. G. Poonacha",
                        "slug": "P.-G.-Poonacha",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Poonacha",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. G. Poonacha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144527832"
                        ],
                        "name": "S. Chaudhuri",
                        "slug": "S.-Chaudhuri",
                        "structuredName": {
                            "firstName": "Subhasis",
                            "lastName": "Chaudhuri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Chaudhuri"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 10
                            }
                        ],
                        "text": "method in [123] uses an HMM to learn the face to nonface"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 30
                            }
                        ],
                        "text": "patterns, the first method in [123] uses higher order"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11398552,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5c610939c03a450617a2d5290f0a4fa731407333",
            "isKey": false,
            "numCitedBy": 110,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Two new schemes are presented for finding human faces in a photograph. The first scheme approximates the unknown distributions of the face and the face-like manifolds wing higher order statistics (HOS). An HOS-based data clustering algorithm is also proposed. In the second scheme, the face to non-face and non-face to face transitions are learnt using a hidden Markov model (HMM). The HMM parameters are estimated corresponding to a given photograph and the faces are located by examining the optimal state sequence of the HMM. Experimental results are presented on the performance of both the schemes."
            },
            "slug": "Finding-faces-in-photographs-Rajagopalan-Kumar",
            "title": {
                "fragments": [],
                "text": "Finding faces in photographs"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "Two new schemes for finding human faces in a photograph are presented, one of which approximates the unknown distributions of the face and the face-like manifolds wing higher order statistics (HOS)."
            },
            "venue": {
                "fragments": [],
                "text": "Sixth International Conference on Computer Vision (IEEE Cat. No.98CH36271)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118579343"
                        ],
                        "name": "Jie Yang",
                        "slug": "Jie-Yang",
                        "structuredName": {
                            "firstName": "Jie",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jie Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724972"
                        ],
                        "name": "A. Waibel",
                        "slug": "A.-Waibel",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Waibel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Waibel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 190,
                                "start": 185
                            }
                        ],
                        "text": "Although different people have different skin color, several studies have shown that the major difference lies largely between their intensity rather than their chrominance [54], [55], [172]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1154755,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "002aaf4412f91d0828b79511f35c0863a1a32c47",
            "isKey": false,
            "numCitedBy": 656,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors present a real-time face tracker. The system has achieved a rate of 30+ frames/second using an HP-9000 workstation with a frame grabber and a Canon VC-Cl camera. It can track a person's face while the person moves freely (e.g., walks, jumps, sits down and stands up) in a room. Three types of models have been employed in developing the system. First, they present a stochastic model to characterize skin color distributions of human faces. The information provided by the model is sufficient for tracking a human face in various poses and views. This model is adaptable to different people and different lighting conditions in real-time. Second, a motion model is used to estimate image motion and to predict the search window. Third, a camera model is used to predict and compensate for camera motion. The system can be applied to teleconferencing and many HCI applications including lip reading and gaze tracking. The principle in developing this system can be extended to other tracking problems such as tracking the human hand."
            },
            "slug": "A-real-time-face-tracker-Yang-Waibel",
            "title": {
                "fragments": [],
                "text": "A real-time face tracker"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "The authors present a real-time face tracker that can track a person's face while the person moves freely in a room and can be applied to teleconferencing and many HCI applications including lip reading and gaze tracking."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Third IEEE Workshop on Applications of Computer Vision. WACV'96"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753210"
                        ],
                        "name": "Trevor Darrell",
                        "slug": "Trevor-Darrell",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Darrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Darrell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40302807"
                        ],
                        "name": "G. Gordon",
                        "slug": "G.-Gordon",
                        "structuredName": {
                            "firstName": "Gaile",
                            "lastName": "Gordon",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Gordon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143833523"
                        ],
                        "name": "M. Harville",
                        "slug": "M.-Harville",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Harville",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Harville"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1803592"
                        ],
                        "name": "J. Woodfill",
                        "slug": "J.-Woodfill",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Woodfill",
                            "middleNames": [
                                "Iselin"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Woodfill"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Index Terms\u2014Face detection, face recognition, object recognition, view-based recognition, statistical pattern recognition, machine learning."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10236323,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c7deb3faa8dc3f39344eb2e778bf83991464708e",
            "isKey": false,
            "numCitedBy": 455,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an approach to real-time person tracking in crowded and/or unknown environments using integration of multiple visual modalities. We combine stereo, color, and face detection modules into a single robust system, and show an initial application in an interactive, face-responsive display. Dense, real-time stereo processing is used to isolate users from other objects and people in the background. Skin-hue classification identifies and tracks likely body parts within the silhouette of a user. Face pattern detection discriminates and localizes the face within the identified body parts. Faces and bodies of users are tracked over several temporal scales: short-term (user stays within the field of view), medium-term (user exits/reenters within minutes), and long term (user returns after hours or days). Short-term tracking is performed using simple region position and size correspondences, while medium and long-term tracking are based on statistics of user appearance. We discuss the failure modes of each individual module, describe our integration method, and report results with the complete system in trials with thousands of users."
            },
            "slug": "Integrated-Person-Tracking-Using-Stereo,-Color,-and-Darrell-Gordon",
            "title": {
                "fragments": [],
                "text": "Integrated Person Tracking Using Stereo, Color, and Pattern Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work combines stereo, color, and face detection modules into a single robust system, shows an initial application in an interactive, face-responsive display, and discusses the failure modes of each individual module."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2503366"
                        ],
                        "name": "S. Sirohey",
                        "slug": "S.-Sirohey",
                        "structuredName": {
                            "firstName": "Saad",
                            "lastName": "Sirohey",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sirohey"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 109
                            }
                        ],
                        "text": "Sirohey proposed a localization method to segment a face from a cluttered background for face identification [145]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60899982,
            "fieldsOfStudy": [
                "Geology"
            ],
            "id": "769bcd88eed1f6f3db767ddcc30a63aa4ba7105e",
            "isKey": false,
            "numCitedBy": 125,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "This thesis considers segmentation and identiication of human faces from grey scale images with clutter. The segmentation developed utilizes the elliptical structure of the human head. It uses the information present in the edge map of the image and through some preprocessing separates the head from the background clutter. An ellipse is then tted to mark the boundary between the head region and the background. The identiication procedure nds feature points in the segmented face through a Gabor wavelet decomposition and performs graph matching. The segmentation and identiication algorithms were tested on a database of 48 images of 16 persons with encouraging results. Dedication To my grandmother Masooda Begum and my parents Iftikhar A. Sirohey and Zarina Sirohey ii Acknowledgements I would like to take this opportunity to thank all the people who have been instrumental the accomplishment of this task. First of all Professor Rama Chellappa, with his superb guidance and support during the course of this thesis. Dr. Q. Zheng for his time and eeort and the lively discussions that we had concerning the topic. and David for putting up with me for the last year. Our discussions together brought new insight for me regarding the research. And of course this work would not have been possible if not for the undying love, support and encouragement of my parents, Iftikhar A. Sirohey and Zarina Sirohey. To them any gratitude I may express will not be enough."
            },
            "slug": "Human-Face-Segmentation-and-Identification-Sirohey",
            "title": {
                "fragments": [],
                "text": "Human Face Segmentation and Identification"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "The segmentation developed utilizes the elliptical structure of the human head and uses the information present in the edge map of the image and through some preprocessing separates the head from the background clutter."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143920486"
                        ],
                        "name": "F. Samaria",
                        "slug": "F.-Samaria",
                        "structuredName": {
                            "firstName": "Ferdinando",
                            "lastName": "Samaria",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Samaria"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145259603"
                        ],
                        "name": "S. Young",
                        "slug": "S.-Young",
                        "structuredName": {
                            "firstName": "Steve",
                            "lastName": "Young",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Young"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 10798965,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4bcb88b6842b8fa0fb257e904f4113487e7af554",
            "isKey": false,
            "numCitedBy": 370,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "HMM-based-architecture-for-face-identification-Samaria-Young",
            "title": {
                "fragments": [],
                "text": "HMM-based architecture for face identification"
            },
            "venue": {
                "fragments": [],
                "text": "Image Vis. Comput."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715634"
                        ],
                        "name": "Ming-Hsuan Yang",
                        "slug": "Ming-Hsuan-Yang",
                        "structuredName": {
                            "firstName": "Ming-Hsuan",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming-Hsuan Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145237406"
                        ],
                        "name": "N. Ahuja",
                        "slug": "N.-Ahuja",
                        "structuredName": {
                            "firstName": "Narendra",
                            "lastName": "Ahuja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Ahuja"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1765887"
                        ],
                        "name": "D. Kriegman",
                        "slug": "D.-Kriegman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Kriegman",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Kriegman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16705499,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "29fe0a5209ad7c538aeaf9819644abac532bcce9",
            "isKey": false,
            "numCitedBy": 120,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "We present two methods using mixtures of linear sub-spaces for face detection in gray level images. One method uses a mixture of factor analyzers to concurrently perform clustering and, within each cluster, perform local dimensionality reduction. The parameters of the mixture model are estimated using an EM algorithm. A face is detected if the probability of an input sample is above a predefined threshold. The other mixture of subspaces method uses Kohonen's self-organizing map for clustering and Fisher linear discriminant to find the optimal projection for pattern classification, and a Gaussian distribution to model the class-conditioned density function of the projected samples for each class. The parameters of the class-conditioned density functions are maximum likelihood estimates and the decision rule is also based on maximum likelihood. A wide range of face images including ones in different poses, with different expressions and under different lighting conditions are used as the training set to capture the variations of human faces. Our methods have been tested on three sets of 225 images which contain 871 faces. Experimental results on the first two datasets show that our methods perform as well as the best methods in the literature, yet have fewer false detects."
            },
            "slug": "Face-detection-using-mixtures-of-linear-subspaces-Yang-Ahuja",
            "title": {
                "fragments": [],
                "text": "Face detection using mixtures of linear subspaces"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "Two methods using mixtures of linear sub-spaces for face detection in gray level images using Kohonen's self-organizing map for clustering and Fisher linear discriminant to find the optimal projection for pattern classification are presented."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145266088"
                        ],
                        "name": "T. Leung",
                        "slug": "T.-Leung",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Leung",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Leung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2511655"
                        ],
                        "name": "M. Burl",
                        "slug": "M.-Burl",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Burl",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Burl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 70
                            }
                        ],
                        "text": "Combinations of such areas are then evaluated with classifiers, to determine whether and where a face is present."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2904067,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0d50d0e2af0b45cc7ed25fe4aa97af900c9bd32a",
            "isKey": false,
            "numCitedBy": 389,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "An algorithm for locating quasi-frontal views of human faces in cluttered scenes is presented. The algorithm works by coupling a set of local feature detectors with a statistical model of the mutual distances between facial features it is invariant with respect to translation, rotation (in the plane), and scale and can handle partial occlusions of the face. On a challenging database with complicated and varied backgrounds, the algorithm achieved a correct localization rate of 95% in images where the face appeared quasi-frontally.<<ETX>>"
            },
            "slug": "Finding-faces-in-cluttered-scenes-using-random-Leung-Burl",
            "title": {
                "fragments": [],
                "text": "Finding faces in cluttered scenes using random labeled graph matching"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "An algorithm for locating quasi-frontal views of human faces in cluttered scenes is presented and it is found that it is invariant with respect to translation, rotation, and scale and can handle partial occlusions of the face."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE International Conference on Computer Vision"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144994682"
                        ],
                        "name": "A. Pentland",
                        "slug": "A.-Pentland",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Pentland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pentland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780935"
                        ],
                        "name": "B. Moghaddam",
                        "slug": "B.-Moghaddam",
                        "structuredName": {
                            "firstName": "Baback",
                            "lastName": "Moghaddam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Moghaddam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738894"
                        ],
                        "name": "T. Starner",
                        "slug": "T.-Starner",
                        "structuredName": {
                            "firstName": "Thad",
                            "lastName": "Starner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Starner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 136280,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b0bf5d558220d39698ce96d59ee5772e8e1a0663",
            "isKey": false,
            "numCitedBy": 2234,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe experiments with eigenfaces for recognition and interactive search in a large-scale face database. Accurate visual recognition is demonstrated using a database of O(10/sup 3/) faces. The problem of recognition under general viewing orientation is also examined. A view-based multiple-observer eigenspace technique is proposed for use in face recognition under variable pose. In addition, a modular eigenspace description technique is used which incorporates salient features such as the eyes, nose and mouth, in an eigenfeature layer. This modular representation yields higher recognition rates as well as a more robust framework for face recognition. An automatic feature extraction technique using feature eigentemplates is also demonstrated.<<ETX>>"
            },
            "slug": "View-based-and-modular-eigenspaces-for-face-Pentland-Moghaddam",
            "title": {
                "fragments": [],
                "text": "View-based and modular eigenspaces for face recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A modular eigenspace description technique is used which incorporates salient features such as the eyes, nose and mouth, in an eigenfeature layer, which yields higher recognition rates as well as a more robust framework for face recognition."
            },
            "venue": {
                "fragments": [],
                "text": "1994 Proceedings of IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1775043"
                        ],
                        "name": "H. Graf",
                        "slug": "H.-Graf",
                        "structuredName": {
                            "firstName": "Hans",
                            "lastName": "Graf",
                            "middleNames": [
                                "Peter"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Graf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3165487"
                        ],
                        "name": "E. Cosatto",
                        "slug": "E.-Cosatto",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Cosatto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Cosatto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2387879"
                        ],
                        "name": "D. Gibbon",
                        "slug": "D.-Gibbon",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Gibbon",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Gibbon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2535899"
                        ],
                        "name": "M. Kocheisen",
                        "slug": "M.-Kocheisen",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Kocheisen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kocheisen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2920582"
                        ],
                        "name": "E. Petajan",
                        "slug": "E.-Petajan",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Petajan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Petajan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1249176,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ff3faa698efae2c58ee1782654b2c4769dbb11be",
            "isKey": false,
            "numCitedBy": 152,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "We designed a modular system using a combination of shape analysis, color segmentation and motion information for locating reliably heads and faces of different sizes and orientations in complex images. The first of the system's three channels does a shape analysis on gray-level images to determine the location of individual facial features as well as the outlines of heads. In the second channel the color space is analyzed with a clustering algorithm to find areas of skin colors. The color space is first calibrated, using the results from the other channels. In the third channel motion information is extracted from frame differences. Head outlines are determined by analyzing the shapes of areas with large motion vectors. All three channels produce lists of shapes, each marking an area of the image where a facial feature or a part of the outline of a head may be present. Combinations of such shapes are evaluated with n-gram searches to produce a list of likely head positions and the locations of facial features. We tested the system for tracking faces of people sitting in front of terminals and video phones and used it to track people entering through a doorway."
            },
            "slug": "Multi-modal-system-for-locating-heads-and-faces-Graf-Cosatto",
            "title": {
                "fragments": [],
                "text": "Multi-modal system for locating heads and faces"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "A modular system using a combination of shape analysis, color segmentation and motion information for locating reliably heads and faces of different sizes and orientations in complex images for tracking faces of people sitting in front of terminals and video phones is designed."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Second International Conference on Automatic Face and Gesture Recognition"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2085375589"
                        ],
                        "name": "H. Schneiderman",
                        "slug": "H.-Schneiderman",
                        "structuredName": {
                            "firstName": "Henry",
                            "lastName": "Schneiderman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Schneiderman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733113"
                        ],
                        "name": "T. Kanade",
                        "slug": "T.-Kanade",
                        "structuredName": {
                            "firstName": "Takeo",
                            "lastName": "Kanade",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanade"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1060186,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bbe488bb190d75f4b665d43e306bcab1ab228890",
            "isKey": false,
            "numCitedBy": 459,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we describe an algorithm for object recognition that explicitly models and estimated the posterior probability function, P(object/image). We have chosen a functional form of the posterior probability function that captures the joint statistics of local appearance and position on the object as well as the statistics of local appearance in the visual world at large. We use a discrete representation of local appearance consisting of approximately 10/sup 6/ patterns. We compute an estimate of P(object/image) in closed form by counting the frequency of occurrence of these patterns over various sets of training images. We have used this method for detecting human faces from frontal and profile views. The algorithm for frontal views has shown a detection rate of 93.0% with 88 false alarms on a set of 125 images containing 483 faces combining the MIT test set of Sung and Poggio with the CMU test sets of Rowley, Baluja, and Kanade. The algorithm for detection of profile views has also demonstrated promising results."
            },
            "slug": "Probabilistic-modeling-of-local-appearance-and-for-Schneiderman-Kanade",
            "title": {
                "fragments": [],
                "text": "Probabilistic modeling of local appearance and spatial relationships for object recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "An algorithm for object recognition that explicitly models and estimated the posterior probability function, P(object/image) in closed form is described, which captures the joint statistics of local appearance and position on the object as well as the statistics ofLocal appearance in the visual world at large."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 1998 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.98CB36231)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768120"
                        ],
                        "name": "T. Jebara",
                        "slug": "T.-Jebara",
                        "structuredName": {
                            "firstName": "Tony",
                            "lastName": "Jebara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jebara"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2707321"
                        ],
                        "name": "Kenneth B. Russell",
                        "slug": "Kenneth-B.-Russell",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Russell",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenneth B. Russell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144994682"
                        ],
                        "name": "A. Pentland",
                        "slug": "A.-Pentland",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Pentland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pentland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 84
                            }
                        ],
                        "text": "Several color spaces have been utilized to label pixels as skin including RGB [66], [67], [137], normalized RGB [102], [29], [149], [172], [30], [105], [171], [77], [151], [120], HSV (or HSI) [138], [79], [147], [146], YCrCb [167], [17], YIQ [31], [32], YES [131], CIE XYZ [19], and CIE LUV [173]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2112235,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "86fd73a94640da250e7d456482f0690095707eb3",
            "isKey": false,
            "numCitedBy": 74,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a face modeling system which estimates complete facial structure and texture from a real-time video stream. The system begins with a face trading algorithm which detects and stabilizes live facial images into a canonical 3D pose. The resulting canonical texture is then processed by a statistical model to filter imperfections and estimate unknown components such as missing pixels and underlying 3D structure. This statistical model is a soft mixture of eigenfeature selectors which span the 3D deformations and texture changes across a training set of laser scanned faces. An iterative algorithm is introduced for determining the dimensional partitioning of the eigenfeatures to maximize their generalization capability over a cross-validation set of data. The model's abilities to filter and estimate absent facial components are then demonstrated over incomplete 3D data. This ultimately allows the model to span known and regress unknown facial information front stabilized natural video sequences generated by a face tracking algorithm. The resulting continuous and dynamic estimation of the model's parameters over a video sequence generates a compact temporal description of the 3D deformations and texture changes of the face."
            },
            "slug": "Mixtures-of-eigenfeatures-for-real-time-structure-Jebara-Russell",
            "title": {
                "fragments": [],
                "text": "Mixtures of eigenfeatures for real-time structure from texture"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "A face modeling system which estimates complete facial structure and texture from a real-time video stream and allows the model to span known and regress unknown facial information front stabilized natural video sequences generated by a face tracking algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "Sixth International Conference on Computer Vision (IEEE Cat. No.98CH36271)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783224"
                        ],
                        "name": "A. Colmenarez",
                        "slug": "A.-Colmenarez",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Colmenarez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Colmenarez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153652752"
                        ],
                        "name": "Thomas S. Huang",
                        "slug": "Thomas-S.-Huang",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Huang",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas S. Huang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 9192390,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e744c3eef4fbc4ac52b2458eb2d545a4432bcb86",
            "isKey": false,
            "numCitedBy": 196,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present a visual learning technique that maximizes the discrimination between positive and negative examples in a training set. We demonstrate our technique in the context of face detection with complex background without color or motion information, which has proven to be a challenging problem. We use a family of discrete Markov processes to model the face and background patterns and estimate the probability models using the data statistics. Then, we convert the learning process into an optimization, selecting the Markov process that optimizes the information-based discrimination between the two classes. The detection process is carried out by computing the likelihood ratio using the probability model obtained from the learning procedure. We show that because of the discrete nature of these models, the detection process is at least two orders of magnitude less computationally expensive than neural network approaches. However, no improvement in terms of correct-answer/false-alarm tradeoff is achieved."
            },
            "slug": "Face-detection-with-information-based-maximum-Colmenarez-Huang",
            "title": {
                "fragments": [],
                "text": "Face detection with information-based maximum discrimination"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "A visual learning technique that maximizes the discrimination between positive and negative examples in a training set by using a family of discrete Markov processes to model the face and background patterns and estimate the probability models using the data statistics."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1830709"
                        ],
                        "name": "A. Lanitis",
                        "slug": "A.-Lanitis",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Lanitis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Lanitis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144482985"
                        ],
                        "name": "C. Taylor",
                        "slug": "C.-Taylor",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Taylor",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7205190"
                        ],
                        "name": "Tim Cootes",
                        "slug": "Tim-Cootes",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Cootes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tim Cootes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 122941737,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7b65b06ecdd9df916d8f688e73ac41a2bb0fd63f",
            "isKey": false,
            "numCitedBy": 253,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Automatic-face-identification-system-using-flexible-Lanitis-Taylor",
            "title": {
                "fragments": [],
                "text": "Automatic face identification system using flexible appearance models"
            },
            "venue": {
                "fragments": [],
                "text": "Image Vis. Comput."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3042261"
                        ],
                        "name": "M. Augusteijn",
                        "slug": "M.-Augusteijn",
                        "structuredName": {
                            "firstName": "Marijke",
                            "lastName": "Augusteijn",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Augusteijn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "71223939"
                        ],
                        "name": "T. L. Skufca",
                        "slug": "T.-L.-Skufca",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Skufca",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. L. Skufca"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 125
                            }
                        ],
                        "text": "Augusteijn and Skufca developed a method that infers the presence of a face through the identification of face-like textures [6]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 61487400,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "24860a5f4fd20ff8c9f988080c9b869f78d73e83",
            "isKey": false,
            "numCitedBy": 62,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "A method is presented to infer the presence of a human face in an image through the identification of face-like textures. The selected textures are those of human hair and skin. The second-order statistics method is used for texture representation. This method employs a set of co-occurrence matrices, from which features can be calculated that can characterize a texture. The cascade-correlation neural network architecture is used for supervised classification of textures. The Kohonen self-organizing feature map shows the clustering of the different texture types. Classification performance is generally above 80%, which is sufficient to clearly outline a face in an image.<<ETX>>"
            },
            "slug": "Identification-of-human-faces-through-texture-based-Augusteijn-Skufca",
            "title": {
                "fragments": [],
                "text": "Identification of human faces through texture-based feature recognition and neural network technology"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "A method is presented to infer the presence of a human face in an image through the identification of face-like textures, using the second-order statistics method and the cascade-correlation neural network architecture."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE International Conference on Neural Networks"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2080791815"
                        ],
                        "name": "C. Burly",
                        "slug": "C.-Burly",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Burly",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burly"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2087868060"
                        ],
                        "name": "T. K. Leungz",
                        "slug": "T.-K.-Leungz",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Leungz",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. K. Leungz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 21
                            }
                        ],
                        "text": "One advantage of these methods is that partially occluded faces can be located."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1226614,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2052cab4162051486813b7ed0c2c5f6f4c0667ee",
            "isKey": false,
            "numCitedBy": 79,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, a face localization system is proposed in which local detectors are coupled with a statistical model of the spatial arrangement of facial features to yield robust performance. The outputs from the local detectors are treated as candidate locations and constellations are formed from these. The eeects of translation, rotation, and scale are eliminated by mapping to a set of shape variables. The constellations are then ranked according to the likelihood that the shape variables correspond to a face versus an alternative model. Incomplete constellations , which occur when some of the true features are missed, are handled in a principled way."
            },
            "slug": "Face-Localization-via-Shape-Statistics-Burly-Leungz",
            "title": {
                "fragments": [],
                "text": "Face Localization via Shape Statistics"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "A face localization system is proposed in which local detectors are coupled with a statistical model of the spatial arrangement of facial features to yield robust performance and incomplete constellations are handled in a principled way."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2275441"
                        ],
                        "name": "K. Yow",
                        "slug": "K.-Yow",
                        "structuredName": {
                            "firstName": "Kin",
                            "lastName": "Yow",
                            "middleNames": [
                                "Choong"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Yow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745672"
                        ],
                        "name": "R. Cipolla",
                        "slug": "R.-Cipolla",
                        "structuredName": {
                            "firstName": "Roberto",
                            "lastName": "Cipolla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Cipolla"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13890870,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a62e9acb12d9ab6225a8cf70f68b8de0384becad",
            "isKey": false,
            "numCitedBy": 77,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Present approaches to human face detection have made several assumptions that restrict their ability to be extended to general imaging conditions. We identify that the key factor in a generic and robust system is that of exploiting a large amount of evidence, related and reinforced by model knowledge through a probabilistic framework. In this paper, we propose a face detection framework that groups image features into meaningful entities-using perceptual organization, assigns probabilities to each of them, and reinforce there probabilities using Bayesian reasoning techniques. True hypotheses of faces will be reinforced to a high probability. The detection of faces under scale, orientation and viewpoint variations will be examined in a subsequent paper."
            },
            "slug": "A-probabilistic-framework-for-perceptual-grouping-Yow-Cipolla",
            "title": {
                "fragments": [],
                "text": "A probabilistic framework for perceptual grouping of features for human face detection"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A face detection framework that groups image features into meaningful entities-using perceptual organization, assigns probabilities to each of them, and reinforce there probabilities using Bayesian reasoning techniques is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Second International Conference on Automatic Face and Gesture Recognition"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145235303"
                        ],
                        "name": "Jun Miao",
                        "slug": "Jun-Miao",
                        "structuredName": {
                            "firstName": "Jun",
                            "lastName": "Miao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jun Miao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714354"
                        ],
                        "name": "Baocai Yin",
                        "slug": "Baocai-Yin",
                        "structuredName": {
                            "firstName": "Baocai",
                            "lastName": "Yin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Baocai Yin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2641821"
                        ],
                        "name": "Kongqiao Wang",
                        "slug": "Kongqiao-Wang",
                        "structuredName": {
                            "firstName": "Kongqiao",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kongqiao Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1808618"
                        ],
                        "name": "Lansun Shen",
                        "slug": "Lansun-Shen",
                        "structuredName": {
                            "firstName": "Lansun",
                            "lastName": "Shen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lansun Shen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46772962"
                        ],
                        "name": "Xuecun Chen",
                        "slug": "Xuecun-Chen",
                        "structuredName": {
                            "firstName": "Xuecun",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xuecun Chen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6464488,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4db205eb15a72c177438275746eebeaabf7e6b1a",
            "isKey": false,
            "numCitedBy": 135,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-hierarchical-multiscale-and-multiangle-system-for-Miao-Yin",
            "title": {
                "fragments": [],
                "text": "A hierarchical multiscale and multiangle system for human face detection in a complex background using gravity-center template"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39682833"
                        ],
                        "name": "H. Rowley",
                        "slug": "H.-Rowley",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Rowley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Rowley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1767244"
                        ],
                        "name": "S. Baluja",
                        "slug": "S.-Baluja",
                        "structuredName": {
                            "firstName": "Shumeet",
                            "lastName": "Baluja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Baluja"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733113"
                        ],
                        "name": "T. Kanade",
                        "slug": "T.-Kanade",
                        "structuredName": {
                            "firstName": "Takeo",
                            "lastName": "Kanade",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanade"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 52
                            }
                        ],
                        "text": "First, morphological operations such as closing, clipped difference, and thresh-\nolding are applied to extract pixels at which the intensity values change significantly."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 40120983,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3d76ef8e61395a6e9c32627f1f108772d084e2e9",
            "isKey": false,
            "numCitedBy": 4156,
            "numCiting": 134,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a neural network-based face detection system. A retinally connected neural network examines small windows of an image and decides whether each window contains a face. The system arbitrates between multiple networks to improve performance over a single network. We use a bootstrap algorithm for training the networks, which adds false detections into the training set as training progresses. This eliminates the difficult task of manually selecting non-face training examples, which must be chosen to span the entire space of non-face images. Comparisons with other state-of-the-art face detection systems are presented; our system has better performance in terms of detection and false-positive rates."
            },
            "slug": "Neural-network-based-face-detection-Rowley-Baluja",
            "title": {
                "fragments": [],
                "text": "Neural Network-Based Face Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "A neural network-based face detection system that arbitrates between multiple networks to improve performance over a single network using a bootstrap algorithm, which eliminates the difficult task of manually selecting non-face training examples."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2334021"
                        ],
                        "name": "D. M. Saxe",
                        "slug": "D.-M.-Saxe",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Saxe",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. M. Saxe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3159744"
                        ],
                        "name": "R. Foulds",
                        "slug": "R.-Foulds",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Foulds",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Foulds"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 192
                            }
                        ],
                        "text": "Several color spaces have been utilized to label pixels as skin including RGB [66], [67], [137], normalized RGB [102], [29], [149], [172], [30], [105], [171], [77], [151], [120], HSV (or HSI) [138], [79], [147], [146], YCrCb [167], [17], YIQ [31], [32], YES [131], CIE XYZ [19], and CIE LUV [173]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 21800551,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fe77870698fbb65819feee822a533825f205b551",
            "isKey": false,
            "numCitedBy": 152,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "There are many applications where it is desirable to segment a video image into regions defined by color. Among these are the recognition of gesture from the image (as opposed to instrumented gloves), facial expression and orientation, and video teleconferencing. In these examples, the important elements of the images are human hands and face, which share common skin coloration of the subject. This paper describes an approach to the identification of skin-colored regions of the image that is robust in terms of variations in skin pigmentation in a single subject, differences in skin pigmentation across a population of potential users, and subject clothing and image background. The paper also discusses the potential for being robust over a wide range of illuminating conditions."
            },
            "slug": "Toward-robust-skin-identification-in-video-images-Saxe-Foulds",
            "title": {
                "fragments": [],
                "text": "Toward robust skin identification in video images"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "An approach to the identification of skin-colored regions of the image that is robust in terms of variations in skin pigmentation in a single subject, differences in skin Pigmentation across a population of potential users, and subject clothing and image background is described."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Second International Conference on Automatic Face and Gesture Recognition"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145030811"
                        ],
                        "name": "C. Papageorgiou",
                        "slug": "C.-Papageorgiou",
                        "structuredName": {
                            "firstName": "Constantine",
                            "lastName": "Papageorgiou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Papageorgiou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145328018"
                        ],
                        "name": "M. Oren",
                        "slug": "M.-Oren",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Oren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Oren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6476085,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "76f560991d56ad689ec32f9e9d13291e0193f4cf",
            "isKey": false,
            "numCitedBy": 1604,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a general trainable framework for object detection in static images of cluttered scenes. The detection technique we develop is based on a wavelet representation of an object class derived from a statistical analysis of the class instances. By learning an object class in terms of a subset of an overcomplete dictionary of wavelet basis functions, we derive a compact representation of an object class which is used as an input to a support vector machine classifier. This representation overcomes both the problem of in-class variability and provides a low false detection rate in unconstrained environments. We demonstrate the capabilities of the technique in two domains whose inherent information content differs significantly. The first system is face detection and the second is the domain of people which, in contrast to faces, vary greatly in color, texture, and patterns. Unlike previous approaches, this system learns from examples and does not rely on any a priori (hand-crafted) models or motion-based segmentation. The paper also presents a motion-based extension to enhance the performance of the detection algorithm over video sequences. The results presented here suggest that this architecture may well be quite general."
            },
            "slug": "A-general-framework-for-object-detection-Papageorgiou-Oren",
            "title": {
                "fragments": [],
                "text": "A general framework for object detection"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A general trainable framework for object detection in static images of cluttered scenes based on a wavelet representation of an object class derived from a statistical analysis of the class instances and a motion-based extension to enhance the performance of the detection algorithm over video sequences is presented."
            },
            "venue": {
                "fragments": [],
                "text": "Sixth International Conference on Computer Vision (IEEE Cat. No.98CH36271)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1887281"
                        ],
                        "name": "Hualu Wang",
                        "slug": "Hualu-Wang",
                        "structuredName": {
                            "firstName": "Hualu",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hualu Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9546964"
                        ],
                        "name": "Shih-Fu Chang",
                        "slug": "Shih-Fu-Chang",
                        "structuredName": {
                            "firstName": "Shih-Fu",
                            "lastName": "Chang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shih-Fu Chang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 230,
                                "start": 225
                            }
                        ],
                        "text": "Several color spaces have been utilized to label pixels as skin including RGB [66], [67], [137], normalized RGB [102], [29], [149], [172], [30], [105], [171], [77], [151], [120], HSV (or HSI) [138], [79], [147], [146], YCrCb [167], [17], YIQ [31], [32], YES [131], CIE XYZ [19], and CIE LUV [173]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 521920,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "14e49e2807db93b2c5d524876436516e55169a70",
            "isKey": false,
            "numCitedBy": 346,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Human faces provide a useful cue in indexing video content. We present a highly efficient system that can rapidly detect human face regions in MPEG video sequences. The underlying algorithm takes the inverse quantized discrete cosine transform (DCT) coefficients of MPEG video as the input, and outputs the locations of the detected face regions. The algorithm consists of three stages, where chrominance, shape, and frequency information are used, respectively. By detecting faces directly in the compressed domain, there is no need to carry out the inverse DCT transform, so that the algorithm can run faster than the real time. In our experiments, the algorithm detected 85-92% of the faces in three test sets, including both intraframe and interframe coded image frames from news video. The average run time ranges from 13-33 ms per frame. The algorithm can be applied to JPEG unconstrained images or motion JPEG video as well."
            },
            "slug": "A-highly-efficient-system-for-automatic-face-region-Wang-Chang",
            "title": {
                "fragments": [],
                "text": "A highly efficient system for automatic face region detection in MPEG video"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "A highly efficient system that can rapidly detect human face regions in MPEG video sequences by detecting faces directly in the compressed domain, and there is no need to carry out the inverse DCT transform, so that the algorithm can run faster than the real time."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Circuits Syst. Video Technol."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38817267"
                        ],
                        "name": "K. Sung",
                        "slug": "K.-Sung",
                        "structuredName": {
                            "firstName": "Kah",
                            "lastName": "Sung",
                            "middleNames": [
                                "Kay"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Sung"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 17583351,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f3317b98195fe0be4acf7b450f015c1abca13ab9",
            "isKey": false,
            "numCitedBy": 248,
            "numCiting": 99,
            "paperAbstract": {
                "fragments": [],
                "text": "Object and pattern detection is a classical computer vision problem with many potential applications, ranging from automatic target recognition to image-based industrial inspection tasks in assembly lines. While there have been some successful object and pattern detection systems in the past, most such systems handle only specific rigid objects or patterns that can be accurately described by fixed geometric models or pictorial templates. \nThis thesis presents a learning based approach for detecting classes of objects and patterns with variable image appearance but highly predictable image boundaries. Some examples of such object and pattern classes include human faces, aerial views of structured terrain features like volcanoes, localized material defect signatures in industrial parts, certain tissue anomalies in medical images, and instances of a given digit or character, which may be written or printed in many different styles. \nThe thesis consists of two parts. In part one, we introduce our object and pattern detection approach using a concrete human face detection example. The approach first builds a distribution-based model of the target pattern class in an appropriate feature space to describe the target's variable image appearance. It then learns from examples a similarity measure for matching new patterns against the distribution-based target model. We also discuss some pertinent learning issues, including ideas on virtual example generation and example selection. The approach makes few assumptions about the target pattern class and should therefore be fairly general, as long as the target class has predictable image boundaries. We show that this is indeed the case by demonstrating the technique on two other pattern detection/recognition problems. \nBecause our object and pattern detection approach is very much learning-based, how well a system eventually performs depends heavily on the quality of training examples it receives. The second part of this thesis looks at how one can select high quality examples for function approximation learning tasks. Active learning is an area of research that investigates how a learner can intelligently select future training examples to get better approximation results with less data. We propose an active learning formulation for function approximation, and show for three specific approximation function classes, that the active example selection strategy learns its target with fewer data samples than random sampling. Finally, we simplify the original active learning formulation, and show how it leads to a tractable example selection paradigm, suitable for use in many object and pattern detection problems. (Copies available exclusively from MIT Libraries, Rm. 14-0551, Cambridge, MA 02139-4307. Ph. 617-253-5668; Fax 617-253-1690.)"
            },
            "slug": "Learning-and-example-selection-for-object-and-Sung",
            "title": {
                "fragments": [],
                "text": "Learning and example selection for object and pattern detection"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This thesis presents a learning based approach for detecting classes of objects and patterns with variable image appearance but highly predictable image boundaries, and proposes an active learning formulation for function approximation, and shows that the active example selection strategy learns its target with fewer data samples than random sampling."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2283680"
                        ],
                        "name": "P. Juell",
                        "slug": "P.-Juell",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Juell",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Juell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145127047"
                        ],
                        "name": "R. Marsh",
                        "slug": "R.-Marsh",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Marsh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Marsh"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 43374055,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "64b0c20b2737f71b995750b00578ae79e08724bf",
            "isKey": false,
            "numCitedBy": 113,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-hierarchical-neural-network-for-human-face-Juell-Marsh",
            "title": {
                "fragments": [],
                "text": "A hierarchical neural network for human face detection"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715634"
                        ],
                        "name": "Ming-Hsuan Yang",
                        "slug": "Ming-Hsuan-Yang",
                        "structuredName": {
                            "firstName": "Ming-Hsuan",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming-Hsuan Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144590225"
                        ],
                        "name": "D. Roth",
                        "slug": "D.-Roth",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Roth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Roth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145237406"
                        ],
                        "name": "N. Ahuja",
                        "slug": "N.-Ahuja",
                        "structuredName": {
                            "firstName": "Narendra",
                            "lastName": "Ahuja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Ahuja"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "proposed a method that uses SNoW learning architecture [125], [16] to detect faces with different features and expressions, in different poses, and under different lighting conditions [176]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1709452,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "23234a0f211a44d9706b2570d474427b8f899ec1",
            "isKey": false,
            "numCitedBy": 354,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "A novel learning approach for human face detection using a network of linear units is presented. The SNoW learning architecture is a sparse network of linear functions over a pre-defined or incrementally learned feature space and is specifically tailored for learning in the presence of a very large number of features. A wide range of face images in different poses, with different expressions and under different lighting conditions are used as a training set to capture the variations of human faces. Experimental results on commonly used benchmark data sets of a wide range of face images show that the SNoW-based approach outperforms methods that use neural networks, Bayesian methods, support vector machines and others. Furthermore, learning and evaluation using the SNoW-based method are significantly more efficient than with other methods."
            },
            "slug": "A-SNoW-Based-Face-Detector-Yang-Roth",
            "title": {
                "fragments": [],
                "text": "A SNoW-Based Face Detector"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Experimental results on commonly used benchmark data sets of a wide range of face images show that the SNoW-based approach outperforms methods that use neural networks, Bayesian methods, support vector machines and others."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118579343"
                        ],
                        "name": "Jie Yang",
                        "slug": "Jie-Yang",
                        "structuredName": {
                            "firstName": "Jie",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jie Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742325"
                        ],
                        "name": "R. Stiefelhagen",
                        "slug": "R.-Stiefelhagen",
                        "structuredName": {
                            "firstName": "Rainer",
                            "lastName": "Stiefelhagen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Stiefelhagen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145352356"
                        ],
                        "name": "U. Meier",
                        "slug": "U.-Meier",
                        "structuredName": {
                            "firstName": "Uwe",
                            "lastName": "Meier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Meier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724972"
                        ],
                        "name": "A. Waibel",
                        "slug": "A.-Waibel",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Waibel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Waibel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 152
                            }
                        ],
                        "text": "Several color spaces have been utilized to label pixels as skin including RGB [66], [67], [137], normalized RGB [102], [29], [149], [172], [30], [105], [171], [77], [151], [120], HSV (or HSI) [138], [79], [147], [146], YCrCb [167], [17], YIQ [31], [32], YES [131], CIE XYZ [19], and CIE LUV [173]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 526599,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "15748d1ced740637ce4e46ddfe7871399214db1f",
            "isKey": false,
            "numCitedBy": 80,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present visual tracking techniques for multimodal human computer interaction. First, we discuss techniques for tracking human faces in which human skin-color is used as a major feature. An adaptive stochastic model has been developed to characterize the skin-color distributions. Based on the maximum likelihood method, the model parameters can be adapted for different people and different lighting conditions. The feasibility of the model has been demonstrated by the development of a real-time face tracker. The system has achieved a rate of 30-t- frames/second using a low-end workstation with a framegrabber and a camera. We also present a top-down approach for tracking facial features such as eyes, nostrils, and lip comers. These real-time visual tracking techniques have been successfully applied to many applications such as gaze tracking, and lipreading. The face tracker has been combined with a microphone array for extracting speech signal from a specific person. The gaze tracker has been combined with a speech recognizer in a multimodal interface for controlling a panoramic image viewer."
            },
            "slug": "Visual-tracking-for-multimodal-human-computer-Yang-Stiefelhagen",
            "title": {
                "fragments": [],
                "text": "Visual tracking for multimodal human computer interaction"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "An adaptive stochastic model has been developed to characterize the skin-color distributions and these real-time visual tracking techniques have been successfully applied to many applications such as gaze tracking, and lipreading."
            },
            "venue": {
                "fragments": [],
                "text": "CHI"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34948290"
                        ],
                        "name": "B. Tak\u00e1cs",
                        "slug": "B.-Tak\u00e1cs",
                        "structuredName": {
                            "firstName": "Barnab\u00e1s",
                            "lastName": "Tak\u00e1cs",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Tak\u00e1cs"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143979395"
                        ],
                        "name": "H. Wechsler",
                        "slug": "H.-Wechsler",
                        "structuredName": {
                            "firstName": "Harry",
                            "lastName": "Wechsler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Wechsler"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 111
                            }
                        ],
                        "text": "The overall detection rate on a test set of 110 images of faces with different scales, orientations, and viewpoints is 85 percent [179]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14078099,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3353e2f67b1d03de770c4f115b2fa4e21243f1a6",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a novel dynamic vision scheme and its application for locating face images. The approach is based on a biologically motivated feature extraction model involving retinal sampling along M-type lattices and small oscillatory eye movements. The bounding box of the face is then found based on the conspicuity surface emerging while scanning the input and deriving the features. Simulation results on over 400 images showed an error rate of less than 5 % and demonstrated the applicability of our approach."
            },
            "slug": "Face-Location-Using-A-Dynamic-Model-of-Retinal-Tak\u00e1cs-Wechsler",
            "title": {
                "fragments": [],
                "text": "Face Location Using A Dynamic Model of Retinal Feature Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A novel dynamic vision scheme based on a biologically motivated feature extraction model involving retinal sampling along M-type lattices and small oscillatory eye movements is described and its application for locating face images is demonstrated."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145328018"
                        ],
                        "name": "M. Oren",
                        "slug": "M.-Oren",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Oren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Oren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145030811"
                        ],
                        "name": "C. Papageorgiou",
                        "slug": "C.-Papageorgiou",
                        "structuredName": {
                            "firstName": "Constantine",
                            "lastName": "Papageorgiou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Papageorgiou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46597039"
                        ],
                        "name": "P. Sinha",
                        "slug": "P.-Sinha",
                        "structuredName": {
                            "firstName": "Pawan",
                            "lastName": "Sinha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Sinha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781874"
                        ],
                        "name": "E. Osuna",
                        "slug": "E.-Osuna",
                        "structuredName": {
                            "firstName": "Edgar",
                            "lastName": "Osuna",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Osuna"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 79
                            }
                        ],
                        "text": "SVMs have also been used to detect faces and pedestrians in the wavelet domain [106], [108], [109]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7967646,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bd0534a87e09b3d64b7e7462e2684c60c9aca1f5",
            "isKey": false,
            "numCitedBy": 837,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a trainable object detection architecture that is applied to detecting people in static images of cluttered scenes. This problem poses several challenges. People are highly non-rigid objects with a high degree of variability in size, shape, color, and texture. Unlike previous approaches, this system learns from examples and does not rely on any a priori (hand-crafted) models or on motion. The detection technique is based on the novel idea of the wavelet template that defines the shape of an object in terms of a subset of the wavelet coefficients of the image. It is invariant to changes in color and texture and can be used to robustly define a rich and complex class of objects such as people. We show how the invariant properties and computational efficiency of the wavelet template make it an effective tool for object detection."
            },
            "slug": "Pedestrian-detection-using-wavelet-templates-Oren-Papageorgiou",
            "title": {
                "fragments": [],
                "text": "Pedestrian detection using wavelet templates"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "This paper presents a trainable object detection architecture that is applied to detecting people in static images of cluttered scenes and shows how the invariant properties and computational efficiency of the wavelet template make it an effective tool for object detection."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48926774"
                        ],
                        "name": "Ying Dai",
                        "slug": "Ying-Dai",
                        "structuredName": {
                            "firstName": "Ying",
                            "lastName": "Dai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ying Dai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737939"
                        ],
                        "name": "Y. Nakano",
                        "slug": "Y.-Nakano",
                        "structuredName": {
                            "firstName": "Yasuaki",
                            "lastName": "Nakano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Nakano"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 246,
                                "start": 242
                            }
                        ],
                        "text": "Several color spaces have been utilized to label pixels as skin including RGB [66], [67], [137], normalized RGB [102], [29], [149], [172], [30], [105], [171], [77], [151], [120], HSV (or HSI) [138], [79], [147], [146], YCrCb [167], [17], YIQ [31], [32], YES [131], CIE XYZ [19], and CIE LUV [173]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 56766771,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f277e2bf0ed8117740a8399da8055a209ab31061",
            "isKey": false,
            "numCitedBy": 46,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "It is the key step for face recognition systems to extract the facial parts from the complex backgrounds. In this paper, we propose a new method for the face extraction in the color complex backgrounds. By transforming color images from RGB color represention to YIQ color one, the orange-like parts including the face areas are enhanced in the original images, if the I-componet of YIQ color system is only used. The facial texture model based on the space gray level dependence (SGLD) matrices is applied to these images. Using this model, facial parts are detected as those regions which satisfy a set of inequalities. The weight coefficients in the inequalities are decided by the conventional method of learning. Using this textural model, we design a kind of scanning scheme for face detection in the complex backgrounds. The experiments show that this method could locate the face position in the complex backgrounds effectively. \u30ad\u30fc \u30ef\u30fc \u30c9:\u9854 \u753b\u50cf \u691c \u51fa\u3001\u5171 \u8d77\u884c\u5217"
            },
            "slug": "Extraction-of-Facial-Images-from-the-Complex-Using-Dai-Nakano",
            "title": {
                "fragments": [],
                "text": "Extraction of Facial Images from the Complex Background Using Color Information and SGLD Matrices"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "A new method for the face extraction in the color complex backgrounds by transforming color images from RGB color represention to YIQ color one, the orange-like parts including the face areas are enhanced in the original images, if the I-componet of Y IQ color system is only used."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145081362"
                        ],
                        "name": "A. Yuille",
                        "slug": "A.-Yuille",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Yuille",
                            "middleNames": [
                                "Loddon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yuille"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36427051"
                        ],
                        "name": "Peter W. Hallinan",
                        "slug": "Peter-W.-Hallinan",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Hallinan",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter W. Hallinan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115017312"
                        ],
                        "name": "David S. Cohen",
                        "slug": "David-S.-Cohen",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Cohen",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David S. Cohen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 8700837,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "52956a66b0d6951906f8bfd76574327ef90b672b",
            "isKey": false,
            "numCitedBy": 1500,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a method for detecting and describing features of faces using deformable templates. The feature of interest, an eye for example, is described by a parameterized template. An energy function is defined which links edges, peaks, and valleys in the image intensity to corresponding properties of the template. The template then interacts dynamically with the image by altering its parameter values to minimize the energy function, thereby deforming itself to find the best fit. The final parametr values can be used as descriptors for the feature. We illustrate this method by showing deformable templates detecting eyes and mouths in real images. We demonstrate their ability for tracking features."
            },
            "slug": "Feature-extraction-from-faces-using-deformable-Yuille-Hallinan",
            "title": {
                "fragments": [],
                "text": "Feature extraction from faces using deformable templates"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "De deformable templates are illustrated by showing their ability for tracking features and altering parameter values to minimize the energy function, thereby deforming itself to find the best fit."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings CVPR '89: IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2510362"
                        ],
                        "name": "K. Hotta",
                        "slug": "K.-Hotta",
                        "structuredName": {
                            "firstName": "Kazuhiro",
                            "lastName": "Hotta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Hotta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145375983"
                        ],
                        "name": "T. Kurita",
                        "slug": "T.-Kurita",
                        "structuredName": {
                            "firstName": "Takio",
                            "lastName": "Kurita",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kurita"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35797142"
                        ],
                        "name": "T. Mishima",
                        "slug": "T.-Mishima",
                        "structuredName": {
                            "firstName": "Taketoshi",
                            "lastName": "Mishima",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Mishima"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 12233931,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "680ca8763560e93db88c9b8e74780ec2ccd78c0c",
            "isKey": false,
            "numCitedBy": 77,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a scale invariant face detection method which combines higher-order local autocorrelation (HLAC) features extracted from a log-polar transformed image with linear discriminant analysis for \"face\" and \"not face\" classification. Since HLAC features of log-polar images are sensitive to shifts of a face, we utilize this property and develop a face detection method. HLAC features extracted from a log-polar image become scale and rotation invariant because scalings and rotations of a face are expressed as shifts in a log-polar image (coordinate). By combining these features with the linear discriminant analysis which is extended to treat \"face\" and \"not face\" classes, a scale invariant face detection system can be realized."
            },
            "slug": "Scale-invariant-face-detection-method-using-local-Hotta-Kurita",
            "title": {
                "fragments": [],
                "text": "Scale invariant face detection method using higher-order local autocorrelation features extracted from log-polar image"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "A scale invariant face detection method which combines higher-order local autocorrelation (HLAC) features extracted from a log-polar transformed image with linear discriminant analysis for \"face\" and \"not face\" classification is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Third IEEE International Conference on Automatic Face and Gesture Recognition"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052858104"
                        ],
                        "name": "D. B. Graham",
                        "slug": "D.-B.-Graham",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Graham",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. B. Graham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50729465"
                        ],
                        "name": "N. Allinson",
                        "slug": "N.-Allinson",
                        "structuredName": {
                            "firstName": "Nigel",
                            "lastName": "Allinson",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Allinson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 4
                            }
                        ],
                        "text": "The UMIST database consists of 564 images of 20 people with varying pose."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 86
                            }
                        ],
                        "text": "The images of each subject cover a range of poses from right profile to frontal views [56]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 87
                            }
                        ],
                        "text": "In training the SNoW-based face detector, 1,681 face images from Olivetti [136], UMIST [56], Harvard [57], Yale [7], and FERET [115] databases are used to capture the variations in face patterns."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 141914838,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8d40689398f8b055dc0111da1405fcedf16a403e",
            "isKey": false,
            "numCitedBy": 591,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe an eigenspace manifold for the representation and recognition of pose-varying faces. The distribution of faces in this manifold allows us to determine theoretical recognition characteristics which are then verified experimentally. Using this manifold a framework is proposed which can be used for both familiar and unfamiliar face recognition. A simple implementation demonstrates the pose dependent nature of the system over the transition from unfamiliar to familiar face recognition. Furthermore we show that multiple test images, whether real or virtual, can be used to augment the recognition process. The results compare favourably with reported human face recognition experiments. Finally, we describe how this framework can be used as a mechanism for characterising faces from video for general purpose recognition."
            },
            "slug": "Characterising-Virtual-Eigensignatures-for-General-Graham-Allinson",
            "title": {
                "fragments": [],
                "text": "Characterising Virtual Eigensignatures for General Purpose Face Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "An eigenspace manifold for the representation and recognition of pose-varying faces is described and a framework is proposed which can be used for both familiar and unfamiliar face recognition."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48926774"
                        ],
                        "name": "Ying Dai",
                        "slug": "Ying-Dai",
                        "structuredName": {
                            "firstName": "Ying",
                            "lastName": "Dai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ying Dai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737939"
                        ],
                        "name": "Y. Nakano",
                        "slug": "Y.-Nakano",
                        "structuredName": {
                            "firstName": "Yasuaki",
                            "lastName": "Nakano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Nakano"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 252,
                                "start": 248
                            }
                        ],
                        "text": "Several color spaces have been utilized to label pixels as skin including RGB [66], [67], [137], normalized RGB [102], [29], [149], [172], [30], [105], [171], [77], [151], [120], HSV (or HSI) [138], [79], [147], [146], YCrCb [167], [17], YIQ [31], [32], YES [131], CIE XYZ [19], and CIE LUV [173]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 59
                            }
                        ],
                        "text": "Dai and Nakano also applied SGLD model to face detection [32]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12479014,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a8e47dcdcd15180d9218fa80f107675a0d5cb74e",
            "isKey": false,
            "numCitedBy": 277,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Face-texture-model-based-on-SGLD-and-its-in-face-in-Dai-Nakano",
            "title": {
                "fragments": [],
                "text": "Face-texture model based on SGLD and its application in face detection in a color scene"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2023498"
                        ],
                        "name": "Thomas D. Rikert",
                        "slug": "Thomas-D.-Rikert",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Rikert",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas D. Rikert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145319478"
                        ],
                        "name": "Michael J. Jones",
                        "slug": "Michael-J.-Jones",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jones",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Jones"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731948"
                        ],
                        "name": "Paul A. Viola",
                        "slug": "Paul-A.-Viola",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Viola",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paul A. Viola"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 17251623,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "25bb1db7eff259fcc4e906f81184a1e07db9ff29",
            "isKey": false,
            "numCitedBy": 57,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents an approach to object detection which is based on recent work in statistical models for texture synthesis and recognition. Our method follows the texture recognition work of De Bonet and Viola (1998). We use feature vectors which capture the joint occurrence of local features at multiple resolutions. The distribution of feature vectors for a set of training images of an object class is estimated by clustering the data and then forming a mixture of Gaussian models. The mixture model is further refined by determining which clusters are the most discriminative for the class and retaining only those clusters. After the model is learned, test images are classified by computing the likelihood of their feature vectors with respect to the model. We present promising results in applying our technique to face detection and car detection."
            },
            "slug": "A-cluster-based-statistical-model-for-object-Rikert-Jones",
            "title": {
                "fragments": [],
                "text": "A cluster-based statistical model for object detection"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "This paper presents an approach to object detection which is based on recent work in statistical models for texture synthesis and recognition, and presents promising results in applying the technique to face detection and car detection."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Seventh IEEE International Conference on Computer Vision"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145319478"
                        ],
                        "name": "Michael J. Jones",
                        "slug": "Michael-J.-Jones",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jones",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Jones"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144177248"
                        ],
                        "name": "James M. Rehg",
                        "slug": "James-M.-Rehg",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Rehg",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James M. Rehg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2527220,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "69ef0027729dae1807afb6f7acd9aad0220140f4",
            "isKey": false,
            "numCitedBy": 1206,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "The existence of large image datasets such as the set of photos on the World Wide Web make it possible to build powerful generic models for low-level image attributes like color using simple histogram learning techniques. We describe the construction of color models for skin and non-skin classes from a dataset of nearly 1 billion labelled pixels. These classes exhibit a surprising degree of separability which we exploit by building a skin pixel detector achieving a detection rate of 80% with 8.5% false positives. We compare the performance of histogram and mixture models in skin detection and find histogram models to be superior in accuracy and computational cost. Using aggregate features computed from the skin pixel detector we build a surprisingly effective detector for naked people. Our results suggest that color can be a more powerful cue for detecting people in unconstrained imagery than was previously suspected. We believe this work is the most comprehensive and detailed exploration of skin color models to date."
            },
            "slug": "Statistical-Color-Models-with-Application-to-Skin-Jones-Rehg",
            "title": {
                "fragments": [],
                "text": "Statistical Color Models with Application to Skin Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work describes the construction of color models for skin and non-skin classes from a dataset of nearly 1 billion labelled pixels and suggests that color can be a more powerful cue for detecting people in unconstrained imagery than was previously suspected."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145687022"
                        ],
                        "name": "J. Crowley",
                        "slug": "J.-Crowley",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Crowley",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Crowley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "146688280"
                        ],
                        "name": "F. B\u00e9rard",
                        "slug": "F.-B\u00e9rard",
                        "structuredName": {
                            "firstName": "Fran\u00e7ois",
                            "lastName": "B\u00e9rard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. B\u00e9rard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Index Terms\u2014Face detection, face recognition, object recognition, view-based recognition, statistical pattern recognition, machine learning."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 139
                            }
                        ],
                        "text": "Several color spaces have been utilized to label pixels as skin including RGB [66], [67], [137], normalized RGB [102], [29], [149], [172], [30], [105], [171], [77], [151], [120], HSV (or HSI) [138], [79], [147], [146], YCrCb [167], [17], YIQ [31], [32], YES [131], CIE XYZ [19], and CIE LUV [173]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 257283,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1dc544677e8176a2043f7d5080eba3499a928316",
            "isKey": false,
            "numCitedBy": 253,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Visual processes to detect and track faces for video compression and transmission. The system is based on an architecture in which a supervisor selects and activates visual processes in cyclic manner. Control of visual processes is made possible by a confidence factor which accompanies each observation. Fusion of results into a unified estimation for tracking is made possible by estimating a covariance matrix with each observation. Visual processes for face tracking are described using blink detection, normalised color histogram matching, and cross correlation (SSD and NCC). Ensembles of visual processes are organised into processing states so as to provide robust tracking. Transition between states is determined by events detected by processes. The result of face detection is fed into recursive estimator (Kalman filter). The output from the estimator drives a PD controller for a pan/tilt/zoom camera. The resulting system provides robust and precise tracking which operates continuously at approximately 20 images per second on a 150 megahertz computer workstation."
            },
            "slug": "Multi-modal-tracking-of-faces-for-video-Crowley-B\u00e9rard",
            "title": {
                "fragments": [],
                "text": "Multi-modal tracking of faces for video communications"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "Visual processes to detect and track faces for video compression and transmission based on an architecture in which a supervisor selects and activates visual processes in cyclic manner provides robust and precise tracking."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2060131247"
                        ],
                        "name": "N. Oliver",
                        "slug": "N.-Oliver",
                        "structuredName": {
                            "firstName": "N.",
                            "lastName": "Oliver",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Oliver"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144994682"
                        ],
                        "name": "A. Pentland",
                        "slug": "A.-Pentland",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Pentland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pentland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "146688280"
                        ],
                        "name": "F. B\u00e9rard",
                        "slug": "F.-B\u00e9rard",
                        "structuredName": {
                            "firstName": "Fran\u00e7ois",
                            "lastName": "B\u00e9rard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. B\u00e9rard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 145
                            }
                        ],
                        "text": "Several color spaces have been utilized to label pixels as skin including RGB [66], [67], [137], normalized RGB [102], [29], [149], [172], [30], [105], [171], [77], [151], [120], HSV (or HSI) [138], [79], [147], [146], YCrCb [167], [17], YIQ [31], [32], YES [131], CIE XYZ [19], and CIE LUV [173]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 241110,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "25fd0a96ba4930e551efdee3e1eaa3b98fd87ac4",
            "isKey": false,
            "numCitedBy": 229,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes an active-camera real-time system for tracking, shape description, and classification of the human face and mouth using only an SGI Indy computer. The system is based on use of 2-D blob features, which are spatially-compact clusters of pixels that are similar in terms of low-level image properties. Patterns of behavior (e.g., facial expressions and head movements) can be classified in real-time using Hidden Markov Model (HMM) methods. The system has been tested on hundreds of users and has demonstrated extremely reliable and accurate performance. Typical classification accuracies are near 100%."
            },
            "slug": "LAFTER:-lips-and-face-real-time-tracker-Oliver-Pentland",
            "title": {
                "fragments": [],
                "text": "LAFTER: lips and face real time tracker"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "An active-camera real-time system for tracking, shape description, and classification of the human face and mouth using only an SGI Indy computer using 2-D blob features, which are spatially-compact clusters of pixels that are similar in terms of low-level image properties."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704975"
                        ],
                        "name": "G. Burel",
                        "slug": "G.-Burel",
                        "structuredName": {
                            "firstName": "Gilles",
                            "lastName": "Burel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Burel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2081671334"
                        ],
                        "name": "Dominique Carel",
                        "slug": "Dominique-Carel",
                        "structuredName": {
                            "firstName": "Dominique",
                            "lastName": "Carel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dominique Carel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 10
                            }
                        ],
                        "text": "and Carel [12] proposed a neural network for face detection in"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 0
                            }
                        ],
                        "text": "Burel and Carel [12] proposed a neural network for face detection in which the large number of training examples of faces and nonfaces are compressed into fewer examples using a Kohonen\u2019s SOM algorithm [80]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 35
                            }
                        ],
                        "text": "performance, while Burel and Carel [12] used a single"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 102
                            }
                        ],
                        "text": "They also used multiple neural networks and several arbitration methods to improve performance, while Burel and Carel [12] used a single network, and Vaillant et al. [164] used two networks for classification."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14746961,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1cdb510aed4f8b04b73240287da954f9eff39318",
            "isKey": true,
            "numCitedBy": 94,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Detection-and-localization-of-faces-on-digital-Burel-Carel",
            "title": {
                "fragments": [],
                "text": "Detection and localization of faces on digital images"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit. Lett."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2987811"
                        ],
                        "name": "M. Swain",
                        "slug": "M.-Swain",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Swain",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Swain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691804"
                        ],
                        "name": "D. Ballard",
                        "slug": "D.-Ballard",
                        "structuredName": {
                            "firstName": "Dana",
                            "lastName": "Ballard",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ballard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 23
                            }
                        ],
                        "text": "Histogram intersection [155] is used to compare the control histogram and current histogram."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8167136,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5b1e1696564e5a3021ac3a501c9deeb6c0fbc637",
            "isKey": false,
            "numCitedBy": 5039,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Computer vision is embracing a new research focus in which the aim is to develop visual skills for robots that allow them to interact with a dynamic, realistic environment. To achieve this aim, new kinds of vision algorithms need to be developed which run in real time and subserve the robot's goals. Two fundamental goals are determining the location of a known object. Color can be successfully used for both tasks.This article demonstrates that color histograms of multicolored objects provide a robust, efficient cue for indexing into a large database of models. It shows that color histograms are stable object representations in the presence of occlusion and over change in view, and that they can differentiate among a large number of objects. For solving the identification problem, it introduces a technique calledHistogram Intersection, which matches model and image histograms and a fast incremental version of Histogram Intersection, which allows real-time indexing into a large database of stored models. For solving the location problem it introduces an algorithm calledHistogram Backprojection, which performs this task efficiently in crowded scenes."
            },
            "slug": "Color-indexing-Swain-Ballard",
            "title": {
                "fragments": [],
                "text": "Color indexing"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is demonstrated that color histograms of multicolored objects provide a robust, efficient cue for indexing into a large database of models and that they can differentiate among a large number of objects."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682773"
                        ],
                        "name": "A. Pentland",
                        "slug": "A.-Pentland",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Pentland",
                            "middleNames": [
                                "'Sandy'"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pentland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144075899"
                        ],
                        "name": "Tanzeem Choudhury",
                        "slug": "Tanzeem-Choudhury",
                        "structuredName": {
                            "firstName": "Tanzeem",
                            "lastName": "Choudhury",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tanzeem Choudhury"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 95
                            }
                        ],
                        "text": "Given a single image, the goal of face detection is to identify all image regions which contain a face regardless of its three-dimensional position, orientation, and lighting conditions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14470115,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "13425bb41d326982ec6b3c6f3034aa978a1300ac",
            "isKey": false,
            "numCitedBy": 277,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Smart environments, wearable computers, and ubiquitous computing in general are the coming \"fourth generation\" of computing and information technology. But that technology will be a stillbirth without new interfaces for interaction, minus a keyboard or mouse. To win wide consumer acceptance, these interactions must be friendly and personalized; the next generation interfaces must recognize people in their immediate environment and, at a minimum, know who they are. In this article, the authors discuss face recognition technology, how it works, problems to be overcome, current technologies, and future developments and possible applications. Twenty years ago, the problem of face recognition was considered among the most difficult in artificial intelligence and computer vision. Today, however, there are several companies that sell commercial face recognition software that is capable of high-accuracy recognition with databases of more than 1,000 people. The authors describe the face recognition technology used, explaining the algorithms for face recognition as well as novel applications, such as behavior monitoring that assesses emotions based on facial expressions."
            },
            "slug": "Face-Recognition-for-Smart-Environments-Pentland-Choudhury",
            "title": {
                "fragments": [],
                "text": "Face Recognition for Smart Environments"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The authors describe the face recognition technology used, explaining the algorithms for face recognition as well as novel applications, such as behavior monitoring that assesses emotions based on facial expressions."
            },
            "venue": {
                "fragments": [],
                "text": "Computer"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145237406"
                        ],
                        "name": "N. Ahuja",
                        "slug": "N.-Ahuja",
                        "structuredName": {
                            "firstName": "Narendra",
                            "lastName": "Ahuja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Ahuja"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 42408532,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "922224add217ed1400f700459c396575f264e340",
            "isKey": false,
            "numCitedBy": 135,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Describes a transform to extract image regions at all geometric and photometric scales. It is argued that linear approaches have the shortcoming that they require a priori models of region shape. The proposed transform avoids this by letting the structure emerge, bottom-up, from interactions among pixels. The transform involves global computations on pairs of pixels followed by vector integration of the results. An attraction force field is computed over the image in which pixels belonging to the same region are mutually attracted and the region is characterized by a convergent flow. It is shown that the transform possesses properties that allow multiscale segmentation, or extraction of original, unblurred structure at all different geometric and photometric scales present. This is in contrast with much previous work wherein multiscale structure is viewed as the smoothed structure in a multiscale signal decimation. Scale is an integral parameter of the force computation, and the number and values of scale parameters associated with the image can be estimated automatically. Regions are detected at all a priori unknown scales resulting in automatic construction of a segmentation tree, in which each pixel is annotated with descriptions of all the regions it belongs to. Transform properties are presented for piecewise-constant images but hold for more general ones. Thus the proposed method is intended as a solution to the problem of multiscale, integrated edge and region detection, or low-level image segmentation. Experimental results are given."
            },
            "slug": "A-Transform-for-Multiscale-Image-Segmentation-by-Ahuja",
            "title": {
                "fragments": [],
                "text": "A Transform for Multiscale Image Segmentation by Integrated Edge and Region Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "It is shown that the transform possesses properties that allow multiscale segmentation, or extraction of original, unblurred structure at all different geometric and photometric scales present, in contrast with much previous work whereinMultiscale structure is viewed as the smoothed structure in a multiscales signal decimation."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38480590"
                        ],
                        "name": "Wenyi Zhao",
                        "slug": "Wenyi-Zhao",
                        "structuredName": {
                            "firstName": "Wenyi",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wenyi Zhao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9215658"
                        ],
                        "name": "R. Chellappa",
                        "slug": "R.-Chellappa",
                        "structuredName": {
                            "firstName": "Rama",
                            "lastName": "Chellappa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Chellappa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2030268"
                        ],
                        "name": "A. Krishnaswamy",
                        "slug": "A.-Krishnaswamy",
                        "structuredName": {
                            "firstName": "Arvindh",
                            "lastName": "Krishnaswamy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Krishnaswamy"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 7032646,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d8e6580749e9cd3eebf7f9b95b58645c23d043ea",
            "isKey": false,
            "numCitedBy": 628,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we describe a face recognition method based on PCA (Principal Component Analysis) and LDA (Linear Discriminant Analysis). The method consists of two steps: first we project the face image from the original vector space to a face subspace via PCA, second we use LDA to obtain a best linear classifier. The basic idea of combining PCA and LDA is to improve the generalization capability of LDA when only few samples per class are available. Using PCA, we are able to construct a face subspace in which we apply LDA to perform classification. Using FERET dataset we demonstrate a significant improvement when principal components rather than original images are fed to the LDA classifier. The hybrid classifier using PCA and LDA provides a useful framework for other image recognition tasks as well."
            },
            "slug": "Discriminant-analysis-of-principal-components-for-Zhao-Chellappa",
            "title": {
                "fragments": [],
                "text": "Discriminant analysis of principal components for face recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A hybrid classifier using PCA and LDA provides a useful framework for other image recognition tasks as well and demonstrates a significant improvement when principal components rather than original images are fed to the LDA classifier."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Third IEEE International Conference on Automatic Face and Gesture Recognition"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145266088"
                        ],
                        "name": "T. Leung",
                        "slug": "T.-Leung",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Leung",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Leung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2511655"
                        ],
                        "name": "M. Burl",
                        "slug": "M.-Burl",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Burl",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Burl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 27
                            }
                        ],
                        "text": "One advantage of these methods is that partially occluded faces can be located."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 29742099,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "846b99d151892e70046e5ca541430be4ff67209f",
            "isKey": false,
            "numCitedBy": 57,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Under a weak perspective camera model, the image plane coordinates in different views of a planar object are related by an affine transformation. Because of this property, researchers have attempted to use affine invariants for recognition. However, there are two problems with this approach: (1) objects or object classes with inherent variability cannot be adequately treated using invariants; and (2) in practice the calculated affine invariants can be quite sensitive to errors in the image plane measurements. In this paper we use probability distributions to address both of these difficulties. Under the assumption that the feature positions of a planar object can be modeled using a jointly Gaussian density, we have derived the joint density over the corresponding set of affine coordinates. Even when the assumptions of a planar object and a weak perspective camera model do not strictly hold, the results are useful because deviations from the ideal can be treated as deformability in the underlying object model."
            },
            "slug": "Probabilistic-affine-invariants-for-recognition-Leung-Burl",
            "title": {
                "fragments": [],
                "text": "Probabilistic affine invariants for recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Under the assumption that the feature positions of a planar object can be modeled using a jointly Gaussian density, the joint density over the corresponding set of affine coordinates is derived."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 1998 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.98CB36231)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2739350"
                        ],
                        "name": "R. Kjeldsen",
                        "slug": "R.-Kjeldsen",
                        "structuredName": {
                            "firstName": "Rick",
                            "lastName": "Kjeldsen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kjeldsen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719062"
                        ],
                        "name": "J. Kender",
                        "slug": "J.-Kender",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Kender",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kender"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 203,
                                "start": 199
                            }
                        ],
                        "text": "Several color spaces have been utilized to label pixels as skin including RGB [66], [67], [137], normalized RGB [102], [29], [149], [172], [30], [105], [171], [77], [151], [120], HSV (or HSI) [138], [79], [147], [146], YCrCb [167], [17], YIQ [31], [32], YES [131], CIE XYZ [19], and CIE LUV [173]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 29027548,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5883515e99c1a8da1a178df9ebe5116018fe656a",
            "isKey": false,
            "numCitedBy": 350,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes the techniques used to separate the hand from a cluttered background in a gesture recognition system. Target colors are identified using a histogram-like structure called a Color Predicate, which is trained in real-time using a novel algorithm. Running on standard PC hardware, the segmentation is of sufficient speed and quality to support an interactive user interface. The method has shown its flexibility in a range of different office environments, segmenting users with many different skin-tones. Variations have been applied to other problems including finding face candidates in video sequences."
            },
            "slug": "Finding-skin-in-color-images-Kjeldsen-Kender",
            "title": {
                "fragments": [],
                "text": "Finding skin in color images"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "The techniques used to separate the hand from a cluttered background in a gesture recognition system are described, which is of sufficient speed and quality to support an interactive user interface."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Second International Conference on Automatic Face and Gesture Recognition"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710238"
                        ],
                        "name": "R. Haralick",
                        "slug": "R.-Haralick",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Haralick",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Haralick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3144171"
                        ],
                        "name": "K. Shanmugam",
                        "slug": "K.-Shanmugam",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Shanmugam",
                            "middleNames": [
                                "Sam"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Shanmugam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686153"
                        ],
                        "name": "I. Dinstein",
                        "slug": "I.-Dinstein",
                        "structuredName": {
                            "firstName": "Its'hak",
                            "lastName": "Dinstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Dinstein"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 72
                            }
                        ],
                        "text": "The texture are computed using second-order statistical features (SGLD) [59] on subimages of 16 16 pixels."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 28
                            }
                        ],
                        "text": "Dai and Nakano also applied SGLD model to face detection [32]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 206786900,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "1fdb62555eb650662dbe2a6f3985d390861597c2",
            "isKey": false,
            "numCitedBy": 19247,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Texture is one of the important characteristics used in identifying objects or regions of interest in an image, whether the image be a photomicrograph, an aerial photograph, or a satellite image. This paper describes some easily computable textural features based on gray-tone spatial dependancies, and illustrates their application in category-identification tasks of three different kinds of image data: photomicrographs of five kinds of sandstones, 1:20 000 panchromatic aerial photographs of eight land-use categories, and Earth Resources Technology Satellite (ERTS) multispecial imagery containing seven land-use categories. We use two kinds of decision rules: one for which the decision regions are convex polyhedra (a piecewise linear decision rule), and one for which the decision regions are rectangular parallelpipeds (a min-max decision rule). In each experiment the data set was divided into two parts, a training set and a test set. Test set identification accuracy is 89 percent for the photomicrographs, 82 percent for the aerial photographic imagery, and 83 percent for the satellite imagery. These results indicate that the easily computable textural features probably have a general applicability for a wide variety of image-classification applications."
            },
            "slug": "Textural-Features-for-Image-Classification-Haralick-Shanmugam",
            "title": {
                "fragments": [],
                "text": "Textural Features for Image Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "These results indicate that the easily computable textural features based on gray-tone spatial dependancies probably have a general applicability for a wide variety of image-classification applications."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Syst. Man Cybern."
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768120"
                        ],
                        "name": "T. Jebara",
                        "slug": "T.-Jebara",
                        "structuredName": {
                            "firstName": "Tony",
                            "lastName": "Jebara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jebara"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144994682"
                        ],
                        "name": "A. Pentland",
                        "slug": "A.-Pentland",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Pentland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pentland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 78
                            }
                        ],
                        "text": "Several color spaces have been utilized to label pixels as skin including RGB [66], [67], [137], normalized RGB [102], [29], [149], [172], [30], [105], [171], [77], [151], [120], HSV (or HSI) [138], [79], [147], [146], YCrCb [167], [17], YIQ [31], [32], YES [131], CIE XYZ [19], and CIE LUV [173]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 71
                            }
                        ],
                        "text": "Crowley and Coutaz used a histogram h\u00f0r; g\u00de of \u00f0r; g\u00de values in normalized RGB color space to obtain the probability of obtaining a particular RGBvector given that the pixel observes skin [29], [30]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 140
                            }
                        ],
                        "text": "Recently, Jones and Rehg conducted a large-scale experiment in which nearly 1 billion labeled skin tone pixels are collected (in normalized RGB color space) [69]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 44
                            }
                        ],
                        "text": "Using a Gaussian distribution in normalized RGB color space, segmented regions with a skinlike color are classified as faces."
                    },
                    "intents": []
                }
            ],
            "corpusId": 249000,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c775b46b6fc80b16cbd27da65eb6ddac30868645",
            "isKey": true,
            "numCitedBy": 307,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "A real-time system is described for automatically detecting, modeling and tracking faces in 3D. A closed loop approach is proposed which utilizes structure from motion to generate a 3D model of a face and then feed back the estimated structure to constrain feature tracking in the next frame. The system initializes by using skin classification, symmetry operations, 3D warping and eigenfaces to find a face. Feature trajectories are then computed by SSD or correlation-based tracking. The trajectories are simultaneously processed by an extended Kalman filter to stably recover 3D structure, camera geometry and facial pose. Adaptively weighted estimation is used in this filter by modeling the noise characteristics of the 2D image patch tracking technique. In addition, the structural estimate is constrained by using parametrized models of facial structure (eigen-heads). The Kalman filter's estimate of the 3D state and motion of the face predicts the trajectory of the features which constrains the search space for the next frame in the video sequence. The feature tracking and Kalman filtering closed loop system operates at 25 Hz."
            },
            "slug": "Parametrized-structure-from-motion-for-3D-adaptive-Jebara-Pentland",
            "title": {
                "fragments": [],
                "text": "Parametrized structure from motion for 3D adaptive feedback tracking of faces"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "A real-time system is described for automatically detecting, modeling and tracking faces in 3D, which utilizes structure from motion to generate a 3D model of a face and then feeds back the estimated structure to constrain feature tracking in the next frame."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794240"
                        ],
                        "name": "Haiyuan Wu",
                        "slug": "Haiyuan-Wu",
                        "structuredName": {
                            "firstName": "Haiyuan",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Haiyuan Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "122439944"
                        ],
                        "name": "Qian Chen",
                        "slug": "Qian-Chen",
                        "structuredName": {
                            "firstName": "Qian",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qian Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735941"
                        ],
                        "name": "M. Yachida",
                        "slug": "M.-Yachida",
                        "structuredName": {
                            "firstName": "Masahiko",
                            "lastName": "Yachida",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Yachida"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5869722,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "080f57ebfe8b6e5e26b3c9589c95ee84feeb2a71",
            "isKey": false,
            "numCitedBy": 297,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a new method to detect faces in color images based on the fuzzy theory. We make two fuzzy models to describe the skin color and hair color, respectively. In these models, we use a perceptually uniform color space to describe the color information to increase the accuracy and stableness. We use the two models to extract the skin color regions and the hair color regions, and then comparing them with the prebuilt head-shape models by using a fuzzy theory based pattern-matching method to detect face candidates."
            },
            "slug": "Face-Detection-From-Color-Images-Using-a-Fuzzy-Wu-Chen",
            "title": {
                "fragments": [],
                "text": "Face Detection From Color Images Using a Fuzzy Pattern Matching Method"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Two fuzzy models are made to describe the skin color and hair color and then compared with the prebuilt head-shape models by using a fuzzy theory based pattern-matching method to detect face candidates."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2586567"
                        ],
                        "name": "M. Collobert",
                        "slug": "M.-Collobert",
                        "structuredName": {
                            "firstName": "Michel",
                            "lastName": "Collobert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collobert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3430625"
                        ],
                        "name": "R. F\u00e9raud",
                        "slug": "R.-F\u00e9raud",
                        "structuredName": {
                            "firstName": "Rapha\u00ebl",
                            "lastName": "F\u00e9raud",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. F\u00e9raud"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2084746211"
                        ],
                        "name": "G. Tourneur",
                        "slug": "G.-Tourneur",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Tourneur",
                            "middleNames": [
                                "Le"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Tourneur"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3118328"
                        ],
                        "name": "O. Bernier",
                        "slug": "O.-Bernier",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Bernier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Bernier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35094984"
                        ],
                        "name": "J. Viallet",
                        "slug": "J.-Viallet",
                        "structuredName": {
                            "firstName": "Jean-Emmanuel",
                            "lastName": "Viallet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Viallet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052697"
                        ],
                        "name": "Y. Mahieux",
                        "slug": "Y.-Mahieux",
                        "structuredName": {
                            "firstName": "Yannick",
                            "lastName": "Mahieux",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Mahieux"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2444622"
                        ],
                        "name": "D. Collobert",
                        "slug": "D.-Collobert",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Collobert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Collobert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17288542,
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "id": "3af7faa8548ff45dd64c530a12344725bf8a1c84",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Both visual and acoustical informations provide effective means of telecommunication between persons. In this context, the face is the most important part of the person both visually and acoustically. We describe how the cooperation of image and audio processing allows to track a person's face and to collect the audio information it produces. We present detection techniques of regions of interest (e.g. Moving regions of skin color), coupled with a neural network based face detector with a low false alarm rate, to locate and track faces. The system is connected to a nine microphone array adaptive beam forming which performs immediate beam forming. Visual and acoustical informations from the speaker face are thus obtained in real time."
            },
            "slug": "LISTEN:-a-system-for-locating-and-tracking-speakers-Collobert-F\u00e9raud",
            "title": {
                "fragments": [],
                "text": "LISTEN: a system for locating and tracking individual speakers"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work describes how the cooperation of image and audio processing allows to track a person's face and to collect the audio information it produces, coupled with a neural network based face detector with a low false alarm rate, to locate and track faces."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Second International Conference on Automatic Face and Gesture Recognition"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781874"
                        ],
                        "name": "E. Osuna",
                        "slug": "E.-Osuna",
                        "structuredName": {
                            "firstName": "Edgar",
                            "lastName": "Osuna",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Osuna"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1771659"
                        ],
                        "name": "R. Freund",
                        "slug": "R.-Freund",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Freund",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2845602,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9008cdacbdcff8a218a6928e94fe7c6dfc237b24",
            "isKey": false,
            "numCitedBy": 2841,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate the application of Support Vector Machines (SVMs) in computer vision. SVM is a learning technique developed by V. Vapnik and his team (AT&T Bell Labs., 1985) that can be seen as a new method for training polynomial, neural network, or Radial Basis Functions classifiers. The decision surfaces are found by solving a linearly constrained quadratic programming problem. This optimization problem is challenging because the quadratic form is completely dense and the memory requirements grow with the square of the number of data points. We present a decomposition algorithm that guarantees global optimality, and can be used to train SVM's over very large data sets. The main idea behind the decomposition is the iterative solution of sub-problems and the evaluation of optimality conditions which are used both to generate improved iterative values, and also establish the stopping criteria for the algorithm. We present experimental results of our implementation of SVM, and demonstrate the feasibility of our approach on a face detection problem that involves a data set of 50,000 data points."
            },
            "slug": "Training-support-vector-machines:-an-application-to-Osuna-Freund",
            "title": {
                "fragments": [],
                "text": "Training support vector machines: an application to face detection"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A decomposition algorithm that guarantees global optimality, and can be used to train SVM's over very large data sets is presented, and the feasibility of the approach on a face detection problem that involves a data set of 50,000 data points is demonstrated."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1816827"
                        ],
                        "name": "Gianluca Donato",
                        "slug": "Gianluca-Donato",
                        "structuredName": {
                            "firstName": "Gianluca",
                            "lastName": "Donato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gianluca Donato"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2218905"
                        ],
                        "name": "M. Bartlett",
                        "slug": "M.-Bartlett",
                        "structuredName": {
                            "firstName": "Marian",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "Stewart"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Bartlett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2072657855"
                        ],
                        "name": "J. C. Hager",
                        "slug": "J.-C.-Hager",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Hager",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. C. Hager"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "21451088"
                        ],
                        "name": "P. Ekman",
                        "slug": "P.-Ekman",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Ekman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Ekman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Index Terms\u2014Face detection, face recognition, object recognition, view-based recognition, statistical pattern recognition, machine learning."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4517874,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d306e3382a119d61d5fdc243408bf2d433f1e38b",
            "isKey": false,
            "numCitedBy": 1117,
            "numCiting": 165,
            "paperAbstract": {
                "fragments": [],
                "text": "The Facial Action Coding System (FACS) [23] is an objective method for quantifying facial movement in terms of component actions. This system is widely used in behavioral investigations of emotion, cognitive processes, and social interaction. The coding is presently performed by highly trained human experts. This paper explores and compares techniques for automatically recognizing facial actions in sequences of images. These techniques include analysis of facial motion through estimation of optical flow; holistic spatial analysis, such as principal component analysis, independent component analysis, local feature analysis, and linear discriminant analysis; and methods based on the outputs of local filters, such as Gabor wavelet representations and local principal components. Performance of these systems is compared to naive and expert human subjects. Best performances were obtained using the Gabor wavelet representation and the independent component representation, both of which achieved 96 percent accuracy for classifying 12 facial actions of the upper and lower face. The results provide converging evidence for the importance of using local filters, high spatial frequencies, and statistical independence for classifying facial actions."
            },
            "slug": "Classifying-Facial-Actions-Donato-Bartlett",
            "title": {
                "fragments": [],
                "text": "Classifying Facial Actions"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper explores and compares techniques for automatically recognizing facial actions in sequences of images and provides converging evidence for the importance of using local filters, high spatial frequencies, and statistical independence for classifying facial actions."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749650"
                        ],
                        "name": "B. Frey",
                        "slug": "B.-Frey",
                        "structuredName": {
                            "firstName": "Brendan",
                            "lastName": "Frey",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Frey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783224"
                        ],
                        "name": "A. Colmenarez",
                        "slug": "A.-Colmenarez",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Colmenarez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Colmenarez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153652752"
                        ],
                        "name": "Thomas S. Huang",
                        "slug": "Thomas-S.-Huang",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Huang",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas S. Huang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15581671,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "739e4adbd26835e35911e04c7295f601abd9eaff",
            "isKey": false,
            "numCitedBy": 82,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Traditional subspace methods for face recognition compute a measure of similarity between images after projecting them onto a fixed linear subspace that is spanned by some principal component vectors (a.k.a. \"eigenfaces\") of a training set of images. By supposing a parametric Gaussian distribution over the subspace and a symmetric Gaussian noise model for the image given a point in the subspace, we can endow this framework with a probabilistic interpretation so that Bayes-optimal decisions can be made. However, we expect that different image clusters (corresponding, say, to different poses and expressions) will be best represented by different subspaces. In this paper, we study the recognition performance of a mixture of local linear subspaces model that can be fit to training data using the expectation maximization algorithm. The mixture model outperforms a nearest-neighbor classifier that operates in a PCA subspace."
            },
            "slug": "Mixtures-of-local-linear-subspaces-for-face-Frey-Colmenarez",
            "title": {
                "fragments": [],
                "text": "Mixtures of local linear subspaces for face recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper studies the recognition performance of a mixture of local linear subspaces model that can be fit to training data using the expectation maximization algorithm and finds that the mixture model outperforms a nearest-neighbor classifier that operates in a PCA subspace."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 1998 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.98CB36231)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145030811"
                        ],
                        "name": "C. Papageorgiou",
                        "slug": "C.-Papageorgiou",
                        "structuredName": {
                            "firstName": "Constantine",
                            "lastName": "Papageorgiou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Papageorgiou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13308232,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e6c20ed0c3f375f403ab5d750a6e9699d5c3af6a",
            "isKey": false,
            "numCitedBy": 1404,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a general, trainable system for object detection in unconstrained, cluttered scenes. The system derives much of its power from a representation that describes an object class in terms of an overcomplete dictionary of local, oriented, multiscale intensity differences between adjacent regions, efficiently computable as a Haar wavelet transform. This example-based learning approach implicitly derives a model of an object class by training a support vector machine classifier using a large set of positive and negative examples. We present results on face, people, and car detection tasks using the same architecture. In addition, we quantify how the representation affects detection performance by considering several alternate representations including pixels and principal components. We also describe a real-time application of our person detection system as part of a driver assistance system."
            },
            "slug": "A-Trainable-System-for-Object-Detection-Papageorgiou-Poggio",
            "title": {
                "fragments": [],
                "text": "A Trainable System for Object Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "A general, trainable system for object detection in unconstrained, cluttered scenes that derives much of its power from a representation that describes an object class in terms of an overcomplete dictionary of local, oriented, multiscale intensity differences between adjacent regions, efficiently computable as a Haar wavelet transform."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3430625"
                        ],
                        "name": "R. F\u00e9raud",
                        "slug": "R.-F\u00e9raud",
                        "structuredName": {
                            "firstName": "Rapha\u00ebl",
                            "lastName": "F\u00e9raud",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. F\u00e9raud"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "autoassociative neural networks [43], [42], [44]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 129215622,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "97a1e3537cb2776d94d173d87c5b9b1975f564e6",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "A generative neural network model, constrained by non-face examples chosen by an iterative algorithm, is applied to face detection. To extend the detection ability in orientation and to decrease the number of false alarms, different combinations of networks are tested: ensemble, conditional ensemble and conditional mixture of networks. The use of a conditional mixture of networks obtains better results on different benchmark face databases than state-of-the-art."
            },
            "slug": "PCA,-neural-networks-and-estimation-for-face-F\u00e9raud",
            "title": {
                "fragments": [],
                "text": "PCA, neural networks and estimation for face detection"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "A generative neural network model, constrained by non-face examples chosen by an iterative algorithm, is applied to face detection, and the use of a conditional mixture of networks obtains better results on different benchmark face databases than state-of-the-art."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1803182"
                        ],
                        "name": "A. Nefian",
                        "slug": "A.-Nefian",
                        "structuredName": {
                            "firstName": "Ara",
                            "lastName": "Nefian",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Nefian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144449603"
                        ],
                        "name": "M. Hayes",
                        "slug": "M.-Hayes",
                        "structuredName": {
                            "firstName": "Monson",
                            "lastName": "Hayes",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hayes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12576529,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dcdd358dd2e79da3fff78cae37417a251b0170ed",
            "isKey": false,
            "numCitedBy": 167,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "The work presented in this paper describes a hidden Markov model (HMM)-based framework for face recognition and face detection. The observation vectors used to characterize the states of the HMM are obtained using the coefficients of the Karhunen-Loeve transform (KLT). The face recognition method presented reduces significantly the computational complexity of previous HMM-based face recognition systems, while slightly improving the recognition rate. Consistent with the HMM model of the face, this paper introduces a novel HMM-based face detection approach using the same feature extraction techniques used for face recognition."
            },
            "slug": "Face-detection-and-recognition-using-hidden-Markov-Nefian-Hayes",
            "title": {
                "fragments": [],
                "text": "Face detection and recognition using hidden Markov models"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A novel HMM-based face detection approach using the same feature extraction techniques used for face recognition using the coefficients of the Karhunen-Loeve transform is introduced."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 1998 International Conference on Image Processing. ICIP98 (Cat. No.98CB36269)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144847940"
                        ],
                        "name": "K. Lam",
                        "slug": "K.-Lam",
                        "structuredName": {
                            "firstName": "Kin-Man",
                            "lastName": "Lam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Lam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152996923"
                        ],
                        "name": "Hong Yan",
                        "slug": "Hong-Yan",
                        "structuredName": {
                            "firstName": "Hong",
                            "lastName": "Yan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hong Yan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Index Terms\u2014Face detection, face recognition, object recognition, view-based recognition, statistical pattern recognition, machine learning."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 26635890,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d0eb349667f9e25c87bce8d945b4b225f6041c82",
            "isKey": false,
            "numCitedBy": 55,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The first step for human face recognition is to locate the head boundary in a head-and-shoulders image. An approach that uses adaptive contour models or \"snakes\" is described to solve this problem. Since we have a priori knowledge of the shape of a head, this active contour model is tailor-made for representing the head boundary. In this paper, a reliable method to locate the approximate position of the head and to estimate the head boundary is proposed. The effect of the parameters for snakes is investigated by locating the head boundary, and a best set of the parameters is suggested. A fast algorithm based on the greedy algorithm for active contour modeling is also presented. The computational complexity of this new algorithm is analyzed and compared with the greedy algorithm. This fast algorithm has a performance capability comparable to the greedy algorithm and reduces the execution time by more than 30% on the average."
            },
            "slug": "Fast-algorithm-for-locating-head-boundaries-Lam-Yan",
            "title": {
                "fragments": [],
                "text": "Fast algorithm for locating head boundaries"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A reliable method to locate the approximate position of the head and to estimate the head boundary is proposed and a fast algorithm based on the greedy algorithm for active contour modeling is presented."
            },
            "venue": {
                "fragments": [],
                "text": "J. Electronic Imaging"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144986880"
                        ],
                        "name": "I. Craw",
                        "slug": "I.-Craw",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Craw",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Craw"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3263505"
                        ],
                        "name": "D. Tock",
                        "slug": "D.-Tock",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Tock",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Tock"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056963263"
                        ],
                        "name": "Alan Bennett",
                        "slug": "Alan-Bennett",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Bennett",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alan Bennett"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Index Terms\u2014Face detection, face recognition, object recognition, view-based recognition, statistical pattern recognition, machine learning."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17481367,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e2e6bc4db498566a9f95f122970fb4488eaf3392",
            "isKey": false,
            "numCitedBy": 332,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a computer program which understands a greyscale image of a face well enough to locate individual face features such as eyes and mouth. The program has two distinct components: modules designed to locate particular face features, usually in a restricted area; and the overall control strategy which activates modules on the basis of the current solution state, and assesses and integrates the results of each module."
            },
            "slug": "Finding-Face-Features-Craw-Tock",
            "title": {
                "fragments": [],
                "text": "Finding Face Features"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "A computer program which understands a greyscale image of a face well enough to locate individual face features such as eyes and mouth is described."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7205190"
                        ],
                        "name": "Tim Cootes",
                        "slug": "Tim-Cootes",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Cootes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tim Cootes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144482985"
                        ],
                        "name": "C. Taylor",
                        "slug": "C.-Taylor",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Taylor",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Taylor"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 29326531,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f0ad8f033087944c3b6fa8a443c443dde7782e0d",
            "isKey": false,
            "numCitedBy": 53,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a method of locating hypotheses for the positions of faces in an image. We use statistical feature detectors to locate candidates for features, then use a statistical model of the shape and orientation of the features to test combinations of such features to find the most plausible. The best sets can be used as the initial position of an Active Shape Model, which can then accurately locate the full face."
            },
            "slug": "Locating-faces-using-statistical-feature-detectors-Cootes-Taylor",
            "title": {
                "fragments": [],
                "text": "Locating faces using statistical feature detectors"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "A method of locating hypotheses for the positions of faces in an image by using statistical feature detectors to locate candidates for features, then using a statistical model of the shape and orientation of the features to test combinations of such features to find the most plausible."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Second International Conference on Automatic Face and Gesture Recognition"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6435894"
                        ],
                        "name": "S. McKenna",
                        "slug": "S.-McKenna",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "McKenna",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. McKenna"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144784813"
                        ],
                        "name": "S. Gong",
                        "slug": "S.-Gong",
                        "structuredName": {
                            "firstName": "Shaogang",
                            "lastName": "Gong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Gong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120597446"
                        ],
                        "name": "Y. Raja",
                        "slug": "Y.-Raja",
                        "structuredName": {
                            "firstName": "Yogesh",
                            "lastName": "Raja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Raja"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 8447172,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ee461d060da58d6053d2f4988b54eff8655ecede",
            "isKey": false,
            "numCitedBy": 263,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Modelling-facial-colour-and-identity-with-Gaussian-McKenna-Gong",
            "title": {
                "fragments": [],
                "text": "Modelling facial colour and identity with Gaussian mixtures"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2085375589"
                        ],
                        "name": "H. Schneiderman",
                        "slug": "H.-Schneiderman",
                        "structuredName": {
                            "firstName": "Henry",
                            "lastName": "Schneiderman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Schneiderman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733113"
                        ],
                        "name": "T. Kanade",
                        "slug": "T.-Kanade",
                        "structuredName": {
                            "firstName": "Takeo",
                            "lastName": "Kanade",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanade"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12209481,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3565c5a65842f26091578b9d71d496cc1561239d",
            "isKey": false,
            "numCitedBy": 1292,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we describe a statistical method for 3D object detection. We represent the statistics of both object appearance and \"non-object\" appearance using a product of histograms. Each histogram represents the joint statistics of a subset of wavelet coefficients and their position on the object. Our approach is to use many such histograms representing a wide variety of visual attributes. Using this method, we have developed the first algorithm that can reliably detect human faces with out-of-plane rotation and the first algorithm that can reliably detect passenger cars over a wide range of viewpoints."
            },
            "slug": "A-statistical-method-for-3D-object-detection-to-and-Schneiderman-Kanade",
            "title": {
                "fragments": [],
                "text": "A statistical method for 3D object detection applied to faces and cars"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Using this method, this work has developed the first algorithm that can reliably detect human faces with out-of-plane rotation and the first algorithms thatCan reliably detect passenger cars over a wide range of viewpoints."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings IEEE Conference on Computer Vision and Pattern Recognition. CVPR 2000 (Cat. No.PR00662)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717217"
                        ],
                        "name": "R. J. Qian",
                        "slug": "R.-J.-Qian",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Qian",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. J. Qian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153652752"
                        ],
                        "name": "Thomas S. Huang",
                        "slug": "Thomas-S.-Huang",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Huang",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas S. Huang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 9413838,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b54f72748b74fc815aa802aaef90c3685ca8f3d6",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a new scale, position and orientation invariant approach to object detection. The proposed method first chooses attention regions in an image based on the region detection result on the image. Within the attention regions, the method then detects targets using a novel object detection algorithm that combines template matching methods with feature-based methods via hierarchical MRF and MAP estimation. Hierarchical MRF and MAP estimation provide a flexible framework to incorporate various visual clues. The combination of template matching and feature detection helps to achieve robustness against complex backgrounds and partial occlusions in object detection. Experimental results are given in the paper."
            },
            "slug": "Object-detection-using-hierarchical-MRF-and-MAP-Qian-Huang",
            "title": {
                "fragments": [],
                "text": "Object detection using hierarchical MRF and MAP estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A novel object detection algorithm that combines template matching methods with feature-based methods via hierarchical MRF and MAP estimation is presented, which helps to achieve robustness against complex backgrounds and partial occlusions in object detection."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731570"
                        ],
                        "name": "M. Lew",
                        "slug": "M.-Lew",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Lew",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lew"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14363121,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1e3cb790313c793cebfe86d5bfdcd300b0d759e1",
            "isKey": false,
            "numCitedBy": 51,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes information theoretic methods for the determination of the optimal subset of pixels for the problem of face detection in complex backgrounds. A view-based method is described which has limitations due to misalignments. This motivates the modular feature based method which minimizes the misalignment problem. Empirical comparisons between the view-based, modular, and sum of squared difference methods are made using four databases from three universities."
            },
            "slug": "Information-theoretic-view-based-and-modular-face-Lew",
            "title": {
                "fragments": [],
                "text": "Information theoretic view-based and modular face detection"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "Information theoretic methods for the determination of the optimal subset of pixels for the problem of face detection in complex backgrounds are described and the modular feature based method which minimizes the misalignment problem is described."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Second International Conference on Automatic Face and Gesture Recognition"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7343787"
                        ],
                        "name": "Michael D. Heath",
                        "slug": "Michael-D.-Heath",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Heath",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael D. Heath"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145306925"
                        ],
                        "name": "Sudeep Sarkar",
                        "slug": "Sudeep-Sarkar",
                        "structuredName": {
                            "firstName": "Sudeep",
                            "lastName": "Sarkar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sudeep Sarkar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3164225"
                        ],
                        "name": "T. Sanocki",
                        "slug": "T.-Sanocki",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Sanocki",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sanocki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143759604"
                        ],
                        "name": "K. Bowyer",
                        "slug": "K.-Bowyer",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Bowyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Bowyer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1169888,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "761d29cb1478ef9a28ead80bd799b02b41b7692a",
            "isKey": false,
            "numCitedBy": 554,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "A new method for evaluating edge detection algorithms is presented and applied to measure the relative performance of algorithms by Canny, Nalwa-Binford, Iverson-Zucker, Bergholm, and Rothwell. The basic measure of performance is a visual rating score which indicates the perceived quality of the edges for identifying an object. The process of evaluating edge detection algorithms with this performance measure requires the collection of a set of gray-scale images, optimizing the input parameters for each algorithm, conducting visual evaluation experiments and applying statistical analysis methods. The novel aspect of this work is the use of a visual task and real images of complex scenes in evaluating edge detectors. The method is appealing because, by definition, the results agree with visual evaluations of the edge images."
            },
            "slug": "Robust-Visual-Method-for-Assessing-the-Relative-of-Heath-Sarkar",
            "title": {
                "fragments": [],
                "text": "Robust Visual Method for Assessing the Relative Performance of Edge-Detection Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "A new method for evaluating edge detection algorithms is presented and applied to measure the relative performance of algorithms by Canny, Nalwa-Binford, Iverson-Zucker, Bergholm, and Rothwell, and the results agree with visual evaluations of the edge images."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "117208225"
                        ],
                        "name": "Venu Govindaraju",
                        "slug": "Venu-Govindaraju",
                        "structuredName": {
                            "firstName": "Venu",
                            "lastName": "Govindaraju",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Venu Govindaraju"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696384"
                        ],
                        "name": "S. Srihari",
                        "slug": "S.-Srihari",
                        "structuredName": {
                            "firstName": "Sargur",
                            "lastName": "Srihari",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Srihari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35100689"
                        ],
                        "name": "D. Sher",
                        "slug": "D.-Sher",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Sher",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Sher"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 44712344,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "b7f8860bf69a05a6af7e6969d63ef68687913abe",
            "isKey": false,
            "numCitedBy": 128,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors adopted a model-based approach, where the shape of the object is defined in terms of several mini-templates. The mini-templates are abstract descriptions of simple geometric features like arcs and corners. Relationships between mini-templates are not rigid. Rather, they are represented by springs that allow deformation of a template in terms of its size and orientation. Cost functionals are determined empirically. The authors expect their system to generate candidate regions in a given photograph associated with a rank of its goodness.<<ETX>>"
            },
            "slug": "A-computational-model-for-face-location-Govindaraju-Srihari",
            "title": {
                "fragments": [],
                "text": "A computational model for face location"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "The authors adopted a model-based approach, where the shape of the object is defined in terms of several mini-templates, represented by springs that allow deformation of a template in Terms of its size and orientation."
            },
            "venue": {
                "fragments": [],
                "text": "[1990] Proceedings Third International Conference on Computer Vision"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1729041"
                        ],
                        "name": "J. Canny",
                        "slug": "J.-Canny",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Canny",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Canny"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 18
                            }
                        ],
                        "text": "The underlying assumption is based on the observation that humans can effortlessly detect faces and objects in different poses and lighting conditions and, so, there must exist properties or features which are invariant over these variabilities."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13284142,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fcf9fc4e23b45345c2404ce7d6cb0fc9dea2c9ec",
            "isKey": false,
            "numCitedBy": 27662,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a computational approach to edge detection. The success of the approach depends on the definition of a comprehensive set of goals for the computation of edge points. These goals must be precise enough to delimit the desired behavior of the detector while making minimal assumptions about the form of the solution. We define detection and localization criteria for a class of edges, and present mathematical forms for these criteria as functionals on the operator impulse response. A third criterion is then added to ensure that the detector has only one response to a single edge. We use the criteria in numerical optimization to derive detectors for several common image features, including step edges. On specializing the analysis to step edges, we find that there is a natural uncertainty principle between detection and localization performance, which are the two main goals. With this principle we derive a single operator shape which is optimal at any scale. The optimal detector has a simple approximate implementation in which edges are marked at maxima in gradient magnitude of a Gaussian-smoothed image. We extend this simple detector using operators of several widths to cope with different signal-to-noise ratios in the image. We present a general method, called feature synthesis, for the fine-to-coarse integration of information from operators at different scales. Finally we show that step edge detector performance improves considerably as the operator point spread function is extended along the edge."
            },
            "slug": "A-Computational-Approach-to-Edge-Detection-Canny",
            "title": {
                "fragments": [],
                "text": "A Computational Approach to Edge Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "There is a natural uncertainty principle between detection and localization performance, which are the two main goals, and with this principle a single operator shape is derived which is optimal at any scale."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713089"
                        ],
                        "name": "D. Huttenlocher",
                        "slug": "D.-Huttenlocher",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Huttenlocher",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Huttenlocher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2901136"
                        ],
                        "name": "G. Klanderman",
                        "slug": "G.-Klanderman",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Klanderman",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Klanderman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116003"
                        ],
                        "name": "W. Rucklidge",
                        "slug": "W.-Rucklidge",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Rucklidge",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Rucklidge"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8027136,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "85efeeb25d8e363606d94c8fadaa922ba9b93a37",
            "isKey": false,
            "numCitedBy": 3913,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "The Hausdorff distance measures the extent to which each point of a model set lies near some point of an image set and vice versa. Thus, this distance can be used to determine the degree of resemblance between two objects that are superimposed on one another. Efficient algorithms for computing the Hausdorff distance between all possible relative positions of a binary image and a model are presented. The focus is primarily on the case in which the model is only allowed to translate with respect to the image. The techniques are extended to rigid motion. The Hausdorff distance computation differs from many other shape comparison methods in that no correspondence between the model and the image is derived. The method is quite tolerant of small position errors such as those that occur with edge detectors and other feature extraction methods. It is shown that the method extends naturally to the problem of comparing a portion of a model against an image. >"
            },
            "slug": "Comparing-Images-Using-the-Hausdorff-Distance-Huttenlocher-Klanderman",
            "title": {
                "fragments": [],
                "text": "Comparing Images Using the Hausdorff Distance"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Efficient algorithms for computing the Hausdorff distance between all possible relative positions of a binary image and a model are presented and it is shown that the method extends naturally to the problem of comparing a portion of a model against an image."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1399006281"
                        ],
                        "name": "F. Fogelman-Souli\u00e9",
                        "slug": "F.-Fogelman-Souli\u00e9",
                        "structuredName": {
                            "firstName": "Fran\u00e7oise",
                            "lastName": "Fogelman-Souli\u00e9",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Fogelman-Souli\u00e9"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2885735"
                        ],
                        "name": "E. Viennet",
                        "slug": "E.-Viennet",
                        "structuredName": {
                            "firstName": "Emmanuel",
                            "lastName": "Viennet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Viennet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2059697180"
                        ],
                        "name": "Bertrand Lamy",
                        "slug": "Bertrand-Lamy",
                        "structuredName": {
                            "firstName": "Bertrand",
                            "lastName": "Lamy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bertrand Lamy"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 45254388,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fa5915225543e25f1c95a47406bc9a23829d5804",
            "isKey": false,
            "numCitedBy": 74,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "In practical applications, recognition accuracy is sometimes not the only criterion; capability to reject erroneous patterns might also be needed. We show that there is a trade-off between these two properties. An efficient solution to this trade-off is brought about by the use of different algorithms implemented in various modules, i.e. multi-modular architectures. We present a general mechanism for designing and training multi-modular architectures, integrating various neural networks into a unique pattern recognition system, which is globally trained. It is possible to realize, within the system, feature extraction and recognition in successive modules which are cooperatively trained. We discuss various rejection criteria for neural networks and multi-modular architectures. We then give two examples of such systems, study their rejection capabilities and show how to use them for segmentation. In handwritten optical character recognition, our system achieves performances at state-of-the-art level, but is eight times faster. In human face recognition, our system is intended to work in the real world."
            },
            "slug": "Multi-Modular-Neural-Network-Architectures:-in-and-Fogelman-Souli\u00e9-Viennet",
            "title": {
                "fragments": [],
                "text": "Multi-Modular Neural Network Architectures: Applications in Optical Character and Human Face Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A general mechanism for designing and training multi-modular architectures, integrating various neural networks into a unique pattern recognition system, which is globally trained and possible to realize, within the system, feature extraction and recognition in successive modules which are cooperatively trained."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Pattern Recognit. Artif. Intell."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1384255355"
                        ],
                        "name": "Aleix M. Martinez",
                        "slug": "Aleix-M.-Martinez",
                        "structuredName": {
                            "firstName": "Aleix M.",
                            "lastName": "Martinez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aleix M. Martinez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703247"
                        ],
                        "name": "A. Kak",
                        "slug": "A.-Kak",
                        "structuredName": {
                            "firstName": "Avinash",
                            "lastName": "Kak",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kak"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 5
                            }
                        ],
                        "text": "(See [97] for a discussion about training set size."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5523504,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d544475dc01daa0c4f9847ef72adb8878df8ce99",
            "isKey": false,
            "numCitedBy": 3193,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "In the context of the appearance-based paradigm for object recognition, it is generally believed that algorithms based on LDA (linear discriminant analysis) are superior to those based on PCA (principal components analysis). In this communication, we show that this is not always the case. We present our case first by using intuitively plausible arguments and, then, by showing actual results on a face database. Our overall conclusion is that when the training data set is small, PCA can outperform LDA and, also, that PCA is less sensitive to different training data sets."
            },
            "slug": "PCA-versus-LDA-Martinez-Kak",
            "title": {
                "fragments": [],
                "text": "PCA versus LDA"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "The overall conclusion is that when the training data set is small, PCA can outperform LDA and, also, that PCA is less sensitive to different training data sets."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47953439"
                        ],
                        "name": "R\u00e9gis Vaillant",
                        "slug": "R\u00e9gis-Vaillant",
                        "structuredName": {
                            "firstName": "R\u00e9gis",
                            "lastName": "Vaillant",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R\u00e9gis Vaillant"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3208918"
                        ],
                        "name": "C. Monrocq",
                        "slug": "C.-Monrocq",
                        "structuredName": {
                            "firstName": "Christophe",
                            "lastName": "Monrocq",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Monrocq"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9326933"
                        ],
                        "name": "Y. L. Cun",
                        "slug": "Y.-L.-Cun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "Cun",
                            "middleNames": [
                                "le"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. L. Cun"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62763570,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "09ebd9ad4fa21c0d56433ac57a4cd69e94c72281",
            "isKey": false,
            "numCitedBy": 286,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "An original approach is presented for the localisation of objects in an image which approach is neuronal and has two steps. In the first step, a rough localisation is performed by presenting each pixel with its neighbourhood to a neural net which is able to indicate whether this pixel and its neighbourhood are the image of the search object. This first filter does not discriminate for position. From its result, areas which might contain an image of the object can be selected. In the second step, these areas are presented to another neural net which can determine the exact position of the object in each area. This algorithm is applied to the problem of localising faces in images."
            },
            "slug": "Original-approach-for-the-localisation-of-objects-Vaillant-Monrocq",
            "title": {
                "fragments": [],
                "text": "Original approach for the localisation of objects in images"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "An original approach is presented for the localisation of objects in an image which approach is neuronal and has two steps and is applied to the problem of localising faces in images."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700567"
                        ],
                        "name": "S. Satoh",
                        "slug": "S.-Satoh",
                        "structuredName": {
                            "firstName": "Shin\u2019ichi",
                            "lastName": "Satoh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Satoh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "24265259"
                        ],
                        "name": "Yuichi Nakamura",
                        "slug": "Yuichi-Nakamura",
                        "structuredName": {
                            "firstName": "Yuichi",
                            "lastName": "Nakamura",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuichi Nakamura"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733113"
                        ],
                        "name": "T. Kanade",
                        "slug": "T.-Kanade",
                        "structuredName": {
                            "firstName": "Takeo",
                            "lastName": "Kanade",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanade"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 90
                            }
                        ],
                        "text": "Several color spaces have been utilized to label pixels as skin including RGB [66], [67], [137], normalized RGB [102], [29], [149], [172], [30], [105], [171], [77], [151], [120], HSV (or HSI) [138], [79], [147], [146], YCrCb [167], [17], YIQ [31], [32], YES [131], CIE XYZ [19], and CIE LUV [173]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14592234,
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "id": "fad265c5e322a6740009a12edc173860f5f516b1",
            "isKey": false,
            "numCitedBy": 311,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We developed Name-It, a system that associates faces and names in news videos. It processes information from the videos and can infer possible name candidates for a given face or locate a face in news videos by name. To accomplish this task, the system takes a multimodal video analysis approach: face sequence extraction and similarity evaluation from videos, name extraction from transcripts, and video-caption recognition."
            },
            "slug": "Name-It:-Naming-and-Detecting-Faces-in-News-Videos-Satoh-Nakamura",
            "title": {
                "fragments": [],
                "text": "Name-It: Naming and Detecting Faces in News Videos"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "Name-It, a system that associates faces and names in news videos, takes a multimodal video analysis approach: face sequence extraction and similarity evaluation from videos, name extraction from transcripts, and video-caption recognition."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Multim."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144016256"
                        ],
                        "name": "D. Forsyth",
                        "slug": "D.-Forsyth",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Forsyth",
                            "middleNames": [
                                "Alexander"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Forsyth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14341320,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "71da81d32922bc2df46b0f1cfc508f5169fb4685",
            "isKey": false,
            "numCitedBy": 613,
            "numCiting": 78,
            "paperAbstract": {
                "fragments": [],
                "text": "AbstractColor constancy is the skill by which it is possible to tell the color of an object even under a colored light. I interpret the color of an object as its color under a fixed canonical light, rather than as a surface reflectance function. This leads to an analysis that shows two distinct sets of circumstances under which color constancy is possible. In this framework, color constancy requires estimating the illuminant under which the image was taken. The estimate is then used to choose one of a set of linear maps, which is applied to the image to yield a color descriptor at each point. This set of maps is computed in advance.The illuminant can be estimated using image measurements alone, because, given a number of weak assumptions detailed in the text, the color of the illuminant is constrained by the colors observed in the image. This constraint arises from the fact that surfaces can reflect no more light than is cast on them. For example, if one observes a patch that excites the red receptor strongly, the illuminant cannot have been deep blue.Two algorithms are possible using this constraint, corresponding to different assumptions about the world. The first algorithm, Crule will work for any surface reflectance. Crule corresponds to a form of coefficient rule, but obtains the coefficients by using constraints on illuminant color. The set of illuminants for which Crule will be successful depends strongly on the choice of photoreceptors: for narrowband photoreceptors, Crule will work in an unrestricted world. The second algorithm, Mwext, requires that both surface reflectances and illuminants be chosen from finite dimensional spaces; but under these restrictive conditions it can recover a large number of parameters in the illuminant, and is not an attractive model of human color constancy.\nCrule has been tested on real images of Mondriaans, and works well. I show results for Crule and for the Retinex algorithm of Land (Land 1971; Land 1983; Land 1985) operating on a number of real images. The experimental work shows that for good constancy, a color constancy system will need to adjust the gain of the receptors it employs in a fashion analagous to adaptation in humans."
            },
            "slug": "A-novel-algorithm-for-color-constancy-Forsyth",
            "title": {
                "fragments": [],
                "text": "A novel algorithm for color constancy"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The experimental work shows that for good constancy, a color constancy system will need to adjust the gain of the receptors it employs in a fashion analagous to adaptation in humans."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780935"
                        ],
                        "name": "B. Moghaddam",
                        "slug": "B.-Moghaddam",
                        "structuredName": {
                            "firstName": "Baback",
                            "lastName": "Moghaddam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Moghaddam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144994682"
                        ],
                        "name": "A. Pentland",
                        "slug": "A.-Pentland",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Pentland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pentland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 483975,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "74b312560b79929540734067e58de46966b96130",
            "isKey": false,
            "numCitedBy": 1684,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an unsupervised technique for visual learning, which is based on density estimation in high-dimensional spaces using an eigenspace decomposition. Two types of density estimates are derived for modeling the training data: a multivariate Gaussian (for unimodal distributions) and a mixture-of-Gaussians model (for multimodal distributions). Those probability densities are then used to formulate a maximum-likelihood estimation framework for visual search and target detection for automatic object recognition and coding. Our learning technique is applied to the probabilistic visual modeling, detection, recognition, and coding of human faces and nonrigid objects, such as hands."
            },
            "slug": "Probabilistic-Visual-Learning-for-Object-Moghaddam-Pentland",
            "title": {
                "fragments": [],
                "text": "Probabilistic Visual Learning for Object Representation"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "An unsupervised technique for visual learning is presented, which is based on density estimation in high-dimensional spaces using an eigenspace decomposition and is applied to the probabilistic visual modeling, detection, recognition, and coding of human faces and nonrigid objects."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "21472040"
                        ],
                        "name": "Irfan Essa",
                        "slug": "Irfan-Essa",
                        "structuredName": {
                            "firstName": "Irfan",
                            "lastName": "Essa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Irfan Essa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144994682"
                        ],
                        "name": "A. Pentland",
                        "slug": "A.-Pentland",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Pentland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pentland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Index Terms\u2014Face detection, face recognition, object recognition, view-based recognition, statistical pattern recognition, machine learning."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18558796,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6d651edcb564547b960a8b8a8f72116986657470",
            "isKey": false,
            "numCitedBy": 291,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Previous efforts at facial expression recognition have been based on the Facial Action Coding System (FACS), a representation developed in order to allow human psychologists to code expression from static facial \"mugshots.\" We develop new more accurate representations for facial expression by building a video database of facial expressions and then probabilistically characterizing the facial muscle activation associated with each expression using a detailed physical model of the skin and muscles. This produces a muscle based representation of facial motion, which is then used to recognize facial expressions in two different ways. The first method uses the physics based model directly, by recognizing expressions through comparison of estimated muscle activations. The second method uses the physics based model to generate spatio temporal motion energy templates of the whole face for each different expression. These simple, biologically plausible motion energy \"templates\" are then used for recognition. Both methods show substantially greater accuracy at expression recognition than has been previously achieved.<<ETX>>"
            },
            "slug": "Facial-expression-recognition-using-a-dynamic-model-Essa-Pentland",
            "title": {
                "fragments": [],
                "text": "Facial expression recognition using a dynamic model and motion energy"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "New more accurate representations for facial expression are developed by building a video database of facial expressions and then probabilistically characterizing the facial muscle activation associated with each expression using a detailed physical model of the skin and muscles."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE International Conference on Computer Vision"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3430625"
                        ],
                        "name": "R. F\u00e9raud",
                        "slug": "R.-F\u00e9raud",
                        "structuredName": {
                            "firstName": "Rapha\u00ebl",
                            "lastName": "F\u00e9raud",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. F\u00e9raud"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3118328"
                        ],
                        "name": "O. Bernier",
                        "slug": "O.-Bernier",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Bernier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Bernier"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14652912,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fab5729c8fb48b5542c393fac36debb75969e033",
            "isKey": false,
            "numCitedBy": 13,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "A new learning model based on autoassociative neural networks is developped and applied to face detection. To extend the detection ability in orientation and to decrease the number of false alarms, different combinations of networks are tested: ensemble, conditional ensemble and conditional mixture of networks. The use of a conditional mixture of networks allows to obtain state of the art results on different benchmark face databases."
            },
            "slug": "Ensemble-and-Modular-Approaches-for-Face-Detection:-F\u00e9raud-Bernier",
            "title": {
                "fragments": [],
                "text": "Ensemble and Modular Approaches for Face Detection: A Comparison"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A new learning model based on autoassociative neural networks is developped and applied to face detection and the use of a conditional mixture of networks allows to obtain state of the art results on different benchmark face databases."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4801654"
                        ],
                        "name": "Y. Amit",
                        "slug": "Y.-Amit",
                        "structuredName": {
                            "firstName": "Yali",
                            "lastName": "Amit",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Amit"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35651617"
                        ],
                        "name": "Kenneth Wilder",
                        "slug": "Kenneth-Wilder",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Wilder",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenneth Wilder"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 28
                            }
                        ],
                        "text": "Detection follows two stages: focusing and intensive classification."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2023689,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9f7c640ea1fe32e017c68005ef5e18969039b3f4",
            "isKey": false,
            "numCitedBy": 209,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a very large family of binary features for two-dimensional shapes. The salient ones for separating particular shapes are determined by inductive learning during the construction of classification trees. There is a feature for every possible geometric arrangement of local topographic codes. The arrangements express coarse constraints on relative angles and distances among the code locations and are nearly invariant to substantial affine and nonlinear deformations. They are also partially ordered, which makes it possible to narrow the search for informative ones at each node of the tree. Different trees correspond to different aspects of shape. They are statistically and weakly dependent due to randomization and are aggregated in a simple way. Adapting the algorithm to a shape family is then fully automatic once training samples are provided. As an illustration, we classified handwritten digits from the NIST database; the error rate was 0.7 percent."
            },
            "slug": "Joint-Induction-of-Shape-Features-and-Tree-Amit-Wilder",
            "title": {
                "fragments": [],
                "text": "Joint Induction of Shape Features and Tree Classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "A very large family of binary features for two-dimensional shapes determined by inductive learning during the construction of classification trees is introduced, which makes it possible to narrow the search for informative ones at each node of the tree."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781060"
                        ],
                        "name": "M. Venkatraman",
                        "slug": "M.-Venkatraman",
                        "structuredName": {
                            "firstName": "Mahesh",
                            "lastName": "Venkatraman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Venkatraman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723877"
                        ],
                        "name": "V. Govindaraju",
                        "slug": "V.-Govindaraju",
                        "structuredName": {
                            "firstName": "Venu",
                            "lastName": "Govindaraju",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Govindaraju"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 43781938,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ed361263f4d16e2dfe3186aeea4b410848c89095",
            "isKey": false,
            "numCitedBy": 13,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we address the task of segmentation of objects from photographs. A method of extraction of features based on the zero-crossings of a wavelet transform is described. The wavelet transform basis functions are derived from the second derivative of a Gaussian function. The extracted features are then used in a multilevel hypothesis generate and test algorithm to locate the objects of interest. The matching algorithm is based on the springs and templates framework of Fischler and Eschlanger (1973). The zero-crossings of the wavelet coefficients at different scales are combined in the model-matching stage to generate possible candidates. We apply this method to segment human faces from newspaper photographs."
            },
            "slug": "Zero-crossings-of-a-non-orthogonal-wavelet-for-Venkatraman-Govindaraju",
            "title": {
                "fragments": [],
                "text": "Zero crossings of a non-orthogonal wavelet transform for object location"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "This paper addresses the task of segmentation of objects from photographs by method of extraction of features based on the zero-crossings of a wavelet transform, and applies this method to segment human faces from newspaper photographs."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings., International Conference on Image Processing"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745250"
                        ],
                        "name": "F. Leymarie",
                        "slug": "F.-Leymarie",
                        "structuredName": {
                            "firstName": "Frederic",
                            "lastName": "Leymarie",
                            "middleNames": [
                                "Fol"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Leymarie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3631473"
                        ],
                        "name": "M. Levine",
                        "slug": "M.-Levine",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Levine",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Levine"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 34118350,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "ef4d4bf00727dfd34cf4f87308fd40e5d64e4b86",
            "isKey": false,
            "numCitedBy": 670,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "The problems of segmenting a noisy intensity image and tracking a nonrigid object in the plane are discussed. In evaluating these problems, a technique based on an active contour model commonly called a snake is examined. The technique is applied to cell locomotion and tracking studies. The snake permits both the segmentation and tracking problems to be simultaneously solved in constrained cases. A detailed analysis of the snake model, emphasizing its limitations and shortcomings, is presented, and improvements to the original description of the model are proposed. Problems of convergence of the optimization scheme are considered. In particular, an improved terminating criterion for the optimization scheme that is based on topographic features of the graph of the intensity image is proposed. Hierarchical filtering methods, as well as a continuation method based on a discrete sale-space representation, are discussed. Results for both segmentation and tracking are presented. Possible failures of the method are discussed. >"
            },
            "slug": "Tracking-Deformable-Objects-in-the-Plane-Using-an-Leymarie-Levine",
            "title": {
                "fragments": [],
                "text": "Tracking Deformable Objects in the Plane Using an Active Contour Model"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "An improved terminating criterion for the optimization scheme that is based on topographic features of the graph of the intensity image is proposed, as well as a continuation method based on a discrete sale-space representation."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111048229"
                        ],
                        "name": "Kyujin Cho",
                        "slug": "Kyujin-Cho",
                        "structuredName": {
                            "firstName": "Kyujin",
                            "lastName": "Cho",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kyujin Cho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145776090"
                        ],
                        "name": "P. Meer",
                        "slug": "P.-Meer",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Meer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Meer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053191965"
                        ],
                        "name": "Javier Cabrera",
                        "slug": "Javier-Cabrera",
                        "structuredName": {
                            "firstName": "Javier",
                            "lastName": "Cabrera",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Javier Cabrera"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12597688,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0f7ff12f217d7c05322f2772c11e663f8046564a",
            "isKey": false,
            "numCitedBy": 84,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "A new performance evaluation paradigm for computer vision systems is proposed. In real situation, the complexity of the input data and/or of the computational procedure can make traditional error propagation methods infeasible. The new approach exploits a resampling technique recently introduced in statistics, the bootstrap. Distributions for the output variables are obtained by perturbing the nuisance properties of the input, i.e., properties with no relevance for the output under ideal conditions. From these bootstrap distributions, the confidence in the adequacy of the assumptions embedded into the computational procedure for the given input is derived. As an example, the new paradigm is applied to the task of edge detection. The performance of several edge detection methods is compared both for synthetic data and real images. The confidence in the output can be used to obtain an edgemap independent of the gradient magnitude."
            },
            "slug": "Performance-Assessment-Through-Bootstrap-Cho-Meer",
            "title": {
                "fragments": [],
                "text": "Performance Assessment Through Bootstrap"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "A new performance evaluation paradigm for computer vision systems is proposed that exploits a resampling technique recently introduced in statistics, the bootstrap, to derive confidence in the adequacy of the assumptions embedded into the computational procedure for the given input."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2995060"
                        ],
                        "name": "D. Swets",
                        "slug": "D.-Swets",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Swets",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Swets"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145926447"
                        ],
                        "name": "J. Weng",
                        "slug": "J.-Weng",
                        "structuredName": {
                            "firstName": "Juyang",
                            "lastName": "Weng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weng"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 10952196,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5e28e81e757009d2f76b8674e0da431f5845884a",
            "isKey": false,
            "numCitedBy": 1773,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes the automatic selection of features from an image training set using the theories of multidimensional discriminant analysis and the associated optimal linear projection. We demonstrate the effectiveness of these most discriminating features for view-based class retrieval from a large database of widely varying real-world objects presented as \"well-framed\" views, and compare it with that of the principal component analysis."
            },
            "slug": "Using-Discriminant-Eigenfeatures-for-Image-Swets-Weng",
            "title": {
                "fragments": [],
                "text": "Using Discriminant Eigenfeatures for Image Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "This paper describes the automatic selection of features from an image training set using the theories of multidimensional discriminant analysis and the associated optimal linear projection, and demonstrates the effectiveness of these most discriminating features for view-based class retrieval from a large database of widely varying real-world objects."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3118328"
                        ],
                        "name": "O. Bernier",
                        "slug": "O.-Bernier",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Bernier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Bernier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2586567"
                        ],
                        "name": "M. Collobert",
                        "slug": "M.-Collobert",
                        "structuredName": {
                            "firstName": "Michel",
                            "lastName": "Collobert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collobert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3430625"
                        ],
                        "name": "R. F\u00e9raud",
                        "slug": "R.-F\u00e9raud",
                        "structuredName": {
                            "firstName": "Rapha\u00ebl",
                            "lastName": "F\u00e9raud",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. F\u00e9raud"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066206356"
                        ],
                        "name": "V. Lemaire",
                        "slug": "V.-Lemaire",
                        "structuredName": {
                            "firstName": "V",
                            "lastName": "Lemaire",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Lemaire"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35094984"
                        ],
                        "name": "J. Viallet",
                        "slug": "J.-Viallet",
                        "structuredName": {
                            "firstName": "Jean-Emmanuel",
                            "lastName": "Viallet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Viallet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2444622"
                        ],
                        "name": "D. Collobert",
                        "slug": "D.-Collobert",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Collobert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Collobert"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 8742282,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "242c7e54293575318f594d7808136c81d2be1994",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "A real-time system is described for automatic detection and tracking of multiple persons, in the context of video-conferencing systems. This system, called MULTRAK (multiperson locating and tracking automatic kernel) is able to continuously detect and track the position of faces in its field of view. The heart of the system as a modular neural network based face detector, giving fast and accurate face detection."
            },
            "slug": "MULTRAK:-a-system-for-automatic-multiperson-and-in-Bernier-Collobert",
            "title": {
                "fragments": [],
                "text": "MULTRAK: a system for automatic multiperson localization and tracking in real-time"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "A real-time system is described for automatic detection and tracking of multiple persons, in the context of video-conferencing systems, able to continuously detect and track the position of faces in its field of view."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 1998 International Conference on Image Processing. ICIP98 (Cat. No.98CB36269)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715634"
                        ],
                        "name": "Ming-Hsuan Yang",
                        "slug": "Ming-Hsuan-Yang",
                        "structuredName": {
                            "firstName": "Ming-Hsuan",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming-Hsuan Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145237406"
                        ],
                        "name": "N. Ahuja",
                        "slug": "N.-Ahuja",
                        "structuredName": {
                            "firstName": "Narendra",
                            "lastName": "Ahuja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Ahuja"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14053944,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4de2e3e2873571a7fa145c1a1febe1b28e472b8e",
            "isKey": false,
            "numCitedBy": 364,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper is concerned with estimating a probability density function of human skin color, using a finite Gaussian mixture model, whose parameters are estimated through the EM algorithm. Hawkins' statistical test on the normality and homoscedasticity (common covariance matrix) of the estimated Gaussian mixture models is performed and McLachlan's bootstrap method is used to test the number of components in a mixture. Experimental results show that the estimated Gaussian mixture model fits skin images from a large database. Applications of the estimated density function in image and video databases are presented."
            },
            "slug": "Gaussian-mixture-model-for-human-skin-color-and-its-Yang-Ahuja",
            "title": {
                "fragments": [],
                "text": "Gaussian mixture model for human skin color and its applications in image and video databases"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Experimental results show that the estimated Gaussian mixture model fits skin images from a large database and applications of the estimated density function in image and video databases are presented."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38995786"
                        ],
                        "name": "S. Pigeon",
                        "slug": "S.-Pigeon",
                        "structuredName": {
                            "firstName": "St\u00e9phane",
                            "lastName": "Pigeon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Pigeon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698047"
                        ],
                        "name": "L. Vandendorpe",
                        "slug": "L.-Vandendorpe",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Vandendorpe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Vandendorpe"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 45537808,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "75898aaf072431a4355b5c86ad6c53c004e55787",
            "isKey": false,
            "numCitedBy": 156,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "The primary goal of the M2VTS project is to address the issue of secured access to buildings or multi-media services by the use of automatic person verification based on multimodal strategies (secured access based on speech, face images and other information). This paper presents an overview of the multimodal face database recorded at UCL premises for the purpose of research applications inside the M2VTS project. This database offers synchronized video and speech data as well as image sequences allowing to access multiple views of a face. This material should permit the design and the testing of identification strategies based on speech andro labial analysis, frontal and/or profile face analysis as well as 3-D analysis thanks to the multiple views. The M2VTS Database is available to any non-commercial user on request to the European Language Resource Agency."
            },
            "slug": "The-M2VTS-Multimodal-Face-Database-(Release-1.00)-Pigeon-Vandendorpe",
            "title": {
                "fragments": [],
                "text": "The M2VTS Multimodal Face Database (Release 1.00)"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper presents an overview of the multimodal face database recorded at UCL premises for the purpose of research applications inside the M2VTS project, offering synchronized video and speech data as well as image sequences allowing to access multiple views of a face."
            },
            "venue": {
                "fragments": [],
                "text": "AVBPA"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790646"
                        ],
                        "name": "P. Dayan",
                        "slug": "P.-Dayan",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Dayan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Dayan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2713302"
                        ],
                        "name": "M. Revow",
                        "slug": "M.-Revow",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Revow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Revow"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 996158,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "62f4d89a3c1441b47170c7e1380137fb388d0799",
            "isKey": false,
            "numCitedBy": 416,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes two new methods for modeling the manifolds of digitized images of handwritten digits. The models allow a priori information about the structure of the manifolds to be combined with empirical data. Accurate modeling of the manifolds allows digits to be discriminated using the relative probability densities under the alternative models. One of the methods is grounded in principal components analysis, the other in factor analysis. Both methods are based on locally linear low-dimensional approximations to the underlying data manifold. Links with other methods that model the manifold are discussed."
            },
            "slug": "Modeling-the-manifolds-of-images-of-handwritten-Hinton-Dayan",
            "title": {
                "fragments": [],
                "text": "Modeling the manifolds of images of handwritten digits"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "Two new methods for modeling the manifolds of digitized images of handwritten digits of principal components analysis and factor analysis are described, based on locally linear low-dimensional approximations to the underlying data manifold."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2910910"
                        ],
                        "name": "Jefferey A. Shufelt",
                        "slug": "Jefferey-A.-Shufelt",
                        "structuredName": {
                            "firstName": "Jefferey",
                            "lastName": "Shufelt",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jefferey A. Shufelt"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 19092404,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "4e7127bf6010be268e37cb10da16dd914dab5b38",
            "isKey": false,
            "numCitedBy": 209,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Research in monocular building extraction from aerial imagery has neglected performance evaluation in three areas: unbiased metrics for quantifying detection and delineation performance, an evaluation methodology for applying these metrics to a representative body of test imagery, and an approach for understanding the impact of image and scene content on building extraction algorithms. This paper addresses these areas with an end-to-end performance evaluation of four existing monocular building extraction systems, using image space and object space-based metrics on 83 test images of 18 sites. This analysis is supplemented by an examination of the effects of image obliquity and object complexity on system performance, as well as a case study on the effects of edge fragmentation. This widely applicable performance evaluation approach highlights the consequences of various traditional assumptions about camera geometry, image content and scene structure, and demonstrates the utility of rigorous photogrammetric object space modeling and primitive-based representations for building extraction."
            },
            "slug": "Performance-Evaluation-and-Analysis-of-Monocular-Shufelt",
            "title": {
                "fragments": [],
                "text": "Performance Evaluation and Analysis of Monocular Building Extraction From Aerial Imagery"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "An end-to-end performance evaluation of four existing monocular building extraction systems, using image space and object space-based metrics on 83 test images of 18 sites, and demonstrates the utility of rigorous photogrammetric object space modeling and primitive-based representations for building extraction."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144994682"
                        ],
                        "name": "A. Pentland",
                        "slug": "A.-Pentland",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Pentland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pentland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2591356,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f2b7a0c608ab2557b8897cddd1dd7ebd56978a85",
            "isKey": false,
            "numCitedBy": 458,
            "numCiting": 259,
            "paperAbstract": {
                "fragments": [],
                "text": "The research topic of looking at people, that is, giving machines the ability to detect, track, and identify people and more generally, to interpret human behavior, has become a central topic in machine vision research. Initially thought to be the research problem that would be hardest to solve, it has proven remarkably tractable and has even spawned several thriving commercial enterprises. The principle driving application for this technology is \"fourth generation\" embedded computing: \"smart\" environments and portable or wearable devices. The key technical goals are to determine the computer's context with respect to nearby humans (e.g., who, what, when, where, and why) so that the computer can act or respond appropriately without detailed instructions. The paper examines the mathematical tools that have proven successful, provides a taxonomy of the problem domain, and then examines the state of the art. Four areas receive particular attention: person identification, surveillance/monitoring, 3D methods, and smart rooms/perceptual user interfaces. Finally, the paper discusses some of the research challenges and opportunities."
            },
            "slug": "Looking-at-People:-Sensing-for-Ubiquitous-and-Pentland",
            "title": {
                "fragments": [],
                "text": "Looking at People: Sensing for Ubiquitous and Wearable Computing"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The paper examines the mathematical tools that have proven successful, provides a taxonomy of the problem domain, and then examines the state of the art: person identification, surveillance/monitoring, 3D methods, and smart rooms/perceptual user interfaces."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736143"
                        ],
                        "name": "Constantine Kotropoulos",
                        "slug": "Constantine-Kotropoulos",
                        "structuredName": {
                            "firstName": "Constantine",
                            "lastName": "Kotropoulos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Constantine Kotropoulos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737071"
                        ],
                        "name": "A. Tefas",
                        "slug": "A.-Tefas",
                        "structuredName": {
                            "firstName": "Anastasios",
                            "lastName": "Tefas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Tefas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144064571"
                        ],
                        "name": "I. Pitas",
                        "slug": "I.-Pitas",
                        "structuredName": {
                            "firstName": "Ioannis",
                            "lastName": "Pitas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Pitas"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 18823033,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "beaf4be5f96b32447bcac5f2e972016a6502d198",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Two variants of dynamic link matching based on mathematical morphology are developed and tested for frontal face authentication, namely, the morphological dynamic link architecture and the morphological signal decomposition-dynamic link architecture. Local coefficients which weigh the contribution of each node in elastic graph matching according to its discriminatory power are derived. The performance of the proposed algorithms is evaluated in terms of their receiver operating characteristic and the equal error rate (EER) achieved in the M2VTS database. The comparison with other frontal face authentication algorithms developed within M2VTS project indicates that morphological dynamic link architecture with discriminatory power coefficients is ranked as the best algorithm in terms of the EER."
            },
            "slug": "Frontal-face-authentication-using-variants-of-link-Kotropoulos-Tefas",
            "title": {
                "fragments": [],
                "text": "Frontal face authentication using variants of dynamic link matching based on mathematical morphology"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "The comparison with other frontal face authentication algorithms developed within M2VTS project indicates that morphological dynamic link architecture with discriminatory power coefficients is ranked as the best algorithm in terms of the EER."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 1998 International Conference on Image Processing. ICIP98 (Cat. No.98CB36269)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102881550"
                        ],
                        "name": "D. Kendall",
                        "slug": "D.-Kendall",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Kendall",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Kendall"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 50702498,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "9367399b05cc0193396275476e9734c3be3c8313",
            "isKey": false,
            "numCitedBy": 1393,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "The shape-space l. k m whose points a represent the shapes of not totally degenerate /c-ads in IR m is introduced as a quotient space carrying the quotient metric. When m = 1, we find that Y\\ = S k ~ 2 ; when m ^ 3, the shape-space contains singularities. This paper deals mainly with the case m = 2, when the shape-space I* ca n be identified with a version of CP*~ 2 . Of special importance are the shape-measures induced on CP k ~ 2 by any assigned diffuse law of distribution for the k vertices. We determine several such shape-measures, we resolve some of the technical problems associated with the graphic presentation and statistical analysis of empirical shape distributions, and among applications we discuss the relevance of these ideas to testing for the presence of non-accidental multiple alignments in collections of (i) neolithic stone monuments and (ii) quasars. Finally the recently introduced Ambartzumian density is examined from the present point of view, its norming constant is found, and its connexion with random Crofton polygons is established."
            },
            "slug": "SHAPE-MANIFOLDS,-PROCRUSTEAN-METRICS,-AND-COMPLEX-Kendall",
            "title": {
                "fragments": [],
                "text": "SHAPE MANIFOLDS, PROCRUSTEAN METRICS, AND COMPLEX PROJECTIVE SPACES"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745672"
                        ],
                        "name": "R. Cipolla",
                        "slug": "R.-Cipolla",
                        "structuredName": {
                            "firstName": "Roberto",
                            "lastName": "Cipolla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Cipolla"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145162067"
                        ],
                        "name": "A. Blake",
                        "slug": "A.-Blake",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Blake",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Blake"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 48
                            }
                        ],
                        "text": "Each facial feature and grouping is then evaluated using a Bayesian network."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 33185112,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "6178079398696f058c0c1e85120aeb465bbc778d",
            "isKey": false,
            "numCitedBy": 157,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors develop previous theories of the analysis of deformation of apparent contours under viewer motion. Earlier results showing how surface curvature can be inferred from acceleration of image features are generalized for arbitrary viewer motion and perspective projection. It is shown that relative image acceleration, based on parallax measurements, is robust to uncertainties in robot motion. The theory has been implemented and extensively tested in a real-time (15 frames per second) tracking system based on deformable contours (snakes). It is shown that focusing attention by means of snakes allows rapid, robust computation of surface curvature, including discrimination of extremal and occluding contours.<<ETX>>"
            },
            "slug": "The-dynamic-analysis-of-apparent-contours-Cipolla-Blake",
            "title": {
                "fragments": [],
                "text": "The dynamic analysis of apparent contours"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown that focusing attention by means of snakes allows rapid, robust computation of surface curvature, including discrimination of extremal and occluding contours."
            },
            "venue": {
                "fragments": [],
                "text": "[1990] Proceedings Third International Conference on Computer Vision"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143602141"
                        ],
                        "name": "M. Kass",
                        "slug": "M.-Kass",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Kass",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kass"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1809905"
                        ],
                        "name": "A. Witkin",
                        "slug": "A.-Witkin",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Witkin",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Witkin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1750924"
                        ],
                        "name": "Demetri Terzopoulos",
                        "slug": "Demetri-Terzopoulos",
                        "structuredName": {
                            "firstName": "Demetri",
                            "lastName": "Terzopoulos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Demetri Terzopoulos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12849354,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "9394a5d5adcb626128b6a42c8810b9505a3c6487",
            "isKey": false,
            "numCitedBy": 15501,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "A snake is an energy-minimizing spline guided by external constraint forces and influenced by image forces that pull it toward features such as lines and edges. Snakes are active contour models: they lock onto nearby edges, localizing them accurately. Scale-space continuation can be used to enlarge the capture region surrounding a feature. Snakes provide a unified account of a number of visual problems, including detection of edges, lines, and subjective contours; motion tracking; and stereo matching. We have used snakes successfully for interactive interpretation, in which user-imposed constraint forces guide the snake near features of interest."
            },
            "slug": "Snakes:-Active-contour-models-Kass-Witkin",
            "title": {
                "fragments": [],
                "text": "Snakes: Active contour models"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This work uses snakes for interactive interpretation, in which user-imposed constraint forces guide the snake near features of interest, and uses scale-space continuation to enlarge the capture region surrounding a feature."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053520352"
                        ],
                        "name": "M. Kirby",
                        "slug": "M.-Kirby",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Kirby",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kirby"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49555086"
                        ],
                        "name": "L. Sirovich",
                        "slug": "L.-Sirovich",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Sirovich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Sirovich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 137
                            }
                        ],
                        "text": "They used a point distribution model (PDM) to characterize the shape vectors over an ensemble of individuals, and an approach similar to Kirby and Sirovich [78] to represent shapenormalized intensity appearance."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 87
                            }
                        ],
                        "text": "vectors over an ensemble of individuals, and an approach similar to Kirby and Sirovich [78] to represent shapenormalized intensity appearance."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 11
                            }
                        ],
                        "text": "Similar to [78], principal component analysis on a training set of face images is performed to generate the Eigenpictures (here called Eigenfaces) which span a subspace (called the face space) of the image space."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 115
                            }
                        ],
                        "text": "Kirby and Sirovich demonstrated that images of faces can be linearly encoded using a modest number of basis images [78]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 570648,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "66d75a5fe9e1b6511c5135d68e9ce8c0da5a7374",
            "isKey": true,
            "numCitedBy": 2852,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "The use of natural symmetries (mirror images) in a well-defined family of patterns (human faces) is discussed within the framework of the Karhunen-Loeve expansion. This results in an extension of the data and imposes even and odd symmetry on the eigenfunctions of the covariance matrix, without increasing the complexity of the calculation. The resulting approximation of faces projected from outside of the data set onto this optimal basis is improved on average. >"
            },
            "slug": "Application-of-the-Karhunen-Loeve-Procedure-for-the-Kirby-Sirovich",
            "title": {
                "fragments": [],
                "text": "Application of the Karhunen-Loeve Procedure for the Characterization of Human Faces"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "The use of natural symmetries (mirror images) in a well-defined family of patterns (human faces) is discussed within the framework of the Karhunen-Loeve expansion, which results in an extension of the data and imposes even and odd symmetry on the eigenfunctions of the covariance matrix."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49168578"
                        ],
                        "name": "J. Phillips",
                        "slug": "J.-Phillips",
                        "structuredName": {
                            "firstName": "Jonathon",
                            "lastName": "Phillips",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Phillips"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144975752"
                        ],
                        "name": "V. Bruce",
                        "slug": "V.-Bruce",
                        "structuredName": {
                            "firstName": "Vicki",
                            "lastName": "Bruce",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Bruce"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66890955"
                        ],
                        "name": "F. F. Souli\u00e9",
                        "slug": "F.-F.-Souli\u00e9",
                        "structuredName": {
                            "firstName": "Fran\u00e7oise",
                            "lastName": "Souli\u00e9",
                            "middleNames": [
                                "Fogelman"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. F. Souli\u00e9"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60461382,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bfec6ad5ab86f9a7bf96184c2c58950a4d5876a8",
            "isKey": false,
            "numCitedBy": 553,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Face-Recognition:-From-Theory-to-Applications-Phillips-Bruce",
            "title": {
                "fragments": [],
                "text": "Face Recognition: From Theory to Applications"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737071"
                        ],
                        "name": "A. Tefas",
                        "slug": "A.-Tefas",
                        "structuredName": {
                            "firstName": "Anastasios",
                            "lastName": "Tefas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Tefas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736143"
                        ],
                        "name": "Constantine Kotropoulos",
                        "slug": "Constantine-Kotropoulos",
                        "structuredName": {
                            "firstName": "Constantine",
                            "lastName": "Kotropoulos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Constantine Kotropoulos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144064571"
                        ],
                        "name": "I. Pitas",
                        "slug": "I.-Pitas",
                        "structuredName": {
                            "firstName": "Ioannis",
                            "lastName": "Pitas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Pitas"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Index Terms\u2014Face detection, face recognition, object recognition, view-based recognition, statistical pattern recognition, machine learning."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3046394,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "566b1b80297ef257a81307771484beb8232c8ce0",
            "isKey": false,
            "numCitedBy": 21,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Two novel variants of dynamic link architecture that are based on mathematical morphology and incorporate coefficients which weigh the contribution of each node in elastic graph matching according to its discriminatory power are developed. They are the so called Morphological Dynamic Link Architecture and the Morphological Signal Decomposition-Dynamic Lint Architecture. The proposed variants are tested for face authentication in a cooperative scenario where the candidates claim an identity to be checked. Their performance is evaluated in terms of their receiver operating characteristic and the equal error rate achieved in M2VTS database. An equal error rate in the range 3.7-6.8% is reported."
            },
            "slug": "Variants-of-dynamic-link-architecture-based-on-for-Tefas-Kotropoulos",
            "title": {
                "fragments": [],
                "text": "Variants of dynamic link architecture based on mathematical morphology for frontal face authentication"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Two novel variants of dynamic link architecture that are based on mathematical morphology and incorporate coefficients which weigh the contribution of each node in elastic graph matching according to its discriminatory power are developed."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 1998 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.98CB36231)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35106875"
                        ],
                        "name": "R. Duda",
                        "slug": "R.-Duda",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Duda",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Duda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3108177"
                        ],
                        "name": "P. Hart",
                        "slug": "P.-Hart",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Hart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hart"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 112
                            }
                        ],
                        "text": "For each subclass, its density is modeled as a Gaussian whose parameters are estimated using maximum-likelihood [36]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 31
                            }
                        ],
                        "text": "Synthetic and real examples in [36], [37], [9], [7] have shown that the projected samples from different classes in the PCA subspace can often be smeared."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12946615,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b07ce649d6f6eb636872527104b0209d3edc8188",
            "isKey": false,
            "numCitedBy": 16926,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Provides a unified, comprehensive and up-to-date treatment of both statistical and descriptive methods for pattern recognition. The topics treated include Bayesian decision theory, supervised and unsupervised learning, nonparametric techniques, discriminant analysis, clustering, preprosessing of pictorial data, spatial filtering, shape description techniques, perspective transformations, projective invariants, linguistic procedures, and artificial intelligence techniques for scene analysis."
            },
            "slug": "Pattern-classification-and-scene-analysis-Duda-Hart",
            "title": {
                "fragments": [],
                "text": "Pattern classification and scene analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "The topics treated include Bayesian decision theory, supervised and unsupervised learning, nonparametric techniques, discriminant analysis, clustering, preprosessing of pictorial data, spatial filtering, shape description techniques, perspective transformations, projective invariants, linguistic procedures, and artificial intelligence techniques for scene analysis."
            },
            "venue": {
                "fragments": [],
                "text": "A Wiley-Interscience publication"
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145308667"
                        ],
                        "name": "Y. Miyake",
                        "slug": "Y.-Miyake",
                        "structuredName": {
                            "firstName": "Yoichi",
                            "lastName": "Miyake",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Miyake"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "89804514"
                        ],
                        "name": "H. Saitoh",
                        "slug": "H.-Saitoh",
                        "structuredName": {
                            "firstName": "Hiroyasu",
                            "lastName": "Saitoh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Saitoh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3224866"
                        ],
                        "name": "H. Yaguchi",
                        "slug": "H.-Yaguchi",
                        "structuredName": {
                            "firstName": "Hirohisa",
                            "lastName": "Yaguchi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Yaguchi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "65950288"
                        ],
                        "name": "N. Tsukada",
                        "slug": "N.-Tsukada",
                        "structuredName": {
                            "firstName": "Norishige",
                            "lastName": "Tsukada",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Tsukada"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 112
                            }
                        ],
                        "text": "Several color spaces have been utilized to label pixels as skin including RGB [66], [67], [137], normalized RGB [102], [29], [149], [172], [30], [105], [171], [77], [151], [120], HSV (or HSI) [138], [79], [147], [146], YCrCb [167], [17], YIQ [31], [32], YES [131], CIE XYZ [19], and CIE LUV [173]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 71
                            }
                        ],
                        "text": "Crowley and Coutaz used a histogram h\u00f0r; g\u00de of \u00f0r; g\u00de values in normalized RGB color space to obtain the probability of obtaining a particular RGBvector given that the pixel observes skin [29], [30]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 140
                            }
                        ],
                        "text": "Recently, Jones and Rehg conducted a large-scale experiment in which nearly 1 billion labeled skin tone pixels are collected (in normalized RGB color space) [69]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 44
                            }
                        ],
                        "text": "Using a Gaussian distribution in normalized RGB color space, segmented regions with a skinlike color are classified as faces."
                    },
                    "intents": []
                }
            ],
            "corpusId": 188485204,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "661f27fc978c26561812dc9bf89a752110ec87a0",
            "isKey": true,
            "numCitedBy": 43,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "A new method was introduced to detect skin color and facial pattern regions from NTSC television pictures. About 4000 skin colors in the TV picture were measured and the chromaticities r, g of those colors were calculated. A skin color chart was made from measured regions and color transformation matrix was calculated for printing from a TV picture"
            },
            "slug": "Facial-pattern-detection-and-color-correction-from-Miyake-Saitoh",
            "title": {
                "fragments": [],
                "text": "Facial pattern detection and color correction from television picture for newspaper printing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52131656"
                        ],
                        "name": "R. Kauth",
                        "slug": "R.-Kauth",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Kauth",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kauth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144994682"
                        ],
                        "name": "A. Pentland",
                        "slug": "A.-Pentland",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Pentland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pentland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117269371"
                        ],
                        "name": "G. Thomas",
                        "slug": "G.-Thomas",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Thomas",
                            "middleNames": [
                                "S.",
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Thomas"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 129379138,
            "fieldsOfStudy": [
                "Environmental Science",
                "Mathematics"
            ],
            "id": "47611be5b61d0e2edd076b32d50bd462534ce6e2",
            "isKey": false,
            "numCitedBy": 47,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "A basic concept of Multispectral Scanner data processing was developed for use in agricultural inventories; namely, to introduce spatial coordinates of each pixel into the vector description of the pixel and to use this information along with the spectral channel values in a conventional unsupervised clustering of the scene. The result is to isolate spectrally homogeneous field-like patches (called blobs). The spectral mean vector of a blob can be regarded as a defined feature and used in a conventional pattern recognition procedure. The benefits of use are: ease in locating training units in imagery; data compression of from 10 to 30 depending on the application; reduction of scanner noise and consequently potential improvements in classification/proportion estimation performances."
            },
            "slug": "Blob-An-unsupervised-clustering-approach-to-spatial-Kauth-Pentland",
            "title": {
                "fragments": [],
                "text": "Blob - An unsupervised clustering approach to spatial preprocessing of MSS imagery"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752722"
                        ],
                        "name": "F. Provost",
                        "slug": "F.-Provost",
                        "structuredName": {
                            "firstName": "Foster",
                            "lastName": "Provost",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Provost"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145421658"
                        ],
                        "name": "Tom Fawcett",
                        "slug": "Tom-Fawcett",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Fawcett",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tom Fawcett"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5415722,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "159e7b1128d122f2bde24edd2ad72b6eea750375",
            "isKey": false,
            "numCitedBy": 1268,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "In real-world environments it usually is difficult to specify target operating conditions precisely, for example, target misclassification costs. This uncertainty makes building robust classification systems problematic. We show that it is possible to build a hybrid classifier that will perform at least as well as the best available classifier for any target conditions. In some cases, the performance of the hybrid actually can surpass that of the best known classifier. This robust performance extends across a wide variety of comparison frameworks, including the optimization of metrics such as accuracy, expected cost, lift, precision, recall, and workforce utilization. The hybrid also is efficient to build, to store, and to update. The hybrid is based on a method for the comparison of classifier performance that is robust to imprecise class distributions and misclassification costs. The ROC convex hull (ROCCH) method combines techniques from ROC analysis, decision analysis and computational geometry, and adapts them to the particulars of analyzing learned classifiers. The method is efficient and incremental, minimizes the management of classifier performance data, and allows for clear visual comparisons and sensitivity analyses. Finally, we point to empirical evidence that a robust hybrid classifier indeed is needed for many real-world problems."
            },
            "slug": "Robust-Classification-for-Imprecise-Environments-Provost-Fawcett",
            "title": {
                "fragments": [],
                "text": "Robust Classification for Imprecise Environments"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown that it is possible to build a hybrid classifier that will perform at least as well as the best available classifier for any target conditions, and in some cases, the performance of the hybrid actually can surpass that of the best known classifier."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724972"
                        ],
                        "name": "A. Waibel",
                        "slug": "A.-Waibel",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Waibel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Waibel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40396597"
                        ],
                        "name": "Toshiyuki Hanazawa",
                        "slug": "Toshiyuki-Hanazawa",
                        "structuredName": {
                            "firstName": "Toshiyuki",
                            "lastName": "Hanazawa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Toshiyuki Hanazawa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9243990"
                        ],
                        "name": "K. Shikano",
                        "slug": "K.-Shikano",
                        "structuredName": {
                            "firstName": "Kiyohiro",
                            "lastName": "Shikano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Shikano"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49464494"
                        ],
                        "name": "Kevin J. Lang",
                        "slug": "Kevin-J.-Lang",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Lang",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin J. Lang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9563026,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cd62c9976534a6a2096a38244f6cbb03635a127e",
            "isKey": false,
            "numCitedBy": 2787,
            "numCiting": 92,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors present a time-delay neural network (TDNN) approach to phoneme recognition which is characterized by two important properties: (1) using a three-layer arrangement of simple computing units, a hierarchy can be constructed that allows for the formation of arbitrary nonlinear decision surfaces, which the TDNN learns automatically using error backpropagation; and (2) the time-delay arrangement enables the network to discover acoustic-phonetic features and the temporal relationships between them independently of position in time and therefore not blurred by temporal shifts in the input. As a recognition task, the speaker-dependent recognition of the phonemes B, D, and G in varying phonetic contexts was chosen. For comparison, several discrete hidden Markov models (HMM) were trained to perform the same task. Performance evaluation over 1946 testing tokens from three speakers showed that the TDNN achieves a recognition rate of 98.5% correct while the rate obtained by the best of the HMMs was only 93.7%. >"
            },
            "slug": "Phoneme-recognition-using-time-delay-neural-Waibel-Hanazawa",
            "title": {
                "fragments": [],
                "text": "Phoneme recognition using time-delay neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The authors present a time-delay neural network (TDNN) approach to phoneme recognition which is characterized by two important properties: using a three-layer arrangement of simple computing units, a hierarchy can be constructed that allows for the formation of arbitrary nonlinear decision surfaces, which the TDNN learns automatically using error backpropagation."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Acoust. Speech Signal Process."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145833095"
                        ],
                        "name": "S. Kothari",
                        "slug": "S.-Kothari",
                        "structuredName": {
                            "firstName": "Suresh",
                            "lastName": "Kothari",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kothari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681982"
                        ],
                        "name": "H. Oh",
                        "slug": "H.-Oh",
                        "structuredName": {
                            "firstName": "Heekuck",
                            "lastName": "Oh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Oh"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 43
                            }
                        ],
                        "text": "Synthetic and real examples in [36], [37], [9], [7] have shown that the projected samples from different classes in the PCA subspace can often be smeared."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 177751,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dbc0a468ab103ae29717703d4aa9f682f6a2b664",
            "isKey": false,
            "numCitedBy": 15338,
            "numCiting": 64,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Neural-Networks-for-Pattern-Recognition-Kothari-Oh",
            "title": {
                "fragments": [],
                "text": "Neural Networks for Pattern Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Adv. Comput."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144994682"
                        ],
                        "name": "A. Pentland",
                        "slug": "A.-Pentland",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Pentland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pentland"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 88
                            }
                        ],
                        "text": "Given a single image, the goal of face detection is to identify all image regions which contain a face regardless of its three-dimensional position, orientation, and lighting conditions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 41239330,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2e77f3b5a027ad4416abe696f5b2c7605b5bbd77",
            "isKey": false,
            "numCitedBy": 109,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "The objects that surround us --desks, cars, shoes and coats --are deaf and blind; this limits their ability to adapt to our needs and thus to be useful. We have therefore developed computer systems that can follow people's actions, recognizing their faces, gestures, and expressions. Using this technology we have begun to make \"smart rooms\" and \"smart clothes\" that can help people in day-to-day life without chaining them to keyboards, pointing devices or special goggles."
            },
            "slug": "Perceptual-Intelligence-Pentland",
            "title": {
                "fragments": [],
                "text": "Perceptual Intelligence"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Using computer systems that can follow people's actions, recognizing their faces, gestures, and expressions, this technology has begun to make \"smart rooms\" and \"smart clothes\" that can help people in day-to-day life without chaining them to keyboards, pointing devices or special goggles."
            },
            "venue": {
                "fragments": [],
                "text": "HUC"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35674406"
                        ],
                        "name": "Shigeo Abe DrEng",
                        "slug": "Shigeo-Abe-DrEng",
                        "structuredName": {
                            "firstName": "Shigeo",
                            "lastName": "DrEng",
                            "middleNames": [
                                "Abe"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shigeo Abe DrEng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9384346,
            "fieldsOfStudy": [
                "Mathematics",
                "Environmental Science"
            ],
            "id": "65a69968bb8c41aad0113cec4c2d981bddf50bc8",
            "isKey": false,
            "numCitedBy": 13095,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Classification \u2022 Supervised \u2013 parallelpiped \u2013 minimum distance \u2013 maximum likelihood (Bayes Rule) > non-parametric > parametric \u2013 support vector machines \u2013 neural networks \u2013 context classification \u2022 Unsupervised (clustering) \u2013 K-Means \u2013 ISODATA \u2022 Pattern recognition in remote sensing has been based on the intuitive notion that pixels belonging to the same class should have similar gray values in a given band. \u2013 Given two spectral bands, pixels from the same class plotted in a two-dimensional histogram should appear as a localized cluster. \u2013 If n images, each in a different spectral band, are available, pixels from the same class should form a localized cluster in n-space."
            },
            "slug": "Pattern-Classification-DrEng",
            "title": {
                "fragments": [],
                "text": "Pattern Classification"
            },
            "venue": {
                "fragments": [],
                "text": "Springer London"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144590225"
                        ],
                        "name": "D. Roth",
                        "slug": "D.-Roth",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Roth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Roth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1785,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3ed17a1114e2dc48597ab17cc8d5234006f525c9",
            "isKey": false,
            "numCitedBy": 222,
            "numCiting": 70,
            "paperAbstract": {
                "fragments": [],
                "text": "We analyze a few of the commonly used statistics based and machine learning algorithms for natural language disambiguation tasks and observe that they can be recast as learning linear separators in the feature space. Each of the methods makes a priori assumptions which it employs, given the data, when searching for its hypothesis. Nevertheless, as we show, it searches a space that is as rich as the space of all linear separators. We use this to build an argument for a data driven approach which merely searches for a good linear separator in the feature space, without further assumptions on the domain or a specific problem.We present such an approach - a sparse network of linear separators, utilizing the Winnow learning algorithm - and show how to use it in a variety of ambiguity resolution problems. The learning approach presented is attribute-efficient and, therefore, appropriate for domains having very large number of attributes.In particular, we present an extensive experimental comparison of our approach with other methods on several well studied lexical disambiguation tasks such as context-sensitive spelling correction, prepositional phrase attachment and part of speech tagging. In all cases we show that our approach either outperforms other methods tried for these tasks or performs comparably to the best."
            },
            "slug": "Learning-to-Resolve-Natural-Language-Ambiguities:-A-Roth",
            "title": {
                "fragments": [],
                "text": "Learning to Resolve Natural Language Ambiguities: A Unified Approach"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "An extensive experimental comparison of the approach with other methods on several well studied lexical disambiguation tasks such as context-sensitive spelling correction, prepositional phrase attachment and part of speech tagging shows that it outperforms other methods tried for these tasks or performs comparably to the best."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI/IAAI"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682773"
                        ],
                        "name": "A. Pentland",
                        "slug": "A.-Pentland",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Pentland",
                            "middleNames": [
                                "'Sandy'"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pentland"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 679222,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "a2cda75f91a093d55b73d3bd0f8d85fa4c417003",
            "isKey": false,
            "numCitedBy": 80,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "changing from static, inanimate objects into adaptive, reactive systems that can be more friendly, useful, and efficient. Or, of course, these new systems could be even more difficult to use than current systems; it depends how we design the interface between the world of humans and the world of this new generation of machines. To change inanimate objects like offices, houses, cars, or glasses into smart, active helpmates they need what I call \u201cperceptual intelligence.\u201d Translated, perceptual intelligence is paying attention to people and the surrounding situation in the same way another person would, thus allowing these new devices to learn to adapt their behavior to suit us, rather than adapting to them as we do today. This approach is grounded in the theory that most appropriate, adaptive biological behavior results from perceptual apparatus classifying the situation correctly, which then triggers fairly simple, situation-specific learned responses. It is an ethological view of behavior, and stands in strong contrast to cognitive theories that hold that adaptive behavior is primarily the result of complex reasoning mechanisms. From this theoretical perspective the problem with current computers is they are incredibly isolated. If you imagine yourself living in a closed, dark, soundproof box with only a telegraph connection to the outside world, you can get some sense of how difficult it is for computers to act intelligently or be helpful. They exist in a world almost completely disconnected from ours, so how can they know what they should do in order to be helpful? In the language of cognitive science, perceptual intelligence is the ability to deal with the frame problem: It is the ability to classify the P U I Alex Pentland PERCEPTUAL INTELLIGENCE"
            },
            "slug": "Perceptual-user-interfaces:-perceptual-intelligence-Pentland",
            "title": {
                "fragments": [],
                "text": "Perceptual user interfaces: perceptual intelligence"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "To change inanimate objects like offices, houses, cars, or glasses into smart, active helpmates they need what I call \u201cperceptual intelligence,\u201d which is paying attention to people and the surrounding situation in the same way another person would, thus allowing these new devices to learn to adapt their behavior to suit us, rather than adapting to them as the authors do today."
            },
            "venue": {
                "fragments": [],
                "text": "CACM"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6435894"
                        ],
                        "name": "S. McKenna",
                        "slug": "S.-McKenna",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "McKenna",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. McKenna"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120597446"
                        ],
                        "name": "Y. Raja",
                        "slug": "Y.-Raja",
                        "structuredName": {
                            "firstName": "Yogesh",
                            "lastName": "Raja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Raja"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144784813"
                        ],
                        "name": "S. Gong",
                        "slug": "S.-Gong",
                        "structuredName": {
                            "firstName": "Shaogang",
                            "lastName": "Gong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Gong"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15654336,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cad09eae64302e3bf89ce20b476374de3b56a607",
            "isKey": false,
            "numCitedBy": 377,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Tracking-colour-objects-using-adaptive-mixture-McKenna-Raja",
            "title": {
                "fragments": [],
                "text": "Tracking colour objects using adaptive mixture models"
            },
            "venue": {
                "fragments": [],
                "text": "Image Vis. Comput."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711777"
                        ],
                        "name": "C. Breazeal",
                        "slug": "C.-Breazeal",
                        "structuredName": {
                            "firstName": "Cynthia",
                            "lastName": "Breazeal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Breazeal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792053"
                        ],
                        "name": "B. Scassellati",
                        "slug": "B.-Scassellati",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Scassellati",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Scassellati"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2675817,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "56d411f1eb8a9a3e89f9c5cc02fdb52e50806440",
            "isKey": false,
            "numCitedBy": 420,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents part of an on-going project to integrate perception, attention, drives, emotions, behavior arbitration, and expressive acts for a robot designed to interact socially with humans. We present the design of a visual attention system based on a model of human visual search behavior from Wolfe (1994). The attention system integrates perceptions (motion detection, color saliency, and face popouts) with habituation effects and influences from the robot's motivational and behavioral state to create a context-dependent attention activation map. This activation map is used to direct eye movements and to satiate the drives of the motivational system."
            },
            "slug": "A-Context-Dependent-Attention-System-for-a-Social-Breazeal-Scassellati",
            "title": {
                "fragments": [],
                "text": "A Context-Dependent Attention System for a Social Robot"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The design of a visual attention system based on a model of human visual search behavior from Wolfe (1994) is presented, which integrates perceptions with habituation effects and influences from the robot's motivational and behavioral state to create a context-dependent attention activation map."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712517"
                        ],
                        "name": "L. Rabiner",
                        "slug": "L.-Rabiner",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Rabiner",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Rabiner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143604406"
                        ],
                        "name": "B. Juang",
                        "slug": "B.-Juang",
                        "structuredName": {
                            "firstName": "Biing-Hwang",
                            "lastName": "Juang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Juang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 85
                            }
                        ],
                        "text": "an HMM model with the standard Viterbi segmentation method and Baum-Welch algorithms [122]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7788300,
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "id": "df50c6e1903b1e2d657f78c28ab041756baca86a",
            "isKey": false,
            "numCitedBy": 8924,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "1. Fundamentals of Speech Recognition. 2. The Speech Signal: Production, Perception, and Acoustic-Phonetic Characterization. 3. Signal Processing and Analysis Methods for Speech Recognition. 4. Pattern Comparison Techniques. 5. Speech Recognition System Design and Implementation Issues. 6. Theory and Implementation of Hidden Markov Models. 7. Speech Recognition Based on Connected Word Models. 8. Large Vocabulary Continuous Speech Recognition. 9. Task-Oriented Applications of Automatic Speech Recognition."
            },
            "slug": "Fundamentals-of-speech-recognition-Rabiner-Juang",
            "title": {
                "fragments": [],
                "text": "Fundamentals of speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This book presents a meta-modelling framework for speech recognition that automates the very labor-intensive and therefore time-heavy and therefore expensive and expensive process of manually modeling speech."
            },
            "venue": {
                "fragments": [],
                "text": "Prentice Hall signal processing series"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2961287"
                        ],
                        "name": "N. Littlestone",
                        "slug": "N.-Littlestone",
                        "structuredName": {
                            "firstName": "Nick",
                            "lastName": "Littlestone",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Littlestone"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6334230,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a1dace286582d91916fe470d08f30381cf453f20",
            "isKey": false,
            "numCitedBy": 1612,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "Valiant (1984) and others have studied the problem of learning various classes of Boolean functions from examples. Here we discuss incremental learning of these functions. We consider a setting in which the learner responds to each example according to a current hypothesis. Then the learner updates the hypothesis, if necessary, based on the correct classification of the example. One natural measure of the quality of learning in this setting is the number of mistakes the learner makes. For suitable classes of functions, learning algorithms are available that make a bounded number of mistakes, with the bound independent of the number of examples seen by the learner. We present one such algorithm that learns disjunctive Boolean functions, along with variants for learning other classes of Boolean functions. The basic method can be expressed as a linear-threshold algorithm. A primary advantage of this algorithm is that the number of mistakes grows only logarithmically with the number of irrelevant attributes in the examples. At the same time, the algorithm is computationally efficient in both time and space."
            },
            "slug": "Learning-Quickly-When-Irrelevant-Attributes-Abound:-Littlestone",
            "title": {
                "fragments": [],
                "text": "Learning Quickly When Irrelevant Attributes Abound: A New Linear-Threshold Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work presents one such algorithm that learns disjunctive Boolean functions, along with variants for learning other classes of Boolean functions."
            },
            "venue": {
                "fragments": [],
                "text": "28th Annual Symposium on Foundations of Computer Science (sfcs 1987)"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2168816"
                        ],
                        "name": "D. Stuss",
                        "slug": "D.-Stuss",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Stuss",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Stuss"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47248134"
                        ],
                        "name": "A. Hamer",
                        "slug": "A.-Hamer",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Hamer",
                            "middleNames": [
                                "M",
                                "P"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hamer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6918045"
                        ],
                        "name": "L. Palumbo",
                        "slug": "L.-Palumbo",
                        "structuredName": {
                            "firstName": "Letizia",
                            "lastName": "Palumbo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Palumbo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2080862548"
                        ],
                        "name": "C. Dempster",
                        "slug": "C.-Dempster",
                        "structuredName": {
                            "firstName": "C",
                            "lastName": "Dempster",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Dempster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "95349405"
                        ],
                        "name": "R. Binns",
                        "slug": "R.-Binns",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Binns",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Binns"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "93562020"
                        ],
                        "name": "M. Levine",
                        "slug": "M.-Levine",
                        "structuredName": {
                            "firstName": "Marvin",
                            "lastName": "Levine",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Levine"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1397844999"
                        ],
                        "name": "B. Izuakawa",
                        "slug": "B.-Izuakawa",
                        "structuredName": {
                            "firstName": "B",
                            "lastName": "Izuakawa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Izuakawa"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Index Terms\u2014Face detection, face recognition, object recognition, view-based recognition, statistical pattern recognition, machine learning."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 889956,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "9b74a8d8da4a17096480b04192f467748b8be093",
            "isKey": false,
            "numCitedBy": 715,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "A functional neuroimaging study of the variables that generate category-specific object processing differences. On the interaction of selective attention and lexical knowledge: A connectionist account of neglect dyslexia. D 1998 The effects of focal anterior and posterior brain lesions on verbal fluency. The discipline has emerged in the 1990s at the interface between the neural sciences and the cognitive and computational sciences. On one side, it grows out of the traditions of cognitive psychology and neuro-psychology, which use behavioral experiments to uncover the processes and mechanisms lying behind human cognitive functions, and of computational approaches within cognitive psychology, which rely on computational models to develop explicit mech-anistic accounts of these functions. On the other side, it grows out of the traditions of behavioral, functional, and systems neuroscience, which use neurophysio-logical and neuroanatomical methods to explore the mechanisms underlying complex functions. It draws on findings and principles of cellular and molecular neuroscience. It joins these approaches with the use of new functional brain imaging methods, such as functional magnetic imaging (fMRI), positron emission tomography (PET), as well as other methods including electroencephalography (EEG) and mag-netoencephalography (MEG), and with a growing research tradition in computational neuroscience. A starting point for cognitive neuroscience is the idea that a cognitive or mental state consists of a pattern of activity distributed over many neurons. For example, the experience an individual has when holding, sniffing , and viewing a rose is a complex pattern of neural activity, distributed over many brain regions, including the participation of neurons in visual, somato-sensory, and olfactory, and possibly extending to language areas participating in representing the sound of the word 'rose' and\\or other areas where activity represents the content of an associated memory that may be evoked by the experience. These patterns of activation arise from excitatory and inhibitory interactions among the participating neurons, mediated by connections called synapses. The inputs neurons receive cause them to 'fire' or emit impulses called spikes or action potentials, which travel down their axons to synaptic terminals where they cause the release of chemicals that then have excitatory or inhibitory influences on the neurons on the other side of the synapse. The combined effect of the incoming signals to each neuron, together with its recent history, determines whether it will fire at a particular moment. Figure 1 indicates something of the fundamental circuitry involved, though it should be noted that only one out of \u2026"
            },
            "slug": "Cognitive-neuroscience.-Stuss-Hamer",
            "title": {
                "fragments": [],
                "text": "Cognitive neuroscience."
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Cognitive neuroscience has emerged in the 1990s at the interface between the neural sciences and the cognitive and computational sciences and joins these approaches with the use of new functional brain imaging methods, such as functional magnetic imaging (fMRI), positron emission tomography (PET), as well as other methods including electroencephalography (EEG) and mag-netoencephalographic (MEG)."
            },
            "venue": {
                "fragments": [],
                "text": "Current opinion in neurobiology"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758714"
                        ],
                        "name": "S. Fahlman",
                        "slug": "S.-Fahlman",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Fahlman",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Fahlman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749342"
                        ],
                        "name": "C. Lebiere",
                        "slug": "C.-Lebiere",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Lebiere",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lebiere"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 49
                            }
                        ],
                        "text": "They used a cascade correlation neural network [41] for supervised classification of textures and a Kohonen self-organizing feature map [80] to form clusters for different texture classes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 30443043,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "995a3b11cc8a4751d8e167abc4aa937abc934df0",
            "isKey": false,
            "numCitedBy": 2938,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Cascade-Correlation is a new architecture and supervised learning algorithm for artificial neural networks. Instead of just adjusting the weights in a network of fixed topology. Cascade-Correlation begins with a minimal network, then automatically trains and adds new hidden units one by one, creating a multi-layer structure. Once a new hidden unit has been added to the network, its input-side weights are frozen. This unit then becomes a permanent feature-detector in the network, available for producing outputs or for creating other, more complex feature detectors. The Cascade-Correlation architecture has several advantages over existing algorithms: it learns very quickly, the network determines its own size and topology, it retains the structures it has built even if the training set changes, and it requires no back-propagation of error signals through the connections of the network."
            },
            "slug": "The-Cascade-Correlation-Learning-Architecture-Fahlman-Lebiere",
            "title": {
                "fragments": [],
                "text": "The Cascade-Correlation Learning Architecture"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "The Cascade-Correlation architecture has several advantages over existing algorithms: it learns very quickly, the network determines its own size and topology, it retains the structures it has built even if the training set changes, and it requires no back-propagation of error signals through the connections of the network."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3209910"
                        ],
                        "name": "K. Mardia",
                        "slug": "K.-Mardia",
                        "structuredName": {
                            "firstName": "Kanti",
                            "lastName": "Mardia",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Mardia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1835233"
                        ],
                        "name": "I. Dryden",
                        "slug": "I.-Dryden",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Dryden",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Dryden"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "In [177], [178], Yow and Cipolla presented a featurebased method that uses a large amount of evidence from the visual image and their contextual evidence."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 123104803,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "73e0ceaf8ab52448c6c17d5dde58ab429d669887",
            "isKey": false,
            "numCitedBy": 99,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper obtains the exact distribution of Bookstein's shape variables under his plausible model for landmark data. We consider its properties including invariances, marginal distributions and the relationship with Kendall's uniform measure. Particular cases for triangles and quadrilaterals are highlighted. A normal approximation to the distribution is obtained, extending Bookstein's result for three landmarks. The adequacy of these approximations is also studied."
            },
            "slug": "Shape-distributions-for-landmark-data-Mardia-Dryden",
            "title": {
                "fragments": [],
                "text": "Shape distributions for landmark data"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Applied Probability"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "100942540"
                        ],
                        "name": "M. Lo\u00e8ve",
                        "slug": "M.-Lo\u00e8ve",
                        "structuredName": {
                            "firstName": "Michel",
                            "lastName": "Lo\u00e8ve",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lo\u00e8ve"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 123533290,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "f18307ebabf398bb7fd1b3375b2f09a7f9f6c5be",
            "isKey": false,
            "numCitedBy": 6129,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "These notes cover the basic definitions of discrete probability theory, and then present some results including Bayes' rule, inclusion-exclusion formula, Chebyshev's inequality, and the weak law of large numbers. 1 Sample spaces and events To treat probability rigorously, we define a sample space S whose elements are the possible outcomes of some process or experiment. For example, the sample space might be the outcomes of the roll of a die, or flips of a coin. To each element x of the sample space, we assign a probability, which will be a non-negative number between 0 and 1, which we will denote by p(x). We require that x\u2208S p(x) = 1, so the total probability of the elements of our sample space is 1. What this means intuitively is that when we perform our process, exactly one of the things in our sample space will happen. Example. The sample space could be S = {a, b, c}, and the probabilities could be p(a) = 1/2, p(b) = 1/3, p(c) = 1/6. If all elements of our sample space have equal probabilities, we call this the uniform probability distribution on our sample space. For example, if our sample space was the outcomes of a die roll, the sample space could be denoted S = {x 1 , x 2 ,. .. , x 6 }, where the event x i correspond to rolling i. The uniform distribution, in which every outcome x i has probability 1/6 describes the situation for a fair die. Similarly, if we consider tossing a fair coin, the outcomes would be H (heads) and T (tails), each with probability 1/2. In this situation we have the uniform probability distribution on the sample space S = {H, T }. We define an event A to be a subset of the sample space. For example, in the roll of a die, if the event A was rolling an even number, then A = {x 2 , x 4 , x 6 }. The probability of an event A, denoted by P(A), is the sum of the probabilities of the corresponding elements in the sample space. For rolling an even number, we have P(A) = p(x 2) + p(x 4) + p(x 6) = 1 2 Given an event A of our sample space, there is a complementary event which consists of all points in our sample space that are not \u2026"
            },
            "slug": "Probability-Theory-I-Lo\u00e8ve",
            "title": {
                "fragments": [],
                "text": "Probability Theory I"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "These notes cover the basic definitions of discrete probability theory, and then present some results including Bayes' rule, inclusion-exclusion formula, Chebyshev's inequality, and the weak law of large numbers."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1963
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758350"
                        ],
                        "name": "M. Kramer",
                        "slug": "M.-Kramer",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Kramer",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kramer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15907287,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "87c280d0dc204ca5db0d325991a21c211aeec866",
            "isKey": false,
            "numCitedBy": 2274,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Nonlinear principal component analysis is a novel technique for multivariate data analysis, similar to the well-known method of principal component analysis. NLPCA, like PCA, is used to identify and remove correlations among problem variables as an aid to dimensionality reduction, visualization, and exploratory data analysis. While PCA identifies only linear correlations between variables, NLPCA uncovers both linear and nonlinear correlations, without restriction on the character of the nonlinearities present in the data. NLPCA operates by training a feedforward neural network to perform the identity mapping, where the network inputs are reproduced at the output layer. The network contains an internal \u201cbottleneck\u201d layer (containing fewer nodes than input or output layers), which forces the network to develop a compact representation of the input data, and two additional hidden layers. The NLPCA method is demonstrated using time-dependent, simulated batch reaction data. Results show that NLPCA successfully reduces dimensionality and produces a feature space map resembling the actual distribution of the underlying system parameters."
            },
            "slug": "Nonlinear-principal-component-analysis-using-neural-Kramer",
            "title": {
                "fragments": [],
                "text": "Nonlinear principal component analysis using autoassociative neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The NLPCA method is demonstrated using time-dependent, simulated batch reaction data and shows that it successfully reduces dimensionality and produces a feature space map resembling the actual distribution of the underlying system parameters."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744109"
                        ],
                        "name": "S. Salzberg",
                        "slug": "S.-Salzberg",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Salzberg",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Salzberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1800681"
                        ],
                        "name": "Alberto Maria Segre",
                        "slug": "Alberto-Maria-Segre",
                        "structuredName": {
                            "firstName": "Alberto",
                            "lastName": "Segre",
                            "middleNames": [
                                "Maria"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alberto Maria Segre"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 12
                            }
                        ],
                        "text": "5 algorithm [121] to learn a decision tree from positive and negative examples of face patterns [64]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60499165,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7feb0fc888cd55360949554db032d7d1cba9e947",
            "isKey": false,
            "numCitedBy": 7028,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "Algorithms for constructing decision trees are among the most well known and widely used of all machine learning methods. Among decision tree algorithms, J. Ross Quinlan's ID3 and its successor, C4.5, are probably the most popular in the machine learning community. These algorithms and variations on them have been the subject of numerous research papers since Quinlan introduced ID3. Until recently, most researchers looking for an introduction to decision trees turned to Quinlan's seminal 1986 Machine Learning journal article [Quinlan, 1986]. In his new book, C4.5: Programs for Machine Learning, Quinlan has put together a definitive, much needed description of his complete system, including the latest developments. As such, this book will be a welcome addition to the library of many researchers and students."
            },
            "slug": "Programs-for-Machine-Learning-Salzberg-Segre",
            "title": {
                "fragments": [],
                "text": "Programs for Machine Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "In his new book, C4.5: Programs for Machine Learning, Quinlan has put together a definitive, much needed description of his complete system, including the latest developments, which will be a welcome addition to the library of many researchers and students."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52476463"
                        ],
                        "name": "A. Vogler",
                        "slug": "A.-Vogler",
                        "structuredName": {
                            "firstName": "Anja",
                            "lastName": "Vogler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Vogler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 16
                            }
                        ],
                        "text": "Factor analysis [5] is then applied to fit these training features and obtain a distribution function."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 63409966,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "9ecae668ad78e5ab6ef099808fea219f815cf5c4",
            "isKey": false,
            "numCitedBy": 3862,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": " "
            },
            "slug": "An-Introduction-to-Multivariate-Statistical-Vogler",
            "title": {
                "fragments": [],
                "text": "An Introduction to Multivariate Statistical Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "The introduction to multivariate statistical analysis is universally compatible with any devices to read, and will help you to cope with some harmful bugs inside their desktop computer."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781060"
                        ],
                        "name": "M. Venkatraman",
                        "slug": "M.-Venkatraman",
                        "structuredName": {
                            "firstName": "Mahesh",
                            "lastName": "Venkatraman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Venkatraman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723877"
                        ],
                        "name": "V. Govindaraju",
                        "slug": "V.-Govindaraju",
                        "structuredName": {
                            "firstName": "Venu",
                            "lastName": "Govindaraju",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Govindaraju"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59735565,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "35990ab419e44994a6a6866c674ec61c4d948a39",
            "isKey": false,
            "numCitedBy": 5,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Zero-Crossings-of-a-Nonorthogonal-Wavelet-Transform-Venkatraman-Govindaraju",
            "title": {
                "fragments": [],
                "text": "Zero-Crossings of a Nonorthogonal Wavelet Transform for Complex Object Location"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145687022"
                        ],
                        "name": "J. Crowley",
                        "slug": "J.-Crowley",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Crowley",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Crowley"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 119
                            }
                        ],
                        "text": "Several color spaces have been utilized to label pixels as skin including RGB [66], [67], [137], normalized RGB [102], [29], [149], [172], [30], [105], [171], [77], [151], [120], HSV (or HSI) [138], [79], [147], [146], YCrCb [167], [17], YIQ [31], [32], YES [131], CIE XYZ [19], and CIE LUV [173]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10350805,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b9c428d519f16bbd332a66c38564e67d3363419d",
            "isKey": false,
            "numCitedBy": 83,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Integration-and-control-of-reactive-visual-Crowley",
            "title": {
                "fragments": [],
                "text": "Integration and control of reactive visual processes"
            },
            "venue": {
                "fragments": [],
                "text": "Robotics Auton. Syst."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688681"
                        ],
                        "name": "T. Kohonen",
                        "slug": "T.-Kohonen",
                        "structuredName": {
                            "firstName": "Teuvo",
                            "lastName": "Kohonen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kohonen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 138
                            }
                        ],
                        "text": "They used a cascade correlation neural network [41] for supervised classification of textures and a Kohonen self-organizing feature map [80] to form clusters for different texture classes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 222292199,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "10055eb6f2f711a36d9aa8f759d3b3f01ebddb5d",
            "isKey": false,
            "numCitedBy": 6561,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "1. Various Aspects of Memory.- 1.1 On the Purpose and Nature of Biological Memory.- 1.1.1 Some Fundamental Concepts.- 1.1.2 The Classical Laws of Association.- 1.1.3 On Different Levels of Modelling.- 1.2 Questions Concerning the Fundamental Mechanisms of Memory.- 1.2.1 Where Do the Signals Relating to Memory Act Upon?.- 1.2.2 What Kind of Encoding is Used for Neural Signals?.- 1.2.3 What are the Variable Memory Elements?.- 1.2.4 How are Neural Signals Addressed in Memory?.- 1.3 Elementary Operations Implemented by Associative Memory.- 1.3.1 Associative Recall.- 1.3.2 Production of Sequences from the Associative Memory.- 1.3.3 On the Meaning of Background and Context.- 1.4 More Abstract Aspects of Memory.- 1.4.1 The Problem of Infinite-State Memory.- 1.4.2 Invariant Representations.- 1.4.3 Symbolic Representations.- 1.4.4 Virtual Images.- 1.4.5 The Logic of Stored Knowledge.- 2. Pattern Mathematics.- 2.1 Mathematical Notations and Methods.- 2.1.1 Vector Space Concepts.- 2.1.2 Matrix Notations.- 2.1.3 Further Properties of Matrices.- 2.1.4 Matrix Equations.- 2.1.5 Projection Operators.- 2.1.6 On Matrix Differential Calculus.- 2.2 Distance Measures for Patterns.- 2.2.1 Measures of Similarity and Distance in Vector Spaces.- 2.2.2 Measures of Similarity and Distance Between Symbol Strings.- 2.2.3 More Accurate Distance Measures for Text.- 3. Classical Learning Systems.- 3.1 The Adaptive Linear Element (Adaline).- 3.1.1 Description of Adaptation by the Stochastic Approximation.- 3.2 The Perceptron.- 3.3 The Learning Matrix.- 3.4 Physical Realization of Adaptive Weights.- 3.4.1 Perceptron and Adaline.- 3.4.2 Classical Conditioning.- 3.4.3 Conjunction Learning Switches.- 3.4.4 Digital Representation of Adaptive Circuits.- 3.4.5 Biological Components.- 4. A New Approach to Adaptive Filters.- 4.1 Survey of Some Necessary Functions.- 4.2 On the \"Transfer Function\" of the Neuron.- 4.3 Models for Basic Adaptive Units.- 4.3.1 On the Linearization of the Basic Unit.- 4.3.2 Various Cases of Adaptation Laws.- 4.3.3 Two Limit Theorems.- 4.3.4 The Novelty Detector.- 4.4 Adaptive Feedback Networks.- 4.4.1 The Autocorrelation Matrix Memory.- 4.4.2 The Novelty Filter.- 5. Self-Organizing Feature Maps.- 5.1 On the Feature Maps of the Brain.- 5.2 Formation of Localized Responses by Lateral Feedback.- 5.3 Computational Simplification of the Process.- 5.3.1 Definition of the Topology-Preserving Mapping.- 5.3.2 A Simple Two-Dimensional Self-Organizing System.- 5.4 Demonstrations of Simple Topology-Preserving Mappings.- 5.4.1 Images of Various Distributions of Input Vectors.- 5.4.2 \"The Magic TV\".- 5.4.3 Mapping by a Feeler Mechanism.- 5.5 Tonotopic Map.- 5.6 Formation of Hierarchical Representations.- 5.6.1 Taxonomy Example.- 5.6.2 Phoneme Map.- 5.7 Mathematical Treatment of Self-Organization.- 5.7.1 Ordering of Weights.- 5.7.2 Convergence Phase.- 5.8 Automatic Selection of Feature Dimensions.- 6. Optimal Associative Mappings.- 6.1 Transfer Function of an Associative Network.- 6.2 Autoassociative Recall as an Orthogonal Projection.- 6.2.1 Orthogonal Projections.- 6.2.2 Error-Correcting Properties of Projections.- 6.3 The Novelty Filter.- 6.3.1 Two Examples of Novelty Filter.- 6.3.2 Novelty Filter as an Autoassociative Memory.- 6.4 Autoassociative Encoding.- 6.4.1 An Example of Autoassociative Encoding.- 6.5 Optimal Associative Mappings.- 6.5.1 The Optimal Linear Associative Mapping.- 6.5.2 Optimal Nonlinear Associative Mappings.- 6.6 Relationship Between Associative Mapping, Linear Regression, and Linear Estimation.- 6.6.1 Relationship of the Associative Mapping to Linear Regression.- 6.6.2 Relationship of the Regression Solution to the Linear Estimator.- 6.7 Recursive Computation of the Optimal Associative Mapping.- 6.7.1 Linear Corrective Algorithms.- 6.7.2 Best Exact Solution (Gradient Projection).- 6.7.3 Best Approximate Solution (Regression).- 6.7.4 Recursive Solution in the General Case.- 6.8 Special Cases.- 6.8.1 The Correlation Matrix Memory.- 6.8.2 Relationship Between Conditional Averages and Optimal Estimator.- 7. Pattern Recognition.- 7.1 Discriminant Functions.- 7.2 Statistical Formulation of Pattern Classification.- 7.3 Comparison Methods.- 7.4 The Subspace Methods of Classification.- 7.4.1 The Basic Subspace Method.- 7.4.2 The Learning Subspace Method (LSM).- 7.5 Learning Vector Quantization.- 7.6 Feature Extraction.- 7.7 Clustering.- 7.7.1 Simple Clustering (Optimization Approach).- 7.7.2 Hierarchical Clustering (Taxonomy Approach).- 7.8 Structural Pattern Recognition Methods.- 8. More About Biological Memory.- 8.1 Physiological Foundations of Memory.- 8.1.1 On the Mechanisms of Memory in Biological Systems.- 8.1.2 Structural Features of Some Neural Networks.- 8.1.3 Functional Features of Neurons.- 8.1.4 Modelling of the Synaptic Plasticity.- 8.1.5 Can the Memory Capacity Ensue from Synaptic Changes?.- 8.2 The Unified Cortical Memory Model.- 8.2.1 The Laminar Network Organization.- 8.2.2 On the Roles of Interneurons.- 8.2.3 Representation of Knowledge Over Memory Fields.- 8.2.4 Self-Controlled Operation of Memory.- 8.3 Collateral Reading.- 8.3.1 Physiological Results Relevant to Modelling.- 8.3.2 Related Modelling.- 9. Notes on Neural Computing.- 9.1 First Theoretical Views of Neural Networks.- 9.2 Motives for the Neural Computing Research.- 9.3 What Could the Purpose of the Neural Networks be?.- 9.4 Definitions of Artificial \"Neural Computing\" and General Notes on Neural Modelling.- 9.5 Are the Biological Neural Functions Localized or Distributed?.- 9.6 Is Nonlinearity Essential to Neural Computing?.- 9.7 Characteristic Differences Between Neural and Digital Computers.- 9.7.1 The Degree of Parallelism of the Neural Networks is Still Higher than that of any \"Massively Parallel\" Digital Computer.- 9.7.2 Why the Neural Signals Cannot be Approximated by Boolean Variables.- 9.7.3 The Neural Circuits do not Implement Finite Automata.- 9.7.4 Undue Views of the Logic Equivalence of the Brain and Computers on a High Level.- 9.8 \"Connectionist Models\".- 9.9 How can the Neural Computers be Programmed?.- 10. Optical Associative Memories.- 10.1 Nonholographic Methods.- 10.2 General Aspects of Holographic Memories.- 10.3 A Simple Principle of Holographic Associative Memory.- 10.4 Addressing in Holographic Memories.- 10.5 Recent Advances of Optical Associative Memories.- Bibliography on Pattern Recognition.- References."
            },
            "slug": "Self-Organization-and-Associative-Memory-Kohonen",
            "title": {
                "fragments": [],
                "text": "Self-Organization and Associative Memory"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "The purpose and nature of Biological Memory, as well as some of the aspects of Memory Aspects, are explained."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744700"
                        ],
                        "name": "Zoubin Ghahramani",
                        "slug": "Zoubin-Ghahramani",
                        "structuredName": {
                            "firstName": "Zoubin",
                            "lastName": "Ghahramani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zoubin Ghahramani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18270595,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "09ef86868035bbfd4803a9e1c98640804bf8f4a4",
            "isKey": false,
            "numCitedBy": 700,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Factor analysis, a statistical method for modeling the covariance structure of high dimensional data using a small number of latent variables, can be extended by allowing di erent local factor models in di erent regions of the input space. This results in a model which concurrently performs clustering and dimensionality reduction, and can be thought of as a reduced dimension mixture of Gaussians. We present an exact Expectation{Maximization algorithm for tting the parameters of this mixture of factor analyzers."
            },
            "slug": "The-EM-algorithm-for-mixtures-of-factor-analyzers-Ghahramani-Hinton",
            "title": {
                "fragments": [],
                "text": "The EM algorithm for mixtures of factor analyzers"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work presents an exact Expectation{Maximization algorithm for determining the parameters of this mixture of factor analyzers which concurrently performs clustering and dimensionality reduction, and can be thought of as a reduced dimension mixture of Gaussians."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752732"
                        ],
                        "name": "T. Cover",
                        "slug": "T.-Cover",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Cover",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Cover"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115282352"
                        ],
                        "name": "Joy A. Thomas",
                        "slug": "Joy-A.-Thomas",
                        "structuredName": {
                            "firstName": "Joy",
                            "lastName": "Thomas",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joy A. Thomas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 190432,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7dbdb4209626fd92d2436a058663206216036e68",
            "isKey": false,
            "numCitedBy": 42793,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface to the Second Edition. Preface to the First Edition. Acknowledgments for the Second Edition. Acknowledgments for the First Edition. 1. Introduction and Preview. 1.1 Preview of the Book. 2. Entropy, Relative Entropy, and Mutual Information. 2.1 Entropy. 2.2 Joint Entropy and Conditional Entropy. 2.3 Relative Entropy and Mutual Information. 2.4 Relationship Between Entropy and Mutual Information. 2.5 Chain Rules for Entropy, Relative Entropy, and Mutual Information. 2.6 Jensen's Inequality and Its Consequences. 2.7 Log Sum Inequality and Its Applications. 2.8 Data-Processing Inequality. 2.9 Sufficient Statistics. 2.10 Fano's Inequality. Summary. Problems. Historical Notes. 3. Asymptotic Equipartition Property. 3.1 Asymptotic Equipartition Property Theorem. 3.2 Consequences of the AEP: Data Compression. 3.3 High-Probability Sets and the Typical Set. Summary. Problems. Historical Notes. 4. Entropy Rates of a Stochastic Process. 4.1 Markov Chains. 4.2 Entropy Rate. 4.3 Example: Entropy Rate of a Random Walk on a Weighted Graph. 4.4 Second Law of Thermodynamics. 4.5 Functions of Markov Chains. Summary. Problems. Historical Notes. 5. Data Compression. 5.1 Examples of Codes. 5.2 Kraft Inequality. 5.3 Optimal Codes. 5.4 Bounds on the Optimal Code Length. 5.5 Kraft Inequality for Uniquely Decodable Codes. 5.6 Huffman Codes. 5.7 Some Comments on Huffman Codes. 5.8 Optimality of Huffman Codes. 5.9 Shannon-Fano-Elias Coding. 5.10 Competitive Optimality of the Shannon Code. 5.11 Generation of Discrete Distributions from Fair Coins. Summary. Problems. Historical Notes. 6. Gambling and Data Compression. 6.1 The Horse Race. 6.2 Gambling and Side Information. 6.3 Dependent Horse Races and Entropy Rate. 6.4 The Entropy of English. 6.5 Data Compression and Gambling. 6.6 Gambling Estimate of the Entropy of English. Summary. Problems. Historical Notes. 7. Channel Capacity. 7.1 Examples of Channel Capacity. 7.2 Symmetric Channels. 7.3 Properties of Channel Capacity. 7.4 Preview of the Channel Coding Theorem. 7.5 Definitions. 7.6 Jointly Typical Sequences. 7.7 Channel Coding Theorem. 7.8 Zero-Error Codes. 7.9 Fano's Inequality and the Converse to the Coding Theorem. 7.10 Equality in the Converse to the Channel Coding Theorem. 7.11 Hamming Codes. 7.12 Feedback Capacity. 7.13 Source-Channel Separation Theorem. Summary. Problems. Historical Notes. 8. Differential Entropy. 8.1 Definitions. 8.2 AEP for Continuous Random Variables. 8.3 Relation of Differential Entropy to Discrete Entropy. 8.4 Joint and Conditional Differential Entropy. 8.5 Relative Entropy and Mutual Information. 8.6 Properties of Differential Entropy, Relative Entropy, and Mutual Information. Summary. Problems. Historical Notes. 9. Gaussian Channel. 9.1 Gaussian Channel: Definitions. 9.2 Converse to the Coding Theorem for Gaussian Channels. 9.3 Bandlimited Channels. 9.4 Parallel Gaussian Channels. 9.5 Channels with Colored Gaussian Noise. 9.6 Gaussian Channels with Feedback. Summary. Problems. Historical Notes. 10. Rate Distortion Theory. 10.1 Quantization. 10.2 Definitions. 10.3 Calculation of the Rate Distortion Function. 10.4 Converse to the Rate Distortion Theorem. 10.5 Achievability of the Rate Distortion Function. 10.6 Strongly Typical Sequences and Rate Distortion. 10.7 Characterization of the Rate Distortion Function. 10.8 Computation of Channel Capacity and the Rate Distortion Function. Summary. Problems. Historical Notes. 11. Information Theory and Statistics. 11.1 Method of Types. 11.2 Law of Large Numbers. 11.3 Universal Source Coding. 11.4 Large Deviation Theory. 11.5 Examples of Sanov's Theorem. 11.6 Conditional Limit Theorem. 11.7 Hypothesis Testing. 11.8 Chernoff-Stein Lemma. 11.9 Chernoff Information. 11.10 Fisher Information and the Cram-er-Rao Inequality. Summary. Problems. Historical Notes. 12. Maximum Entropy. 12.1 Maximum Entropy Distributions. 12.2 Examples. 12.3 Anomalous Maximum Entropy Problem. 12.4 Spectrum Estimation. 12.5 Entropy Rates of a Gaussian Process. 12.6 Burg's Maximum Entropy Theorem. Summary. Problems. Historical Notes. 13. Universal Source Coding. 13.1 Universal Codes and Channel Capacity. 13.2 Universal Coding for Binary Sequences. 13.3 Arithmetic Coding. 13.4 Lempel-Ziv Coding. 13.5 Optimality of Lempel-Ziv Algorithms. Compression. Summary. Problems. Historical Notes. 14. Kolmogorov Complexity. 14.1 Models of Computation. 14.2 Kolmogorov Complexity: Definitions and Examples. 14.3 Kolmogorov Complexity and Entropy. 14.4 Kolmogorov Complexity of Integers. 14.5 Algorithmically Random and Incompressible Sequences. 14.6 Universal Probability. 14.7 Kolmogorov complexity. 14.9 Universal Gambling. 14.10 Occam's Razor. 14.11 Kolmogorov Complexity and Universal Probability. 14.12 Kolmogorov Sufficient Statistic. 14.13 Minimum Description Length Principle. Summary. Problems. Historical Notes. 15. Network Information Theory. 15.1 Gaussian Multiple-User Channels. 15.2 Jointly Typical Sequences. 15.3 Multiple-Access Channel. 15.4 Encoding of Correlated Sources. 15.5 Duality Between Slepian-Wolf Encoding and Multiple-Access Channels. 15.6 Broadcast Channel. 15.7 Relay Channel. 15.8 Source Coding with Side Information. 15.9 Rate Distortion with Side Information. 15.10 General Multiterminal Networks. Summary. Problems. Historical Notes. 16. Information Theory and Portfolio Theory. 16.1 The Stock Market: Some Definitions. 16.2 Kuhn-Tucker Characterization of the Log-Optimal Portfolio. 16.3 Asymptotic Optimality of the Log-Optimal Portfolio. 16.4 Side Information and the Growth Rate. 16.5 Investment in Stationary Markets. 16.6 Competitive Optimality of the Log-Optimal Portfolio. 16.7 Universal Portfolios. 16.8 Shannon-McMillan-Breiman Theorem (General AEP). Summary. Problems. Historical Notes. 17. Inequalities in Information Theory. 17.1 Basic Inequalities of Information Theory. 17.2 Differential Entropy. 17.3 Bounds on Entropy and Relative Entropy. 17.4 Inequalities for Types. 17.5 Combinatorial Bounds on Entropy. 17.6 Entropy Rates of Subsets. 17.7 Entropy and Fisher Information. 17.8 Entropy Power Inequality and Brunn-Minkowski Inequality. 17.9 Inequalities for Determinants. 17.10 Inequalities for Ratios of Determinants. Summary. Problems. Historical Notes. Bibliography. List of Symbols. Index."
            },
            "slug": "Elements-of-Information-Theory-Cover-Thomas",
            "title": {
                "fragments": [],
                "text": "Elements of Information Theory"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The author examines the role of entropy, inequality, and randomness in the design of codes and the construction of codes in the rapidly changing environment."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2295441"
                        ],
                        "name": "K. Fukunaga",
                        "slug": "K.-Fukunaga",
                        "structuredName": {
                            "firstName": "Keinosuke",
                            "lastName": "Fukunaga",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Fukunaga"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "13849278"
                        ],
                        "name": "W. Koontz",
                        "slug": "W.-Koontz",
                        "structuredName": {
                            "firstName": "W.L.G.",
                            "lastName": "Koontz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Koontz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 206617934,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "6ed4a2833ae3adaebaa73d24fd5f06bc9d9a02c9",
            "isKey": false,
            "numCitedBy": 540,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "The Karhunen-Lo6ve expansion has been used previously to extract important features for representing samples taken from a given distribution. A method is developed herein to use the Karhunen-Loeve expansion to extract features relevant to classification of a sample taken from one of two pattern classes. Numerical examples are presented to illustrate the technique."
            },
            "slug": "Application-of-the-Karhunen-Lo\u00e8ve-Expansion-to-and-Fukunaga-Koontz",
            "title": {
                "fragments": [],
                "text": "Application of the Karhunen-Lo\u00e8ve Expansion to Feature Selection and Ordering"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "A method is developed herein to use the Karhunen-Loeve expansion to extract features relevant to classification of a sample taken from one of two pattern classes."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Computers"
            },
            "year": 1970
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1432639666"
                        ],
                        "name": "Karl Pearson F.R.S.",
                        "slug": "Karl-Pearson-F.R.S.",
                        "structuredName": {
                            "firstName": "Karl",
                            "lastName": "F.R.S.",
                            "middleNames": [
                                "Pearson"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karl Pearson F.R.S."
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 125037489,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "cac33f91e59f0a137b46176d74cee55c7010c3f8",
            "isKey": false,
            "numCitedBy": 9520,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "(1901). LIII. On lines and planes of closest fit to systems of points in space. The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science: Vol. 2, No. 11, pp. 559-572."
            },
            "slug": "LIII.-On-lines-and-planes-of-closest-fit-to-systems-F.R.S.",
            "title": {
                "fragments": [],
                "text": "LIII. On lines and planes of closest fit to systems of points in space"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "This paper is concerned with the construction of planes of closest fit to systems of points in space and the relationships between these planes and the planes themselves."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1901
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 3
                            }
                        ],
                        "text": "In QMF, each sample image is divided into a number of blocks, and qualitative features are estimated for each block."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 53
                            }
                        ],
                        "text": "presented a qualitative model for face pattern (QMF) [161], [162]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 65
                            }
                        ],
                        "text": "Tsukamoto et al. presented a qualitative model for face pattern (QMF) [161], [162]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Detection and Tracking of Human Face with Synthesized Templates"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. First Asian Conf. Computer Vision"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 16
                            }
                        ],
                        "text": "Both methods in [175] have been tested using the databases in [128], [154] which together consist of 225 images with 619 faces, and experimental results show that these two methods have detection rates of 92."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 3
                            }
                        ],
                        "text": "In [175], a detection method based on a mixture of factor analyses was proposed."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 19
                            }
                        ],
                        "text": "A second method in [175] uses Fisher\u2019s Linear Discriminant (FLD) to project samples from the high dimensional image space to a lower dimensional feature space."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Mixtures of Linear Subspaces for Face Detection"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. Fourth Int'l Conf. Automatic Face and Gesture Recognition"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117070857"
                        ],
                        "name": "A. Mart\u00ednez",
                        "slug": "A.-Mart\u00ednez",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Mart\u00ednez",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Mart\u00ednez"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This face database has been applied to image and video indexing as well as retrieval [96]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The Purdue AR database contains over 3,276 color images of 126 people (70 males and 56 females) in frontal view [96]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 221632808,
            "fieldsOfStudy": [
                "Materials Science"
            ],
            "id": "cd520bc4b5301bc51b8b6bf1226c3f2f88e8e444",
            "isKey": false,
            "numCitedBy": 2634,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-AR-face-databasae-Mart\u00ednez",
            "title": {
                "fragments": [],
                "text": "The AR face databasae"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2070546375"
                        ],
                        "name": "\uae40\ud615\uace4",
                        "slug": "\uae40\ud615\uace4",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "\uae40\ud615\uace4",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\uae40\ud615\uace4"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "67235998"
                        ],
                        "name": "\uae40\uc0c1\ud6c8",
                        "slug": "\uae40\uc0c1\ud6c8",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "\uae40\uc0c1\ud6c8",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\uae40\uc0c1\ud6c8"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "65977275"
                        ],
                        "name": "\uc548\uc0c1\ucca0",
                        "slug": "\uc548\uc0c1\ucca0",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "\uc548\uc0c1\ucca0",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\uc548\uc0c1\ucca0"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66182185"
                        ],
                        "name": "\uae40\ub0a8\uaddc",
                        "slug": "\uae40\ub0a8\uaddc",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "\uae40\ub0a8\uaddc",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\uae40\ub0a8\uaddc"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 159
                            }
                        ],
                        "text": "Several color spaces have been utilized to label pixels as skin including RGB [66], [67], [137], normalized RGB [102], [29], [149], [172], [30], [105], [171], [77], [151], [120], HSV (or HSI) [138], [79], [147], [146], YCrCb [167], [17], YIQ [31], [32], YES [131], CIE XYZ [19], and CIE LUV [173]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 203710113,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "10da098f300c40c34b66bd9eb6e8afdf9743a1d8",
            "isKey": false,
            "numCitedBy": 3,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Object-oriented-face-detection-using-range-and-\uae40\ud615\uace4-\uae40\uc0c1\ud6c8",
            "title": {
                "fragments": [],
                "text": "Object oriented face detection using range and color information"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47652868"
                        ],
                        "name": "H. Hotelling",
                        "slug": "H.-Hotelling",
                        "structuredName": {
                            "firstName": "Harold",
                            "lastName": "Hotelling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hotelling"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 144828484,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "9ebb5c0d6d54707a4d6181a693b6f755ec8a45a9",
            "isKey": false,
            "numCitedBy": 8492,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Analysis-of-a-complex-of-statistical-variables-into-Hotelling",
            "title": {
                "fragments": [],
                "text": "Analysis of a complex of statistical variables into principal components."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1933
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "101655384"
                        ],
                        "name": "K. Karhunen",
                        "slug": "K.-Karhunen",
                        "structuredName": {
                            "firstName": "Kari",
                            "lastName": "Karhunen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Karhunen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This demonstration is based on the Karhunen-Lo\u00e8ve transform [72], [93], [48], which also goes by other names, e."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 115303271,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "023b81a379bc957edcaa72429ea4c182dfd49ad2",
            "isKey": false,
            "numCitedBy": 987,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "\u00dcber-lineare-Methoden-in-der-Karhunen",
            "title": {
                "fragments": [],
                "text": "\u00dcber lineare Methoden in der Wahrscheinlichkeitsrechnung"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1947
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2295441"
                        ],
                        "name": "K. Fukunaga",
                        "slug": "K.-Fukunaga",
                        "structuredName": {
                            "firstName": "Keinosuke",
                            "lastName": "Fukunaga",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Fukunaga"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60735762,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "71c0f082a41c7f0b102c3ca9e4cf6b31f361d06a",
            "isKey": false,
            "numCitedBy": 4228,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Introduction-to-statistical-pattern-recognition-Fukunaga",
            "title": {
                "fragments": [],
                "text": "Introduction to statistical pattern recognition (2nd ed.)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69439712"
                        ],
                        "name": "R. C. Gonzales",
                        "slug": "R.-C.-Gonzales",
                        "structuredName": {
                            "firstName": "Rafael",
                            "lastName": "Gonzales",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. C. Gonzales"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50739223"
                        ],
                        "name": "P. Wintz",
                        "slug": "P.-Wintz",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Wintz",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Wintz"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59850843,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c636437d53514d8f59ed9e7cab165d33b2b86aa2",
            "isKey": false,
            "numCitedBy": 1782,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Digital-image-processing-(2nd-ed.)-Gonzales-Wintz",
            "title": {
                "fragments": [],
                "text": "Digital image processing (2nd ed.)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688681"
                        ],
                        "name": "T. Kohonen",
                        "slug": "T.-Kohonen",
                        "structuredName": {
                            "firstName": "Teuvo",
                            "lastName": "Kohonen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kohonen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59773108,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "015e5c48abbd59309e6986aaa94550e40562f100",
            "isKey": false,
            "numCitedBy": 3128,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Self-organization-and-associative-memory:-3rd-Kohonen",
            "title": {
                "fragments": [],
                "text": "Self-organization and associative memory: 3rd edition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "12047785"
                        ],
                        "name": "M. Propp",
                        "slug": "M.-Propp",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Propp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Propp"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144013393"
                        ],
                        "name": "A. Samal",
                        "slug": "A.-Samal",
                        "structuredName": {
                            "firstName": "Ashok",
                            "lastName": "Samal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Samal"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 81
                            }
                        ],
                        "text": "Propp and Samal developed one of the earliest neural networks for face detection [117]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 59630685,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b0a39564e02ccd0fc15aa9756c4324639d5ec301",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Artificial-Neural-Network-architectures-for-human-Propp-Samal",
            "title": {
                "fragments": [],
                "text": "Artificial Neural Network architectures for human face detection"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1384255355"
                        ],
                        "name": "Aleix M. Martinez",
                        "slug": "Aleix-M.-Martinez",
                        "structuredName": {
                            "firstName": "Aleix M.",
                            "lastName": "Martinez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aleix M. Martinez"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 85
                            }
                        ],
                        "text": "This face database has been applied to image and video indexing as well as retrieval [96]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 112
                            }
                        ],
                        "text": "The Purdue AR database contains over 3,276 color images of 126 people (70 males and 56 females) in frontal view [96]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 57227467,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6d96f946aaabc734af7fe3fc4454cf8547fcd5ed",
            "isKey": false,
            "numCitedBy": 3767,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-AR-face-database-Martinez",
            "title": {
                "fragments": [],
                "text": "The AR face database"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6435894"
                        ],
                        "name": "S. McKenna",
                        "slug": "S.-McKenna",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "McKenna",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. McKenna"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144784813"
                        ],
                        "name": "S. Gong",
                        "slug": "S.-Gong",
                        "structuredName": {
                            "firstName": "Shaogang",
                            "lastName": "Gong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Gong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3038814"
                        ],
                        "name": "R. W\u00fcrtz",
                        "slug": "R.-W\u00fcrtz",
                        "structuredName": {
                            "firstName": "Rolf",
                            "lastName": "W\u00fcrtz",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. W\u00fcrtz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144411844"
                        ],
                        "name": "J. Tanner",
                        "slug": "J.-Tanner",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Tanner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tanner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2220947"
                        ],
                        "name": "D. Banin",
                        "slug": "D.-Banin",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Banin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Banin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We suggest ways to further improve these methods in Section 4."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8962129,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "144d056830387b04b8c5e5fa9e9bea84c4600d27",
            "isKey": false,
            "numCitedBy": 421,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Audio-and-Video-based-Biometric-Person-McKenna-Gong",
            "title": {
                "fragments": [],
                "text": "Audio- and Video-based Biometric Person Authentication"
            },
            "venue": {
                "fragments": [],
                "text": "Lecture Notes in Computer Science"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3108177"
                        ],
                        "name": "P. Hart",
                        "slug": "P.-Hart",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Hart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35106875"
                        ],
                        "name": "R. Duda",
                        "slug": "R.-Duda",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Duda",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Duda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2586918"
                        ],
                        "name": "D. Stork",
                        "slug": "D.-Stork",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Stork",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Stork"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 196008710,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "78053512af13466c569e5946acfc3953bbfc9d36",
            "isKey": false,
            "numCitedBy": 18024,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Pattern-Classification-Hart-Duda",
            "title": {
                "fragments": [],
                "text": "Pattern Classification"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1973
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 263,
                                "start": 258
                            }
                        ],
                        "text": "Several color spaces have been utilized to label pixels as skin including RGB [66], [67], [137], normalized RGB [102], [29], [149], [172], [30], [105], [171], [77], [151], [120], HSV (or HSI) [138], [79], [147], [146], YCrCb [167], [17], YIQ [31], [32], YES [131], CIE XYZ [19], and CIE LUV [173]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 91
                            }
                        ],
                        "text": "Skin/nonskin classification is carried out using the class-conditional density function in YES color space followed by smoothing in order to yield contiguous regions."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Tekalp, \u00aaFrontal-View Face Detection and Facial Feature Extraction Using Color, Shape and Symmetry Based Cost Functions,\u00ba Pattern Recognition Letters"
            },
            "venue": {
                "fragments": [],
                "text": "Tekalp, \u00aaFrontal-View Face Detection and Facial Feature Extraction Using Color, Shape and Symmetry Based Cost Functions,\u00ba Pattern Recognition Letters"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Shufelt, \u00aaPerformance Evaluation and Analysis of Monocular Building Extraction"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Analysis and Machine Intelligence"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "5: Programs for Machine Learning"
            },
            "venue": {
                "fragments": [],
                "text": "5: Programs for Machine Learning"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 277,
                                "start": 273
                            }
                        ],
                        "text": "Several color spaces have been utilized to label pixels as skin including RGB [66], [67], [137], normalized RGB [102], [29], [149], [172], [30], [105], [171], [77], [151], [120], HSV (or HSI) [138], [79], [147], [146], YCrCb [167], [17], YIQ [31], [32], YES [131], CIE XYZ [19], and CIE LUV [173]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 86
                            }
                        ],
                        "text": "They used two fuzzy models to describe the distribution of skin and hair color in CIE XYZ color space."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Face Detection by Fuzzy Matching"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. Fifth IEEE Int'l Conf. Computer Vision"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Index Terms\u2014Face detection, face recognition, object recognition, view-based recognition, statistical pattern recognition, machine learning."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "J. Sejnowski, \u00aaClassifying Facial Actions IEEE Trans. Pattern Analysis and Machine Intelligence"
            },
            "venue": {
                "fragments": [],
                "text": "J. Sejnowski, \u00aaClassifying Facial Actions IEEE Trans. Pattern Analysis and Machine Intelligence"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00aaA SNoW-Based Face Detector,\u00ba Advances in Neural Information Processing Systems"
            },
            "venue": {
                "fragments": [],
                "text": "\u00aaA SNoW-Based Face Detector,\u00ba Advances in Neural Information Processing Systems"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 97
                            }
                        ],
                        "text": "Sinha used a small set of spatial image invariants to describe the space of face patterns [143], [144]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Processing and Recognizing 3D Forms"
            },
            "venue": {
                "fragments": [],
                "text": "Massachusetts Inst. of Technology"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Akamatsu, \u00aaDetection of Human Faces in Complex Scene Images by Use of a Skin Color Model and Invariant Fourier-Mellin Moments"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. Int'l Conf. Pattern Recognition"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 210,
                                "start": 205
                            }
                        ],
                        "text": "Several color spaces have been utilized to label pixels as skin including RGB [66], [67], [137], normalized RGB [102], [29], [149], [172], [30], [105], [171], [77], [151], [120], HSV (or HSI) [138], [79], [147], [146], YCrCb [167], [17], YIQ [31], [32], YES [131], CIE XYZ [19], and CIE LUV [173]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Face Localization and Feature Extraction Based on Shape and Color Information"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IEEE Int'l Conf. Image Processing"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Index Terms\u2014Face detection, face recognition, object recognition, view-based recognition, statistical pattern recognition, machine learning."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Probabilistic Visual Learning for Object Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Analysis and Machine Intelligence"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 127
                            }
                        ],
                        "text": "In contrast to template matching, the models (or templates) are learned from a set of training images which should capture the representative variability of facial appearance."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00aaPicture Processing by Computer Complex and Recognition of Human Faces"
            },
            "venue": {
                "fragments": [],
                "text": "\u00aaPicture Processing by Computer Complex and Recognition of Human Faces"
            },
            "year": 1973
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 122
                            }
                        ],
                        "text": "They used a set of 150 images for experiments in which a face is considered correctly detected if any constellation correctly locates three or more features on the faces."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00aaShape Distributions for Landmark Data,\u00ba Advanced Applied Probability"
            },
            "venue": {
                "fragments": [],
                "text": "\u00aaShape Distributions for Landmark Data,\u00ba Advanced Applied Probability"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 93
                            }
                        ],
                        "text": "SVMs have also been used to detect faces and pedestrians in the wavelet domain [106], [108], [109]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 169
                            }
                        ],
                        "text": "The idea of using intensity differences between local adjacent regions has later been extended to a wavelet-based representation for pedestrian, car, and face detection [109]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Trainable System for Object Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Int'l J. Computer Vision"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00aaThe SNoW Learning Architecture"
            },
            "venue": {
                "fragments": [],
                "text": "\u00aaThe SNoW Learning Architecture"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 101
                            }
                        ],
                        "text": "In training the SNoW-based face detector, 1,681 face images from Olivetti [136], UMIST [56], Harvard [57], Yale [7], and FERET [115] databases are used to capture the variations in face patterns."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 112
                            }
                        ],
                        "text": "The Harvard database consists of cropped, masked frontal face images taken from a wide variety of light sources [57]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Deformable Model for Face Recognition Under Arbitrary Lighting Conditions"
            },
            "venue": {
                "fragments": [],
                "text": "A Deformable Model for Face Recognition Under Arbitrary Lighting Conditions"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 109
                            }
                        ],
                        "text": "One attractive feature of this method is that a coarse-to-fine or focus-of-attention strategy is used to reduce the required computation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The M2VTS Multimodal Face Database"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. First Int'l Conf. Audio-and Video-Based Biometric Person Authentication"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00aaArtificial Neural Network Architectures for Human Face Detection,\u00ba Intelligent Eng"
            },
            "venue": {
                "fragments": [],
                "text": "Systems through Artificial Neural Networks"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 109
                            }
                        ],
                        "text": "In contrast to the knowledge-based top-down approach, researchers have been trying to find invariant features of faces for detection."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Sirohey, \u00aaHuman Face Segmentation and Identification"
            },
            "venue": {
                "fragments": [],
                "text": "Sirohey, \u00aaHuman Face Segmentation and Identification"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00aaAutomatic Extraction of Face Features,\u00ba Pattern Recognition Letters"
            },
            "venue": {
                "fragments": [],
                "text": "\u00aaAutomatic Extraction of Face Features,\u00ba Pattern Recognition Letters"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00aaAutomatic Extraction of Face Features,\u00ba Pattern Recognition Letters"
            },
            "venue": {
                "fragments": [],
                "text": "\u00aaAutomatic Extraction of Face Features,\u00ba Pattern Recognition Letters"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A SNoW-Based Face Detector Advances in Neural Information Processing Systems"
            },
            "venue": {
                "fragments": [],
                "text": "A SNoW-Based Face Detector Advances in Neural Information Processing Systems"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 44
                            }
                        ],
                        "text": "autoassociative neural networks [43], [42], [44]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Fast and Accuract Face Detector Based on Neural Networks"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Analysis and Machine Intelligence"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Index Terms\u2014Face detection, face recognition, object recognition, view-based recognition, statistical pattern recognition, machine learning."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00aaProbabilistic Visual Learning for Object Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Analysis and Machine Intelligence"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 88
                            }
                        ],
                        "text": "A localization method based on multiple templates for facial components was proposed in [150]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Detection of Face Orientation and Facial Components Using Distributed Appearance Modeling"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. First Int'l Workshop Automatic Face and Gesture Recognition"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 210,
                                "start": 205
                            }
                        ],
                        "text": "Several color spaces have been utilized to label pixels as skin including RGB [66], [67], [137], normalized RGB [102], [29], [149], [172], [30], [105], [171], [77], [151], [120], HSV (or HSI) [138], [79], [147], [146], YCrCb [167], [17], YIQ [31], [32], YES [131], CIE XYZ [19], and CIE LUV [173]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Pitas, \u00aaFace Localization and Feature Extraction Based on Shape and Color Information"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IEEE Int'l Conf. Image Processing"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00aaExtraction of FaceRecognition from Monochromatic Photographs Using Neural Networks"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. Second Int'l Conf. Automation, Robotics, and Computer Vision"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00aaA Deformable Model for Face Recognition Under Arbitrary Lighting Conditions"
            },
            "venue": {
                "fragments": [],
                "text": "\u00aaA Deformable Model for Face Recognition Under Arbitrary Lighting Conditions"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Extraction of FaceRecognition from Monochromatic Photographs Using Neural Networks"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. Second Int'l Conf. Automation, Robotics, and Computer Vision"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 26
                            }
                        ],
                        "text": "A multiresolution hierarchy of images is created by averaging and subsampling, and an example is shown in Fig."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Picture Processing by Computer Complex and Recognition of Human Faces"
            },
            "venue": {
                "fragments": [],
                "text": "Picture Processing by Computer Complex and Recognition of Human Faces"
            },
            "year": 1973
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 203,
                                "start": 199
                            }
                        ],
                        "text": "Several color spaces have been utilized to label pixels as skin including RGB [66], [67], [137], normalized RGB [102], [29], [149], [172], [30], [105], [171], [77], [151], [120], HSV (or HSI) [138], [79], [147], [146], YCrCb [167], [17], YIQ [31], [32], YES [131], CIE XYZ [19], and CIE LUV [173]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00aaFinding Skin in Color Images,\u00ba Proc. Second Int'l Conf. Automatic Face and Gesture Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "\u00aaFinding Skin in Color Images,\u00ba Proc. Second Int'l Conf. Automatic Face and Gesture Recognition"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00aaFace Detection Using Templates,\u00ba Proc. Int'l Conf. Pattern Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "\u00aaFace Detection Using Templates,\u00ba Proc. Int'l Conf. Pattern Recognition"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 2
                            }
                        ],
                        "text": "It uses an edge map (Canny detector [15]) and heuristics to remove and group edges so that only the ones on the face\ncontour are preserved."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00aaMultiresolution Face Detection,\u00ba Theoretical Foundations of Computer Vision"
            },
            "venue": {
                "fragments": [],
                "text": "\u00aaMultiresolution Face Detection,\u00ba Theoretical Foundations of Computer Vision"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 100
                            }
                        ],
                        "text": "A mixture model of factor analyzers has recently been extended [49] and applied to face recognition [46]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Mixtures of Local Subspaces for Face Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IEEE Conf. Computer Vision and Pattern Recognition"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "For more information on this or any other computing topic, please visit our Digital Library at http://computer.org/publications/dlib"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE"
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00aaFinding Faces in Photographs,\u00ba Proc. Sixth IEEE Int'l Conf. Computer Vision"
            },
            "venue": {
                "fragments": [],
                "text": "\u00aaFinding Faces in Photographs,\u00ba Proc. Sixth IEEE Int'l Conf. Computer Vision"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 151
                            }
                        ],
                        "text": "Ranking of constellations is based on a probability density function that a constellation corresponds to a face versus the probability it was generated by an alternative mechanism (i.e., nonface)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00aaFace Localization via Shape Statistics,\u00ba Proc. First Int'l Workshop Automatic Face and Gesture Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "\u00aaFace Localization via Shape Statistics,\u00ba Proc. First Int'l Workshop Automatic Face and Gesture Recognition"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "[148] scans an input image with a time-delay neural network [166] (with a receptive field of 20 25 pixels) to detect faces."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Multi-Modular Neural Network Architectures: Pattern Recognition Applications in Optical Character Recognition and Human Face Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Int'l J. Pattern Recognition and Artificial Intelligence"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "J. Royal Statistical Soc"
            },
            "venue": {
                "fragments": [],
                "text": "J. Royal Statistical Soc"
            },
            "year": 1978
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Srihari, \u00aaLocating Human Faces in Newspaper Photographs,\u00ba Proc. IEEE Conf. Computer Vision and Pattern Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Srihari, \u00aaLocating Human Faces in Newspaper Photographs,\u00ba Proc. IEEE Conf. Computer Vision and Pattern Recognition"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 81
                            }
                        ],
                        "text": "Given a single image, the goal of face detection is to identify all image regions which contain a face regardless of its three-dimensional position, orientation, and lighting conditions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Analysis and Machine Intelligence"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Analysis and Machine Intelligence"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Srihari, \u00aaLocating Human Faces in Newspaper Photographs,\u00ba Proc. IEEE Conf. Computer Vision and Pattern Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Srihari, \u00aaLocating Human Faces in Newspaper Photographs,\u00ba Proc. IEEE Conf. Computer Vision and Pattern Recognition"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Collobert, \u00aaA Fast and Accuract Face Detector Based on Neural Networks"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Analysis and Machine Intelligence"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 125
                            }
                        ],
                        "text": "Augusteijn and Skufca developed a method that infers the presence of a face through the identification of face-like textures [6]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Skujca, \u00aaIdentification of Human Faces through Texture-Based Feature Recognition and Neural Network Technology"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IEEE Conf. Neural Networks"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 165
                            }
                        ],
                        "text": "Several color spaces have been utilized to label pixels as skin including RGB [66], [67], [137], normalized RGB [102], [29], [149], [172], [30], [105], [171], [77], [151], [120], HSV (or HSI) [138], [79], [147], [146], YCrCb [167], [17], YIQ [31], [32], YES [131], CIE XYZ [19], and CIE LUV [173]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00aaFace Detection Based on Color and Local Symmetry Information,\u00ba Proc. Third Int'l Conf. Automatic Face and Gesture Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "\u00aaFace Detection Based on Color and Local Symmetry Information,\u00ba Proc. Third Int'l Conf. Automatic Face and Gesture Recognition"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 125
                            }
                        ],
                        "text": "Several color spaces have been utilized to label pixels as skin including RGB [66], [67], [137], normalized RGB [102], [29], [149], [172], [30], [105], [171], [77], [151], [120], HSV (or HSI) [138], [79], [147], [146], YCrCb [167], [17], YIQ [31], [32], YES [131], CIE XYZ [19], and CIE LUV [173]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Real-Time ASL Recognition from Video Using HMM's"
            },
            "venue": {
                "fragments": [],
                "text": "Real-Time ASL Recognition from Video Using HMM's"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 82
                            }
                        ],
                        "text": "Such issues have attracted much attention in numerous vision problems [21], [60], [142], [115]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Performance Evaluation and Analysis of Monocular Building Extraction"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Analysis and Machine Intelligence"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Index Terms\u2014Face detection, face recognition, object recognition, view-based recognition, statistical pattern recognition, machine learning."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00aaFinding Face Features,\u00ba Proc. Second European Conf. Computer Vision"
            },
            "venue": {
                "fragments": [],
                "text": "\u00aaFinding Face Features,\u00ba Proc. Second European Conf. Computer Vision"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Analysis and Machine Intelligence"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Analysis and Machine Intelligence"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00aaDetection and Tracking of Human Face with Synthesized Templates"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. First Asian Conf. Computer Vision"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 62
                            }
                        ],
                        "text": "proposed a method that uses SNoW learning architecture [125], [16] to detect faces with different features and expressions, in different poses, and under different lighting conditions [176]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The SNoW Learning Architecture"
            },
            "venue": {
                "fragments": [],
                "text": "The SNoW Learning Architecture"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 146
                            }
                        ],
                        "text": "Face localization aims to determine the image position of a single face; this is a simplified detection problem with the assumption that an input image contains only one face [85], [103]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 35
                            }
                        ],
                        "text": "One problem with these feature-based algorithms is that the image features can be severely corrupted due to illumination, noise, and occlusion."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 136
                            }
                        ],
                        "text": "In contrast to template matching, the models (or templates) are learned from a set of training images which should capture the representative variability of facial appearance."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00aaHuman Face Detection in Complex Background,\u00ba Pattern Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "\u00aaHuman Face Detection in Complex Background,\u00ba Pattern Recognition"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Index Terms\u2014Face detection, face recognition, object recognition, view-based recognition, statistical pattern recognition, machine learning."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00aaAutomatic Recognition and Analysis of Human Faces and Facial Expressions: A Survey,\u00ba Pattern Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "\u00aaAutomatic Recognition and Analysis of Human Faces and Facial Expressions: A Survey,\u00ba Pattern Recognition"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00aaMixtures of Local Subspaces for Face Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IEEE Conf. Computer Vision and Pattern Recognition"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Methoden in der Wahrscheinlichkeitsrechnung ,\u00ba Annales Academiae Sciientiarum Fennicae, Series AI: Mathematica-Physica"
            },
            "venue": {
                "fragments": [],
                "text": "Methoden in der Wahrscheinlichkeitsrechnung ,\u00ba Annales Academiae Sciientiarum Fennicae, Series AI: Mathematica-Physica"
            },
            "year": 1946
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 179
                            }
                        ],
                        "text": "Although different people have different skin color, several studies have shown that the major difference lies largely between their intensity rather than their chrominance [54], [55], [172]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00aaMultimodal System for Locating Heads and Faces,\u00ba Proc. Second Int'l Conf. Automatic Face and Gesture Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "\u00aaMultimodal System for Locating Heads and Faces,\u00ba Proc. Second Int'l Conf. Automatic Face and Gesture Recognition"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Index Terms\u2014Face detection, face recognition, object recognition, view-based recognition, statistical pattern recognition, machine learning."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Tefas, and I. Pitas, \u00aaFrontal Face Authentication Uing Variants of Dynamic Link Matching Based on Mathematical Morphology"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IEEE Int'l Conf. Image Processing"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 14
                            }
                        ],
                        "text": "Duta and Jain [38] presented a method to learn the face concept using Mitchell\u2019s Find-S algorithm [101]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning the Human Face Concept from Black and White Pictures"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. Int'l Conf. Pattern Recognition"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 42
                            }
                        ],
                        "text": "Graf et al. developed a method to locate facial features and faces in gray scale images [54]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Multiresolution Face Detection"
            },
            "venue": {
                "fragments": [],
                "text": "Theoretical Foundations of Computer Vision"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00aaObject Recognition via Image Invariants: A Case Study"
            },
            "venue": {
                "fragments": [],
                "text": "Investigative Ophthalmology and Visual Science"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00aaA Trainable System for Object Recognition,\u00ba Int'l J. Computer Vision"
            },
            "venue": {
                "fragments": [],
                "text": "\u00aaA Trainable System for Object Recognition,\u00ba Int'l J. Computer Vision"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 145
                            }
                        ],
                        "text": "A set of hypotheses for the existence of a face is then defined in terms of the hypotheses for facial components using the DempsterShafer theory [34]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Generalization of Bayesian Theory"
            },
            "venue": {
                "fragments": [],
                "text": "J. Royal Statistical Soc"
            },
            "year": 1978
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Index Terms\u2014Face detection, face recognition, object recognition, view-based recognition, statistical pattern recognition, machine learning."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 173
                            }
                        ],
                        "text": "Although different people have different skin color, several studies have shown that the major difference lies largely between their intensity rather than their chrominance [54], [55], [172]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 100
                            }
                        ],
                        "text": "Combinations of such areas are then evaluated with classifiers, to determine whether and where a face is present."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Locating Faces and Facial Parts"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. First Int'l Workshop Automatic Face and Gesture Recognition"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00aaPedestrian Detection Using Wavelet Templates,\u00ba Proc. IEEE Conf. Computer Vision and Pattern Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "\u00aaPedestrian Detection Using Wavelet Templates,\u00ba Proc. IEEE Conf. Computer Vision and Pattern Recognition"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00aaA Novel Approach to Color Constancy,\u00ba Int"
            },
            "venue": {
                "fragments": [],
                "text": "J. Computer Vision"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "J. Computer Vision"
            },
            "venue": {
                "fragments": [],
                "text": "J. Computer Vision"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 128
                            }
                        ],
                        "text": "For example, template matching methods usually use a face model and subtemplates to extract facial features [132], [27], [180], [143], [51], and then use these features to locate or detect faces."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 90
                            }
                        ],
                        "text": "Sinha used a small set of spatial image invariants to describe the space of face patterns [143], [144]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 210,
                                "start": 205
                            }
                        ],
                        "text": "Furthermore, the boundary between knowledge-based methods and some template matching methods is blurry since the latter usually implicitly applies human knowledge to define the face templates [132], [28], [143]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Object Recognition via Image Invariants: A Case Study"
            },
            "venue": {
                "fragments": [],
                "text": "Investigative Ophthalmology and Visual Science"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00aaDetection and Localization of Faces on Digital Images,\u00ba Pattern Recognition Letters"
            },
            "venue": {
                "fragments": [],
                "text": "\u00aaDetection and Localization of Faces on Digital Images,\u00ba Pattern Recognition Letters"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 277,
                                "start": 273
                            }
                        ],
                        "text": "Several color spaces have been utilized to label pixels as skin including RGB [66], [67], [137], normalized RGB [102], [29], [149], [172], [30], [105], [171], [77], [151], [120], HSV (or HSI) [138], [79], [147], [146], YCrCb [167], [17], YIQ [31], [32], YES [131], CIE XYZ [19], and CIE LUV [173]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 86
                            }
                        ],
                        "text": "They used two fuzzy models to describe the distribution of skin and hair color in CIE XYZ color space."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00aaFace Detection by Fuzzy Matching,\u00ba Proc. Fifth IEEE Int'l Conf. Computer Vision"
            },
            "venue": {
                "fragments": [],
                "text": "\u00aaFace Detection by Fuzzy Matching,\u00ba Proc. Fifth IEEE Int'l Conf. Computer Vision"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Kotropoulos and Pitas [81] presented a rule-based localization method which is similar to [71] and [170]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "First, facial features are located with a projection method that Kanade successfully used to locate the boundary of a face [71]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Picture processing by computer complex and recogniton of human faces"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1973
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 125
                            }
                        ],
                        "text": "Several color spaces have been utilized to label pixels as skin including RGB [66], [67], [137], normalized RGB [102], [29], [149], [172], [30], [105], [171], [77], [151], [120], HSV (or HSI) [138], [79], [147], [146], YCrCb [167], [17], YIQ [31], [32], YES [131], CIE XYZ [19], and CIE LUV [173]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00aaReal-Time ASL Recognition from Video Using HMM's"
            },
            "venue": {
                "fragments": [],
                "text": "\u00aaReal-Time ASL Recognition from Video Using HMM's"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 95
                            }
                        ],
                        "text": "They used a set of 150 images for experiments in which a face is considered correctly detected if any constellation correctly locates three or more features on the faces."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Procrustean Metrics, and Complex Projective Shapes,\u00ba Bull"
            },
            "venue": {
                "fragments": [],
                "text": "London Math. Soc"
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00aaHuman Face Detection in Visual Scenes,\u00ba Advances in Neural Information Processing Systems"
            },
            "venue": {
                "fragments": [],
                "text": "\u00aaHuman Face Detection in Visual Scenes,\u00ba Advances in Neural Information Processing Systems"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00aaMulti-Modular Neural Network Architectures: Pattern Recognition Applications in Optical Character Recognition and Human Face Recognition,\u00ba Int"
            },
            "venue": {
                "fragments": [],
                "text": "J. Pattern Recognition and Artificial Intelligence"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00aaLocating Human Faces in Photographs,\u00ba Int"
            },
            "venue": {
                "fragments": [],
                "text": "J. Computer Vision"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Sirovich, \u00aaApplication of the Karhunen-Loe \u00c1ve Procedure for the Characterization of Human Faces"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Analysis and Machine Intelligence"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 11
                            }
                        ],
                        "text": "The model uses streaks to represent the outlines of the faces, eyebrows, and lips."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Index Terms\u2014Face detection, face recognition, object recognition, view-based recognition, statistical pattern recognition, machine learning."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 173
                            }
                        ],
                        "text": "Although different people have different skin color, several studies have shown that the major difference lies largely between their intensity rather than their chrominance [54], [55], [172]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00aaLocating Faces and Facial Parts,\u00ba Proc. First Int'l Workshop Automatic Face and Gesture Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "\u00aaLocating Faces and Facial Parts,\u00ba Proc. First Int'l Workshop Automatic Face and Gesture Recognition"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00aaLearning the Human Face Concept from Black and White Pictures"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. Int'l Conf. Pattern Recognition"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00aaHuman Face Detection Using Silhouettes,\u00ba Int"
            },
            "venue": {
                "fragments": [],
                "text": "J. Pattern Recognition and Artificial Intelligence"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u00aaDetection of Face Orientation and Facial Components Using Distributed Appearance Modeling"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. First Int'l Workshop Automatic Face and Gesture Recognition"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Kriegman, \u00aaMixtures of Linear Subspaces for Face Detection"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. Fourth Int'l Conf. Automatic Face and Gesture Recognition"
            },
            "year": 2000
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 50,
            "methodology": 79,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 251,
        "totalPages": 26
    },
    "page_url": "https://www.semanticscholar.org/paper/Detecting-Faces-in-Images:-A-Survey-Yang-Kriegman/ebb34b75982f628f9ce5995821fff81fd967dc2d?sort=total-citations"
}