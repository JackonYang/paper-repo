{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3727644"
                        ],
                        "name": "Satoshi Tsutsui",
                        "slug": "Satoshi-Tsutsui",
                        "structuredName": {
                            "firstName": "Satoshi",
                            "lastName": "Tsutsui",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Satoshi Tsutsui"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2821130"
                        ],
                        "name": "David J. Crandall",
                        "slug": "David-J.-Crandall",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Crandall",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David J. Crandall"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 134
                            }
                        ],
                        "text": "Shared tasks such as ImageCLEF [9, 10] also helped drive more attention to compound figure detection [29], compound figure separation [23], medical image annotation [17], among other tasks related to medical images."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15751448,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "391da573d7960249b0bab04e2b8791e88d0c946a",
            "isKey": false,
            "numCitedBy": 32,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "A key problem in automatic analysis and understanding of scientific papers is to extract semantic information from non-textual paper components like figures, diagrams, tables, etc. Much of this work requires a very first preprocessing step: decomposing compound multi-part figures into individual sub-figures. Previous work in compound figure separation has been based on manually designed features and separation rules, which often fail for less common figure types and layouts. Moreover, few implementations for compound figure decomposition are publicly available. This paper proposes a data driven approach to separate compound figures using modern deep Convolutional Neural Networks (CNNs) to train the separator in an end-to-end manner. CNNs eliminate the need for manually designing features and separation rules, but require a large amount of annotated training data. We overcome this challenge using transfer learning as well as automatically synthesizing training exemplars. We evaluate our technique on the ImageCLEF Medical dataset, achieving 85.9% accuracy and outperforming previous techniques. We have released our implementation as an easy-to-use Python library, aiming to promote further research in scientific figure mining."
            },
            "slug": "A-Data-Driven-Approach-for-Compound-Figure-Using-Tsutsui-Crandall",
            "title": {
                "fragments": [],
                "text": "A Data Driven Approach for Compound Figure Separation Using Convolutional Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A data driven approach to separate compound figures using modern deep Convolutional Neural Networks to train the separator in an end-to-end manner is proposed, using transfer learning as well as automatically synthesizing training exemplars."
            },
            "venue": {
                "fragments": [],
                "text": "2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1500370330"
                        ],
                        "name": "Noah Siegel",
                        "slug": "Noah-Siegel",
                        "structuredName": {
                            "firstName": "Noah",
                            "lastName": "Siegel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noah Siegel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2803574"
                        ],
                        "name": "Zachary Horvitz",
                        "slug": "Zachary-Horvitz",
                        "structuredName": {
                            "firstName": "Zachary",
                            "lastName": "Horvitz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zachary Horvitz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145872888"
                        ],
                        "name": "Roie Levin",
                        "slug": "Roie-Levin",
                        "structuredName": {
                            "firstName": "Roie",
                            "lastName": "Levin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Roie Levin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2038685"
                        ],
                        "name": "S. Divvala",
                        "slug": "S.-Divvala",
                        "structuredName": {
                            "firstName": "Santosh",
                            "lastName": "Divvala",
                            "middleNames": [
                                "Kumar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Divvala"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143787583"
                        ],
                        "name": "Ali Farhadi",
                        "slug": "Ali-Farhadi",
                        "structuredName": {
                            "firstName": "Ali",
                            "lastName": "Farhadi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ali Farhadi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 92
                            }
                        ],
                        "text": "Researchers have considered a range of tasks from extracting the underlying data from plots [5, 20], to the use of figures in search engines and broader information extraction systems [4, 24, 26]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 108
                            }
                        ],
                        "text": "Although crowdsourcing has been successfully used to construct useful image datasets such as ImageNet [11], [20] found that crowdsourcing figure annotations in research papers yielded low inter-annotator agreement and significant noise due to workers\u2019 lack of familiarity with scholarly documents."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7857660,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0d0a05fa74dd5ed8aed59bd7a92420e034d64cf1",
            "isKey": false,
            "numCitedBy": 96,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "\u2018Which are the pedestrian detectors that yield a precision above 95 % at 25 % recall?\u2019 Answering such a complex query involves identifying and analyzing the results reported in figures within several research papers. Despite the availability of excellent academic search engines, retrieving such information poses a cumbersome challenge today as these systems have primarily focused on understanding the text content of scholarly documents. In this paper, we introduce FigureSeer, an end-to-end framework for parsing result-figures, that enables powerful search and retrieval of results in research papers. Our proposed approach automatically localizes figures from research papers, classifies them, and analyses the content of the result-figures. The key challenge in analyzing the figure content is the extraction of the plotted data and its association with the legend entries. We address this challenge by formulating a novel graph-based reasoning approach using a CNN-based similarity metric. We present a thorough evaluation on a real-word annotated dataset to demonstrate the efficacy of our approach."
            },
            "slug": "FigureSeer:-Parsing-Result-Figures-in-Research-Siegel-Horvitz",
            "title": {
                "fragments": [],
                "text": "FigureSeer: Parsing Result-Figures in Research Papers"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper introduces FigureSeer, an end-to-end framework for parsing result-figures, that enables powerful search and retrieval of results in research papers and formulates a novel graph-based reasoning approach using a CNN-based similarity metric."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143997772"
                        ],
                        "name": "Christopher Clark",
                        "slug": "Christopher-Clark",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Clark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher Clark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2038685"
                        ],
                        "name": "S. Divvala",
                        "slug": "S.-Divvala",
                        "structuredName": {
                            "firstName": "Santosh",
                            "lastName": "Divvala",
                            "middleNames": [
                                "Kumar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Divvala"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 82
                            }
                        ],
                        "text": "In particular, [6] extract figures in research papers at NIPS, ICML and AAAI, and [7] extend their work to address papers in computer science more generally; however, stylistic conventions vary widely across academic fields, and since previous methods relied primarily on hand-designed features from computer science papers, they do not generalize well to other scientific domains, as we show in section 5."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 43
                            }
                        ],
                        "text": "Our evaluation methodology follows that of [7]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 106
                            }
                        ],
                        "text": "Previous work on this task has focused only on limited domains; [6] focused only on 3 AI conferences, and [7] concentrated on papers only within computer science."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 16
                            }
                        ],
                        "text": "In keeping with [7], a predicted bounding"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 64
                            }
                        ],
                        "text": "As shown in Table 3, DeepFigures underperorms by 3 F1 points on \u201cCS-Large\u201d, but achieves a 17 point improvement on the \u201cPubMed\u201d dataset."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 4
                            }
                        ],
                        "text": "See [7] for more details on matching captions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 85
                            }
                        ],
                        "text": "Given that PDFFigures 2.0 is a rule-based method that was tuned specifically for the \u201cCS-Large\u201d test set, it is unsurprising to see that it works better than DeepFigures for this domain."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 25
                            }
                        ],
                        "text": "\u2022 The \u201cCS-Large\u201d dataset [7]: To our knowledge, this was previously the largest dataset for the task of figure extraction."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 51
                            }
                        ],
                        "text": "We compare with two manually labeled datasets:\n\u2022 The \u201cCS-Large\u201d dataset [7]: To our knowledge, this was previously the largest dataset for the task of figure extraction."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 99
                            }
                        ],
                        "text": "Our figure extraction pipeline extracts captions\u2019 text and bounding boxes using the same method as [7], finding paragraphs starting with a string that matches a regular expression capturing variations of \u201cFigure N."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 89
                            }
                        ],
                        "text": "We run evaluation on two datasets: the \u201cCS-Large\u201d computer science dataset introduced by [7], and a new dataset we introduce using papers randomly sampled from PubMed."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 2
                            }
                        ],
                        "text": "0 [7], the previous state of the art for the task of figure extraction."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 82
                            }
                        ],
                        "text": "cur_dist = sum(table_words) Dataset Manually-labeled Induced labels name CS-Large [7] PubMed LaTeX XML"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2998907,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fe2e70569abf4aa90f1ebf9b90b51120ae6de57e",
            "isKey": true,
            "numCitedBy": 83,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Figures and tables are key sources of information in many scholarly documents. However, current academic search engines do not make use of figures and tables when semantically parsing documents or presenting document summaries to users. To facilitate these applications we develop an algorithm that extracts figures, tables, and captions from documents called \u201cPDFFigures 2.0.\u201d Our proposed approach analyzes the structure of individual pages by detecting captions, graphical elements, and chunks of body text, and then locates figures and tables by reasoning about the empty regions within that text. To evaluate our work, we introduce a new dataset of computer science papers, along with ground truth labels for the locations of the figures, tables, and captions within them. Our algorithm achieves impressive results (94% precision at 90% recall) on this dataset surpassing previous state of the art. Further, we show how our framework was used to extract figures from a corpus of over one million papers, and how the resulting extractions were integrated into the user interface of a smart academic search engine, Semantic Scholar (www.semanticscholar.org). Finally, we present results of exploratory data analysis completed on the extracted figures as well as an extension of our method for the task of section title extraction. We release our dataset and code on our project webpage for enabling future research (http://pdffigures2.allenai.org)."
            },
            "slug": "PDFFigures-2.0:-Mining-figures-from-research-papers-Clark-Divvala",
            "title": {
                "fragments": [],
                "text": "PDFFigures 2.0: Mining figures from research papers"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "An algorithm that extracts figures, tables, and captions from documents called \u201cPDFFigures 2.0\u201d that analyzes the structure of individual pages by detecting captions, graphical elements, and chunks of body text, and then locates figures and tables by reasoning about the empty regions within that text."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE/ACM Joint Conference on Digital Libraries (JCDL)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112096491"
                        ],
                        "name": "Xiao Yang",
                        "slug": "Xiao-Yang",
                        "structuredName": {
                            "firstName": "Xiao",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiao Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8020964"
                        ],
                        "name": "Ersin Yumer",
                        "slug": "Ersin-Yumer",
                        "structuredName": {
                            "firstName": "Ersin",
                            "lastName": "Yumer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ersin Yumer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2934421"
                        ],
                        "name": "Paul Asente",
                        "slug": "Paul-Asente",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Asente",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paul Asente"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1389971134"
                        ],
                        "name": "Mike Kraley",
                        "slug": "Mike-Kraley",
                        "structuredName": {
                            "firstName": "Mike",
                            "lastName": "Kraley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mike Kraley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1852261"
                        ],
                        "name": "Daniel Kifer",
                        "slug": "Daniel-Kifer",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Kifer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Kifer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145157784"
                        ],
                        "name": "C. Lee Giles",
                        "slug": "C.-Lee-Giles",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Giles",
                            "middleNames": [
                                "Lee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lee Giles"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2272015,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9baae0bdc2884bcf0aa4063914b87d60952cb678",
            "isKey": false,
            "numCitedBy": 145,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an end-to-end, multimodal, fully convolutional network for extracting semantic structures from document images. We consider document semantic structure extraction as a pixel-wise segmentation task, and propose a unified model that classifies pixels based not only on their visual appearance, as in the traditional page segmentation task, but also on the content of underlying text. Moreover, we propose an efficient synthetic document generation process that we use to generate pretraining data for our network. Once the network is trained on a large set of synthetic documents, we fine-tune the network on unlabeled real documents using a semi-supervised approach. We systematically study the optimum network architecture and show that both our multimodal approach and the synthetic data pretraining significantly boost the performance."
            },
            "slug": "Learning-to-Extract-Semantic-Structure-from-Using-Yang-Yumer",
            "title": {
                "fragments": [],
                "text": "Learning to Extract Semantic Structure from Documents Using Multimodal Fully Convolutional Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "An end-to-end, multimodal, fully convolutional network for extracting semantic structures from document images using a unified model that classifies pixels based not only on their visual appearance, as in the traditional page segmentation task, but also on the content of underlying text."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3418028"
                        ],
                        "name": "Yuhai Yu",
                        "slug": "Yuhai-Yu",
                        "structuredName": {
                            "firstName": "Yuhai",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuhai Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37553559"
                        ],
                        "name": "Hongfei Lin",
                        "slug": "Hongfei-Lin",
                        "structuredName": {
                            "firstName": "Hongfei",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hongfei Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39595959"
                        ],
                        "name": "Jiana Meng",
                        "slug": "Jiana-Meng",
                        "structuredName": {
                            "firstName": "Jiana",
                            "lastName": "Meng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiana Meng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "19320049"
                        ],
                        "name": "Xiaocong Wei",
                        "slug": "Xiaocong-Wei",
                        "structuredName": {
                            "firstName": "Xiaocong",
                            "lastName": "Wei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaocong Wei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3009486"
                        ],
                        "name": "Zhehuan Zhao",
                        "slug": "Zhehuan-Zhao",
                        "structuredName": {
                            "firstName": "Zhehuan",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhehuan Zhao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 101
                            }
                        ],
                        "text": "Shared tasks such as ImageCLEF [9, 10] also helped drive more attention to compound figure detection [29], compound figure separation [23], medical image annotation [17], among other tasks related to medical images."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 26628450,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ce565192b2bcfbd12e31351ef929e1d421856153",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Compound figure detection on figures and associated captions is the first step to making medical figures from biomedical literature available for further analysis. The performance of traditional methods is limited to the choice of hand-engineering features and prior domain knowledge. We train multiple convolutional neural networks (CNNs), long short-term memory (LSTM) networks, and gated recurrent unit (GRU) networks on top of pre-trained word vectors to learn textual features from captions and employ deep CNNs to learn visual features from figures. We then identify compound figures by combining textual and visual prediction. Our proposed architecture obtains remarkable performance in three run types\u2014textual, visual and mixed\u2014and achieves better performance in ImageCLEF2015 and ImageCLEF2016."
            },
            "slug": "Assembling-Deep-Neural-Networks-for-Medical-Figure-Yu-Lin",
            "title": {
                "fragments": [],
                "text": "Assembling Deep Neural Networks for Medical Compound Figure Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work trains multiple convolutional neural networks, long short-term memory networks, and gated recurrent unit networks on top of pre-trained word vectors to learn textual features from captions and employ deep CNNs to learn visual features from figures."
            },
            "venue": {
                "fragments": [],
                "text": "Inf."
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143997772"
                        ],
                        "name": "Christopher Clark",
                        "slug": "Christopher-Clark",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Clark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher Clark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2038685"
                        ],
                        "name": "S. Divvala",
                        "slug": "S.-Divvala",
                        "structuredName": {
                            "firstName": "Santosh",
                            "lastName": "Divvala",
                            "middleNames": [
                                "Kumar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Divvala"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 64
                            }
                        ],
                        "text": "Previous work on this task has focused only on limited domains; [6] focused only on 3 AI conferences, and [7] concentrated on papers only within computer science."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 15
                            }
                        ],
                        "text": "In particular, [6] extract figures in research papers at NIPS, ICML and AAAI, and [7] extend their work to address papers in computer science more generally; however, stylistic conventions vary widely across academic fields, and since previous methods relied primarily on hand-designed features from computer science papers, they do not generalize well to other scientific domains, as we show in section 5."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 51
                            }
                        ],
                        "text": "Finally, the baseline caption detection model from [6] performs very well."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16285667,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cdb1159ff29454df38667bc17e550cc437826931",
            "isKey": false,
            "numCitedBy": 67,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Identifying and extracting figures and tables along with their captions from scholarly articles is important both as a way of providing tools for article summarization, and as part of larger systems that seek to gain deeper, semantic understanding of these articles. While many \"off-the-shelf\" tools exist that can extract embedded images from these documents, e.g. PDFBox, Poppler, etc., these tools are unable to extract tables, captions, and figures composed of vector graphics. Our proposed approach analyzes the structure of individual pages of a document by detecting chunks of body text, and locates the areas wherein figures or tables could reside by reasoning about the empty regions within that text. This method can extract a wide variety of figures because it does not make strong assumptions about the format of the figures embedded in the document, as long as they can be differentiated from the main article's text. Our algorithm also demonstrates a caption-to-figure matching component that is effective even in cases where individual captions are adjacent to multiple figures. Our contribution also includes methods for leveraging particular consistency and formatting assumptions to identify titles, body text and captions within each article. We introduce a new dataset of 150 computer science papers along with ground truth labels for the locations of the figures, tables and captions within them. Our algorithm achieves 96% precision at 92% recall when tested against this dataset, surpassing previous state of the art. We release our dataset, code, and evaluation scripts on our project website for enabling future research."
            },
            "slug": "Looking-Beyond-Text:-Extracting-Figures,-Tables-and-Clark-Divvala",
            "title": {
                "fragments": [],
                "text": "Looking Beyond Text: Extracting Figures, Tables and Captions from Computer Science Papers"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work introduces a new dataset of 150 computer science papers along with ground truth labels for the locations of the figures, tables and captions within them and demonstrates a caption-to-figure matching component that is effective even in cases where individual captions are adjacent to multiple figures."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI Workshop: Scholarly Big Data"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3324957"
                        ],
                        "name": "Sagnik Ray Choudhury",
                        "slug": "Sagnik-Ray-Choudhury",
                        "structuredName": {
                            "firstName": "Sagnik",
                            "lastName": "Choudhury",
                            "middleNames": [
                                "Ray"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sagnik Ray Choudhury"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3339720"
                        ],
                        "name": "Shuting Wang",
                        "slug": "Shuting-Wang",
                        "structuredName": {
                            "firstName": "Shuting",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shuting Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143930195"
                        ],
                        "name": "P. Mitra",
                        "slug": "P.-Mitra",
                        "structuredName": {
                            "firstName": "Prasenjit",
                            "lastName": "Mitra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Mitra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145157784"
                        ],
                        "name": "C. Lee Giles",
                        "slug": "C.-Lee-Giles",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Giles",
                            "middleNames": [
                                "Lee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lee Giles"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 92
                            }
                        ],
                        "text": "Researchers have considered a range of tasks from extracting the underlying data from plots [5, 20], to the use of figures in search engines and broader information extraction systems [4, 24, 26]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 29962856,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bbb5570f2aba15d40565f85b0c81b0206475390c",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Line graphs are ubiquitous in scholarly papers. They are usually generated from a data table and often used to compare performances of various methods. The data in these figures can not be accessed. Manual extraction of this data is hard and not scalable. On the other hand, automated systems for such data extraction task is not yet available. We report an analysis of line graphs to explain the challenges of building a fully automated data extraction system. Next, we describe a system for automated data extraction from color line graphs. Our system has multiple components: image classification for identifying line graphs; text extraction from the figures and curve extraction. For the classification, we show that unsupervised feature learning outperforms traditional low-level image descriptors by 10%. For the text extraction, our heuristics outperforms the accuracy of the previous method by 29%. We also propose a novel curve extraction method that has an average accuracy of 82%. A large partially annotated dataset for future research is described."
            },
            "slug": "Automated-Data-Extraction-from-Scholarly-Line-Choudhury-Wang",
            "title": {
                "fragments": [],
                "text": "Automated Data Extraction from Scholarly Line Graphs"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "An analysis of line graphs is reported to explain the challenges of building a fully automated data extraction system and a novel curve extraction method is proposed that has an average accuracy of 82%."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36181176"
                        ],
                        "name": "Mike D. Mintz",
                        "slug": "Mike-D.-Mintz",
                        "structuredName": {
                            "firstName": "Mike",
                            "lastName": "Mintz",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mike D. Mintz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "87299088"
                        ],
                        "name": "Steven Bills",
                        "slug": "Steven-Bills",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Bills",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steven Bills"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144621026"
                        ],
                        "name": "R. Snow",
                        "slug": "R.-Snow",
                        "structuredName": {
                            "firstName": "Rion",
                            "lastName": "Snow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Snow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746807"
                        ],
                        "name": "Dan Jurafsky",
                        "slug": "Dan-Jurafsky",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Jurafsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dan Jurafsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 13
                            }
                        ],
                        "text": "For example, [18] used relations between entities in Freebase [1] to induce labels between pairs of corresponding entity mentions in unlabeled text, making the strong assumption that the Freebase relation is described in each sentence where both entities are mentioned."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10910955,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d84b57362e2010f6f65357267df7e0157af30684",
            "isKey": false,
            "numCitedBy": 2479,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Modern models of relation extraction for tasks like ACE are based on supervised learning of relations from small hand-labeled corpora. We investigate an alternative paradigm that does not require labeled corpora, avoiding the domain dependence of ACE-style algorithms, and allowing the use of corpora of any size. Our experiments use Freebase, a large semantic database of several thousand relations, to provide distant supervision. For each pair of entities that appears in some Freebase relation, we find all sentences containing those entities in a large unlabeled corpus and extract textual features to train a relation classifier. Our algorithm combines the advantages of supervised IE (combining 400,000 noisy pattern features in a probabilistic classifier) and unsupervised IE (extracting large numbers of relations from large corpora of any domain). Our model is able to extract 10,000 instances of 102 relations at a precision of 67.6%. We also analyze feature performance, showing that syntactic parse features are particularly helpful for relations that are ambiguous or lexically distant in their expression."
            },
            "slug": "Distant-supervision-for-relation-extraction-without-Mintz-Bills",
            "title": {
                "fragments": [],
                "text": "Distant supervision for relation extraction without labeled data"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work investigates an alternative paradigm that does not require labeled corpora, avoiding the domain dependence of ACE-style algorithms, and allowing the use of corpora of any size."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3493665"
                        ],
                        "name": "Dafang He",
                        "slug": "Dafang-He",
                        "structuredName": {
                            "firstName": "Dafang",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dafang He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145823372"
                        ],
                        "name": "Scott D. Cohen",
                        "slug": "Scott-D.-Cohen",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Cohen",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Scott D. Cohen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31844147"
                        ],
                        "name": "Brian L. Price",
                        "slug": "Brian-L.-Price",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Price",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Brian L. Price"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1852261"
                        ],
                        "name": "Daniel Kifer",
                        "slug": "Daniel-Kifer",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Kifer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Kifer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145157784"
                        ],
                        "name": "C. Lee Giles",
                        "slug": "C.-Lee-Giles",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Giles",
                            "middleNames": [
                                "Lee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lee Giles"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 162
                            }
                        ],
                        "text": "Recent work has used neural networks for semantic page segmentation, suggesting that these models can be applied to synthetic documents as well as natural scenes [3, 13, 28]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 29473897,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b3cf063c3e25b2ab30e44ba49920b811d40f7702",
            "isKey": false,
            "numCitedBy": 62,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Page segmentation and table detection play an important role in understanding the structure of documents. We present a page segmentation algorithm that incorporates state-of-the-art deep learning methods for segmenting three types of document elements: text blocks, tables, and figures. We propose a multi-scale, multi-task fully convolutional neural network (FCN) for the tasks of semantic page segmentation and element contour detection. The semantic segmentation network accurately predicts the probability at each pixel of the three element classes. The contour detection network accurately predicts instance level \"edges\" around each element occurrence. We propose a conditional random field (CRF) that uses features output from the semantic segmentation and contour networks to improve upon the semantic segmentation network output. Given the semantic segmentation output, we also extract individual table instances from the page using some heuristic rules and a verification network to remove false positives. We show that although we only consider a page image as input, we produce comparable results with other methods that relies on PDF file information and heuristics and hand crafted features tailored to specific types of documents. Our approach learns the representative features for page segmentation from real and synthetic training data. %, and produces good results on real documents. The learning-based property makes it a more general method than existing methods in terms of document types and element appearances. For example, our method reliably detects sparsely lined tables which are hard for rule-based or heuristic methods."
            },
            "slug": "Multi-Scale-Multi-Task-FCN-for-Semantic-Page-and-He-Cohen",
            "title": {
                "fragments": [],
                "text": "Multi-Scale Multi-Task FCN for Semantic Page Segmentation and Table Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "This work presents a page segmentation algorithm that incorporates state-of-the-art deep learning methods for segmenting three types of document elements: text blocks, tables, and figures and proposes a conditional random field (CRF) that uses features output from the semantic segmentsation and contour networks to improve upon the semantic segmentation network output."
            },
            "venue": {
                "fragments": [],
                "text": "2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46365617"
                        ],
                        "name": "Jian Wu",
                        "slug": "Jian-Wu",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "150150405"
                        ],
                        "name": "Jason Killian",
                        "slug": "Jason-Killian",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Killian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jason Killian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2156109369"
                        ],
                        "name": "Huaiyu Yang",
                        "slug": "Huaiyu-Yang",
                        "structuredName": {
                            "firstName": "Huaiyu",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huaiyu Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46890115"
                        ],
                        "name": "Kyle Williams",
                        "slug": "Kyle-Williams",
                        "structuredName": {
                            "firstName": "Kyle",
                            "lastName": "Williams",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kyle Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3324957"
                        ],
                        "name": "Sagnik Ray Choudhury",
                        "slug": "Sagnik-Ray-Choudhury",
                        "structuredName": {
                            "firstName": "Sagnik",
                            "lastName": "Choudhury",
                            "middleNames": [
                                "Ray"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sagnik Ray Choudhury"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2038777"
                        ],
                        "name": "Suppawong Tuarob",
                        "slug": "Suppawong-Tuarob",
                        "structuredName": {
                            "firstName": "Suppawong",
                            "lastName": "Tuarob",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Suppawong Tuarob"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690656"
                        ],
                        "name": "Cornelia Caragea",
                        "slug": "Cornelia-Caragea",
                        "structuredName": {
                            "firstName": "Cornelia",
                            "lastName": "Caragea",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cornelia Caragea"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145157784"
                        ],
                        "name": "C. Lee Giles",
                        "slug": "C.-Lee-Giles",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Giles",
                            "middleNames": [
                                "Lee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lee Giles"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 184
                            }
                        ],
                        "text": "Researchers have considered a range of tasks from extracting the underlying data from plots [5, 20], to the use of figures in search engines and broader information extraction systems [4, 24, 26]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "In [26], researchers introduced the system PDFMEF, which incorporated figures into its extracted information."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 162424,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0d4571f57b32a182a34ae49ac68257fbd174c1ba",
            "isKey": false,
            "numCitedBy": 35,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce PDFMEF, a multi-entity knowledge extraction framework for scholarly documents in the PDF format. It is implemented with a framework that encapsulates open-source extraction tools. Currently, it leverages PDFBox and TET for full text extraction, the scholarly document filter described in [5] for document classification, GROBID for header extraction, ParsCit for citation extraction, PDFFigures for figure and table extraction, and algorithm extraction [27]. While it can be run as a whole, the extraction tool in each module is highly customizable. Users can substitute default extractors with other extraction tools they prefer by writing a thin wrapper to implement the abstracts. The framework is designed to be scalable and is capable of running in parallel using a multi-processing technique in Python. Experiments indicate that the system with default setups is CPU bounded, and leaves a small footprint in the memory, which makes it best to run on a multi-core machine. The best performance using a dedicated server of 16 cores takes 1.3 seconds on average to process one PDF document. It is used to index extracted information and help users to quickly locate relevant results in published scholarly documents and to efficiently construct a large knowledge base in order to build a semantic scholarly search engine. Part of it is running on CiteSeerX digital library search engine."
            },
            "slug": "PDFMEF:-A-Multi-Entity-Knowledge-Extraction-for-and-Wu-Killian",
            "title": {
                "fragments": [],
                "text": "PDFMEF: A Multi-Entity Knowledge Extraction Framework for Scholarly Documents and Semantic Search"
            },
            "venue": {
                "fragments": [],
                "text": "K-CAP"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2031669"
                        ],
                        "name": "A. G. S. D. Herrera",
                        "slug": "A.-G.-S.-D.-Herrera",
                        "structuredName": {
                            "firstName": "Alba",
                            "lastName": "Herrera",
                            "middleNames": [
                                "Garc\u00eda",
                                "Seco",
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. G. S. D. Herrera"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1750684"
                        ],
                        "name": "Roger Schaer",
                        "slug": "Roger-Schaer",
                        "structuredName": {
                            "firstName": "Roger",
                            "lastName": "Schaer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Roger Schaer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2986008"
                        ],
                        "name": "S. Bromuri",
                        "slug": "S.-Bromuri",
                        "structuredName": {
                            "firstName": "Stefano",
                            "lastName": "Bromuri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Bromuri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2151194032"
                        ],
                        "name": "H. M\u00fcller",
                        "slug": "H.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Henning",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. M\u00fcller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14558859,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4208a299d76183df72c8fa77452aecd7d590c24b",
            "isKey": false,
            "numCitedBy": 53,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "ImageCLEF is the image retrieval task of the Conference and Labs of the Evaluation Forum (CLEF). ImageCLEF has historically focused on the multimodal and language\u2013independent retrieval of images. Many tasks are related to image classification and the annotation of image data as well. The medical task has focused more on image retrieval in the beginning and then retrieval and classification tasks in subsequent years. In 2016 a main focus was the creation of meta data for a collection of medical images taken from articles of the the biomedical scientific literature. In total 8 teams participated in the four tasks and 69 runs were submitted. No team participated in the caption prediction task, a totally new task. Deep learning has now been used for several of the ImageCLEF tasks and by many of the participants obtaining very good results. A majority of runs was submitting using deep learning and this follows general trends in machine learning. In several of the tasks multimodal approaches clearly led to best results."
            },
            "slug": "Overview-of-the-ImageCLEF-2016-Medical-Task-Herrera-Schaer",
            "title": {
                "fragments": [],
                "text": "Overview of the ImageCLEF 2016 Medical Task"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A main focus was the creation of meta data for a collection of medical images taken from articles of the the biomedical scientific literature and a majority of runs was submitting using deep learning, following general trends in machine learning."
            },
            "venue": {
                "fragments": [],
                "text": "CLEF"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2031669"
                        ],
                        "name": "A. G. S. D. Herrera",
                        "slug": "A.-G.-S.-D.-Herrera",
                        "structuredName": {
                            "firstName": "Alba",
                            "lastName": "Herrera",
                            "middleNames": [
                                "Garc\u00eda",
                                "Seco",
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. G. S. D. Herrera"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2151194032"
                        ],
                        "name": "H. M\u00fcller",
                        "slug": "H.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Henning",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. M\u00fcller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2986008"
                        ],
                        "name": "S. Bromuri",
                        "slug": "S.-Bromuri",
                        "structuredName": {
                            "firstName": "Stefano",
                            "lastName": "Bromuri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Bromuri"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 31
                            }
                        ],
                        "text": "Shared tasks such as ImageCLEF [9, 10] also helped drive more attention to compound figure detection [29], compound figure separation [23], medical image annotation [17], among other tasks related to medical images."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14499280,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2f9916a5a7e19e720a3db98171484e0911b4b116",
            "isKey": false,
            "numCitedBy": 52,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "This articles describes the ImageCLEF 2015 Medical Clas-sification task. The task contains several subtasks that all use a dataset of figures from the biomedical open access literature (PubMed Cen-tral). Particularly compound figures are targeted that are frequent inthe literature. For more detailed information analysis and retrieval it isimportant to extract targeted information from the compound figures.The proposed tasks include compound figure detection (separating com-pound from other figures), multi\u2013label classification (define all sub typespresent), figure separation (find boundaries of the subfigures) and modal-ity classification (detecting the figure type of each subfigure). The tasksare described with the participation of international research groups inthe tasks. The results of the participants are then described and analysedto identify promising techniques."
            },
            "slug": "Overview-of-the-ImageCLEF-2015-Medical-Task-Herrera-M\u00fcller",
            "title": {
                "fragments": [],
                "text": "Overview of the ImageCLEF 2015 Medical Classification Task"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The proposed tasks include compound figure detection (separating com-pound from other figures), multi\u2013label classification (define all sub types present), figure separation (find boundaries of the subfigures) and modal-ity classification (detecting the figure type of each subfigure)."
            },
            "venue": {
                "fragments": [],
                "text": "CLEF"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39540928"
                        ],
                        "name": "Ashnil Kumar",
                        "slug": "Ashnil-Kumar",
                        "structuredName": {
                            "firstName": "Ashnil",
                            "lastName": "Kumar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ashnil Kumar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39401174"
                        ],
                        "name": "S. Dyer",
                        "slug": "S.-Dyer",
                        "structuredName": {
                            "firstName": "Shane",
                            "lastName": "Dyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46454386"
                        ],
                        "name": "Jinman Kim",
                        "slug": "Jinman-Kim",
                        "structuredName": {
                            "firstName": "Jinman",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jinman Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1950449"
                        ],
                        "name": "Changyang Li",
                        "slug": "Changyang-Li",
                        "structuredName": {
                            "firstName": "Changyang",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Changyang Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682246"
                        ],
                        "name": "P. Leong",
                        "slug": "P.-Leong",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Leong",
                            "middleNames": [
                                "Heng",
                                "Wai"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Leong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110883"
                        ],
                        "name": "M. Fulham",
                        "slug": "M.-Fulham",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Fulham",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Fulham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145855523"
                        ],
                        "name": "D. Feng",
                        "slug": "D.-Feng",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Feng",
                            "middleNames": [
                                "Dagan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Feng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 165
                            }
                        ],
                        "text": "Shared tasks such as ImageCLEF [9, 10] also helped drive more attention to compound figure detection [29], compound figure separation [23], medical image annotation [17], among other tasks related to medical images."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 28778169,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "929d5a723f79eac70a5a83236d5e531d7a39edf7",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Adapting-content-based-image-retrieval-techniques-Kumar-Dyer",
            "title": {
                "fragments": [],
                "text": "Adapting content-based image retrieval techniques for the semantic annotation of medical images"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Medical Imaging Graph."
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39353098"
                        ],
                        "name": "Kaiming He",
                        "slug": "Kaiming-He",
                        "structuredName": {
                            "firstName": "Kaiming",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaiming He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1771551"
                        ],
                        "name": "X. Zhang",
                        "slug": "X.-Zhang",
                        "structuredName": {
                            "firstName": "X.",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3080683"
                        ],
                        "name": "Shaoqing Ren",
                        "slug": "Shaoqing-Ren",
                        "structuredName": {
                            "firstName": "Shaoqing",
                            "lastName": "Ren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaoqing Ren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Jian Sun",
                        "slug": "Jian-Sun",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Sun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 11
                            }
                        ],
                        "text": "ResNet-101 [14] provides one such feature extraction architecture."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 222,
                                "start": 218
                            }
                        ],
                        "text": "To summarize how our model combines the previously mentioned components, the model generates a 20x15 spatial grid of image embedding vectors with each embedding vector having 1024 dimensions generated using ResNet-101 [14]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 212,
                                "start": 208
                            }
                        ],
                        "text": "The model architecture we use for figure extraction in this paper leverages the great success of convolutional neural networks on a variety of computer vision tasks including object recognition and detection [14], motion analysis [12], and scene reconstruction [27]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 150
                            }
                        ],
                        "text": "In particular, our model is based on TensorBox [21], applying the OverFeat detection architecture [19] to image embeddings generated using ResNet-101 [14]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 95
                            }
                        ],
                        "text": "Numerous highly successful neural network architectures have been proposed for computer vision [14, 15, 22]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 28
                            }
                        ],
                        "text": "Residual networks (ResNets) [14] address the problem by adding identity connections between blocks: rather than each layer receiving as input only the previous layer\u2019s output, some layers also receive the output of several layers before."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 4
                            }
                        ],
                        "text": "See [14] for the full ResNet-101 architecture."
                    },
                    "intents": []
                }
            ],
            "corpusId": 206594692,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2c03df8b48bf3fa39054345bafabfeff15bfd11d",
            "isKey": true,
            "numCitedBy": 95318,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation."
            },
            "slug": "Deep-Residual-Learning-for-Image-Recognition-He-Zhang",
            "title": {
                "fragments": [],
                "text": "Deep Residual Learning for Image Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "This work presents a residual learning framework to ease the training of networks that are substantially deeper than those used previously, and provides comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064160"
                        ],
                        "name": "A. Krizhevsky",
                        "slug": "A.-Krizhevsky",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Krizhevsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Krizhevsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701686"
                        ],
                        "name": "Ilya Sutskever",
                        "slug": "Ilya-Sutskever",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Sutskever",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilya Sutskever"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 95
                            }
                        ],
                        "text": "Numerous highly successful neural network architectures have been proposed for computer vision [14, 15, 22]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 195908774,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "abd1c342495432171beb7ca8fd9551ef13cbd0ff",
            "isKey": false,
            "numCitedBy": 80944,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called \"dropout\" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry."
            },
            "slug": "ImageNet-classification-with-deep-convolutional-Krizhevsky-Sutskever",
            "title": {
                "fragments": [],
                "text": "ImageNet classification with deep convolutional neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A large, deep convolutional neural network was trained to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes and employed a recently developed regularization method called \"dropout\" that proved to be very effective."
            },
            "venue": {
                "fragments": [],
                "text": "Commun. ACM"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2574060"
                        ],
                        "name": "Christian Szegedy",
                        "slug": "Christian-Szegedy",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Szegedy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian Szegedy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2157222093"
                        ],
                        "name": "Wei Liu",
                        "slug": "Wei-Liu",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39978391"
                        ],
                        "name": "Yangqing Jia",
                        "slug": "Yangqing-Jia",
                        "structuredName": {
                            "firstName": "Yangqing",
                            "lastName": "Jia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yangqing Jia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3142556"
                        ],
                        "name": "Pierre Sermanet",
                        "slug": "Pierre-Sermanet",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Sermanet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pierre Sermanet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144828948"
                        ],
                        "name": "Scott E. Reed",
                        "slug": "Scott-E.-Reed",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Reed",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Scott E. Reed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1838674"
                        ],
                        "name": "Dragomir Anguelov",
                        "slug": "Dragomir-Anguelov",
                        "structuredName": {
                            "firstName": "Dragomir",
                            "lastName": "Anguelov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dragomir Anguelov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761978"
                        ],
                        "name": "D. Erhan",
                        "slug": "D.-Erhan",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Erhan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Erhan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2657155"
                        ],
                        "name": "Vincent Vanhoucke",
                        "slug": "Vincent-Vanhoucke",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Vanhoucke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vincent Vanhoucke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39863668"
                        ],
                        "name": "Andrew Rabinovich",
                        "slug": "Andrew-Rabinovich",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Rabinovich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Rabinovich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 95
                            }
                        ],
                        "text": "Numerous highly successful neural network architectures have been proposed for computer vision [14, 15, 22]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 206592484,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e15cf50aa89fee8535703b9f9512fca5bfc43327",
            "isKey": false,
            "numCitedBy": 29480,
            "numCiting": 278,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection."
            },
            "slug": "Going-deeper-with-convolutions-Szegedy-Liu",
            "title": {
                "fragments": [],
                "text": "Going deeper with convolutions"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "A deep convolutional neural network architecture codenamed Inception is proposed that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14)."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153819461"
                        ],
                        "name": "Kai Chen",
                        "slug": "Kai-Chen",
                        "structuredName": {
                            "firstName": "Kai",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2700495"
                        ],
                        "name": "Mathias Seuret",
                        "slug": "Mathias-Seuret",
                        "structuredName": {
                            "firstName": "Mathias",
                            "lastName": "Seuret",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mathias Seuret"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743758"
                        ],
                        "name": "M. Liwicki",
                        "slug": "M.-Liwicki",
                        "structuredName": {
                            "firstName": "Marcus",
                            "lastName": "Liwicki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Liwicki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1722800"
                        ],
                        "name": "J. Hennebert",
                        "slug": "J.-Hennebert",
                        "structuredName": {
                            "firstName": "Jean",
                            "lastName": "Hennebert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hennebert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680326"
                        ],
                        "name": "R. Ingold",
                        "slug": "R.-Ingold",
                        "structuredName": {
                            "firstName": "Rolf",
                            "lastName": "Ingold",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Ingold"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 162
                            }
                        ],
                        "text": "Recent work has used neural networks for semantic page segmentation, suggesting that these models can be applied to synthetic documents as well as natural scenes [3, 13, 28]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9814021,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b90411acf9a597f139651133d42bffce3df78044",
            "isKey": false,
            "numCitedBy": 79,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present an unsupervised feature learning method for page segmentation of historical handwritten documents available as color images. We consider page segmentation as a pixel labeling problem, i.e., each pixel is classified as either periphery, background, text block, or decoration. Traditional methods in this area rely on carefully hand-crafted features or large amounts of prior knowledge. In contrast, we apply convolutional autoencoders to learn features directly from pixel intensity values. Then, using these features to train an SVM, we achieve high quality segmentation without any assumption of specific topologies and shapes. Experiments on three public datasets demonstrate the effectiveness and superiority of the proposed approach."
            },
            "slug": "Page-segmentation-of-historical-document-images-Chen-Seuret",
            "title": {
                "fragments": [],
                "text": "Page segmentation of historical document images with convolutional autoencoders"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper considers page segmentation as a pixel labeling problem, i.e., each pixel is classified as either periphery, background, text block, or decoration, and applies convolutional autoencoders to learn features directly from pixel intensity values."
            },
            "venue": {
                "fragments": [],
                "text": "2015 13th International Conference on Document Analysis and Recognition (ICDAR)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3324957"
                        ],
                        "name": "Sagnik Ray Choudhury",
                        "slug": "Sagnik-Ray-Choudhury",
                        "structuredName": {
                            "firstName": "Sagnik",
                            "lastName": "Choudhury",
                            "middleNames": [
                                "Ray"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sagnik Ray Choudhury"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2038777"
                        ],
                        "name": "Suppawong Tuarob",
                        "slug": "Suppawong-Tuarob",
                        "structuredName": {
                            "firstName": "Suppawong",
                            "lastName": "Tuarob",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Suppawong Tuarob"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143930195"
                        ],
                        "name": "P. Mitra",
                        "slug": "P.-Mitra",
                        "structuredName": {
                            "firstName": "Prasenjit",
                            "lastName": "Mitra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Mitra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732091"
                        ],
                        "name": "L. Rokach",
                        "slug": "L.-Rokach",
                        "structuredName": {
                            "firstName": "Lior",
                            "lastName": "Rokach",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Rokach"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46574165"
                        ],
                        "name": "A. Kirk",
                        "slug": "A.-Kirk",
                        "structuredName": {
                            "firstName": "Andi",
                            "lastName": "Kirk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kirk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47821553"
                        ],
                        "name": "S. Szep",
                        "slug": "S.-Szep",
                        "structuredName": {
                            "firstName": "Silvia",
                            "lastName": "Szep",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Szep"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8786271"
                        ],
                        "name": "D. Pellegrino",
                        "slug": "D.-Pellegrino",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Pellegrino",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Pellegrino"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115262080"
                        ],
                        "name": "Sue Jones",
                        "slug": "Sue-Jones",
                        "structuredName": {
                            "firstName": "Sue",
                            "lastName": "Jones",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sue Jones"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145157784"
                        ],
                        "name": "C. Lee Giles",
                        "slug": "C.-Lee-Giles",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Giles",
                            "middleNames": [
                                "Lee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lee Giles"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7847465,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c581b9c78e581b140585bc901a1f9843fdd54904",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "Academic papers contain multiple figures representing important findings and experimental results; we present a search engine specifically focused on figures in academic documents. This search engine allows users to search on figures in approximately 150,000 chemistry journal articles though the method is easily extendable to other domains. Our system indexes figure caption and mentions extracted from the PDF in documents using a custom built extractor. Recall and precision performance of extracted figures is in the 80 to 90% range. We give the frame work for the extraction algorithm, architecture and ranking function."
            },
            "slug": "A-figure-search-engine-architecture-for-a-chemistry-Choudhury-Tuarob",
            "title": {
                "fragments": [],
                "text": "A figure search engine architecture for a chemistry digital library"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work gives the frame work for the extraction algorithm, architecture and ranking function, and indexes figure caption and mentions extracted from the PDF in documents using a custom built extractor."
            },
            "venue": {
                "fragments": [],
                "text": "JCDL '13"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2841331"
                        ],
                        "name": "A. Dosovitskiy",
                        "slug": "A.-Dosovitskiy",
                        "structuredName": {
                            "firstName": "Alexey",
                            "lastName": "Dosovitskiy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dosovitskiy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152702479"
                        ],
                        "name": "P. Fischer",
                        "slug": "P.-Fischer",
                        "structuredName": {
                            "firstName": "Philipp",
                            "lastName": "Fischer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Fischer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48105320"
                        ],
                        "name": "Eddy Ilg",
                        "slug": "Eddy-Ilg",
                        "structuredName": {
                            "firstName": "Eddy",
                            "lastName": "Ilg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eddy Ilg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2880264"
                        ],
                        "name": "Philip H\u00e4usser",
                        "slug": "Philip-H\u00e4usser",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "H\u00e4usser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philip H\u00e4usser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3322806"
                        ],
                        "name": "Caner Hazirbas",
                        "slug": "Caner-Hazirbas",
                        "structuredName": {
                            "firstName": "Caner",
                            "lastName": "Hazirbas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Caner Hazirbas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2943639"
                        ],
                        "name": "V. Golkov",
                        "slug": "V.-Golkov",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Golkov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Golkov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715782"
                        ],
                        "name": "Patrick van der Smagt",
                        "slug": "Patrick-van-der-Smagt",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "van der Smagt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Patrick van der Smagt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695302"
                        ],
                        "name": "D. Cremers",
                        "slug": "D.-Cremers",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Cremers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Cremers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710872"
                        ],
                        "name": "T. Brox",
                        "slug": "T.-Brox",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Brox",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Brox"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 234,
                                "start": 230
                            }
                        ],
                        "text": "The model architecture we use for figure extraction in this paper leverages the great success of convolutional neural networks on a variety of computer vision tasks including object recognition and detection [14], motion analysis [12], and scene reconstruction [27]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12552176,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c2fb5b39428818d7ec8cc78e152e19c21b7db568",
            "isKey": false,
            "numCitedBy": 2712,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "Convolutional neural networks (CNNs) have recently been very successful in a variety of computer vision tasks, especially on those linked to recognition. Optical flow estimation has not been among the tasks CNNs succeeded at. In this paper we construct CNNs which are capable of solving the optical flow estimation problem as a supervised learning task. We propose and compare two architectures: a generic architecture and another one including a layer that correlates feature vectors at different image locations. Since existing ground truth data sets are not sufficiently large to train a CNN, we generate a large synthetic Flying Chairs dataset. We show that networks trained on this unrealistic data still generalize very well to existing datasets such as Sintel and KITTI, achieving competitive accuracy at frame rates of 5 to 10 fps."
            },
            "slug": "FlowNet:-Learning-Optical-Flow-with-Convolutional-Dosovitskiy-Fischer",
            "title": {
                "fragments": [],
                "text": "FlowNet: Learning Optical Flow with Convolutional Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper constructs CNNs which are capable of solving the optical flow estimation problem as a supervised learning task, and proposes and compares two architectures: a generic architecture and another one including a layer that correlates feature vectors at different image locations."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE International Conference on Computer Vision (ICCV)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3142556"
                        ],
                        "name": "Pierre Sermanet",
                        "slug": "Pierre-Sermanet",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Sermanet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pierre Sermanet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2060028"
                        ],
                        "name": "D. Eigen",
                        "slug": "D.-Eigen",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Eigen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Eigen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46447747"
                        ],
                        "name": "X. Zhang",
                        "slug": "X.-Zhang",
                        "structuredName": {
                            "firstName": "Xiang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143949035"
                        ],
                        "name": "Micha\u00ebl Mathieu",
                        "slug": "Micha\u00ebl-Mathieu",
                        "structuredName": {
                            "firstName": "Micha\u00ebl",
                            "lastName": "Mathieu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Micha\u00ebl Mathieu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276554"
                        ],
                        "name": "R. Fergus",
                        "slug": "R.-Fergus",
                        "structuredName": {
                            "firstName": "Rob",
                            "lastName": "Fergus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fergus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4071727,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5507caf1210b6bfbd04fe02b2669bc14292e23a1",
            "isKey": false,
            "numCitedBy": 4353,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat."
            },
            "slug": "OverFeat:-Integrated-Recognition,-Localization-and-Sermanet-Eigen",
            "title": {
                "fragments": [],
                "text": "OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "This integrated framework for using Convolutional Networks for classification, localization and detection is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 and obtained very competitive results for the detection and classifications tasks."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2667889"
                        ],
                        "name": "Shichao Yang",
                        "slug": "Shichao-Yang",
                        "structuredName": {
                            "firstName": "Shichao",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shichao Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2043965"
                        ],
                        "name": "Daniel Maturana",
                        "slug": "Daniel-Maturana",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Maturana",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Maturana"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32634992"
                        ],
                        "name": "S. Scherer",
                        "slug": "S.-Scherer",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Scherer",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Scherer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 265,
                                "start": 261
                            }
                        ],
                        "text": "The model architecture we use for figure extraction in this paper leverages the great success of convolutional neural networks on a variety of computer vision tasks including object recognition and detection [14], motion analysis [12], and scene reconstruction [27]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 206851949,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2c9aab3a26a1396b997419c44e5774fe913a0d40",
            "isKey": false,
            "numCitedBy": 30,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of understanding the 3D layout of indoor corridor scenes from a single image in real time. Identifying obstacles such as walls is essential for robot navigation, but also challenging due to the diversity in structure, appearance and illumination of real-world corridor scenes. Many current single-image methods make Manhattan-world assumptions, and break down in environments that do not meet this mold. They also may require complicated hand-designed features for image segmentation or clear boundaries to form certain building models. In addition, most cannot run in real time. In this paper, we propose to combine machine learning with geometric modelling to build a simplified 3D model from a single image. We first employ a supervised Convolutional Neural Network (CNN) to provide a dense, but coarse, geometric class labelling of the scene. We then refine this labelling with a fully connected Conditional Random Field (CRF). Finally, we fit line segments along wall-ground boundaries and \u201cpop up\u201d a 3D model using geometric constraints. We assemble a dataset of 967 labelled corridor images. Our experiments on this dataset and another publicly available dataset show our method outperforms other single image scene understanding methods in pixelwise accuracy while labelling images at over 15Hz."
            },
            "slug": "Real-time-3D-scene-layout-from-a-single-image-using-Yang-Maturana",
            "title": {
                "fragments": [],
                "text": "Real-time 3D scene layout from a single image using Convolutional Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper proposes to combine machine learning with geometric modelling to build a simplified 3D model from a single image, and outperforms other single image scene understanding methods in pixelwise accuracy while labelling images at over 15Hz."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE International Conference on Robotics and Automation (ICRA)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3727644"
                        ],
                        "name": "Satoshi Tsutsui",
                        "slug": "Satoshi-Tsutsui",
                        "structuredName": {
                            "firstName": "Satoshi",
                            "lastName": "Tsutsui",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Satoshi Tsutsui"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38441999"
                        ],
                        "name": "Guilin Meng",
                        "slug": "Guilin-Meng",
                        "structuredName": {
                            "firstName": "Guilin",
                            "lastName": "Meng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guilin Meng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2087141571"
                        ],
                        "name": "Xiao-Yan Yao",
                        "slug": "Xiao-Yan-Yao",
                        "structuredName": {
                            "firstName": "Xiao-Yan",
                            "lastName": "Yao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiao-Yan Yao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2821130"
                        ],
                        "name": "David J. Crandall",
                        "slug": "David-J.-Crandall",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Crandall",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David J. Crandall"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111236136"
                        ],
                        "name": "Ying Ding",
                        "slug": "Ying-Ding",
                        "structuredName": {
                            "firstName": "Ying",
                            "lastName": "Ding",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ying Ding"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "In [24], researchers classified figures of brain images in order to provide more relevant information to doctors studying Alzheimer\u2019s Disease."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 184
                            }
                        ],
                        "text": "Researchers have considered a range of tasks from extracting the underlying data from plots [5, 20], to the use of figures in search engines and broader information extraction systems [4, 24, 26]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17574442,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a6d70becda6af1a6ee6405495e69c0d54a37b8c5",
            "isKey": false,
            "numCitedBy": 1,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Which papers focusing on Alzheimer\u2019s disease (AD) include MRI scans of human brains? These images play an important role in clinical detection of AD, but finding them currently requires manual inspection of papers after a keyword search. In order to provide AD researchers with a more efficient way of finding relevant papers, here we focus on three preliminary problems involving automatically identifying figures containing brain images, and solve them as automatic image classification tasks. This is a first step towards efficiently allowing AD researchers to retrieve papers containing a particular type of brain image (e.g. of a patient). We report preliminary results from a larger project, in collaboration with AD researchers."
            },
            "slug": "Analyzing-Figures-of-Brain-Images-from-Alzheimer's-Tsutsui-Meng",
            "title": {
                "fragments": [],
                "text": "Analyzing Figures of Brain Images from Alzheimer's Disease Papers"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Three preliminary problems involving automatically identifying figures containing brain images, and solving them as automatic image classification tasks are focused on, a first step towards efficiently allowing AD researchers to retrieve papers containing a particular type of brain image."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48950628"
                        ],
                        "name": "N. Dalal",
                        "slug": "N.-Dalal",
                        "structuredName": {
                            "firstName": "Navneet",
                            "lastName": "Dalal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Dalal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756114"
                        ],
                        "name": "B. Triggs",
                        "slug": "B.-Triggs",
                        "structuredName": {
                            "firstName": "Bill",
                            "lastName": "Triggs",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Triggs"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 195
                            }
                        ],
                        "text": "Before neural networks, features were generally hand-engineered by researchers or practicioners; an example of one common such feature is the frequencies of edges in various regions of the image [8]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 206590483,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cec734d7097ab6b1e60d95228ffd64248eb89d66",
            "isKey": false,
            "numCitedBy": 29263,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the question of feature sets for robust visual object recognition; adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds."
            },
            "slug": "Histograms-of-oriented-gradients-for-human-Dalal-Triggs",
            "title": {
                "fragments": [],
                "text": "Histograms of oriented gradients for human detection"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection, and the influence of each stage of the computation on performance is studied."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742448"
                        ],
                        "name": "K. Bollacker",
                        "slug": "K.-Bollacker",
                        "structuredName": {
                            "firstName": "Kurt",
                            "lastName": "Bollacker",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Bollacker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065123479"
                        ],
                        "name": "Colin Evans",
                        "slug": "Colin-Evans",
                        "structuredName": {
                            "firstName": "Colin",
                            "lastName": "Evans",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Colin Evans"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2990264"
                        ],
                        "name": "Praveen K. Paritosh",
                        "slug": "Praveen-K.-Paritosh",
                        "structuredName": {
                            "firstName": "Praveen",
                            "lastName": "Paritosh",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Praveen K. Paritosh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1399112633"
                        ],
                        "name": "Tim Sturge",
                        "slug": "Tim-Sturge",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Sturge",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tim Sturge"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110748390"
                        ],
                        "name": "Jamie Taylor",
                        "slug": "Jamie-Taylor",
                        "structuredName": {
                            "firstName": "Jamie",
                            "lastName": "Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jamie Taylor"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 62
                            }
                        ],
                        "text": "For example, [18] used relations between entities in Freebase [1] to induce labels between pairs of corresponding entity mentions in unlabeled text, making the strong assumption that the Freebase relation is described in each sentence where both entities are mentioned."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 207167677,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1976c9eeccc7115d18a04f1e7fb5145db6b96002",
            "isKey": false,
            "numCitedBy": 3825,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "Freebase is a practical, scalable tuple database used to structure general human knowledge. The data in Freebase is collaboratively created, structured, and maintained. Freebase currently contains more than 125,000,000 tuples, more than 4000 types, and more than 7000 properties. Public read/write access to Freebase is allowed through an HTTP-based graph-query API using the Metaweb Query Language (MQL) as a data query and manipulation language. MQL provides an easy-to-use object-oriented interface to the tuple data in Freebase and is designed to facilitate the creation of collaborative, Web-based data-oriented applications."
            },
            "slug": "Freebase:-a-collaboratively-created-graph-database-Bollacker-Evans",
            "title": {
                "fragments": [],
                "text": "Freebase: a collaboratively created graph database for structuring human knowledge"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "MQL provides an easy-to-use object-oriented interface to the tuple data in Freebase and is designed to facilitate the creation of collaborative, Web-based data-oriented applications."
            },
            "venue": {
                "fragments": [],
                "text": "SIGMOD Conference"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47246616"
                        ],
                        "name": "R. Brunelli",
                        "slug": "R.-Brunelli",
                        "structuredName": {
                            "firstName": "Roberto",
                            "lastName": "Brunelli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Brunelli"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 110
                            }
                        ],
                        "text": "Template matching is typically done using image representations such as edge detections or oriented gradients [2], but because we do not have to deal with typical conditions present in natural images such as variations in lighting or pose, we find that template matching in raw pixel space works best."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 147
                            }
                        ],
                        "text": "Labeling figures: Once we have identified the page that a figure is on, we render that page as an image and then use multi-scale template matching [2] to find the position of the figure on the page."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 28287278,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c5ab7a66a0a94137427bf5b4fbcac1ab710b9188",
            "isKey": false,
            "numCitedBy": 696,
            "numCiting": 624,
            "paperAbstract": {
                "fragments": [],
                "text": "The detection and recognition of objects in images is a key research topic in the computer vision community. Within this area, face recognition and interpretation has attracted increasing attention owing to the possibility of unveiling human perception mechanisms, and for the development of practical biometric systems. This book and the accompanying website, focus on template matching, a subset of object recognition techniques of wide applicability, which has proved to be particularly effective for face recognition applications. Using examples from face processing tasks throughout the book to illustrate more general object recognition approaches, Roberto Brunelli: examines the basics of digital image formation, highlighting points critical to the task of template matching; presents basic and advanced template matching techniques, targeting grey-level images, shapes and point sets; discusses recent pattern classification paradigms from a template matching perspective; illustrates the development of a real face recognition system; explores the use of advanced computer graphics techniques in the development of computer vision algorithms. Template Matching Techniques in Computer Vision is primarily aimed at practitioners working on the development of systems for effective object recognition such as biometrics, robot navigation, multimedia retrieval and landmark detection. It is also of interest to graduate students undertaking studies in these areas."
            },
            "slug": "Template-Matching-Techniques-in-Computer-Vision:-Brunelli",
            "title": {
                "fragments": [],
                "text": "Template Matching Techniques in Computer Vision: Theory and Practice"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This book and the accompanying website, focus on template matching, a subset of object recognition techniques of wide applicability, which has proved to be particularly effective for face recognition applications."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145793042"
                        ],
                        "name": "R. Wagner",
                        "slug": "R.-Wagner",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Wagner",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Wagner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145298802"
                        ],
                        "name": "M. Fischer",
                        "slug": "M.-Fischer",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Fischer",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Fischer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 85
                            }
                        ],
                        "text": "We modify the standard WagnerFischer dynamic programming algorithm for edit distance [25] by setting the cost for starting (and ending) at any position in the PDF text to 0."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13381535,
            "fieldsOfStudy": [
                "Mathematics",
                "Education",
                "Physics"
            ],
            "id": "455e1168304e0eb2909093d5ab9b5ec85cda5028",
            "isKey": false,
            "numCitedBy": 3190,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "The string-to-string correction problem is to determine the distance between two strings as measured by the minimum cost sequence of \u201cedit operations\u201d needed to change the one string into the other. The edit operations investigated allow changing one symbol of a string into another single symbol, deleting one symbol from a string, or inserting a single symbol into a string. An algorithm is presented which solves this problem in time proportional to the product of the lengths of the two strings. Possible applications are to the problems of automatic spelling correction and determining the longest subsequence of characters common to two strings."
            },
            "slug": "The-String-to-String-Correction-Problem-Wagner-Fischer",
            "title": {
                "fragments": [],
                "text": "The String-to-String Correction Problem"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "An algorithm is presented which solves the string-to-string correction problem in time proportional to the product of the lengths of the two strings."
            },
            "venue": {
                "fragments": [],
                "text": "JACM"
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2031669"
                        ],
                        "name": "A. G. S. D. Herrera",
                        "slug": "A.-G.-S.-D.-Herrera",
                        "structuredName": {
                            "firstName": "Alba",
                            "lastName": "Herrera",
                            "middleNames": [
                                "Garc\u00eda",
                                "Seco",
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. G. S. D. Herrera"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1750684"
                        ],
                        "name": "Roger Schaer",
                        "slug": "Roger-Schaer",
                        "structuredName": {
                            "firstName": "Roger",
                            "lastName": "Schaer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Roger Schaer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2986008"
                        ],
                        "name": "S. Bromuri",
                        "slug": "S.-Bromuri",
                        "structuredName": {
                            "firstName": "Stefano",
                            "lastName": "Bromuri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Bromuri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2151194032"
                        ],
                        "name": "H. M\u00fcller",
                        "slug": "H.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Henning",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. M\u00fcller"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 31
                            }
                        ],
                        "text": "Shared tasks such as ImageCLEF [9, 10] also helped drive more attention to compound figure detection [29], compound figure separation [23], medical image annotation [17], among other tasks related to medical images."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 208107917,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "52346e4719ffc6914f5bb1b2c6f97edf89160b90",
            "isKey": false,
            "numCitedBy": 16,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Overview-of-the-medical-tasks-in-ImageCLEF-2016-Herrera-Schaer",
            "title": {
                "fragments": [],
                "text": "Overview of the medical tasks in ImageCLEF 2016"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2016
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 113
                            }
                        ],
                        "text": "This is an instance of the linear assignment problem and can be solved efficiently using the Hungarian algorithm [16]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 124889035,
            "fieldsOfStudy": [],
            "id": "e5c6c389be4a8dcfcd11915834cb30e865b506be",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Hungarian method for the assignment problem"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1955
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47246616"
                        ],
                        "name": "R. Brunelli",
                        "slug": "R.-Brunelli",
                        "structuredName": {
                            "firstName": "Roberto",
                            "lastName": "Brunelli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Brunelli"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 64166283,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a8a283977aaaf688f9a84a7e3a93a46257d6958d",
            "isKey": false,
            "numCitedBy": 330,
            "numCiting": 511,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Template-Matching-Techniques-in-Computer-Vision-Brunelli",
            "title": {
                "fragments": [],
                "text": "Template Matching Techniques in Computer Vision"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 18,
            "methodology": 10
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 29,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Extracting-Scientific-Figures-with-Distantly-Neural-Siegel-Lourie/1fec9d41d372267b4474f18cbeadd806c8b67adb?sort=total-citations"
}