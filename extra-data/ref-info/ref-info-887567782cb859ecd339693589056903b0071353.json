{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715634"
                        ],
                        "name": "Ming-Hsuan Yang",
                        "slug": "Ming-Hsuan-Yang",
                        "structuredName": {
                            "firstName": "Ming-Hsuan",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming-Hsuan Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1765887"
                        ],
                        "name": "D. Kriegman",
                        "slug": "D.-Kriegman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Kriegman",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Kriegman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145237406"
                        ],
                        "name": "N. Ahuja",
                        "slug": "N.-Ahuja",
                        "structuredName": {
                            "firstName": "Narendra",
                            "lastName": "Ahuja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Ahuja"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9045232,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ebb34b75982f628f9ce5995821fff81fd967dc2d",
            "isKey": false,
            "numCitedBy": 3968,
            "numCiting": 251,
            "paperAbstract": {
                "fragments": [],
                "text": "Images containing faces are essential to intelligent vision-based human-computer interaction, and research efforts in face processing include face recognition, face tracking, pose estimation and expression recognition. However, many reported methods assume that the faces in an image or an image sequence have been identified and localized. To build fully automated systems that analyze the information contained in face images, robust and efficient face detection algorithms are required. Given a single image, the goal of face detection is to identify all image regions which contain a face, regardless of its 3D position, orientation and lighting conditions. Such a problem is challenging because faces are non-rigid and have a high degree of variability in size, shape, color and texture. Numerous techniques have been developed to detect faces in a single image, and the purpose of this paper is to categorize and evaluate these algorithms. We also discuss relevant issues such as data collection, evaluation metrics and benchmarking. After analyzing these algorithms and identifying their limitations, we conclude with several promising directions for future research."
            },
            "slug": "Detecting-Faces-in-Images:-A-Survey-Yang-Kriegman",
            "title": {
                "fragments": [],
                "text": "Detecting Faces in Images: A Survey"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736143"
                        ],
                        "name": "Constantine Kotropoulos",
                        "slug": "Constantine-Kotropoulos",
                        "structuredName": {
                            "firstName": "Constantine",
                            "lastName": "Kotropoulos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Constantine Kotropoulos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144064571"
                        ],
                        "name": "I. Pitas",
                        "slug": "I.-Pitas",
                        "structuredName": {
                            "firstName": "Ioannis",
                            "lastName": "Pitas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Pitas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 178
                            }
                        ],
                        "text": "The technique of Yang and Huang was recently incorporated into a system for rotation invariant face detection by Lvet al. [119] and an extension of the algorithm is presented in Kotropoulos and Pitas [95]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 19
                            }
                        ],
                        "text": "K. Sobottka and I. Pitas, Extraction of facial regions and features using color and shape information, in Proc. of Int."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 21
                            }
                        ],
                        "text": "A. Nikolaidis and I. Pitas, Facial feature extraction and pose determination,Pattern Recog.33, 2000, 1783\u20131791."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 78
                            }
                        ],
                        "text": "[119] and an extension of the algorithm is presented in Kotropoulos and Pitas [95]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 22
                            }
                        ],
                        "text": "C. Kotropoulos and I. Pitas, Rule-based face detection in frontal views, inProc."
                    },
                    "intents": []
                }
            ],
            "corpusId": 135301,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a1d194fd28c58075efed3425483fe8cfc6df0ae6",
            "isKey": true,
            "numCitedBy": 248,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Face detection is a key problem in building automated systems that perform face recognition. A very attractive approach for face detection is based on multiresolution images (also known as mosaic images). Motivated by the simplicity of this approach, a rule-based face detection algorithm in frontal views is developed that extends the work of G. Yang and T.S. Huang (see Pattern Recognition, vol.27, no.1, p.53-63, 1994). The proposed algorithm has been applied to frontal views extracted from the European ACTS M2VTS database that contains the videosequences of 37 different persons. It has been found that the algorithm provides a correct facial candidate in all cases. However, the success rate of the detected facial features (e.g. eyebrows/eyes, nostrils/nose, and mouth) that validate the choice of a facial candidate is found to be 86.5% under the most strict evaluation conditions."
            },
            "slug": "Rule-based-face-detection-in-frontal-views-Kotropoulos-Pitas",
            "title": {
                "fragments": [],
                "text": "Rule-based face detection in frontal views"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A rule-based face detection algorithm in frontal views is developed that is applied to frontal views extracted from the European ACTS M2VTS database that contains the videosequences of 37 different persons and found that the algorithm provides a correct facial candidate in all cases."
            },
            "venue": {
                "fragments": [],
                "text": "1997 IEEE International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2275441"
                        ],
                        "name": "K. Yow",
                        "slug": "K.-Yow",
                        "structuredName": {
                            "firstName": "Kin",
                            "lastName": "Yow",
                            "middleNames": [
                                "Choong"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Yow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745672"
                        ],
                        "name": "R. Cipolla",
                        "slug": "R.-Cipolla",
                        "structuredName": {
                            "firstName": "Roberto",
                            "lastName": "Cipolla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Cipolla"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 16932868,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "30e2f29d26f31846d6e0294cfa3733adfc618bbb",
            "isKey": false,
            "numCitedBy": 432,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Feature-based-human-face-detection-Yow-Cipolla",
            "title": {
                "fragments": [],
                "text": "Feature-based human face detection"
            },
            "venue": {
                "fragments": [],
                "text": "Image Vis. Comput."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2145032549"
                        ],
                        "name": "Weimin Huang",
                        "slug": "Weimin-Huang",
                        "structuredName": {
                            "firstName": "Weimin",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Weimin Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37495435"
                        ],
                        "name": "Qibin Sun",
                        "slug": "Qibin-Sun",
                        "structuredName": {
                            "firstName": "Qibin",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qibin Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153892802"
                        ],
                        "name": "C. Lam",
                        "slug": "C.-Lam",
                        "structuredName": {
                            "firstName": "Chian-Prong",
                            "lastName": "Lam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109178668"
                        ],
                        "name": "Jian-Kang Wu",
                        "slug": "Jian-Kang-Wu",
                        "structuredName": {
                            "firstName": "Jian-Kang",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian-Kang Wu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 12
                            }
                        ],
                        "text": "Huang et al.[74] also apply a Gaussian filter for pre-processing in a framework based on image feature analysis."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 41321794,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e66bc8179717e0412371279ea2ea3647d859232b",
            "isKey": false,
            "numCitedBy": 51,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Automatic face location in complex scenes is extremely challenging in human face recognition systems. Further more, the facial features detection also plays an important role. The paper presents a scheme for robust face and eyes detection from an image. The scheme uses the Gaussian steerable filter to search and detect the facial feature (preattentive feature) roughly in an image. The face model is investigated to locate the whole face and facial features, such as eyes, nose and mouth. Here, multiple evidences are used in the face location and eyes detection. One important feature is the structural information of the face, i.e. facial components of certain structure. The other is the symmetry property of the face, here only the front face with certain pose variation is considered. It will reduce the computation greatly. For facial components detection, some image features and PCA features are used for verification from the candidates detected before. Experiments show that the algorithm is robust and fast."
            },
            "slug": "A-robust-approach-to-face-and-eyes-detection-from-Huang-Sun",
            "title": {
                "fragments": [],
                "text": "A robust approach to face and eyes detection from images with cluttered background"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A scheme for robust face and eyes detection from an image using the Gaussian steerable filter to search and detect the facial feature (preattentive feature) roughly in an image."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. Fourteenth International Conference on Pattern Recognition (Cat. No.98EX170)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49271339"
                        ],
                        "name": "Lian Hock Koh",
                        "slug": "Lian-Hock-Koh",
                        "structuredName": {
                            "firstName": "Lian",
                            "lastName": "Koh",
                            "middleNames": [
                                "Hock"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lian Hock Koh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144223770"
                        ],
                        "name": "S. Ranganath",
                        "slug": "S.-Ranganath",
                        "structuredName": {
                            "firstName": "Surendra",
                            "lastName": "Ranganath",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ranganath"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2649483"
                        ],
                        "name": "M. Lee",
                        "slug": "M.-Lee",
                        "structuredName": {
                            "firstName": "Mun",
                            "lastName": "Lee",
                            "middleNames": [
                                "Wai"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144834168"
                        ],
                        "name": "Y. Venkatesh",
                        "slug": "Y.-Venkatesh",
                        "structuredName": {
                            "firstName": "Y.",
                            "lastName": "Venkatesh",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Venkatesh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15312045,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "207a44830bffe0825790a431d4fded63dbe82d56",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents an integrated approach to unconstrained face recognition in arbitrary scenes. The front end of the system comprises of a scale- and pose-tolerant face detector. Scale normalization is achieved through a novel combination of a skin color segmentation and log-polar mapping procedure. Principal component analysis is used with the multi-view approach proposed in [10] to handle the pose variations. For a given color input image, the detector encloses a face in a complex scene within a circular boundary and indicates the position of the nose. Next, for recognition, a radial grid mapping centered on the nose yields a feature vector within the circular boundary. As the width of the color segmented region provides an estimated size for the face, the extracted feature vector is scale-normalized by the estimated size. The feature vector is input to a trained neural network classifier for face identification. The system was evaluated using a database of 20 person's faces with varying scale and pose obtained on different complex backgrounds. The face detector was quite robust to all these variations. The performance of the face recognizer was also quite good except for sensitivity to small-scale face images. The integrated system achieved average recognition rates of 87% to 92%."
            },
            "slug": "An-integrated-face-detection-and-recognition-system-Koh-Ranganath",
            "title": {
                "fragments": [],
                "text": "An integrated face detection and recognition system"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "An integrated approach to unconstrained face recognition in arbitrary scenes using a scale- and pose-tolerant face detector and a trained neural network classifier to handle the pose variations is presented."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 10th International Conference on Image Analysis and Processing"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115899190"
                        ],
                        "name": "C. Lee",
                        "slug": "C.-Lee",
                        "structuredName": {
                            "firstName": "Choong",
                            "lastName": "Lee",
                            "middleNames": [
                                "Hwan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109319119"
                        ],
                        "name": "Jun Sung Kim",
                        "slug": "Jun-Sung-Kim",
                        "structuredName": {
                            "firstName": "Jun",
                            "lastName": "Kim",
                            "middleNames": [
                                "Sung"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jun Sung Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1400218066"
                        ],
                        "name": "Kyu Ho Park",
                        "slug": "Kyu-Ho-Park",
                        "structuredName": {
                            "firstName": "Kyu Ho",
                            "lastName": "Park",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kyu Ho Park"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 205015249,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2350cb194cd3f29ff319613859159c54dfb18577",
            "isKey": false,
            "numCitedBy": 144,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Automatic-human-face-location-in-a-complex-using-Lee-Kim",
            "title": {
                "fragments": [],
                "text": "Automatic human face location in a complex background using motion and color information"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2894379"
                        ],
                        "name": "R. Hoogenboom",
                        "slug": "R.-Hoogenboom",
                        "structuredName": {
                            "firstName": "Roel",
                            "lastName": "Hoogenboom",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Hoogenboom"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731570"
                        ],
                        "name": "M. Lew",
                        "slug": "M.-Lew",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Lew",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lew"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 39
                            }
                        ],
                        "text": "A similar system has been developed by Lew and Huijsmans [107]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 675,
                                "start": 672
                            }
                        ],
                        "text": "The number of labeled faces was originally 149 by Sung and Poggio (which is the number used in the reported results in [182] and [107], but it was changed to 155 (which\nTABLE 2 Results Reported in Terms of Percentage Correct Detection (CD) and Number of False Positives (FP), CD/FP, on the CMU and MIT Datasets\nFace detection system CMU-130 CMU-125 MIT-23 MIT-20\nSchneiderman & Kanade\u2014Ea [170] 94.4%/65 Schneiderman & Kanade\u2014Wb [170] 90.2%/110 Yanget al.\u2014FA [217] 92.3%/82 89.4%/3 Yanget al.\u2014LDA [217] 93.6%/74 91.5%/1 Rothet al. [157] 94.8%/78 94.1%/3 Rowleyet al. [158] 86.2%/23 84.5%/8 Feraudet al. [42] 86%/8 Colmenarez & Huang [22] 93.9%/8122 Sung & Poggio [182] 79.9%/5 Lew & Huijsmans [107] 94.1%/64 Osunaet al. [140] 74.2%/20 Lin et al. [113] 72.3%/6 Gu and Li [54] 87.1%/0\na Eigenvector coefficients. b Wavelet coefficients.\nis the number used in the reported results in [158], [140], and [113]) when included in the CMU dataset."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 6
                            }
                        ],
                        "text": "M. S. Lew and N. Huijsmans, Information theory and face detection, inProc. of International Conference on Pattern Recognition, 1996."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 21
                            }
                        ],
                        "text": "R. Hoogenboom and M. Lew, Face detection using local maxima, inIEEE Proc. of 2nd Int."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 22
                            }
                        ],
                        "text": "In Hoogenboom and Lew [64], local maxima, which are defined by a bright pixel surrounded by eight dark neighbors, are used instead to indicate the bright facial spots such as nose tips."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 35406279,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "189f243978e2679b6581a2062a389fa5e746a37b",
            "isKey": true,
            "numCitedBy": 19,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "Automatic human face detection in digital images with a complex environment is still an unsolved problem in computer vision and pattern recognition. It has several uses, such as human face recognition, content based image retrieval and model based video coding. We present an automatic human face detection system where several methods are tested and compared. The underlying principle of the system is to compare subimages of the image pyramid, spanned by the input image, with a set of 'nose-eye' templates. However this comparison is not done on the entire set of subimages of the image pyramid, but on a small subset, which is defined by the 'local maxima method'. False positives are found by using a set of non-face templates. The system is tested on two databases, each include over 1000 images."
            },
            "slug": "Face-detection-using-local-maxima-Hoogenboom-Lew",
            "title": {
                "fragments": [],
                "text": "Face detection using local maxima"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "An automatic human face detection system where several methods are tested and compared to compare subimages of the image pyramid, spanned by the input image, with a set of 'nose-eye' templates."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Second International Conference on Automatic Face and Gesture Recognition"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1684626"
                        ],
                        "name": "B. Heisele",
                        "slug": "B.-Heisele",
                        "structuredName": {
                            "firstName": "Bernd",
                            "lastName": "Heisele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Heisele"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704699"
                        ],
                        "name": "M. Pontil",
                        "slug": "M.-Pontil",
                        "structuredName": {
                            "firstName": "Massimiliano",
                            "lastName": "Pontil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pontil"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14459527,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7b2396cd6a70bae36f19877322b4960919add084",
            "isKey": false,
            "numCitedBy": 215,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : We present a trainable system for detecting frontal and near-frontal views of faces in still gray images using Support Vector Machines (SVMs). We first consider the problem of detecting the whole face pattern by a single SVM classifier. In this context we compare different types of image features, present and evaluate a new method for reducing the number features and discuss practical issues concerning the parameterization of SVMs and the selection of training data. The second part of the paper describes a component-based method for face detection consisting of a two-level hierarchy of SVM classifiers. On the first level, component classifiers independently detect components of a face, such as the eyes, the nose, and the mouth. On the second level, a single classifier checks if the geometrical configuration of the detected components in the image matches a geometrical model of a face."
            },
            "slug": "Face-Detection-in-Still-Gray-Images-Heisele-Poggio",
            "title": {
                "fragments": [],
                "text": "Face Detection in Still Gray Images"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "A trainable system for detecting frontal and near-frontal views of faces in still gray images using Support Vector Machines (SVMs), and a component-based method for face detection consisting of a two-level hierarchy of SVM classifiers."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38595399"
                        ],
                        "name": "Rein-Lien Hsu",
                        "slug": "Rein-Lien-Hsu",
                        "structuredName": {
                            "firstName": "Rein-Lien",
                            "lastName": "Hsu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rein-Lien Hsu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "92329551"
                        ],
                        "name": "M. Abdel-Mottaleb",
                        "slug": "M.-Abdel-Mottaleb",
                        "structuredName": {
                            "firstName": "Mohamed",
                            "lastName": "Abdel-Mottaleb",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Abdel-Mottaleb"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15398205,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a741ea5e483061f2ad1767290770ef35e74fcab4",
            "isKey": false,
            "numCitedBy": 2121,
            "numCiting": 78,
            "paperAbstract": {
                "fragments": [],
                "text": "Human face detection is often the first step in applications such as video surveillance, human computer interface, face recognition, and image database management. We propose a face detection algorithm for color images in the presence of varying lighting conditions as well as complex backgrounds. Our method detects skin regions over the entire image, and then generates face candidates based on the spatial arrangement of these skin patches. The algorithm constructs eye, mouth, and boundary maps for verifying each face candidate. Experimental results demonstrate successful detection over a wide variety of facial variations in color, position, scale, rotation, pose, and expression from several photo collections."
            },
            "slug": "Face-detection-in-color-images-Hsu-Abdel-Mottaleb",
            "title": {
                "fragments": [],
                "text": "Face detection in color images"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A face detection algorithm for color images in the presence of varying lighting conditions as well as complex backgrounds is proposed, and then generates face candidates based on the spatial arrangement of these skin patches."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 2001 International Conference on Image Processing (Cat. No.01CH37205)"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109348440"
                        ],
                        "name": "Xiaobo Li",
                        "slug": "Xiaobo-Li",
                        "structuredName": {
                            "firstName": "Xiaobo",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaobo Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145154665"
                        ],
                        "name": "N. Roeder",
                        "slug": "N.-Roeder",
                        "structuredName": {
                            "firstName": "Nicholas",
                            "lastName": "Roeder",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Roeder"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 83
                            }
                        ],
                        "text": "The Sobel operator was the most common filter among the techniques mentioned above [9, 16, 32, 76, 87, 108]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 121
                            }
                        ],
                        "text": "More recent examples of edge-based techniques can be found in [9, 16, 59, 115, 116] for facial feature extraction and in [32, 50, 58, 60, 70, 76, 108, 201, 222] for face detection."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 28339707,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7f1d51899314af6d668581b05d715017f05f52f0",
            "isKey": false,
            "numCitedBy": 49,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Face-contour-extraction-from-front-view-images-Li-Roeder",
            "title": {
                "fragments": [],
                "text": "Face contour extraction from front-view images"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144013393"
                        ],
                        "name": "A. Samal",
                        "slug": "A.-Samal",
                        "structuredName": {
                            "firstName": "Ashok",
                            "lastName": "Samal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Samal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32130323"
                        ],
                        "name": "Prasana A. Iyengar",
                        "slug": "Prasana-A.-Iyengar",
                        "structuredName": {
                            "firstName": "Prasana",
                            "lastName": "Iyengar",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Prasana A. Iyengar"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 26186431,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3c3592f4cb1c182ef0536f87a6695ed9aa960e2e",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Face detection is integral to any automatic face recognition system. The goal of this research is to develop a system that performs the task of human face detection automatically in a scene. A system to correctly locate and identify human faces will find several applications, some examples are criminal identification and authentication in secure systems. This work presents a new approach based on principal component analysis. Face silhouettes instead of intensity images are used for this research. It results in reduction in both space and processing time. A set of basis face silhouettes are obtained using principal component analysis. These are then used with a Hough-like technique to detect faces. The results show that the approach is robust, accurate and reasonably fast."
            },
            "slug": "Human-Face-Detection-Using-Silhouettes-Samal-Iyengar",
            "title": {
                "fragments": [],
                "text": "Human Face Detection Using Silhouettes"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Face silhouettes instead of intensity images are used for this research, which results in reduction in both space and processing time and shows that the approach is robust, accurate and reasonably fast."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Pattern Recognit. Artif. Intell."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113825663"
                        ],
                        "name": "Gang Wei",
                        "slug": "Gang-Wei",
                        "structuredName": {
                            "firstName": "Gang",
                            "lastName": "Wei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gang Wei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702517"
                        ],
                        "name": "I. Sethi",
                        "slug": "I.-Sethi",
                        "structuredName": {
                            "firstName": "Ishwar",
                            "lastName": "Sethi",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Sethi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 58
                            }
                        ],
                        "text": "The YIQ color model has been applied to face detection in [29, 205]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 27653664,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d9ff511abd115edcc7cdf2fe5bc338522a9faa85",
            "isKey": false,
            "numCitedBy": 53,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Face-detection-for-image-annotation-Wei-Sethi",
            "title": {
                "fragments": [],
                "text": "Face detection for image annotation"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit. Lett."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118443166"
                        ],
                        "name": "Jianguo Wang",
                        "slug": "Jianguo-Wang",
                        "structuredName": {
                            "firstName": "Jianguo",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianguo Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143874948"
                        ],
                        "name": "T. Tan",
                        "slug": "T.-Tan",
                        "structuredName": {
                            "firstName": "Tieniu",
                            "lastName": "Tan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Tan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11321791,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f79c9074fedb9829061bd7c98b66c24ceeb01aa4",
            "isKey": false,
            "numCitedBy": 134,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-new-face-detection-method-based-on-shape-Wang-Tan",
            "title": {
                "fragments": [],
                "text": "A new face detection method based on shape information"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit. Lett."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152292143"
                        ],
                        "name": "Jeffrey R. Huang",
                        "slug": "Jeffrey-R.-Huang",
                        "structuredName": {
                            "firstName": "Jeffrey R.",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeffrey R. Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145699982"
                        ],
                        "name": "S. Gutta",
                        "slug": "S.-Gutta",
                        "structuredName": {
                            "firstName": "Srinivas",
                            "lastName": "Gutta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Gutta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143979395"
                        ],
                        "name": "H. Wechsler",
                        "slug": "H.-Wechsler",
                        "structuredName": {
                            "firstName": "Harry",
                            "lastName": "Wechsler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Wechsler"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 121
                            }
                        ],
                        "text": "More recent examples of edge-based techniques can be found in [9, 16, 59, 115, 116] for facial feature extraction and in [32, 50, 58, 60, 70, 76, 108, 201, 222] for face detection."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 75
                            }
                        ],
                        "text": "The Marr\u2013Hildreth edge operator [124] is a part of the proposed systems in [50, 70]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17638560,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8e3ccd490466445790e246cf7e993d450ee354ba",
            "isKey": false,
            "numCitedBy": 59,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper proposes a novel algorithm for face detection using decision trees (DT) and shows its generality and feasibility using a database consisting of 2340 face images from the FERET database (corresponding to 817 subjects and including 190 sets of duplicates) over a semi-uniform background. The approach used for face detection involves three main stages, those of location, cropping, and post-processing. The first stage finds a rough approximation for the possible location of the face box, the second stage will refine it, and the last stage decider whether a face is present in the image and if the answer is positive would normalize the face image. The algorithm does not require multiple (scale) templates and the accuracy achieved is 96%. Accuracy is based on the visual observation that the face box includes both eyes, nose, and mouth, and that the top side of the box is below the hairline. Experiments were also performed to assess the accuracy of the algorithm in rejecting images where no face is present. Using a small database of 25 images of various but complex backgrounds the algorithm failed on two images for an overall accuracy rate of 92%."
            },
            "slug": "Detection-of-human-faces-using-decision-trees-Huang-Gutta",
            "title": {
                "fragments": [],
                "text": "Detection of human faces using decision trees"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "A novel algorithm for face detection using decision trees (DT) is proposed and its generality and feasibility is shown using a database consisting of 2340 face images from the FERET database over a semi-uniform background."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Second International Conference on Automatic Face and Gesture Recognition"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115746464"
                        ],
                        "name": "Y. Kwon",
                        "slug": "Y.-Kwon",
                        "structuredName": {
                            "firstName": "Young",
                            "lastName": "Kwon",
                            "middleNames": [
                                "Ho"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Kwon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700665"
                        ],
                        "name": "N. Lobo",
                        "slug": "N.-Lobo",
                        "structuredName": {
                            "firstName": "Niels",
                            "lastName": "Lobo",
                            "middleNames": [
                                "da",
                                "Vitoria"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Lobo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 36
                            }
                        ],
                        "text": "Contemporary research in this field [7, 35, 98, 168, 174] has mainly concentrated on issues such as execution time reductions, template modifications, and energy term considerations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 46942178,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c13818310b9019bb10e58177abc8dc0554cd7836",
            "isKey": false,
            "numCitedBy": 56,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "In the problem area of human facial image processing, the first computational task that needs to be solved is that of detecting a face under arbitrary scene conditions. Although some progress towards this has been reported in the literature, face detection remains a difficult problem. In this paper the authors report on a novel face-finding method that appears quite robust. First, \"snakelets\" are used to find candidate edges. Candidate ovals (face-locations) are then found from these snakelets using a voting method. For each of these candidate face-locations, the authors use a method introduced previously to find detailed facial features. If a substantial number of the facial features are found successfully, and their positions satisfy ratio-tests for being standard, the procedure positively reports the existence of a face at this location in the image."
            },
            "slug": "Face-detection-using-templates-Kwon-Lobo",
            "title": {
                "fragments": [],
                "text": "Face detection using templates"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A novel face-finding method that appears quite robust is reported on, using \"snakelets\" to find candidate edges and a voting method to find face-locations."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 12th International Conference on Pattern Recognition"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48636477"
                        ],
                        "name": "G. Yang",
                        "slug": "G.-Yang",
                        "structuredName": {
                            "firstName": "Guangzheng",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153652752"
                        ],
                        "name": "Thomas S. Huang",
                        "slug": "Thomas-S.-Huang",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Huang",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas S. Huang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 38060615,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2bb1ba70d48561ce8c3fbf59739fabc95e7b3d50",
            "isKey": false,
            "numCitedBy": 661,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Human-face-detection-in-a-complex-background-Yang-Huang",
            "title": {
                "fragments": [],
                "text": "Human face detection in a complex background"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103790394"
                        ],
                        "name": "L. Jordao",
                        "slug": "L.-Jordao",
                        "structuredName": {
                            "firstName": "Luis",
                            "lastName": "Jordao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Jordao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144782688"
                        ],
                        "name": "Matteo Perrone",
                        "slug": "Matteo-Perrone",
                        "structuredName": {
                            "firstName": "Matteo",
                            "lastName": "Perrone",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matteo Perrone"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145848221"
                        ],
                        "name": "J. Costeira",
                        "slug": "J.-Costeira",
                        "structuredName": {
                            "firstName": "Jo\u00e3o",
                            "lastName": "Costeira",
                            "middleNames": [
                                "Paulo"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Costeira"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398909021"
                        ],
                        "name": "J. Santos-Victor",
                        "slug": "J.-Santos-Victor",
                        "structuredName": {
                            "firstName": "Jos\u00e9",
                            "lastName": "Santos-Victor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Santos-Victor"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 57
                            }
                        ],
                        "text": "Other color models applied to face detection include HSV [48, 60, 81, 216], YES [160], YCrCb [2, 48, 84, 130, 191, 200], YUV [1, 123], CIE-xyz [15], L\u2217 a\u2217 b\u2217 [13, 109], L\u2217 u\u2217 v \u2217 [63], CSN (a modified rg representation) [90, 91] and UCS/Farnsworth (a perceptually uniform color system was proposed by Farnsworth [210]) [208]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 126
                            }
                        ],
                        "text": "In their study they compare normalized TSL (tint-saturation-luminance [1881]), rg and CIE-xy chrominance spaces, and CIE-DSH, HSV, YIQ, YES, CIE-L \u2217 u \u2217 v \u2217 , and CIE L\u2217 a \u2217 b \u2217 chrominance spaces by modeling skin color distributions with either a single Gaussian or a Gaussian mixture density model in each space."
                    },
                    "intents": []
                }
            ],
            "corpusId": 16832027,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "102bb4e7123c337505399823c0c2f360037b07ea",
            "isKey": false,
            "numCitedBy": 58,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a method for the detection and tracking of human face and facial features. Skin segmentation is learnt from samples of an image. After detecting a moving object, the corresponding area is searched for clusters of pixels with a known distribution. Since we only use the hue (color) component this process is quite insensitive to illumination changes. The face localization procedure looks for areas in the segmented area which resemble a head. Using simple heuristics, the located head is searched and its centroid is fed back to a camera motion control algorithm which tries to keep the face centered in the image using a pan-tilt camera unit. Furthermore the system is capable of tracking, in every frame, the three main features of a human face. Since precise eye location is computationally intensive, an eye and mouth locator using fast morphological and linear filters is developed. This allows for frame-by-frame checking, which reduces the probability of tracking a non-basis feature, yielding a higher success ratio. Velocity and robustness are the main advantages of this fast facial feature detector."
            },
            "slug": "Active-face-and-feature-tracking-Jordao-Perrone",
            "title": {
                "fragments": [],
                "text": "Active face and feature tracking"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "This paper describes a method for the detection and tracking of human face and facial features that is capable of tracking, in every frame, the three main features of a human face."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 10th International Conference on Image Analysis and Processing"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "93353159"
                        ],
                        "name": "C. Chen",
                        "slug": "C.-Chen",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Chen",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120472940"
                        ],
                        "name": "S. Chiang",
                        "slug": "S.-Chiang",
                        "structuredName": {
                            "firstName": "Serena",
                            "lastName": "Chiang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Chiang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 143
                            }
                        ],
                        "text": "Other color models applied to face detection include HSV [48, 60, 81, 216], YES [160], YCrCb [2, 48, 84, 130, 191, 200], YUV [1, 123], CIE-xyz [15], L\u2217 a\u2217 b\u2217 [13, 109], L\u2217 u\u2217 v \u2217 [63], CSN (a modified rg representation) [90, 91] and UCS/Farnsworth (a perceptually uniform color system was proposed by Farnsworth [210]) [208]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 129293707,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "00599af1cbdce577bd9badde13979cf4f2d4bd26",
            "isKey": false,
            "numCitedBy": 54,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "In modern multimedia systems, video and image signals usually need to be indexed or retrieved according to their contents. Colour characteristics are proposed for use in detection of human faces in colour images with complex backgrounds. The proposed face detection method first uses a neural network to classify the images and then segments the candidate face regions. Then, an energy thresholding method which can take the shape, colour and edge characteristics of the face features into the extraction process is devised to extract the lips. Finally, three shape descriptors of the lip feature are used to further verify the existence of the face in the candidate face regions. The experimental results show that this method can detect faces in the images from different sources in an accurate and efficient manner. Since faces are common elements in video and image signals, the proposed face detection method is an advance towards the goal of content-based video and image indexing and retrieval."
            },
            "slug": "Detection-of-human-faces-in-colour-images-Chen-Chiang",
            "title": {
                "fragments": [],
                "text": "Detection of human faces in colour images"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The proposed face detection method first uses a neural network to classify the images and then segments the candidate face regions, and an energy thresholding method which can take the shape, colour and edge characteristics of the face features into the extraction process is devised to extract the lips."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144013393"
                        ],
                        "name": "A. Samal",
                        "slug": "A.-Samal",
                        "structuredName": {
                            "firstName": "Ashok",
                            "lastName": "Samal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Samal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32130323"
                        ],
                        "name": "Prasana A. Iyengar",
                        "slug": "Prasana-A.-Iyengar",
                        "structuredName": {
                            "firstName": "Prasana",
                            "lastName": "Iyengar",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Prasana A. Iyengar"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 205013679,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0be360a2964c4bb91aaad0cc6d1baa6639746028",
            "isKey": false,
            "numCitedBy": 1067,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Automatic-recognition-and-analysis-of-human-faces-a-Samal-Iyengar",
            "title": {
                "fragments": [],
                "text": "Automatic recognition and analysis of human faces and facial expressions: a survey"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "82639669"
                        ],
                        "name": "M. Fouad",
                        "slug": "M.-Fouad",
                        "structuredName": {
                            "firstName": "Mona",
                            "lastName": "Fouad",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Fouad"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2946185"
                        ],
                        "name": "A. Darwish",
                        "slug": "A.-Darwish",
                        "structuredName": {
                            "firstName": "Ahmed",
                            "lastName": "Darwish",
                            "middleNames": [
                                "Mohamed"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Darwish"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47352189"
                        ],
                        "name": "F. Bayoumi",
                        "slug": "F.-Bayoumi",
                        "structuredName": {
                            "firstName": "Fatma",
                            "lastName": "Bayoumi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Bayoumi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2769013"
                        ],
                        "name": "S. Shaheen",
                        "slug": "S.-Shaheen",
                        "structuredName": {
                            "firstName": "Samir",
                            "lastName": "Shaheen",
                            "middleNames": [
                                "Ibrahim"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Shaheen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 227,
                                "start": 223
                            }
                        ],
                        "text": "A similar approach to that of Sung and Poggio based on gray-level features in combination with texture features has been explored in [38] and a similar more computationally efficient method has been proposed by Fouad et al.[44]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1252301,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6f76f4227760ae50fb4f40b1da8efbba22263d37",
            "isKey": false,
            "numCitedBy": 3,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present a model-based face detection approach for determining the region occupied by upright frontal views of human faces in unconstrained natural scenes without any priori information. The process is composed of one offline and two online steps. The first step is offline and consists of the modeling stage that generates a model describing the \"face\" and \"non-face\" patterns by only few clusters. The second step is the pattern matching stage, which matches small patterns cropped from the input image at different positions and scales against the modeled clusters. The third step is the classification stage, which identifies the specified pattern as face or non-face using the nearest neighbor classifier. The main advantage of the proposed approach is the reduced computation complexity with respect to previously reported systems. This is done by an intelligent arbitration strategy that avoids the exhaustive search at different scales. The system is tested on 50 images containing 253 faces. The correct detection rate is approximately 83% while the false detection count is low."
            },
            "slug": "Model-based-human-face-detection-in-unconstrained-Fouad-Darwish",
            "title": {
                "fragments": [],
                "text": "Model-based human face detection in unconstrained scenes"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "A model-based face detection approach for determining the region occupied by upright frontal views of human faces in unconstrained natural scenes without any priori information by an intelligent arbitration strategy that avoids the exhaustive search at different scales."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 2000 International Conference on Image Processing (Cat. No.00CH37101)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712041"
                        ],
                        "name": "K. Mikolajczyk",
                        "slug": "K.-Mikolajczyk",
                        "structuredName": {
                            "firstName": "Krystian",
                            "lastName": "Mikolajczyk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Mikolajczyk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2047253"
                        ],
                        "name": "R. Choudhury",
                        "slug": "R.-Choudhury",
                        "structuredName": {
                            "firstName": "Ragini",
                            "lastName": "Choudhury",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Choudhury"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462253"
                        ],
                        "name": "C. Schmid",
                        "slug": "C.-Schmid",
                        "structuredName": {
                            "firstName": "Cordelia",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schmid"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7498920,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "549b98178134ad855a73258e1fae41edcd745d90",
            "isKey": false,
            "numCitedBy": 53,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a new method for detecting faces in a video sequence where detection is not limited to frontal views. The three novel contributions of the paper are : (1) Accumulation of probabilities of detection over a sequence. This allows to obtain a coherent detection over time as well as independence from thresholds. (2) Prediction of the detection parameters which are position, scale and pose. This guarantees the accuracy of accumulation as well as a continuous detection. (3) The way pose is represented. The representation is based on the combination of two detectors, one for frontal views and one for profiles. Face detection is fully automatic and is based on the method developed by Schneiderman [13]. It uses local histograms of wavelet coefficients represented with respect to a coordinate frame fixed to the object. A probability of detection is obtained for each image position, several scales and the two detectors. The probabilities of detection are propagated over time using a Condensation filter and factored sampling. Prediction is based on a zero order model for position, scale and \"pose\"; update uses the probability maps produced by the detection routine. Experiments show a clear improvement over frame-based detection results."
            },
            "slug": "Face-detection-in-a-video-sequence-a-temporal-Mikolajczyk-Choudhury",
            "title": {
                "fragments": [],
                "text": "Face detection in a video sequence - a temporal approach"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "A new method for detecting faces in a video sequence where detection is not limited to frontal views and based on the method developed by Schneiderman, which guarantees the accuracy of accumulation as well as a continuous detection."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793040"
                        ],
                        "name": "N. Duta",
                        "slug": "N.-Duta",
                        "structuredName": {
                            "firstName": "Nicolae",
                            "lastName": "Duta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Duta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 133
                            }
                        ],
                        "text": "A similar approach to that of Sung and Poggio based on gray-level features in combination with texture features has been explored in [38] and a similar more computationally efficient method has been proposed by Fouad et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1664488,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "371822469e4a3b12fd78daeb130547c4242a69b4",
            "isKey": false,
            "numCitedBy": 16,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Presents a learning approach for the face detection problem. The problem can be stated as follows: given an arbitrary black and white, still image, find the location and size of every human face it contains. Numerous applications of automatic face detection have attracted considerable interest in this problem, but no present face detection system is completely satisfactory from the point of view of detection rate, false alarm rate and detection time. We describe an inductive learning-based detection method that produces a maximally specific hypothesis consistent with the training data. Three different sets of features were considered for defining the concept of a human face. The performance achieved is as follows: 85% detection rate, a false alarm rate of 0.04% of the number of windows analyzed and 1 minute detection table for a 320/spl times/240 image on a Sun Ultrasparc 1."
            },
            "slug": "Learning-the-human-face-concept-in-black-and-white-Duta-Jain",
            "title": {
                "fragments": [],
                "text": "Learning the human face concept in black and white images"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "An inductive learning-based detection method is described that produces a maximally specific hypothesis consistent with the training data and achieves 85% detection rate, a false alarm rate of 0.04% and 1 minute detection table for a 320/spl times/240 image on a Sun Ultrasparc 1."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. Fourteenth International Conference on Pattern Recognition (Cat. No.98EX170)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731570"
                        ],
                        "name": "M. Lew",
                        "slug": "M.-Lew",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Lew",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lew"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2901762"
                        ],
                        "name": "D. P. Huijsmans",
                        "slug": "D.-P.-Huijsmans",
                        "structuredName": {
                            "firstName": "Dionysius",
                            "lastName": "Huijsmans",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. P. Huijsmans"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 129
                            }
                        ],
                        "text": "The number of labeled faces was originally 149 by Sung and Poggio (which is the number used in the reported results in [182] and [107], but it was changed to 155 (which"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 687,
                                "start": 678
                            }
                        ],
                        "text": "The number of labeled faces was originally 149 by Sung and Poggio (which is the number used in the reported results in [182] and [107], but it was changed to 155 (which\nTABLE 2 Results Reported in Terms of Percentage Correct Detection (CD) and Number of False Positives (FP), CD/FP, on the CMU and MIT Datasets\nFace detection system CMU-130 CMU-125 MIT-23 MIT-20\nSchneiderman & Kanade\u2014Ea [170] 94.4%/65 Schneiderman & Kanade\u2014Wb [170] 90.2%/110 Yanget al.\u2014FA [217] 92.3%/82 89.4%/3 Yanget al.\u2014LDA [217] 93.6%/74 91.5%/1 Rothet al. [157] 94.8%/78 94.1%/3 Rowleyet al. [158] 86.2%/23 84.5%/8 Feraudet al. [42] 86%/8 Colmenarez & Huang [22] 93.9%/8122 Sung & Poggio [182] 79.9%/5 Lew & Huijsmans [107] 94.1%/64 Osunaet al. [140] 74.2%/20 Lin et al. [113] 72.3%/6 Gu and Li [54] 87.1%/0\na Eigenvector coefficients. b Wavelet coefficients.\nis the number used in the reported results in [158], [140], and [113]) when included in the CMU dataset."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 57
                            }
                        ],
                        "text": "A similar system has been developed by Lew and Huijsmans [107]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 17
                            }
                        ],
                        "text": "M. S. Lew and N. Huijsmans, Information theory and face detection, inProc. of International Conference on Pattern Recognition, 1996."
                    },
                    "intents": []
                }
            ],
            "corpusId": 629097,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3014bd43e5477a2f1364e82addcb1c98c7961bd9",
            "isKey": true,
            "numCitedBy": 65,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Face detection in complex environments is an unsolved problem which has fundamental importance to face recognition, model based video coding, content based image retrieval, and human computer interaction. In this paper we model the face detection problem using information theory, and formulate information based measures for detecting faces by maximizing the feature class separation. The underlying principle is that search through an image can be viewed as a reduction of uncertainty in the classification of the image. The face detection algorithm is empirically compared using multiple test sets, which include four face databases from three universities."
            },
            "slug": "Information-theory-and-face-detection-Lew-Huijsmans",
            "title": {
                "fragments": [],
                "text": "Information theory and face detection"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper model the face detection problem using information theory, and formulate information based measures for detecting faces by maximizing the feature class separation, which is empirically compared using multiple test sets."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 13th International Conference on Pattern Recognition"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144986880"
                        ],
                        "name": "I. Craw",
                        "slug": "I.-Craw",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Craw",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Craw"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055741802"
                        ],
                        "name": "H. Ellis",
                        "slug": "H.-Ellis",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Ellis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ellis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144579828"
                        ],
                        "name": "J. Lishman",
                        "slug": "J.-Lishman",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Lishman",
                            "middleNames": [
                                "Rowland"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lishman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 1300797,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3d9a1b66ddac96b4bbb3592cfe40db4bb28b24e8",
            "isKey": false,
            "numCitedBy": 232,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Automatic-extraction-of-face-features-Craw-Ellis",
            "title": {
                "fragments": [],
                "text": "Automatic extraction of face-features"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit. Lett."
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50024179"
                        ],
                        "name": "Yongmin Li",
                        "slug": "Yongmin-Li",
                        "structuredName": {
                            "firstName": "Yongmin",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yongmin Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144784813"
                        ],
                        "name": "S. Gong",
                        "slug": "S.-Gong",
                        "structuredName": {
                            "firstName": "Shaogang",
                            "lastName": "Gong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Gong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2934996"
                        ],
                        "name": "H. Liddell",
                        "slug": "H.-Liddell",
                        "structuredName": {
                            "firstName": "Heather",
                            "lastName": "Liddell",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Liddell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1889567,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cf0a5ddc4e80359418e8a66f6f6c068efd12c1d3",
            "isKey": false,
            "numCitedBy": 276,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "A support vector machine-based multi-view face detection and recognition framework is described. Face detection is carried out by constructing several detectors, each of them in charge of one specific view. The symmetrical property of face images is employed to simplify the complexity of the modelling. The estimation of head pose, which is achieved by using the support vector regression technique, provides crucial information for choosing the appropriate face detector. This helps to improve the accuracy and reduce the computation in multi-view face detection compared to other methods. For video sequences, further computational reduction can be achieved by using a pose change smoothing strategy. When face detectors find a face in frontal view, a support vector machine-based multi-class classifier is activated for face recognition. All the above issues are integrated under a support vector machine framework. Test results on four video sequences are presented, among them the detection rate is above 95%, recognition accuracy is above 90%, average pose estimation error is around 10/spl deg/, and the full detection and recognition speed is up to 4 frames/second on a Pentium II 300 PC."
            },
            "slug": "Support-vector-regression-and-classification-based-Li-Gong",
            "title": {
                "fragments": [],
                "text": "Support vector regression and classification based multi-view face detection and recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The estimation of head pose, which is achieved by using the support vector regression technique, provides crucial information for choosing the appropriate face detector, which helps to improve the accuracy and reduce the computation in multi-view face detection compared to other methods."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "92329551"
                        ],
                        "name": "M. Abdel-Mottaleb",
                        "slug": "M.-Abdel-Mottaleb",
                        "structuredName": {
                            "firstName": "Mohamed",
                            "lastName": "Abdel-Mottaleb",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Abdel-Mottaleb"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145159523"
                        ],
                        "name": "A. Elgammal",
                        "slug": "A.-Elgammal",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Elgammal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Elgammal"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 32492126,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "13fe61ecb4a6441e30827c59d1e6b6fa7940e800",
            "isKey": false,
            "numCitedBy": 90,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "The detection of faces in color images is important for many multimedia applications. It is the first step for face recognition and it can be used for classifying specific shots such as anchorperson and talk show shots. In this paper, we present an algorithm for the detection of faces in color images. The algorithm works by first detecting areas of skin color, then it applies a top-down and a bottom-up analysis to the skin colored areas. The algorithm has been tested on images from news clips and other television programs. The results show that the algorithm is robust and works even for cases where there are objects in the background that have colors similar to the skin."
            },
            "slug": "Face-detection-in-complex-environments-from-color-Abdel-Mottaleb-Elgammal",
            "title": {
                "fragments": [],
                "text": "Face detection in complex environments from color images"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The algorithm works by first detecting areas of skin color, then it applies a top-down and a bottom-up analysis to the skin colored areas and shows that the algorithm is robust and works even for cases where there are objects in the background that have colors similar to theskin."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348)"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40212742"
                        ],
                        "name": "T. Kondo",
                        "slug": "T.-Kondo",
                        "structuredName": {
                            "firstName": "Toshiaki",
                            "lastName": "Kondo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kondo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152996923"
                        ],
                        "name": "Hong Yan",
                        "slug": "Hong-Yan",
                        "structuredName": {
                            "firstName": "Hong",
                            "lastName": "Yan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hong Yan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14838563,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "48d0b2bdc73588951d3a1f5757b73935c483a8b3",
            "isKey": false,
            "numCitedBy": 53,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Automatic-human-face-detection-and-recognition-Kondo-Yan",
            "title": {
                "fragments": [],
                "text": "Automatic human face detection and recognition under non-uniform illumination"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2145032549"
                        ],
                        "name": "Weimin Huang",
                        "slug": "Weimin-Huang",
                        "structuredName": {
                            "firstName": "Weimin",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Weimin Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737713"
                        ],
                        "name": "R. Mariani",
                        "slug": "R.-Mariani",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Mariani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mariani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 56
                            }
                        ],
                        "text": "Various types of face constellations have been proposed [4, 11, 73, 110, 117, 131, 180, 197, 221]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16481859,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "67cd3fbc1a5e52dfc0f5cd9acb2bd1304d1a92ad",
            "isKey": false,
            "numCitedBy": 114,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Presents a robust and precise scheme for face detection and precise facial feature location. Multiscale filters are used to obtain the pre-attentive features of objects, based on which different models are investigated to locate the face and facial features such as eyes, nose and mouth. The structural model is used to characterize the geometric pattern of facial components. The texture and feature models are used to verify the face candidates detected before. Since the eyeballs are the only features that are salient and have strong invariant property, the distance between them is used to normalize faces for recognition. Motivated from this, with the face detected and the structural information extracted, a precise eyes location algorithm is applied using contour and region information. It detects, with a subpixellic precision, the center and the radius of the eyeballs of a person. The detected result can be used as an accurate normalization of images, which reduces greatly the number of possible scales used during the face recognition process."
            },
            "slug": "Face-detection-and-precise-eyes-location-Huang-Mariani",
            "title": {
                "fragments": [],
                "text": "Face detection and precise eyes location"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A precise eyes location algorithm is applied using contour and region information and detects, with a subpixellic precision, the center and the radius of the eyeballs of a person."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 15th International Conference on Pattern Recognition. ICPR-2000"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1803435"
                        ],
                        "name": "A. Albiol",
                        "slug": "A.-Albiol",
                        "structuredName": {
                            "firstName": "Alberto",
                            "lastName": "Albiol",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Albiol"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144070753"
                        ],
                        "name": "L. Torres",
                        "slug": "L.-Torres",
                        "structuredName": {
                            "firstName": "Luis",
                            "lastName": "Torres",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Torres"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745655"
                        ],
                        "name": "C. Bouman",
                        "slug": "C.-Bouman",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Bouman",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Bouman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741483"
                        ],
                        "name": "E. Delp",
                        "slug": "E.-Delp",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Delp",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Delp"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6291721,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2b2afa2231e01a62a1f117adc7f05d9050a91161",
            "isKey": false,
            "numCitedBy": 56,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "The objective of this work is to provide a simple and yet efficient tool to detect human faces in video sequences. This information can be very useful for many applications such as video indexing and video browsing. In particular the paper focuses on the significant improvements made to our face detection algorithm presented by Albiol, Bouman and Delp (see IEEE Int. Conference on Image Processing, Kobe, Japan, 1999). Specifically, a novel approach to retrieve skin-like homogeneous regions is presented, which is later used to retrieve face images. Good results have been obtained for a large variety of video sequences."
            },
            "slug": "A-simple-and-efficient-face-detection-algorithm-for-Albiol-Torres",
            "title": {
                "fragments": [],
                "text": "A simple and efficient face detection algorithm for video database applications"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A novel approach to retrieve skin-like homogeneous regions is presented, which is later used to retrieve face images in video sequences, and good results have been obtained for a large variety of video sequences."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 2000 International Conference on Image Processing (Cat. No.00CH37101)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2085062"
                        ],
                        "name": "Gloria Chow",
                        "slug": "Gloria-Chow",
                        "structuredName": {
                            "firstName": "Gloria",
                            "lastName": "Chow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gloria Chow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109348440"
                        ],
                        "name": "Xiaobo Li",
                        "slug": "Xiaobo-Li",
                        "structuredName": {
                            "firstName": "Xiaobo",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaobo Li"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 207,
                                "start": 194
                            }
                        ],
                        "text": "The lengthy processing time is also reduced by using a simple version of the template by trading off the considerations of some parameters that have lesser effects on the overall template shape [17, 69, 211]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[17] employ a two-step approach to the eye extraction task."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 70
                            }
                        ],
                        "text": "Besides eye templates, the use of mouth templates was also introduced [17, 69] using similar strategies."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 27552346,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ca6393f736e59be460288e71590e75aa3bfc1091",
            "isKey": true,
            "numCitedBy": 236,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Towards-a-system-for-automatic-facial-feature-Chow-Li",
            "title": {
                "fragments": [],
                "text": "Towards a system for automatic facial feature detection"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118808101"
                        ],
                        "name": "Sang-Hoon Kim",
                        "slug": "Sang-Hoon-Kim",
                        "structuredName": {
                            "firstName": "Sang-Hoon",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sang-Hoon Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2119401931"
                        ],
                        "name": "Nam-Kyu Kim",
                        "slug": "Nam-Kyu-Kim",
                        "structuredName": {
                            "firstName": "Nam-Kyu",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nam-Kyu Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1853776"
                        ],
                        "name": "S. Ahn",
                        "slug": "S.-Ahn",
                        "structuredName": {
                            "firstName": "Sang",
                            "lastName": "Ahn",
                            "middleNames": [
                                "Chul"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ahn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1931368"
                        ],
                        "name": "Hyoung-Gon Kim",
                        "slug": "Hyoung-Gon-Kim",
                        "structuredName": {
                            "firstName": "Hyoung-Gon",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hyoung-Gon Kim"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 228,
                                "start": 220
                            }
                        ],
                        "text": "Other color models applied to face detection include HSV [48, 60, 81, 216], YES [160], YCrCb [2, 48, 84, 130, 191, 200], YUV [1, 123], CIE-xyz [15], L\u2217 a\u2217 b\u2217 [13, 109], L\u2217 u\u2217 v \u2217 [63], CSN (a modified rg representation) [90, 91] and UCS/Farnsworth (a perceptually uniform color system was proposed by Farnsworth [210]) [208]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7394271,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b3cba0f5a0c3aaac180a8abe0fb27de27fdb6618",
            "isKey": false,
            "numCitedBy": 68,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes an object oriented face detection method using range and color information. Objects are segmented from the background using the stereo disparity histogram that represents the range information of the objects. A matching pixel count (MPC) disparity measure is introduced to enhance the matching accuracy, and removes the effect of unexpected noise in the boundary region. For the high-performance implementation of the MPC disparity histogram, redundancy operations inherent to the area-based search operation are removed. To detect facial regions among segmented objects, a skin-color transform technique is used with the generalized face color distribution (GFCD) modeled by a 2D Gaussian function in a normalized color space. Using GFCD, the input color image can be transformed into a gray-level image enhancing only facial color components. To detect facial information only in the defined range, both results from range segmentation and color transforms are combined effectively. The experimental results show that the proposed algorithm works well in various environments with multiple human objects. Moreover, the processing time for a test image is not exceeding 2 seconds in general purpose workstations. The range information of the objects can be useful in MPEG-4 where natural and synthetic images can be mixed and synthesized."
            },
            "slug": "Object-oriented-face-detection-using-range-and-Kim-Kim",
            "title": {
                "fragments": [],
                "text": "Object oriented face detection using range and color information"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "An object oriented face detection method using range and color information that works well in various environments with multiple human objects and can be useful in MPEG-4 where natural and synthetic images can be mixed and synthesized."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Third IEEE International Conference on Automatic Face and Gesture Recognition"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118808101"
                        ],
                        "name": "Sang-Hoon Kim",
                        "slug": "Sang-Hoon-Kim",
                        "structuredName": {
                            "firstName": "Sang-Hoon",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sang-Hoon Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1931368"
                        ],
                        "name": "Hyoung-Gon Kim",
                        "slug": "Hyoung-Gon-Kim",
                        "structuredName": {
                            "firstName": "Hyoung-Gon",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hyoung-Gon Kim"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 228,
                                "start": 220
                            }
                        ],
                        "text": "Other color models applied to face detection include HSV [48, 60, 81, 216], YES [160], YCrCb [2, 48, 84, 130, 191, 200], YUV [1, 123], CIE-xyz [15], L\u2217 a\u2217 b\u2217 [13, 109], L\u2217 u\u2217 v \u2217 [63], CSN (a modified rg representation) [90, 91] and UCS/Farnsworth (a perceptually uniform color system was proposed by Farnsworth [210]) [208]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 206496369,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4800b1e16045fc0ec1ff77638370e8b05e9b02ee",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes an object-oriented face detection method using multi-modal fusion of range, color and motion information. Objects are segmented from a complex background using a stereo disparity histogram that represents the range information of the objects. A matching pixel count (MPC) disparity measure is introduced to enhance the matching accuracy. To detect the facial regions among segmented objects, a skin-color transform technique is used with the general skin color distribution (GSCD) modeled by a 2D Gaussian function in a color synthetic normalization (CSN) color space. The motion detection technique of AWUPC (adaptive weighted unmatched pixel count) is defined on the skin-color transformed image where the adaptive threshold value for the motion detection is determined according to the probability of skin color. AWUPC transforms the input color image into a gray-level image that represents the probability of both the skin color and motion information. The experimental results show that the proposed algorithm can detect a moving human object in various environments such as skin color noise and complex background. It can be useful in MPEG-4 SNHC."
            },
            "slug": "Face-detection-using-multi-modal-information-Kim-Kim",
            "title": {
                "fragments": [],
                "text": "Face detection using multi-modal information"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The experimental results show that the proposed algorithm can detect a moving human object in various environments such as skin color noise and complex background and can be useful in MPEG-4 SNHC."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39818984"
                        ],
                        "name": "A. R. Mirhosseini",
                        "slug": "A.-R.-Mirhosseini",
                        "structuredName": {
                            "firstName": "Ali",
                            "lastName": "Mirhosseini",
                            "middleNames": [
                                "Reza"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. R. Mirhosseini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152996923"
                        ],
                        "name": "Hong Yan",
                        "slug": "Hong-Yan",
                        "structuredName": {
                            "firstName": "Hong",
                            "lastName": "Yan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hong Yan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144847940"
                        ],
                        "name": "K. Lam",
                        "slug": "K.-Lam",
                        "structuredName": {
                            "firstName": "Kin-Man",
                            "lastName": "Lam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Lam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145525040"
                        ],
                        "name": "T. Pham",
                        "slug": "T.-Pham",
                        "structuredName": {
                            "firstName": "Tuan",
                            "lastName": "Pham",
                            "middleNames": [
                                "Dung"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Pham"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 71
                            }
                        ],
                        "text": "Several recent graph matching systems perform automatic face detection [132, 138], but none have this task as the main goal of the system; thus few extensive quantitative results have been reported on the face detection task specifically."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 45910559,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8f18cf04ab0e52cd09a0c051da5300a0001f8954",
            "isKey": false,
            "numCitedBy": 53,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper a novel analytically-based face recognition system is presented which allows incorporation of the importance of individual facial components in the recognition task. An image gallery of 40 people was used and the images searched to locate the face area and the head boundary. In this system the eyes are detected using learning graph templates, the mouth is detected using deformable templates, and the location of the nose is found by using integral projections based on the mouth and eye locations. Using a 3D model of a head, the facial rotations are estimated in order for the system to compensate for the rotation. The effect of the facial convexity is examined by using an overall recognition index, and an optimum value is used for the rest of the experiments. Each facial feature provides evidence for a classifier, with varying degrees of reliability. Furthermore, a fuzzy information fusion technique is applied to combine the decisions of individual classifiers with all possible combinations of classifiers. The reliability of each classifier is evaluated by an expert using fuzzy density measures in a training phase. An overall classification is derived using a fuzzy evidence aggregation method. The performance of the system is evaluated for various degrees of facial rotation using a cumulative score. The cumulative score provides the normal recognition rate as well as the rank of the next best matches."
            },
            "slug": "Human-Face-Image-Recognition:-An-Evidence-Approach-Mirhosseini-Yan",
            "title": {
                "fragments": [],
                "text": "Human Face Image Recognition: An Evidence Aggregation Approach"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "A novel analytically-based face recognition system is presented which allows incorporation of the importance of individual facial components in the recognition task and a fuzzy information fusion technique is applied to combine the decisions of individual classifiers with all possible combinations of classifiers."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Vis. Image Underst."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4801654"
                        ],
                        "name": "Y. Amit",
                        "slug": "Y.-Amit",
                        "structuredName": {
                            "firstName": "Yali",
                            "lastName": "Amit",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Amit"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707642"
                        ],
                        "name": "D. Geman",
                        "slug": "D.-Geman",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Geman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Geman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2521020"
                        ],
                        "name": "B. Jedynak",
                        "slug": "B.-Jedynak",
                        "structuredName": {
                            "firstName": "Bruno",
                            "lastName": "Jedynak",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Jedynak"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 118052090,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b56ab69d6d2ed840f3850df3c60ccdf1d5f284d9",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an algorithm for shape detection and apply it to frontal views of faces in still grey level images with arbitrary backgrounds. Detection is done in two stages: (i) \u201cfocusing,\u201d during which a relatively small number of regions-of-interest are identified, minimizing computation and false negatives at the (temporary) expense of false positives; and (ii) \u201cintensive classification,\u201d during which a selected region-of-interest is labeled face or background based on multiple decision trees and normalized data. In contrast to most detection algorithms, the processing is then very highly concentrated in the regions near faces and near false positives."
            },
            "slug": "Efficient-Focusing-and-Face-Detection-Amit-Geman",
            "title": {
                "fragments": [],
                "text": "Efficient Focusing and Face Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "An algorithm for shape detection is presented and applied to frontal views of faces in still grey level images with arbitrary backgrounds and a selected region-of-interest is labeled face or background based on multiple decision trees and normalized data."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "23125434"
                        ],
                        "name": "C. Morimoto",
                        "slug": "C.-Morimoto",
                        "structuredName": {
                            "firstName": "Carlos",
                            "lastName": "Morimoto",
                            "middleNames": [
                                "Hitoshi"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Morimoto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712991"
                        ],
                        "name": "M. Flickner",
                        "slug": "M.-Flickner",
                        "structuredName": {
                            "firstName": "Myron",
                            "lastName": "Flickner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Flickner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 128
                            }
                        ],
                        "text": "Approaches which focus on single camera control include LISTEN [19],\nSAVI [60], LAFTER [139], Rits Eye [212], and the system of Morimoto and Flickner [135]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 81
                            }
                        ],
                        "text": "SAVI [60], LAFTER [139], Rits Eye [212], and the system of Morimoto and Flickner [135]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 19
                            }
                        ],
                        "text": "C. Morimoto and M. Flickner, Real-time multiple face detection using active illumination, inProceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition, 2000."
                    },
                    "intents": []
                }
            ],
            "corpusId": 20502596,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "773a5dfed7383ca4c8fc9aa63e97e56c8af6b74d",
            "isKey": false,
            "numCitedBy": 76,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a multiple face detector based on a robust pupil detection technique. The pupil detector uses active illumination that exploits the retro-reflectivity property of eyes to facilitate detection. The detection range of this method is appropriate for interactive desktop and kiosk applications. Once the location of the pupil candidates are computed, the candidates are filtered and grouped into pairs that correspond to faces using heuristic rules. To demonstrate the robustness of the face detection technique, a dual-mode face tracker was developed, which is initialized with the most salient detected face. Recursive estimators are used to guarantee the stability of the process and combine the measurements from the multi-face detector and a feature correlation tracker. The estimated position of the face is used to control a pan-tilt servo mechanism in real-time, that moves the camera to keep the tracked face always centered in the image."
            },
            "slug": "Real-time-multiple-face-detection-using-active-Morimoto-Flickner",
            "title": {
                "fragments": [],
                "text": "Real-time multiple face detection using active illumination"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "A multiple face detector based on a robust pupil detection technique that exploits the retro-reflectivity property of eyes to facilitate detection and is appropriate for interactive desktop and kiosk applications is presented."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144723337"
                        ],
                        "name": "Christophe Garcia",
                        "slug": "Christophe-Garcia",
                        "structuredName": {
                            "firstName": "Christophe",
                            "lastName": "Garcia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christophe Garcia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2441655"
                        ],
                        "name": "G. Tziritas",
                        "slug": "G.-Tziritas",
                        "structuredName": {
                            "firstName": "Georgios",
                            "lastName": "Tziritas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Tziritas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Other color models applied to face detection include HSV [48, 60, 81, 216], YES [160], YCrCb [2, 48, 84, 130, 191, 200], YUV [1, 123], CIE-xyz [15], L \u2217 a \u2217 b \u2217 [13, 109], L \u2217 u \u2217 v \u2217 [63], CSN (a modified rg representation) [90, 91] and UCS/Farnsworth (a perceptually uniform color system was proposed by Farnsworth [210]) [208]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17988718,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ddbb6e0913ac127004be73e2d4097513a8f02d37",
            "isKey": false,
            "numCitedBy": 654,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "Detecting and recognizing human faces automatically in digital images strongly enhance content-based video indexing systems. In this paper, a novel scheme for human faces detection in color images under nonconstrained scene conditions, such as the presence of a complex background and uncontrolled illumination, is presented. Color clustering and filtering using approximations of the YCbCr and HSV skin color subspaces are applied on the original image, providing quantized skin color regions. A merging stage is then iteratively performed on the set of homogeneous skin color regions in the color quantized image, in order to provide a set of potential face areas. Constraints related to shape and size of faces are applied, and face intensity texture is analyzed by performing a wavelet packet decomposition on each face area candidate in order to detect human faces. The wavelet coefficients of the band filtered images characterize the face texture and a set of simple statistical deviations is extracted in order to form compact and meaningful feature vectors. Then, an efficient and reliable probabilistic metric derived from the Bhattacharrya distance is used in order to classify the extracted feature vectors into face or nonface areas, using some prototype face area vectors, acquired in a previous training stage."
            },
            "slug": "Face-Detection-Using-Quantized-Skin-Color-Regions-Garcia-Tziritas",
            "title": {
                "fragments": [],
                "text": "Face Detection Using Quantized Skin Color Regions Merging and Wavelet Packet Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "An efficient and reliable probabilistic metric derived from the Bhattacharrya distance is used in order to classify the extracted feature vectors into face or nonface areas, using some prototype face area vectors, acquired in a previous training stage."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Multim."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5146916"
                        ],
                        "name": "Jian-Gang Wang",
                        "slug": "Jian-Gang-Wang",
                        "structuredName": {
                            "firstName": "Jian-Gang",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian-Gang Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725702"
                        ],
                        "name": "E. Sung",
                        "slug": "E.-Sung",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Sung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Sung"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 34879256,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3d163a3259b467ab95d079da26082a7422ca023f",
            "isKey": false,
            "numCitedBy": 114,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Frontal-view-face-detection-and-facial-feature-and-Wang-Sung",
            "title": {
                "fragments": [],
                "text": "Frontal-view face detection and facial feature extraction using color and morphological operations"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit. Lett."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47128406"
                        ],
                        "name": "Shang-Hung Lin",
                        "slug": "Shang-Hung-Lin",
                        "structuredName": {
                            "firstName": "Shang-Hung",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shang-Hung Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144410963"
                        ],
                        "name": "S. Kung",
                        "slug": "S.-Kung",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Kung",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111062938"
                        ],
                        "name": "Long-Ji Lin",
                        "slug": "Long-Ji-Lin",
                        "structuredName": {
                            "firstName": "Long-Ji",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Long-Ji Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "[113], a fully automatic face recognition system is proposed based on probabilistic decision-based neural networks (PDBNN)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 153
                            }
                        ],
                        "text": "For face recognition, this implies that neural approaches might be applied for all parts of the system, and this had indeed been shown in several papers [14, 113, 163]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 64
                            }
                        ],
                        "text": "is the number used in the reported results in [158], [140], and [113]) when included in the CMU dataset."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 24636915,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fa2603efaf717974c77162c93d800defae61a129",
            "isKey": false,
            "numCitedBy": 669,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a face recognition system, based on probabilistic decision-based neural networks (PDBNN). With technological advance on microelectronic and vision system, high performance automatic techniques on biometric recognition are now becoming economically feasible. Among all the biometric identification methods, face recognition has attracted much attention in recent years because it has potential to be most nonintrusive and user-friendly. The PDBNN face recognition system consists of three modules: First, a face detector finds the location of a human face in an image. Then an eye localizer determines the positions of both eyes in order to generate meaningful feature vectors. The facial region proposed contains eyebrows, eyes, and nose, but excluding mouth (eye-glasses will be allowed). Lastly, the third module is a face recognizer. The PDBNN can be effectively applied to all the three modules. It adopts a hierarchical network structures with nonlinear basis functions and a competitive credit-assignment scheme. The paper demonstrates a successful application of PDBNN to face recognition applications on two public (FERET and ORL) and one in-house (SCR) databases. Regarding the performance, experimental results on three different databases such as recognition accuracies as well as false rejection and false acceptance rates are elaborated. As to the processing speed, the whole recognition process (including PDBNN processing for eye localization, feature extraction, and classification) consumes approximately one second on Sparc10, without using hardware accelerator or co-processor."
            },
            "slug": "Face-recognition/detection-by-probabilistic-neural-Lin-Kung",
            "title": {
                "fragments": [],
                "text": "Face recognition/detection by probabilistic decision-based neural network"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The paper demonstrates a successful application of PDBNN to face recognition applications on two public (FERET and ORL) and one in-house (SCR) databases and experimental results on three different databases such as recognition accuracies as well as false rejection and false acceptance rates are elaborated."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783224"
                        ],
                        "name": "A. Colmenarez",
                        "slug": "A.-Colmenarez",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Colmenarez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Colmenarez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153652752"
                        ],
                        "name": "Thomas S. Huang",
                        "slug": "Thomas-S.-Huang",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Huang",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas S. Huang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 89
                            }
                        ],
                        "text": "Based on an earlier work of maximum likelihood face detection [21], Colmenarez and Huang [22] proposed a system based on Kullback relative information (Kullback divergence)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 26
                            }
                        ],
                        "text": "The technique of Yang and Huang was recently incorporated into a system for rotation invariant face detection by Lvet al. [119] and an extension of the algorithm is presented in Kotropoulos and Pitas [95]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 627,
                                "start": 622
                            }
                        ],
                        "text": "The number of labeled faces was originally 149 by Sung and Poggio (which is the number used in the reported results in [182] and [107], but it was changed to 155 (which\nTABLE 2 Results Reported in Terms of Percentage Correct Detection (CD) and Number of False Positives (FP), CD/FP, on the CMU and MIT Datasets\nFace detection system CMU-130 CMU-125 MIT-23 MIT-20\nSchneiderman & Kanade\u2014Ea [170] 94.4%/65 Schneiderman & Kanade\u2014Wb [170] 90.2%/110 Yanget al.\u2014FA [217] 92.3%/82 89.4%/3 Yanget al.\u2014LDA [217] 93.6%/74 91.5%/1 Rothet al. [157] 94.8%/78 94.1%/3 Rowleyet al. [158] 86.2%/23 84.5%/8 Feraudet al. [42] 86%/8 Colmenarez & Huang [22] 93.9%/8122 Sung & Poggio [182] 79.9%/5 Lew & Huijsmans [107] 94.1%/64 Osunaet al. [140] 74.2%/20 Lin et al. [113] 72.3%/6 Gu and Li [54] 87.1%/0\na Eigenvector coefficients. b Wavelet coefficients.\nis the number used in the reported results in [158], [140], and [113]) when included in the CMU dataset."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 27
                            }
                        ],
                        "text": "A. J. Colmenarez and T. S. Huang, Pattern detection with information-based maximum discrimination and error bootstrapping, inProc. of International Conference on Pattern Recognition, 1998."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 18
                            }
                        ],
                        "text": "G. Yang and T. S. Huang, Human face detection in a complex background,Pattern Recog.27, 1994, 53\u201363."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 15
                            }
                        ],
                        "text": "R. Qian and T. Huang, Object detection using hierarchical mrf and map estimation, inIEEE Proc. of Int."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "Huanget al.[74] also apply a Gaussian filter for pre-processing in a framework based on image feature analysis."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 32
                            }
                        ],
                        "text": "J. P. Phillips, H. Wechsler, J. Huang, and P. Rauss, The FERET database and evaluation procedure for face-recognition algorithms,Image Vision Comput."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 9
                            }
                        ],
                        "text": "In [23], Colmenarez and Huang further improved on this technique by including error bootstrapping (described in Section 4.2), and in [20] the technique was incorporated in a real-time face tracking system."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 27
                            }
                        ],
                        "text": "A. J. Colmenarez and T. S. Huang, Maximum likelihood face detection, inIEEE Proc. of 2nd Int."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 3
                            }
                        ],
                        "text": "J. Huang, S. Gutta, and H. Wechsler, Detection of human faces using decision trees, inIEEE Proc. of 2nd Int."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 37
                            }
                        ],
                        "text": "A. J. Colmenarez, B. Frey, and T. S. Huang, Detection and tracking of faces and facial features, inProceedings International Conference on Image Processing, 1999."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 9
                            }
                        ],
                        "text": "Yang and Huang [214], on the other hand, explore the gray-scale behavior of faces in mosaic (pyramid) images."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 3
                            }
                        ],
                        "text": "J. Huang and H. Wechsler, Eye detection using optimal wavelet packets and radial basis functions (rbfs),Int."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 71
                            }
                        ],
                        "text": "Due to the high computational requirement in the minimization process, Huang and Chen [69] and Lam and Yan [101] both employ fast iteration methods (greedy algorithms) for faster convergence."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 3
                            }
                        ],
                        "text": "J. Huang and H. Wechsler, Visual routines for eye location using learning and evolution, inIEEE Transactions on Evolutionary Computation, 1999."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "Colmenarez and Huang use a large set of 11\u00d7 11 images of faces and nonfaces for training, and the training procedure results in a set of look-up tables with likelihood ratios."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 3
                            }
                        ],
                        "text": "W. Huang, Q. Sun, C. P. Lam, and J. K. Wu, A robust approach to face and eyes detection from images with cluttered background, inProc. of International Conference on Pattern Recognition, 1998."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 6
                            }
                        ],
                        "text": "C. L. Huang and C. W. Chen, Human facial feature extraction for face interpretation and recognition,Pat er Recog.25, 1992, 1435\u20131444."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 3
                            }
                        ],
                        "text": "W. Huang and R. Mariani, Face detection and precise eyes location, inProceedings of the 15th International Conference on Pattern Recognition, 2000, Vol. IV, p. 3B.\n74."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 17
                            }
                        ],
                        "text": "Q. B. Sun, W. M. Huang, and J. K. Wu, Face detection based on color and local symmetry information, in IEEE Proc. of 3rd Int."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 73
                            }
                        ],
                        "text": "Bayes\u2019 decision rule has also been applied for face detection by Qian and Huang [148]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 27
                            }
                        ],
                        "text": "A. J. Colmenarez and T. S. Huang, Face detection with information-based maximum discrimination, inIEEE Proc. of Int."
                    },
                    "intents": []
                }
            ],
            "corpusId": 9192390,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e744c3eef4fbc4ac52b2458eb2d545a4432bcb86",
            "isKey": true,
            "numCitedBy": 196,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present a visual learning technique that maximizes the discrimination between positive and negative examples in a training set. We demonstrate our technique in the context of face detection with complex background without color or motion information, which has proven to be a challenging problem. We use a family of discrete Markov processes to model the face and background patterns and estimate the probability models using the data statistics. Then, we convert the learning process into an optimization, selecting the Markov process that optimizes the information-based discrimination between the two classes. The detection process is carried out by computing the likelihood ratio using the probability model obtained from the learning procedure. We show that because of the discrete nature of these models, the detection process is at least two orders of magnitude less computationally expensive than neural network approaches. However, no improvement in terms of correct-answer/false-alarm tradeoff is achieved."
            },
            "slug": "Face-detection-with-information-based-maximum-Colmenarez-Huang",
            "title": {
                "fragments": [],
                "text": "Face detection with information-based maximum discrimination"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "A visual learning technique that maximizes the discrimination between positive and negative examples in a training set by using a family of discrete Markov processes to model the face and background patterns and estimate the probability models using the data statistics."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9215658"
                        ],
                        "name": "R. Chellappa",
                        "slug": "R.-Chellappa",
                        "structuredName": {
                            "firstName": "Rama",
                            "lastName": "Chellappa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Chellappa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144286385"
                        ],
                        "name": "Charles L. Wilson",
                        "slug": "Charles-L.-Wilson",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Wilson",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charles L. Wilson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2503366"
                        ],
                        "name": "S. Sirohey",
                        "slug": "S.-Sirohey",
                        "structuredName": {
                            "firstName": "Saad",
                            "lastName": "Sirohey",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sirohey"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 106
                            }
                        ],
                        "text": "Many of the current face recognition techniques assume the availability of frontal faces of similar sizes [14, 163]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 153
                            }
                        ],
                        "text": "For face recognition, this implies that neural approaches might be applied for all parts of the system, and this had indeed been shown in several papers [14, 113, 163]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[14] have conducted a detailed survey on face recognition research."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 89
                            }
                        ],
                        "text": "Despite these problems the growth of research interest remained stagnant until the 1990s [14], when practical face recognition and video coding systems started to become a reality."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62185766,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aa1755e87301af36485ca01e3454bf8888dde8d1",
            "isKey": true,
            "numCitedBy": 3007,
            "numCiting": 177,
            "paperAbstract": {
                "fragments": [],
                "text": "The goal of this paper is to present a critical survey of existing literature on human and machine recognition of faces. Machine recognition of faces has several applications, ranging from static matching of controlled photographs as in mug shots matching and credit card verification to surveillance video images. Such applications have different constraints in terms of complexity of processing requirements and thus present a wide range of different technical challenges. Over the last 20 years researchers in psychophysics, neural sciences and engineering, image processing analysis and computer vision have investigated a number of issues related to face recognition by humans and machines. Ongoing research activities have been given a renewed emphasis over the last five years. Existing techniques and systems have been tested on different sets of images of varying complexities. But very little synergism exists between studies in psychophysics and the engineering literature. Most importantly, there exists no evaluation or benchmarking studies using large databases with the image quality that arises in commercial and law enforcement applications In this paper, we first present different applications of face recognition in commercial and law enforcement sectors. This is followed by a brief overview of the literature on face recognition in the psychophysics community. We then present a detailed overview of move than 20 years of research done in the engineering community. Techniques for segmentation/location of the face, feature extraction and recognition are reviewed. Global transform and feature based methods using statistical, structural and neural classifiers are summarized. >"
            },
            "slug": "Human-and-machine-recognition-of-faces:-a-survey-Chellappa-Wilson",
            "title": {
                "fragments": [],
                "text": "Human and machine recognition of faces: a survey"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "A critical survey of existing literature on human and machine recognition of faces is presented, followed by a brief overview of the literature on face recognition in the psychophysics community and a detailed overview of move than 20 years of research done in the engineering community."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109171705"
                        ],
                        "name": "Zhu Liu",
                        "slug": "Zhu-Liu",
                        "structuredName": {
                            "firstName": "Zhu",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhu Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37015240"
                        ],
                        "name": "Yao Wang",
                        "slug": "Yao-Wang",
                        "structuredName": {
                            "firstName": "Yao",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yao Wang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 62
                            }
                        ],
                        "text": "The simplest image-based approaches rely on template matching [62, 114], but these approaches do not perform as well as the more complex techniques presented in the following sections."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16274304,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b857abede45ac99dfcbdba6149e012df30f4c50a",
            "isKey": false,
            "numCitedBy": 55,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Face detection and tracking are important in video content analysis since the most important objects in most video are human beings. This paper proposes a new approach for combined face detection and tracking in video. The face detection algorithm is a fast template matching procedure using iterative dynamic programming (DP). Although the face detection algorithm is designed for frontal face, the same mechanism can also be applied to track non-frontal faces with online adapted face models. Due to the essence of template matching, the algorithm is capable of comparing the similarity among different faces, which makes it suitable for tracking the same face that occur at disjointed temporal locations in video. While the proposed face detection method provides comparable accuracy as the neural network based approach, it is much faster."
            },
            "slug": "Face-detection-and-tracking-in-video-using-dynamic-Liu-Wang",
            "title": {
                "fragments": [],
                "text": "Face detection and tracking in video using dynamic programming"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A new approach for combined face detection and tracking in video that provides comparable accuracy as the neural network based approach, but is much faster."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 2000 International Conference on Image Processing (Cat. No.00CH37101)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700567"
                        ],
                        "name": "S. Satoh",
                        "slug": "S.-Satoh",
                        "structuredName": {
                            "firstName": "Shin\u2019ichi",
                            "lastName": "Satoh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Satoh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 67
                            }
                        ],
                        "text": "This algorithm was applied in a person tracking system in [30] and [166] and for initial face detection in the head tracking system of La Cascia et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7400616,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fedbf93032ac627f8658eecf8ff26a3ca9663717",
            "isKey": false,
            "numCitedBy": 88,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper presents a comparative evaluation of matching methods of face sequences obtained from actual videos. Face information is quite important in videos, especially in news programs, dramas, and movies. Accurate face sequence matching enables many multimedia applications including content-based face retrieval, automated face annotation, video authoring, etc. However, face sequences in videos are subject to variation in lighting condition, pose, facial expression, etc., which cause difficulty in face matching. In order to cope with this problem, several face sequence matching methods are proposed by extending face still image matching, traditional pattern recognition, and recent pattern recognition techniques. They are expected to be applicable to face sequences extracted from actual videos. The performance of these methods is evaluated as the accuracy of face sequence annotation using the methods. The accuracy is evaluated using a considerable amount of actual drama videos. The evaluation results reveal merits and demerits of these methods, and indicate future research directions of face matching for videos."
            },
            "slug": "Comparative-evaluation-of-face-sequence-matching-Satoh",
            "title": {
                "fragments": [],
                "text": "Comparative evaluation of face sequence matching for content-based video access"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "A comparative evaluation of matching methods of face sequences obtained from actual videos by extending face still image matching, traditional pattern recognition, and recent pattern recognition techniques to reveal merits and demerits, and indicate future research directions of face matching for videos."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709633"
                        ],
                        "name": "N. Tsapatsoulis",
                        "slug": "N.-Tsapatsoulis",
                        "structuredName": {
                            "firstName": "Nicolas",
                            "lastName": "Tsapatsoulis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Tsapatsoulis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744904"
                        ],
                        "name": "Yannis Avrithis",
                        "slug": "Yannis-Avrithis",
                        "structuredName": {
                            "firstName": "Yannis",
                            "lastName": "Avrithis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yannis Avrithis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707243"
                        ],
                        "name": "S. Kollias",
                        "slug": "S.-Kollias",
                        "structuredName": {
                            "firstName": "Stefanos",
                            "lastName": "Kollias",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kollias"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 93
                            }
                        ],
                        "text": "Other color models applied to face detection include HSV [48, 60, 81, 216], YES [160], YCrCb [2, 48, 84, 130, 191, 200], YUV [1, 123], CIE-xyz [15], L\u2217 a\u2217 b\u2217 [13, 109], L\u2217 u\u2217 v \u2217 [63], CSN (a modified rg representation) [90, 91] and UCS/Farnsworth (a perceptually uniform color system was proposed by Farnsworth [210]) [208]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 33144359,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7ec5fd6862a3d73ac7c03d227f1d6b0b64e1c933",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Face detection is becoming an important tool in the framework of many multimedia applications. Several face detection algorithms based on skin color characteristics have appeared in the literature. Most of them have generalization problems due to the skin color model they use. We present a study which attempts to minimize the generalization problem by combining the M-RSST color segmentation algorithm with a Gaussian model of the skin color distribution and global shape features. Moreover by associating the resultant segments with a face probability we can index and retrieve facial images from multimedia databases."
            },
            "slug": "Efficient-face-detection-for-multimedia-Tsapatsoulis-Avrithis",
            "title": {
                "fragments": [],
                "text": "Efficient face detection for multimedia applications"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A study which attempts to minimize the generalization problem by combining the M-RSST color segmentation algorithm with a Gaussian model of the skin color distribution and global shape features and can index and retrieve facial images from multimedia databases."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 2000 International Conference on Image Processing (Cat. No.00CH37101)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112233295"
                        ],
                        "name": "Vinay P. Kumar",
                        "slug": "Vinay-P.-Kumar",
                        "structuredName": {
                            "firstName": "Vinay",
                            "lastName": "Kumar",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vinay P. Kumar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 80
                            }
                        ],
                        "text": "The proposed system follows the same framework as the one developed by Sung and Poggio [182], described in Section 4.1 (scanning input images with a 19\u00d7 window)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 59
                            }
                        ],
                        "text": "The number of labeled faces was originally 149 by Sung and Poggio (which is the number used in the reported results in [182] and [107], but it was changed to 155 (which\nTABLE 2 Results Reported in Terms of Percentage Correct Detection (CD) and Number of False Positives (FP), CD/FP, on the CMU and MIT Datasets\nFace detection system CMU-130 CMU-125 MIT-23 MIT-20\nSchneiderman & Kanade\u2014Ea [170] 94.4%/65 Schneiderman & Kanade\u2014Wb [170] 90.2%/110 Yanget al.\u2014FA [217] 92.3%/82 89.4%/3 Yanget al.\u2014LDA [217] 93.6%/74 91.5%/1 Rothet al. [157] 94.8%/78 94.1%/3 Rowleyet al. [158] 86.2%/23 84.5%/8 Feraudet al. [42] 86%/8 Colmenarez & Huang [22] 93.9%/8122 Sung & Poggio [182] 79.9%/5 Lew & Huijsmans [107] 94.1%/64 Osunaet al. [140] 74.2%/20 Lin et al. [113] 72.3%/6 Gu and Li [54] 87.1%/0\na Eigenvector coefficients. b Wavelet coefficients.\nis the number used in the reported results in [158], [140], and [113]) when included in the CMU dataset."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 19
                            }
                        ],
                        "text": "R. Brunelli and T. Poggio, Face recognition: Feature versus templates,IEEE Trans."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 47
                            }
                        ],
                        "text": "Gu and Li [54] present a variation of Sung and Poggio\u2019s approach where linear discriminant analysis is applied for feature selection before training the neural network classifier."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 0
                            }
                        ],
                        "text": "Kumar and Poggio [96] recently incorporated Osunaet l.\u2019s SVM algorithm in a system for real-time tracking and analysis of faces."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 85
                            }
                        ],
                        "text": "Feraudet al.employ a training algorithm based on the bootstrap algorithm of Sung and Poggio (and also a similar preprocessing method consisting of histogram equalization and smoothing)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 16
                            }
                        ],
                        "text": "V. Kumar and T. Poggio, Learning-based approach to real time tracking and analysis of faces, in Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition, 2000."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 45
                            }
                        ],
                        "text": "A method following the framework of Sung and Poggio has also been proposed by Rajagopalanet al.[150] using higher order statistics to model the face and nonface clusters."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 134
                            }
                        ],
                        "text": "They also present a new clustering algorithm using higher order\nstatistics which replaces ak-means clustering algorithm from Sung and Poggio\u2019s work."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 86
                            }
                        ],
                        "text": "The datasets have been collected at CMU by Rowley et al. [158] and at MIT by Sung and Poggio [182]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 53
                            }
                        ],
                        "text": "This pre-processing method was adopted from Sung and Poggio\u2019s system mentioned earlier and is shown in Fig."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 64
                            }
                        ],
                        "text": "This technique was first applied for face detection by Sung and Poggio [182]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 9
                            }
                        ],
                        "text": "Sung and Poggio [182] and Rowleyet al. [158] set the standards for research on this topic, and the performances of their algorithms are still comparable to more recent image-based approaches."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 39
                            }
                        ],
                        "text": "A similar approach to that of Sung and Poggio based on gray-level features in combination with texture features has been explored in [38] and a similar more computationally efficient method has been proposed by Fouadet al.[44]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 60
                            }
                        ],
                        "text": "Also, the proposed bootstrap training algorithm by Sung and Poggio is implemented in some of the systems."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 93
                            }
                        ],
                        "text": "Similar to the previously mentioned methods, Rothet al. use the bootstrap method of Sung and Poggio for generating training samples and preprocess all images with histogram equalization."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 17
                            }
                        ],
                        "text": "Kumar and Poggio [96] recently incorporated Osuna et l."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 18
                            }
                        ],
                        "text": "K.-K. Sung and T. Poggio, Example-based learning for view-based human face detection,IEEE Trans."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 9
                            }
                        ],
                        "text": "Sung and Poggio suggested a training algorithm, known as \u201cboot-strap training,\u201d to partially deal with this problem (a more precise strategy than the one proposed in [10])."
                    },
                    "intents": []
                }
            ],
            "corpusId": 8673716,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "06ba4c28cd9caa5d17c720a49e558d58555ca059",
            "isKey": true,
            "numCitedBy": 60,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a trainable system capable of tracking faces and facial features like eyes and nostrils and estimating basic mouth features such as degrees of openness and smile in real time. In developing this system, we have addressed the twin issues of image representation and algorithms for learning. We have used the invariance properties of image representations based on Haar wavelets to robustly capture various facial features. Similarly, unlike previous approaches this system is entirely trained using examples and does not rely on a priori (hand-crafted) models of facial features based on an optical flow or facial musculature. The system works in several stages that begin with face detection, followed by localization of facial features and estimation of mouth parameters. Each of these stages is formulated as a problem in supervised learning from examples. We apply the new and robust technique of support vector machines (SVM) for classification in the stage of skin segmentation, face detection and eye detection. Estimation of mouth parameters is modeled as a regression from a sparse subset of coefficients (basis functions) of an overcomplete dictionary of Haar wavelets."
            },
            "slug": "Learning-based-approach-to-real-time-tracking-and-Kumar-Poggio",
            "title": {
                "fragments": [],
                "text": "Learning-based approach to real time tracking and analysis of faces"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "This paper describes a trainable system capable of tracking faces and facial features like eyes and nostrils and estimating basic mouth features such as degrees of openness and smile in real time and uses the new and robust technique of support vector machines for classification in the stage of skin segmentation, face detection and eye detection."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680753"
                        ],
                        "name": "Bernhard Fr\u00f6ba",
                        "slug": "Bernhard-Fr\u00f6ba",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Fr\u00f6ba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bernhard Fr\u00f6ba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710154"
                        ],
                        "name": "Christian K\u00fcblbeck",
                        "slug": "Christian-K\u00fcblbeck",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "K\u00fcblbeck",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian K\u00fcblbeck"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 187,
                                "start": 178
                            }
                        ],
                        "text": "In the system of Maio and Maltoni [120], the input images are converted to a directional image using a gradient-type operator over local windows (7\u00d7 7 pixels) (see also Fr\u00a8oba and Ku\u0308blebeck [47])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 193
                            }
                        ],
                        "text": "In the system of Maio and Maltoni [120], the input images are converted to a directional image using a gradient-type operator over local windows (7 \u00d7 7 pixels) (see also Fr \u0308 oba and K\u00fcblebeck [47])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18736707,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d9d72f31a8b4b450e4db2961e303660aa89c0022",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "An important topic in face recognition as well as in video coding or multi-modal human machine interfaces is the automatic localization of faces or head-and-shoulder regions in visual scenes. The algorithms therefore should be computationally efficient and robust against distortions like varying lighting conditions. This paper describes a novel method for segmenting frontal head and shoulder views of persons from grey level images. The segmentation is done by oriented template correlation. This matching method only depends on edge information, especially the orientation of the edges. We describe the generation of a statistical model of edge orientation within a human face from a training sample. In the matching stage we calculate the probability for a face at the current image position using this model. The detection capabilities of the presented algorithm are evaluated on a large database of 1114 images each containing one or more faces."
            },
            "slug": "Orientation-Template-Matching-for-Face-Localization-Fr\u00f6ba-K\u00fcblbeck",
            "title": {
                "fragments": [],
                "text": "Orientation Template Matching for Face Localization in Complex Visual Scenes"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A novel method for segmenting frontal head and shoulder views of persons from grey level images by oriented template correlation, which only depends on edge information, especially the orientation of the edges."
            },
            "venue": {
                "fragments": [],
                "text": "ICIP"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794240"
                        ],
                        "name": "Haiyuan Wu",
                        "slug": "Haiyuan-Wu",
                        "structuredName": {
                            "firstName": "Haiyuan",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Haiyuan Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055712370"
                        ],
                        "name": "T. Yokoyama",
                        "slug": "T.-Yokoyama",
                        "structuredName": {
                            "firstName": "Taro",
                            "lastName": "Yokoyama",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Yokoyama"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2127304"
                        ],
                        "name": "D. Pramadihanto",
                        "slug": "D.-Pramadihanto",
                        "structuredName": {
                            "firstName": "Dadet",
                            "lastName": "Pramadihanto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Pramadihanto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735941"
                        ],
                        "name": "M. Yachida",
                        "slug": "M.-Yachida",
                        "structuredName": {
                            "firstName": "Masahiko",
                            "lastName": "Yachida",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Yachida"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 41
                            }
                        ],
                        "text": "information, the external energy term in [209, 219] includes a skin color function which attracts the contour to the face region."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 15
                            }
                        ],
                        "text": "Elastic energy [55, 69, 209, 219] is used commonly as internal energy."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 72
                            }
                        ],
                        "text": "Active contours, or snakes, are commonly used to locate a head boundary [55, 69, 102, 137, 209, 219]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 46083950,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "20bc70459a93f229cc89a1252e6220261d70ec44",
            "isKey": false,
            "numCitedBy": 65,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents an automatic processing of human face from color images. The system works hierarchically from detecting the position of human face and its features (such as eyes, nose, mouth, etc.) to contours and feature points extraction. The position of human face and its parts are detected from the image by applying the integral projection method, which synthesize the color information (skin and hair color) and the edge information (intensity and sign). In order to extract the contour-line of face features we used a multiple active contour model with color information based energy terms. Facial feature points are decided based on the optimized contours. The proposed system is confirmed to be very effective and robust to deal with the image of faces in the complex background."
            },
            "slug": "Face-and-facial-feature-extraction-from-color-image-Wu-Yokoyama",
            "title": {
                "fragments": [],
                "text": "Face and facial feature extraction from color image"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "The proposed system works hierarchically from detecting the position of human face and its features to contours and feature points extraction, and is confirmed to be very effective and robust to deal with the image of faces in the complex background."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Second International Conference on Automatic Face and Gesture Recognition"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725688"
                        ],
                        "name": "M. Tistarelli",
                        "slug": "M.-Tistarelli",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Tistarelli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Tistarelli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47864942"
                        ],
                        "name": "E. Grosso",
                        "slug": "E.-Grosso",
                        "structuredName": {
                            "firstName": "Enrico",
                            "lastName": "Grosso",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Grosso"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18857469,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eb724fed2a6457a2ee41a205892004116180bbfd",
            "isKey": false,
            "numCitedBy": 54,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Active-vision-based-face-authentication-Tistarelli-Grosso",
            "title": {
                "fragments": [],
                "text": "Active vision-based face authentication"
            },
            "venue": {
                "fragments": [],
                "text": "Image Vis. Comput."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37495435"
                        ],
                        "name": "Qibin Sun",
                        "slug": "Qibin-Sun",
                        "structuredName": {
                            "firstName": "Qibin",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qibin Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2145032549"
                        ],
                        "name": "Weimin Huang",
                        "slug": "Weimin-Huang",
                        "structuredName": {
                            "firstName": "Weimin",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Weimin Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109178668"
                        ],
                        "name": "Jian-Kang Wu",
                        "slug": "Jian-Kang-Wu",
                        "structuredName": {
                            "firstName": "Jian-Kang",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian-Kang Wu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 200,
                                "start": 145
                            }
                        ],
                        "text": "Since the main variation in skin appearance is largely due to luminance change (brightness) [215], normalized RGB colors are generally preferred [27, 53, 75, 88, 92, 165, 181, 196, 202, 207, 213, 215], so that the effect of luminance can be filtered out."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5905541,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b772b99e410333e5edbbe259ad53324671a519c0",
            "isKey": false,
            "numCitedBy": 70,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "As we know, a robust approach to face and facial features detection must be able to handle the variation issues such as changes in imaging conditions, face appearances and image contents. Here we present a method which utilizes color, local symmetry and geometry information of human face based on various models. The algorithm first detects most likely face regions or ROIs (Region-Of-Interest) from the image using face color model and face outline model, produces a face color similarity map. Then it performs local symmetry detection within these ROIs to obtain a local symmetry similarity map. These two maps are fused to obtain potential facial feature points. Finally similarity matching is performed to identify faces between the fusion map and face geometry model under affine transformation. The output results are the detected faces with confidence values. Experimental results have demonstrated its validity and robustness to identify faces under certain variations."
            },
            "slug": "Face-detection-based-on-color-and-local-symmetry-Sun-Huang",
            "title": {
                "fragments": [],
                "text": "Face detection based on color and local symmetry information"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A method which utilizes color, local symmetry and geometry information of human face based on various models to identify faces under certain variations and its validity and robustness have been demonstrated."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Third IEEE International Conference on Automatic Face and Gesture Recognition"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "83345623"
                        ],
                        "name": "Ra\u00fal Pinto El\u00edas",
                        "slug": "Ra\u00fal-Pinto-El\u00edas",
                        "structuredName": {
                            "firstName": "Ra\u00fal",
                            "lastName": "Pinto El\u00edas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ra\u00fal Pinto El\u00edas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3311860"
                        ],
                        "name": "Juan Humberto Sossa Azuela",
                        "slug": "Juan-Humberto-Sossa-Azuela",
                        "structuredName": {
                            "firstName": "Juan",
                            "lastName": "Azuela",
                            "middleNames": [
                                "Humberto",
                                "Sossa"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Juan Humberto Sossa Azuela"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 1436972,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "993f0223d6acf15a301e6d3b758139a0d808a871",
            "isKey": false,
            "numCitedBy": 10,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "A method to automatically detect and locate human face features (eyes and mouth) in a 2D gray level image is presented. The method uses a genetic algorithm (GA) and an invariant description of the facial features to accomplish the task. The descriptors used are the well known first four translation, rotation, and scale moment invariants proposed by Hu (1962). In a first step, an image possibly containing a face or a set of faces is first divided into small cells of fixed size. For each cell, the ordinary moments are next computed. From these quantities, the corresponding Hu's invariants are then derived. Human face feature detection and location is thus accomplished by grouping individual cells using a genetic algorithm by fitting a specific cost function. The cost function corresponds to the invariant description of a specified face feature (eye or mouth) given in terms of the corresponding gray level values."
            },
            "slug": "Automatic-facial-feature-detection-and-location-El\u00edas-Azuela",
            "title": {
                "fragments": [],
                "text": "Automatic facial feature detection and location"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "A method to automatically detect and locate human face features (eyes and mouth) in a 2D gray level image is presented and uses the first four translation, rotation, and scale moment invariants proposed by Hu (1962)."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. Fourteenth International Conference on Pattern Recognition (Cat. No.98EX170)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2037776"
                        ],
                        "name": "D. Reisfeld",
                        "slug": "D.-Reisfeld",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Reisfeld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Reisfeld"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47620675"
                        ],
                        "name": "Y. Yeshurun",
                        "slug": "Y.-Yeshurun",
                        "structuredName": {
                            "firstName": "Yehezkel",
                            "lastName": "Yeshurun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Yeshurun"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 47
                            }
                        ],
                        "text": "A more recent work by Reisfeld and Yeshurun is [155]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 32265260,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "59990778d351b314ce87fd54675214fa73ee3b34",
            "isKey": false,
            "numCitedBy": 76,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "The reliability, speed, and complexity of virtually any face recognition system are substantially improved if the location and the scale of the faces are known. We propose a method for automatic and robust detection of the eyes and mouth using the context freegeneralized symmetry transformand knowledge of faces. The features are extracted from the image of the intensities gradients and are then used to normalize the face images. We show that a normalization procedure based on affine transformations whose anchor points are the locations of the eyes and mouth substantially increases the effectiveness of general purpose classification techniques in face recognition.Other normalization procedures for avoiding the effect of background and varying light conditions are proved to be instrumental as well."
            },
            "slug": "Preprocessing-of-Face-Images:-Detection-of-Features-Reisfeld-Yeshurun",
            "title": {
                "fragments": [],
                "text": "Preprocessing of Face Images: Detection of Features and Pose Normalization"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is shown that a normalization procedure based on affine transformations whose anchor points are the locations of the eyes and mouth substantially increases the effectiveness of general purpose classification techniques in face recognition."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Vis. Image Underst."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3430625"
                        ],
                        "name": "R. F\u00e9raud",
                        "slug": "R.-F\u00e9raud",
                        "structuredName": {
                            "firstName": "Rapha\u00ebl",
                            "lastName": "F\u00e9raud",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. F\u00e9raud"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3118328"
                        ],
                        "name": "O. Bernier",
                        "slug": "O.-Bernier",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Bernier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Bernier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35094984"
                        ],
                        "name": "J. Viallet",
                        "slug": "J.-Viallet",
                        "structuredName": {
                            "firstName": "Jean-Emmanuel",
                            "lastName": "Viallet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Viallet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2586567"
                        ],
                        "name": "M. Collobert",
                        "slug": "M.-Collobert",
                        "structuredName": {
                            "firstName": "Michel",
                            "lastName": "Collobert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collobert"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[42] 86%/8 Colmenarez & Huang [22] 93."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "in [42] to include color information and multiple views and applied to the problem of finding face images on the Web."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14916909,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ecbddc709e20fcec77fb7252941e0dacff6a93d5",
            "isKey": false,
            "numCitedBy": 74,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Detecting faces in images with complex backgrounds is a difficult task. Our approach, which obtains state-of-the-art results, is based on a generative neural network model: the constrained generative model (CGM). To detect side-view faces and to decrease the number of false alarms, a conditional mixture of networks is used. To decrease the computational time cost, a fast search algorithm is proposed. The level of performance reached, in terms of detection accuracy and processing time, allows us to apply this detector to a real-world application: the indexation of face images on the WWW."
            },
            "slug": "A-fast-and-accurate-face-detector-for-indexation-of-F\u00e9raud-Bernier",
            "title": {
                "fragments": [],
                "text": "A fast and accurate face detector for indexation of face images"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The level of performance reached in this approach, in terms of detection accuracy and processing time, allows this detector to apply to a real-world application: the indexation of face images on the WWW."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39682833"
                        ],
                        "name": "H. Rowley",
                        "slug": "H.-Rowley",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Rowley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Rowley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1767244"
                        ],
                        "name": "S. Baluja",
                        "slug": "S.-Baluja",
                        "structuredName": {
                            "firstName": "Shumeet",
                            "lastName": "Baluja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Baluja"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733113"
                        ],
                        "name": "T. Kanade",
                        "slug": "T.-Kanade",
                        "structuredName": {
                            "firstName": "Takeo",
                            "lastName": "Kanade",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanade"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 22
                            }
                        ],
                        "text": "Recently, Rowleyet al.[159] combined this system with a router neural network to detect faces at all angles in the image plane."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1619589,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2fd1c99edbb3d22cec4adc9ba9319cfc2360e903",
            "isKey": false,
            "numCitedBy": 348,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present a neural network-based face detection system. Unlike similar systems which are limited to detecting upright, frontal faces, this system detects faces at any degree of rotation in the image plane. The system employs multiple networks; a \"router\" network first processes each input window to determine its orientation and then uses this information to prepare the window for one or more \"detector\" networks. We present the training methods for both types of networks. We also perform sensitivity analysis on the networks, and present empirical results on a large test set. Finally, we present preliminary results for detecting faces rotated out of the image plane, such as profiles and semi-profiles."
            },
            "slug": "Rotation-invariant-neural-network-based-face-Rowley-Baluja",
            "title": {
                "fragments": [],
                "text": "Rotation invariant neural network-based face detection"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "This paper presents a neural network-based face detection system, which is limited to detecting upright, frontal faces, and presents preliminary results for detecting faces rotated out of the image plane, such as profiles and semi-profiles."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 1998 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.98CB36231)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731948"
                        ],
                        "name": "Paul A. Viola",
                        "slug": "Paul-A.-Viola",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Viola",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paul A. Viola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145319478"
                        ],
                        "name": "Michael J. Jones",
                        "slug": "Michael-J.-Jones",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jones",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Jones"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2715202,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dc6ea0e30e46163b706f2f8bdc9c67ca87f83d63",
            "isKey": false,
            "numCitedBy": 17887,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a machine learning approach for visual object detection which is capable of processing images extremely rapidly and achieving high detection rates. This work is distinguished by three key contributions. The first is the introduction of a new image representation called the \"integral image\" which allows the features used by our detector to be computed very quickly. The second is a learning algorithm, based on AdaBoost, which selects a small number of critical visual features from a larger set and yields extremely efficient classifiers. The third contribution is a method for combining increasingly more complex classifiers in a \"cascade\" which allows background regions of the image to be quickly discarded while spending more computation on promising object-like regions. The cascade can be viewed as an object specific focus-of-attention mechanism which unlike previous approaches provides statistical guarantees that discarded regions are unlikely to contain the object of interest. In the domain of face detection the system yields detection rates comparable to the best previous systems. Used in real-time applications, the detector runs at 15 frames per second without resorting to image differencing or skin color detection."
            },
            "slug": "Rapid-object-detection-using-a-boosted-cascade-of-Viola-Jones",
            "title": {
                "fragments": [],
                "text": "Rapid object detection using a boosted cascade of simple features"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A machine learning approach for visual object detection which is capable of processing images extremely rapidly and achieving high detection rates and the introduction of a new image representation called the \"integral image\" which allows the features used by the detector to be computed very quickly."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144847940"
                        ],
                        "name": "K. Lam",
                        "slug": "K.-Lam",
                        "structuredName": {
                            "firstName": "Kin-Man",
                            "lastName": "Lam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Lam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152996923"
                        ],
                        "name": "Hong Yan",
                        "slug": "Hong-Yan",
                        "structuredName": {
                            "firstName": "Hong",
                            "lastName": "Yan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hong Yan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 17
                            }
                        ],
                        "text": "K. M. Lam and H. Yan, Facial feature location and extraction for computerised human face recognition, in Int."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 17
                            }
                        ],
                        "text": "The technique of Yang and Huang was recently incorporated into a system for rotation invariant face detection by Lvet al. [119] and an extension of the algorithm is presented in Kotropoulos and Pitas [95]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "Yanget al. [217] make this decision based on the rule \u201c . . .a detected face is a successful detect if the subimage contains eyes and mouth.\u201d"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 3
                            }
                        ],
                        "text": "L. Yang, Multiple-face tracking system for general region-of-interest video coding, inProceedings of the 2000 International Conference on Image Processing, 2000, p. MA09."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 445,
                                "start": 442
                            }
                        ],
                        "text": "The number of labeled faces was originally 149 by Sung and Poggio (which is the number used in the reported results in [182] and [107], but it was changed to 155 (which\nTABLE 2 Results Reported in Terms of Percentage Correct Detection (CD) and Number of False Positives (FP), CD/FP, on the CMU and MIT Datasets\nFace detection system CMU-130 CMU-125 MIT-23 MIT-20\nSchneiderman & Kanade\u2014Ea [170] 94.4%/65 Schneiderman & Kanade\u2014Wb [170] 90.2%/110 Yanget al.\u2014FA [217] 92.3%/82 89.4%/3 Yanget al.\u2014LDA [217] 93.6%/74 91.5%/1 Rothet al. [157] 94.8%/78 94.1%/3 Rowleyet al. [158] 86.2%/23 84.5%/8 Feraudet al. [42] 86%/8 Colmenarez & Huang [22] 93.9%/8122 Sung & Poggio [182] 79.9%/5 Lew & Huijsmans [107] 94.1%/64 Osunaet al. [140] 74.2%/20 Lin et al. [113] 72.3%/6 Gu and Li [54] 87.1%/0\na Eigenvector coefficients. b Wavelet coefficients.\nis the number used in the reported results in [158], [140], and [113]) when included in the CMU dataset."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 3
                            }
                        ],
                        "text": "G. Yang and T. S. Huang, Human face detection in a complex background,Pattern Recog.27, 1994, 53\u201363."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 36
                            }
                        ],
                        "text": "For instance, Oliveret al.[139] and Yang and Waibel [215] employ a Gaussian distribution to represent a skin color cluster of thousands of skin color samples taken from difference races."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 17
                            }
                        ],
                        "text": "K. M. Lam and H. Yan, Fast greedy algorithm for locating head boundaries,El ctron."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 73
                            }
                        ],
                        "text": "Examples of such a learning approach have been used by Oliver et al. and Yang and Waibel according to Eq."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 3
                            }
                        ],
                        "text": "J. Yang and A. Waibel, A real-time face tracker, inIEEE Proc. of the 3rd Workshop on Applications of Computer Vision, Florida, 1996."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 22
                            }
                        ],
                        "text": "In the second method, Yanget al.use Kohonen\u2019s self-organizing map (SOM) [93] to divide the training images into 25 face and 25 nonface classes (the number of classes chosen based on the size of the training set)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "Yang and Huang [214], on the other hand, explore the gray-scale behavior of faces in mosaic (pyramid) images."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 17
                            }
                        ],
                        "text": "K. M. Lam and H. Yan, Locating and extracting the eye in human face images,Pattern Recog.29, 1996, 771\u2013779."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 16
                            }
                        ],
                        "text": "T. Kondo and H. Yan, Automatic human face detection and recognition under nonuniform illumination, Pattern Recog.32, 1999, 1707\u20131718."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 95
                            }
                        ],
                        "text": "Due to the high computational requirement in the minimization process, Huang and Chen [69] and Lam and Yan [101] both employ fast iteration methods (greedy algorithms) for faster convergence."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 15
                            }
                        ],
                        "text": "D. Roth, M.-H. Yang, and N. Ahuja, A SNoW-based face detector, inAdvances in Neural Information Processing Systems 12 (NIPS 12), MIT Press, Cambridge, MA, 2000."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 6
                            }
                        ],
                        "text": "M.-H. Yang, N. Ahuja, and D. Kriegman, Face detection using mixtures of linear subspaces, inProceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition, 2000."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 27
                            }
                        ],
                        "text": "Based on this observation, Yang proposed a hierarchical face detection framework."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 22
                            }
                        ],
                        "text": "A. R. Mirhosseini, H. Yan, K.-M. Lam, and T. Pham, Human face image recognition: An evidence aggregation approach,Computer Vision and Image Understanding71, 1998, doi:10.1006/cviu.1998.0710."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "Yanget al.use a mixture of factor analyzers in their first method."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 28
                            }
                        ],
                        "text": "(23)\nBoth of the methods of Yanget al. use the window scanning technique with a 20\u00d7 20 window and scan the input images for 10 iterations with a subsampling factor of 1.2."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 72
                            }
                        ],
                        "text": "Active contours, or snakes, are commonly used to locate a head boundary [55, 69, 102, 137, 209, 219]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 10
                            }
                        ],
                        "text": "J. Hu, H. Yan, and M. Sakalli, Locating head and face boundaries for headshoulder images,Pattern Recog."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 42
                            }
                        ],
                        "text": "In a more recent development, Lam and Yan [102] make use of eye corner information to estimate the initial parameters of the eye template."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "Yanget al. [217] proposed two methods for face detection which also seek to represent the manifold of human faces as a set of subclasses."
                    },
                    "intents": []
                }
            ],
            "corpusId": 33676247,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bd772ebad9f08c5c7910932e82b6cffa895ba068",
            "isKey": true,
            "numCitedBy": 364,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Locating-and-extracting-the-eye-in-human-face-Lam-Yan",
            "title": {
                "fragments": [],
                "text": "Locating and extracting the eye in human face images"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48926774"
                        ],
                        "name": "Ying Dai",
                        "slug": "Ying-Dai",
                        "structuredName": {
                            "firstName": "Ying",
                            "lastName": "Dai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ying Dai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737939"
                        ],
                        "name": "Y. Nakano",
                        "slug": "Y.-Nakano",
                        "structuredName": {
                            "firstName": "Yasuaki",
                            "lastName": "Nakano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Nakano"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 12479014,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a8e47dcdcd15180d9218fa80f107675a0d5cb74e",
            "isKey": false,
            "numCitedBy": 277,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Face-texture-model-based-on-SGLD-and-its-in-face-in-Dai-Nakano",
            "title": {
                "fragments": [],
                "text": "Face-texture model based on SGLD and its application in face detection in a color scene"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117826429"
                        ],
                        "name": "Ying Zhu",
                        "slug": "Ying-Zhu",
                        "structuredName": {
                            "firstName": "Ying",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ying Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732346"
                        ],
                        "name": "S. Schwartz",
                        "slug": "S.-Schwartz",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Schwartz",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Schwartz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9164341"
                        ],
                        "name": "M. Orchard",
                        "slug": "M.-Orchard",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Orchard",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Orchard"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 144
                            }
                        ],
                        "text": "The likelihood decision rule has also recently been applied in a different subspace face detection system based on wavelet packet decomposition [223]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 34874578,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "45cdd04d6a8f6b723d1f34e9d50b7211b719c37d",
            "isKey": false,
            "numCitedBy": 57,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Computation complexity is an important issue for current face detection systems. This paper proposes a subspace approach to capture local discriminative features in the space-frequency domain for fast face detection. Based on orthonormal wavelet packet analysis, we develop a discriminant subspace algorithm to search for the \"minimum cost\" subspace of the high-dimensional signal space, which leads to a set of wavelet features with maximum class discrimination and dimensionality reduction. Detailed (high frequency) information within local facial areas shows noticeable discrimination ability for face detection problem. We demonstrate the algorithm in the context of detecting frontal view faces in a complex background. Discrete pattern distribution functions and fast likelihood ratio detection are adopted by the system. Because of the reduced dimensionality, feature discrimination and the discrete stochastic model, our face detection system consumes much less computation while the performance is comparable with other reported leading systems."
            },
            "slug": "Fast-face-detection-using-subspace-discriminant-Zhu-Schwartz",
            "title": {
                "fragments": [],
                "text": "Fast face detection using subspace discriminant wavelet features"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A discriminant subspace algorithm is developed to search for the \"minimum cost\" subspace of the high-dimensional signal space, which leads to a set of wavelet features with maximum class discrimination and dimensionality reduction for fast face detection."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings IEEE Conference on Computer Vision and Pattern Recognition. CVPR 2000 (Cat. No.PR00662)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699756"
                        ],
                        "name": "K. Sobottka",
                        "slug": "K.-Sobottka",
                        "structuredName": {
                            "firstName": "Karin",
                            "lastName": "Sobottka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Sobottka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144064571"
                        ],
                        "name": "I. Pitas",
                        "slug": "I.-Pitas",
                        "structuredName": {
                            "firstName": "Ioannis",
                            "lastName": "Pitas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Pitas"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 143
                            }
                        ],
                        "text": "Color segmentation can basically be performed using appropriate skin color thresholds where skin color is modeled through histograms or charts [13, 63, 88, 118, 178, 207, 220]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 71
                            }
                        ],
                        "text": "Since the representation strongly relates to human perception of color [106, 178], it is also widely used in face segmentation schemes [51, 83, 118, 125, 175, 178, 187, 220]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5655985,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a54f1bb74352fa15435e4492073b7c9b8480c0be",
            "isKey": false,
            "numCitedBy": 171,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "There are many applications for systems coping with the problem of face localization and recognition, e.g. model-based video coding, security systems and mug shot matching. Due to variations in illumination, back-ground, visual angle and facial expressions, the problem of machine face recognition is complex. In this paper we present a robust approach for the extraction of facial regions and features out of color images. First, face candidates are located based on the color and shape information. Then the topographic grey-level relief of facial regions is evaluated to determine the position of facial features as eyes and month. Results are shown for two example scenes."
            },
            "slug": "Extraction-of-facial-regions-and-features-using-and-Sobottka-Pitas",
            "title": {
                "fragments": [],
                "text": "Extraction of facial regions and features using color and shape information"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper presents a robust approach for the extraction of facial regions and features out of color images based on the color and shape information and results are shown for two example scenes."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 13th International Conference on Pattern Recognition"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1887281"
                        ],
                        "name": "Hualu Wang",
                        "slug": "Hualu-Wang",
                        "structuredName": {
                            "firstName": "Hualu",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hualu Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9546964"
                        ],
                        "name": "Shih-Fu Chang",
                        "slug": "Shih-Fu-Chang",
                        "structuredName": {
                            "firstName": "Shih-Fu",
                            "lastName": "Chang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shih-Fu Chang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 93
                            }
                        ],
                        "text": "Other color models applied to face detection include HSV [48, 60, 81, 216], YES [160], YCrCb [2, 48, 84, 130, 191, 200], YUV [1, 123], CIE-xyz [15], L\u2217 a\u2217 b\u2217 [13, 109], L\u2217 u\u2217 v \u2217 [63], CSN (a modified rg representation) [90, 91] and UCS/Farnsworth (a perceptually uniform color system was proposed by Farnsworth [210]) [208]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 521920,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "14e49e2807db93b2c5d524876436516e55169a70",
            "isKey": false,
            "numCitedBy": 346,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Human faces provide a useful cue in indexing video content. We present a highly efficient system that can rapidly detect human face regions in MPEG video sequences. The underlying algorithm takes the inverse quantized discrete cosine transform (DCT) coefficients of MPEG video as the input, and outputs the locations of the detected face regions. The algorithm consists of three stages, where chrominance, shape, and frequency information are used, respectively. By detecting faces directly in the compressed domain, there is no need to carry out the inverse DCT transform, so that the algorithm can run faster than the real time. In our experiments, the algorithm detected 85-92% of the faces in three test sets, including both intraframe and interframe coded image frames from news video. The average run time ranges from 13-33 ms per frame. The algorithm can be applied to JPEG unconstrained images or motion JPEG video as well."
            },
            "slug": "A-highly-efficient-system-for-automatic-face-region-Wang-Chang",
            "title": {
                "fragments": [],
                "text": "A highly efficient system for automatic face region detection in MPEG video"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "A highly efficient system that can rapidly detect human face regions in MPEG video sequences by detecting faces directly in the compressed domain, and there is no need to carry out the inverse DCT transform, so that the algorithm can run faster than the real time."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Circuits Syst. Video Technol."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2119061171"
                        ],
                        "name": "Liu Yang",
                        "slug": "Liu-Yang",
                        "structuredName": {
                            "firstName": "Liu",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liu Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "26154711"
                        ],
                        "name": "M. Robertson",
                        "slug": "M.-Robertson",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Robertson",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Robertson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 57
                            }
                        ],
                        "text": "Other color models applied to face detection include HSV [48, 60, 81, 216], YES [160], YCrCb [2, 48, 84, 130, 191, 200], YUV [1, 123], CIE-xyz [15], L\u2217 a\u2217 b\u2217 [13, 109], L\u2217 u\u2217 v \u2217 [63], CSN (a modified rg representation) [90, 91] and UCS/Farnsworth (a perceptually uniform color system was proposed by Farnsworth [210]) [208]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 126
                            }
                        ],
                        "text": "In their study they compare normalized TSL (tint-saturation-luminance [1881]), rg and CIE-xy chrominance spaces, and CIE-DSH, HSV, YIQ, YES, CIE-L \u2217 u \u2217 v \u2217 , and CIE L\u2217 a \u2217 b \u2217 chrominance spaces by modeling skin color distributions with either a single Gaussian or a Gaussian mixture density model in each space."
                    },
                    "intents": []
                }
            ],
            "corpusId": 8062858,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d4a898b3220e37af22d16702f2ea9671ac2d81ad",
            "isKey": false,
            "numCitedBy": 10,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "An algorithm has been developed which locates and tracks faces in color video sequences for rise in region-of-interest video compression. Our face-tracking algorithm detects faces in the hue-saturation-value (HSV) color space and reduces false alarms based on various cues, including size, shape position, and aspect ratio. The algorithm also performs temporal filtering to enforce consistency from frame to frame. The face tracker is integrated in the rate control of a video encoder, which allows more bits to be committed to the face regions at the cost of reduced bits in the non-face regions. Thus, image quality can be greatly improved in face regions at the cost of reduced quality in background regions, resulting in better overall subjective quality for many sequences."
            },
            "slug": "Multiple-face-tracking-system-for-general-video-Yang-Robertson",
            "title": {
                "fragments": [],
                "text": "Multiple-face tracking system for general region-of-interest video coding"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The face tracker is integrated in the rate control of a video encoder, which allows more bits to be committed to the face regions at the cost of reduced bits in the non-face regions, resulting in better overall subjective quality for many sequences."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 2000 International Conference on Image Processing (Cat. No.00CH37101)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144739319"
                        ],
                        "name": "R. Lienhart",
                        "slug": "R.-Lienhart",
                        "structuredName": {
                            "firstName": "Rainer",
                            "lastName": "Lienhart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lienhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144789920"
                        ],
                        "name": "A. Kuranov",
                        "slug": "A.-Kuranov",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Kuranov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kuranov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3066817"
                        ],
                        "name": "V. Pisarevsky",
                        "slug": "V.-Pisarevsky",
                        "structuredName": {
                            "firstName": "Vadim",
                            "lastName": "Pisarevsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Pisarevsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8862236,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "67c63f6b3eeb0b619d6306fcf1c4b144710f79b0",
            "isKey": false,
            "numCitedBy": 937,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently Viola et al. have introduced a rapid object detection scheme based on a boosted cascade of simple feature classifiers. In this paper we introduce and empirically analysis two extensions to their approach: Firstly, a novel set of rotated haar-like features is introduced. These novel features significantly enrich the simple features of [6] and can also be calculated efficiently. With these new rotated features our sample face detector shows off on average a 10% lower false alarm rate at a given hit rate. Secondly, we present a through analysis of different boosting algorithms (namely Discrete, Real and Gentle Adaboost) and weak classifiers on the detection performance and computational complexity. We will see that Gentle Adaboost with small CART trees as base classifiers outperform Discrete Adaboost and stumps. The complete object detection training and detection system as well as a trained face detector are available in the Open Computer Vision Library at sourceforge.net [8]."
            },
            "slug": "Empirical-Analysis-of-Detection-Cascades-of-Boosted-Lienhart-Kuranov",
            "title": {
                "fragments": [],
                "text": "Empirical Analysis of Detection Cascades of Boosted Classifiers for Rapid Object Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A novel set of rotated haar-like features is introduced that significantly enrich the simple features of [6] and can also be calculated efficiently and present a through analysis of different boosting algorithms and weak classifiers on the detection performance and computational complexity."
            },
            "venue": {
                "fragments": [],
                "text": "DAGM-Symposium"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40648340"
                        ],
                        "name": "Chiunhsiun Lin",
                        "slug": "Chiunhsiun-Lin",
                        "structuredName": {
                            "firstName": "Chiunhsiun",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chiunhsiun Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143961578"
                        ],
                        "name": "Kuo-Chin Fan",
                        "slug": "Kuo-Chin-Fan",
                        "structuredName": {
                            "firstName": "Kuo-Chin",
                            "lastName": "Fan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kuo-Chin Fan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 56
                            }
                        ],
                        "text": "Various types of face constellations have been proposed [4, 11, 73, 110, 117, 131, 180, 197, 221]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11148859,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b2afaa486527e9b69a7acdced1d410f91f52b71d",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "The proposed system consists of two main parts: the first part is to search the potential face regions that are obtained from the triangles based on the rules of \"the combination of two eyes and one mouth\"; and the second part is to perform the face verification task. The proposed face detection system can locate multiple faces embedded in complicated backgrounds. Moreover, it is able to handle different sizes, different lighting conditions, varying poses and expressions, noises and defocus problems, and the problem of partial occlusion of mouth and sun glasses. Experimental results demonstrate that an approximately 98% success rate was achieved and the relative false detection rate is very low."
            },
            "slug": "Human-face-detection-using-geometric-triangle-Lin-Fan",
            "title": {
                "fragments": [],
                "text": "Human face detection using geometric triangle relationship"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The proposed face detection system can locate multiple faces embedded in complicated backgrounds and is able to handle different sizes, different lighting conditions, varying poses and expressions, noises and defocus problems, and the problem of partial occlusion of mouth and sun glasses."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 15th International Conference on Pattern Recognition. ICPR-2000"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46599154"
                        ],
                        "name": "T. Sakai",
                        "slug": "T.-Sakai",
                        "structuredName": {
                            "firstName": "Toshiyuki",
                            "lastName": "Sakai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sakai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2162080"
                        ],
                        "name": "M. Nagao",
                        "slug": "M.-Nagao",
                        "structuredName": {
                            "firstName": "Makoto",
                            "lastName": "Nagao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Nagao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153311318"
                        ],
                        "name": "Shinya Fujibayashi",
                        "slug": "Shinya-Fujibayashi",
                        "structuredName": {
                            "firstName": "Shinya",
                            "lastName": "Fujibayashi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shinya Fujibayashi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 29357850,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "99fde1986e9ea3dea7af76b86f39dfefcdec9db0",
            "isKey": false,
            "numCitedBy": 112,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Line-extraction-and-pattern-detection-in-a-Sakai-Nagao",
            "title": {
                "fragments": [],
                "text": "Line extraction and pattern detection in a photograph"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1969
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3293655"
                        ],
                        "name": "S. Romdhani",
                        "slug": "S.-Romdhani",
                        "structuredName": {
                            "firstName": "Sami",
                            "lastName": "Romdhani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Romdhani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143635539"
                        ],
                        "name": "P. Torr",
                        "slug": "P.-Torr",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Torr",
                            "middleNames": [
                                "H.",
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Torr"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145162067"
                        ],
                        "name": "A. Blake",
                        "slug": "A.-Blake",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Blake",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Blake"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1952800,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d87d04c221b4dadbacb5ba0778553fa0eafa6820",
            "isKey": false,
            "numCitedBy": 216,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes an algorithm for finding faces within an image. The basis of the algorithm is to run an observation window at all possible positions, scales and orientation within the image. A non-linear support vector machine is used to determine whether or not a face is contained within the observation window. The non-linear support vector machine operates by comparing the input patch to a set of support vectors (which can be thought of as face and anti-face templates). Each support vector is scored by some nonlinear function against the observation window and if the resulting sum is over some threshold a face is indicated. Because of the huge search space that is considered, it is imperative to investigate ways to speed up the support vector machine. Within this paper we suggest a method of speeding up the non-linear support vector machine. A set of reduced set vectors (RVs) are calculated from the support vectors. By considering the RV's sequentially, and if at any point a face is deemed too unlikely to cease the sequential evaluation, obviating the need to evaluate the remaining RVs. The idea being that we only need to apply a subset of the RVs to eliminate things that are obviously not a face (thus reducing the computation). The key then is to explore the RVs in the right order and a method for this is proposed."
            },
            "slug": "Computationally-efficient-face-detection-Romdhani-Torr",
            "title": {
                "fragments": [],
                "text": "Computationally efficient face detection"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A method of speeding up the non-linear support vector machine by considering the RV's sequentially, and if at any point a face is deemed too unlikely to cease the sequential evaluation, obviating the need to evaluate the remaining RVs."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2154569449"
                        ],
                        "name": "Yadong Li",
                        "slug": "Yadong-Li",
                        "structuredName": {
                            "firstName": "Yadong",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yadong Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2312891"
                        ],
                        "name": "A. Goshtasby",
                        "slug": "A.-Goshtasby",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Goshtasby",
                            "middleNames": [
                                "Ardeshir"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Goshtasby"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2061142421"
                        ],
                        "name": "O. Garcia",
                        "slug": "O.-Garcia",
                        "structuredName": {
                            "firstName": "Oscar",
                            "lastName": "Garcia",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Garcia"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 158
                            }
                        ],
                        "text": "Other color models applied to face detection include HSV [48, 60, 81, 216], YES [160], YCrCb [2, 48, 84, 130, 191, 200], YUV [1, 123], CIE-xyz [15], L\u2217 a\u2217 b\u2217 [13, 109], L\u2217 u\u2217 v \u2217 [63], CSN (a modified rg representation) [90, 91] and UCS/Farnsworth (a perceptually uniform color system was proposed by Farnsworth [210]) [208]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5417469,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b9cf32a996ac67ea53c56fe22560fc464faf9c10",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "A method for detecting and tracking human faces in color videos is presented. The method first uses a chroma chart with information about skin colors of various races to determine regions of skin color in the first frame of a video. A new chroma chart is computed for each region, which more precisely represents the colour contents of that region. Chroma charts for different regions that are similar are combined, while those that are considerably different are kept separate. Model facial patterns are then used to detect faces within the skin regions. Once a face is detected, the particular pattern and color of the face are used to track the face. Regions where facial patterns are not detected are expected to correspond to exposed parts of the body or of the background and are ignored. The proposed method can track faces with a high degree of accuracy once they are identified."
            },
            "slug": "Detecting-and-tracking-human-faces-in-videos-Li-Goshtasby",
            "title": {
                "fragments": [],
                "text": "Detecting and tracking human faces in videos"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The proposed method can track faces with a high degree of accuracy once they are identified and can detect faces within the skin regions where facial patterns are not detected."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 15th International Conference on Pattern Recognition. ICPR-2000"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733172"
                        ],
                        "name": "E. Saber",
                        "slug": "E.-Saber",
                        "structuredName": {
                            "firstName": "Eli",
                            "lastName": "Saber",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Saber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747853"
                        ],
                        "name": "A. Tekalp",
                        "slug": "A.-Tekalp",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Tekalp",
                            "middleNames": [
                                "Murat"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Tekalp"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 80
                            }
                        ],
                        "text": "Other color models applied to face detection include HSV [48, 60, 81, 216], YES [160], YCrCb [2, 48, 84, 130, 191, 200], YUV [1, 123], CIE-xyz [15], L\u2217 a\u2217 b\u2217 [13, 109], L\u2217 u\u2217 v \u2217 [63], CSN (a modified rg representation) [90, 91] and UCS/Farnsworth (a perceptually uniform color system was proposed by Farnsworth [210]) [208]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 136
                            }
                        ],
                        "text": "In their study they compare normalized TSL (tint-saturation-luminance [1881]), rg and CIE-xy chrominance spaces, and CIE-DSH, HSV, YIQ, YES, CIE-L \u2217 u \u2217 v \u2217 , and CIE L\u2217 a \u2217 b \u2217 chrominance spaces by modeling skin color distributions with either a single Gaussian or a Gaussian mixture density model in each space."
                    },
                    "intents": []
                }
            ],
            "corpusId": 206037537,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3bc9273d93d8b2ccb23564b4843be118cbc4f254",
            "isKey": false,
            "numCitedBy": 279,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Frontal-view-face-detection-and-facial-feature-and-Saber-Tekalp",
            "title": {
                "fragments": [],
                "text": "Frontal-view face detection and facial feature extraction using color, shape and symmetry based cost functions"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit. Lett."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145030811"
                        ],
                        "name": "C. Papageorgiou",
                        "slug": "C.-Papageorgiou",
                        "structuredName": {
                            "firstName": "Constantine",
                            "lastName": "Papageorgiou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Papageorgiou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145328018"
                        ],
                        "name": "M. Oren",
                        "slug": "M.-Oren",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Oren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Oren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6476085,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "76f560991d56ad689ec32f9e9d13291e0193f4cf",
            "isKey": false,
            "numCitedBy": 1604,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a general trainable framework for object detection in static images of cluttered scenes. The detection technique we develop is based on a wavelet representation of an object class derived from a statistical analysis of the class instances. By learning an object class in terms of a subset of an overcomplete dictionary of wavelet basis functions, we derive a compact representation of an object class which is used as an input to a support vector machine classifier. This representation overcomes both the problem of in-class variability and provides a low false detection rate in unconstrained environments. We demonstrate the capabilities of the technique in two domains whose inherent information content differs significantly. The first system is face detection and the second is the domain of people which, in contrast to faces, vary greatly in color, texture, and patterns. Unlike previous approaches, this system learns from examples and does not rely on any a priori (hand-crafted) models or motion-based segmentation. The paper also presents a motion-based extension to enhance the performance of the detection algorithm over video sequences. The results presented here suggest that this architecture may well be quite general."
            },
            "slug": "A-general-framework-for-object-detection-Papageorgiou-Oren",
            "title": {
                "fragments": [],
                "text": "A general framework for object detection"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A general trainable framework for object detection in static images of cluttered scenes based on a wavelet representation of an object class derived from a statistical analysis of the class instances and a motion-based extension to enhance the performance of the detection algorithm over video sequences is presented."
            },
            "venue": {
                "fragments": [],
                "text": "Sixth International Conference on Computer Vision (IEEE Cat. No.98CH36271)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2427731"
                        ],
                        "name": "T. Kawaguchi",
                        "slug": "T.-Kawaguchi",
                        "structuredName": {
                            "firstName": "Tsuyoshi",
                            "lastName": "Kawaguchi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kawaguchi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40488407"
                        ],
                        "name": "Daisuke Hidaka",
                        "slug": "Daisuke-Hidaka",
                        "structuredName": {
                            "firstName": "Daisuke",
                            "lastName": "Hidaka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daisuke Hidaka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144394088"
                        ],
                        "name": "M. Rizon",
                        "slug": "M.-Rizon",
                        "structuredName": {
                            "firstName": "Mohamed",
                            "lastName": "Rizon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Rizon"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 31442534,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f88b49f1015a76216922ff37c9edd0245d247695",
            "isKey": false,
            "numCitedBy": 16,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we propose a new algorithm to detect the irises of both eyes from a human face in an intensity image. Using the separability filter proposed by Fukui et al. (1997), the algorithm first extracts intensity valleys, which we call blobs in this paper, as the candidates for the irises. Next, for each pair of blobs, the algorithm computes a cost using Hough transform and separability filter to measure the fit of the pair of blobs to the image. And then, the algorithm selects a pair of blobs with the smallest cost as the irises of both eyes. As the result of the experiment using all faces without spectacles in the face database of the University of Bern, the success rate of the proposed algorithm was 96.5% on average."
            },
            "slug": "Robust-extraction-of-eyes-from-face-Kawaguchi-Hidaka",
            "title": {
                "fragments": [],
                "text": "Robust extraction of eyes from face"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "A new algorithm to detect the irises of both eyes from a human face in an intensity image using the separability filter proposed by Fukui et al. (1997), with success rate of 96.5% on average."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 15th International Conference on Pattern Recognition. ICPR-2000"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2510362"
                        ],
                        "name": "K. Hotta",
                        "slug": "K.-Hotta",
                        "structuredName": {
                            "firstName": "Kazuhiro",
                            "lastName": "Hotta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Hotta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35797142"
                        ],
                        "name": "T. Mishima",
                        "slug": "T.-Mishima",
                        "structuredName": {
                            "firstName": "Taketoshi",
                            "lastName": "Mishima",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Mishima"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145375983"
                        ],
                        "name": "T. Kurita",
                        "slug": "T.-Kurita",
                        "structuredName": {
                            "firstName": "Takio",
                            "lastName": "Kurita",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kurita"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744999"
                        ],
                        "name": "S. Umeyama",
                        "slug": "S.-Umeyama",
                        "structuredName": {
                            "firstName": "Shinji",
                            "lastName": "Umeyama",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Umeyama"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 79
                            }
                        ],
                        "text": "Gabor responses have also been applied to face and facial feature detection in [18, 56, 66, 146]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 37321256,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0da424ca9015368003fb7f85fc7daddb11b30b94",
            "isKey": false,
            "numCitedBy": 25,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a face matching method through information theoretical attention points. The attention points are selected as the points where the outputs of Gabor filters applied to the contrast-filtered image (Gabor features) have rich information. The information value of Gabor features of the certain point is used as the weight and the weighed sum of the correlations is used as the similarity measure for the matching. To cope with the scale changes of a face, several images with different scales are generated by interpolation from the input image and the best match is searched. By using the attention points given from the information theoretical point of view, the matching becomes robust under various environments. This matching method is applied to face detection of a known person and face classification. The effectiveness of the proposed method is confirmed by experiments using the face images captured over years under the different environments."
            },
            "slug": "Face-matching-through-information-theoretical-and-Hotta-Mishima",
            "title": {
                "fragments": [],
                "text": "Face matching through information theoretical attention points and its applications to face detection and classification"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This matching method is applied to face detection of a known person and face classification and is confirmed by experiments using the face images captured over years under the different environments."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144187716"
                        ],
                        "name": "Gang Xu",
                        "slug": "Gang-Xu",
                        "structuredName": {
                            "firstName": "Gang",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gang Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3161309"
                        ],
                        "name": "Takeo Sugimoto",
                        "slug": "Takeo-Sugimoto",
                        "structuredName": {
                            "firstName": "Takeo",
                            "lastName": "Sugimoto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Takeo Sugimoto"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 94
                            }
                        ],
                        "text": "Approaches which focus on single camera control include LISTEN [19],\nSAVI [60], LAFTER [139], Rits Eye [212], and the system of Morimoto and Flickner [135]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 34
                            }
                        ],
                        "text": "SAVI [60], LAFTER [139], Rits Eye [212], and the system of Morimoto and Flickner [135]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7035602,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bd2234abb54ed637a8a60480ac6cc77c05b1eb82",
            "isKey": false,
            "numCitedBy": 30,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a working system that can robustly detect and track human faces with unknown sizes and positions in complex backgrounds from color images in real time. Skin color is modeled as and detected by a 3D Gaussian distribution in the RGB color space among different persons within the same race. Detected skin pixels are grouped into regions whose centroids and areas are recorded. If dark regions for eyes, eyebrows and mouth are also detected at certain positions within these regions, they are regarded as faces. Once the faces are detected, the system starts to track one of them using a pan-tilt-zoom controllable camera. Without any special hardware, the system achieves a speed of 0.2 sec for face detection and 0.1 sec for face tracking, on a Pentium 266 MHz PC, including the image acquisition time."
            },
            "slug": "Rits-Eye:-a-software-based-system-for-real-time-and-Xu-Sugimoto",
            "title": {
                "fragments": [],
                "text": "Rits Eye: a software-based system for real-time face detection and tracking using pan-tilt-zoom controllable camera"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "A working system that can robustly detect and track human faces with unknown sizes and positions in complex backgrounds from color images in real time using a pan-tilt-zoom controllable camera is described."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. Fourteenth International Conference on Pattern Recognition (Cat. No.98EX170)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2228528"
                        ],
                        "name": "J. Terrillon",
                        "slug": "J.-Terrillon",
                        "structuredName": {
                            "firstName": "Jean-Christophe",
                            "lastName": "Terrillon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Terrillon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707424"
                        ],
                        "name": "M. N. Shirazi",
                        "slug": "M.-N.-Shirazi",
                        "structuredName": {
                            "firstName": "Mahdad",
                            "lastName": "Shirazi",
                            "middleNames": [
                                "Nouri"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. N. Shirazi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2041939788"
                        ],
                        "name": "Mohamed Sadek",
                        "slug": "Mohamed-Sadek",
                        "structuredName": {
                            "firstName": "Mohamed",
                            "lastName": "Sadek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mohamed Sadek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143906699"
                        ],
                        "name": "H. Fukamachi",
                        "slug": "H.-Fukamachi",
                        "structuredName": {
                            "firstName": "Hideo",
                            "lastName": "Fukamachi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Fukamachi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49052113"
                        ],
                        "name": "S. Akamatsu",
                        "slug": "S.-Akamatsu",
                        "structuredName": {
                            "firstName": "Shigeru",
                            "lastName": "Akamatsu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Akamatsu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "[186], SVMs improved the performance of the face detector compared to the earlier use of a multi-layer perceptron."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7521516,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d546db6875d17a0087c9380ee690865f217a3e4b",
            "isKey": false,
            "numCitedBy": 35,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper present an analysis of the performance of support vector machines (SVMs) for automatic detection of human faces in static color images of complex scenes. Skin color-based image segmentations initially performed for several different chrominance spaces by use of the single Gaussian chrominance model and a Gaussian mixture density model. Feature extraction in the segmented images is then implemented by use of invariant orthogonal Fourier-Mellin moments. For all chrominance spaces, the application of SVMs to the invariant moments obtained from a set of 100 test images yields a higher face detection performance than when applying a 3-layer perceptron neural network (NN), depending on a suitable selection of the kernel function used to train the SVM and of the value of its associated parameter(s). The training of SVMs is easier and faster than that of a NN, always finds a global minimum, and SVMs have a better generalization ability."
            },
            "slug": "Invariant-face-detection-with-support-vector-Terrillon-Shirazi",
            "title": {
                "fragments": [],
                "text": "Invariant face detection with support vector machines"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The application of SVMs to the invariant moments obtained from a set of 100 test images yields a higher face detection performance than when applying a 3-layer perceptron neural network (NN)."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 15th International Conference on Pattern Recognition. ICPR-2000"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3262395"
                        ],
                        "name": "B. Raducanu",
                        "slug": "B.-Raducanu",
                        "structuredName": {
                            "firstName": "Bogdan",
                            "lastName": "Raducanu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Raducanu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144278339"
                        ],
                        "name": "M. Gra\u00f1a",
                        "slug": "M.-Gra\u00f1a",
                        "structuredName": {
                            "firstName": "Manuel",
                            "lastName": "Gra\u00f1a",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gra\u00f1a"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3202871"
                        ],
                        "name": "F. Albizuri",
                        "slug": "F.-Albizuri",
                        "structuredName": {
                            "firstName": "F.",
                            "lastName": "Albizuri",
                            "middleNames": [
                                "Xabier"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Albizuri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "96701098"
                        ],
                        "name": "A. D'Anjou",
                        "slug": "A.-D'Anjou",
                        "structuredName": {
                            "firstName": "Alicia",
                            "lastName": "D'Anjou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. D'Anjou"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 86509424,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "869fd6d4637ea74ad4ff616adc0457654d14027f",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Face-localization-based-on-the-morphological-Raducanu-Gra\u00f1a",
            "title": {
                "fragments": [],
                "text": "Face localization based on the morphological multiscale fingerprints"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit. Lett."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747625"
                        ],
                        "name": "D. Maio",
                        "slug": "D.-Maio",
                        "structuredName": {
                            "firstName": "Dario",
                            "lastName": "Maio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Maio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687735"
                        ],
                        "name": "D. Maltoni",
                        "slug": "D.-Maltoni",
                        "structuredName": {
                            "firstName": "Davide",
                            "lastName": "Maltoni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Maltoni"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15505950,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fb8427550a099124ebdbc44f783f6b62a0583a5e",
            "isKey": false,
            "numCitedBy": 145,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Real-time-face-location-on-gray-scale-static-images-Maio-Maltoni",
            "title": {
                "fragments": [],
                "text": "Real-time face location on gray-scale static images"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34679741"
                        ],
                        "name": "S. Li",
                        "slug": "S.-Li",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144850567"
                        ],
                        "name": "Long Zhu",
                        "slug": "Long-Zhu",
                        "structuredName": {
                            "firstName": "Long",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Long Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2651359"
                        ],
                        "name": "ZhenQiu Zhang",
                        "slug": "ZhenQiu-Zhang",
                        "structuredName": {
                            "firstName": "ZhenQiu",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "ZhenQiu Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145162067"
                        ],
                        "name": "A. Blake",
                        "slug": "A.-Blake",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Blake",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Blake"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108698841"
                        ],
                        "name": "HongJiang Zhang",
                        "slug": "HongJiang-Zhang",
                        "structuredName": {
                            "firstName": "HongJiang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "HongJiang Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144154486"
                        ],
                        "name": "H. Shum",
                        "slug": "H.-Shum",
                        "structuredName": {
                            "firstName": "Harry",
                            "lastName": "Shum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Shum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 32107239,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3c4fdffc12589f9f312a44802b8e2fe8311aa13e",
            "isKey": false,
            "numCitedBy": 462,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "A new boosting algorithm, called FloatBoost, is proposed to overcome the monotonicity problem of the sequential AdaBoost learning. AdaBoost [1, 2] is a sequential forward search procedure using the greedy selection strategy. The premise oyered by the sequential procedure can be broken-down when the monotonicity assumption, i.e. that when adding a new feature to the current set, the value of the performance criterion does not decrease, is violated. FloatBoost incorporates the idea of Floating Search [3] into AdaBoost to solve the non-monotonicity problem encountered in the sequential search of AdaBoost.We then present a system which learns to detect multi-view faces using FloatBoost. The system uses a coarse-to-fine, simple-to-complex architecture called detector-pyramid. FloatBoost learns the component detectors in the pyramid and yields similar or higher classification accuracy than AdaBoost with a smaller number of weak classifiers. This work leads to the first real-time multi-view face detection system in the world. It runs at 200 ms per image of size 320x240 pixels on a Pentium-III CPU of 700 MHz. A live demo will be shown at the conference."
            },
            "slug": "Statistical-Learning-of-Multi-view-Face-Detection-Li-Zhu",
            "title": {
                "fragments": [],
                "text": "Statistical Learning of Multi-view Face Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "FloatBoost incorporates the idea of Floating Search into AdaBoost to solve the non-monotonicity problem encountered in the sequential search of AdaBoost and leads to the first real-time multi-view face detection system in the world."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2228528"
                        ],
                        "name": "J. Terrillon",
                        "slug": "J.-Terrillon",
                        "structuredName": {
                            "firstName": "Jean-Christophe",
                            "lastName": "Terrillon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Terrillon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067641238"
                        ],
                        "name": "Martin David",
                        "slug": "Martin-David",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "David",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Martin David"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49052113"
                        ],
                        "name": "S. Akamatsu",
                        "slug": "S.-Akamatsu",
                        "structuredName": {
                            "firstName": "Shigeru",
                            "lastName": "Akamatsu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Akamatsu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 135
                            }
                        ],
                        "text": "Since the representation strongly relates to human perception of color [106, 178], it is also widely used in face segmentation schemes [51, 83, 118, 125, 175, 178, 187, 220]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2586252,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c724cea5d76cb8611adfcc9bca8d02ce4158d6cf",
            "isKey": false,
            "numCitedBy": 184,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "We use a skin color model based on the Muhulanobis metric and a shape analysis based on invariant moments to automatically detect and locate human faces in two-dimensional natural scene images. First, color segmentation of an input image is performed by thresholding in a perceptually plausible hue-saturation color space where the effects of the variability of human skin color and the dependency of chrominance on changes in illumination are reduced. We then group regions of the resulting binary image which have been classified as candidates into clusters of connected pixels. Performing median filtering on the image and discarding the smallest remaining clusters ensures that only a small number of clusters will be used for further analysis. Fully translation-, scale- anti in-plane rotation invariant moments are calculated for each remaining cluster. Finally, in order to distinguish faces from distractors, a multilayer perceptron neural network is used with the invariant moments as the input vector. Supervised learning of the network is implemented with the backpropagation algorithm, at first for frontal views of faces. Preliminary results show the efficiency of the combination of color segmentation and of invariant moments in detecting faces with a large variety of poses and against relatively complex backgrounds."
            },
            "slug": "Automatic-detection-of-human-faces-in-natural-scene-Terrillon-David",
            "title": {
                "fragments": [],
                "text": "Automatic detection of human faces in natural scene images by use of a skin color model and of invariant moments"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Preliminary results show the efficiency of the combination of color segmentation and of invariant moments in detecting faces with a large variety of poses and against relatively complex backgrounds."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Third IEEE International Conference on Automatic Face and Gesture Recognition"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780935"
                        ],
                        "name": "B. Moghaddam",
                        "slug": "B.-Moghaddam",
                        "structuredName": {
                            "firstName": "Baback",
                            "lastName": "Moghaddam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Moghaddam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144994682"
                        ],
                        "name": "A. Pentland",
                        "slug": "A.-Pentland",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Pentland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pentland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2841459,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6bcdca49ed64ec6b15d975adaea49508e9e941d2",
            "isKey": false,
            "numCitedBy": 224,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we describe experiments using eigenfaces for recognition and interactive search in the FERET face database. A recognition accuracy of 99.35% is obtained using frontal views of 155 individuals. This figure is consistent with the 95% recognition rate obtained previously on a much larger database of 7,562 `mugshots' of approximately 3,000 individuals, consisting of a mix of all age and ethnic groups. We also demonstrate that we can automatically determine head pose without significantly lowering recognition accuracy; this is accomplished by use of a view-based multiple-observer eigenspace technique. In addition, a modular eigenspace description is used which incorporates salient facial features such as the eyes, nose and mouth, in an eigenfeature layer. This modular representation yields slightly higher recognition rates as well as a more robust framework for face recognition. In addition, a robust and automatic feature detection technique using eigentemplates is demonstrated."
            },
            "slug": "Face-recognition-using-view-based-and-modular-Moghaddam-Pentland",
            "title": {
                "fragments": [],
                "text": "Face recognition using view-based and modular eigenspaces"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A modular eigenspace description is used which incorporates salient facial features such as the eyes, nose and mouth, in an eigenfeature layer, which yields slightly higher recognition rates as well as a more robust framework for face recognition."
            },
            "venue": {
                "fragments": [],
                "text": "Optics & Photonics"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38817267"
                        ],
                        "name": "K. Sung",
                        "slug": "K.-Sung",
                        "structuredName": {
                            "firstName": "Kah",
                            "lastName": "Sung",
                            "middleNames": [
                                "Kay"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Sung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 36
                            }
                        ],
                        "text": "[158] and at MIT by Sung and Poggio [182]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 87
                            }
                        ],
                        "text": "The proposed system follows the same framework as the one developed by Sung and Poggio [182], described in Section 4."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 63
                            }
                        ],
                        "text": "The input images are originally 320\u00d7 240 (from the MIT dataset [182]), but are scaled down to approximately 46 \u00d7 35, and a 12\u00d7 12 window is used to scan this image (search step 1 pixel)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 71
                            }
                        ],
                        "text": "This technique was first applied for face detection by Sung and Poggio [182]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 119
                            }
                        ],
                        "text": "The number of labeled faces was originally 149 by Sung and Poggio (which is the number used in the reported results in [182] and [107], but it was changed to 155 (which"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 36
                            }
                        ],
                        "text": "Classification in Sung and Poggio\u2019s [182] system ( c \u00a9 1998/2001 IEEE)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7164794,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "088eb2d102c6bb486f5270d0b2adff76961994cf",
            "isKey": true,
            "numCitedBy": 2061,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an example-based learning approach for locating vertical frontal views of human faces in complex scenes. The technique models the distribution of human face patterns by means of a few view-based \"face\" and \"nonface\" model clusters. At each image location, a difference feature vector is computed between the local image pattern and the distribution-based model. A trained classifier determines, based on the difference feature vector measurements, whether or not a human face exists at the current image location. We show empirically that the distance metric we adopt for computing difference feature vectors, and the \"nonface\" clusters we include in our distribution-based model, are both critical for the success of our system."
            },
            "slug": "Example-Based-Learning-for-View-Based-Human-Face-Sung-Poggio",
            "title": {
                "fragments": [],
                "text": "Example-Based Learning for View-Based Human Face Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "An example-based learning approach for locating vertical frontal views of human faces in complex scenes and shows empirically that the distance metric adopted for computing difference feature vectors, and the \"nonface\" clusters included in the distribution-based model, are both critical for the success of the system."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057569439"
                        ],
                        "name": "F. Weber",
                        "slug": "F.-Weber",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Weber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Weber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109838090"
                        ],
                        "name": "A.H. Hernandez",
                        "slug": "A.H.-Hernandez",
                        "structuredName": {
                            "firstName": "A.H.",
                            "lastName": "Hernandez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A.H. Hernandez"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 188,
                                "start": 178
                            }
                        ],
                        "text": "The eigenvector in the LDA transformation matrix with the largest eigenvalue is known as Fisher\u2019s linear discriminant [43], which by itself has also been used for face detection [179, 203]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 122570047,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "e6a21f5b1e0cc6477286942d9429358ab0cfeb03",
            "isKey": false,
            "numCitedBy": 6,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "We present the application of a new approach for object location to the problem of locating frontal and near-frontal views of faces in still grey-scale images. The approach uses a linear and a quadratic discriminant function in a hierarchical fashion to classify subimages of a fixed size extracted from the input image at various positions and resolutions. The parameters of each discriminant function are chosen such that they maximize an objective function which uses the first and second order statistics of the values of the discriminant function on face and non-face images. The resulting classifier is accurate and very fast, and is part of the commercial face recognition system FaceVACS."
            },
            "slug": "Face-location-by-template-matching-with-a-quadratic-Weber-Hernandez",
            "title": {
                "fragments": [],
                "text": "Face location by template matching with a quadratic discriminant function"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "A new approach for object location to the problem of locating frontal and near-frontal views of faces in still grey-scale images is presented, which is accurate and very fast, and is part of the commercial face recognition system FaceVACS."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings International Workshop on Recognition, Analysis, and Tracking of Faces and Gestures in Real-Time Systems. In Conjunction with ICCV'99 (Cat. No.PR00378)"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2283680"
                        ],
                        "name": "P. Juell",
                        "slug": "P.-Juell",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Juell",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Juell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145127047"
                        ],
                        "name": "R. Marsh",
                        "slug": "R.-Marsh",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Marsh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Marsh"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 172
                            }
                        ],
                        "text": "Apart from face classification, neural networks have also been applied for facial features classification [36, 125, 151, 196] and a method for improving detection time for MLPs is presented in [6]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 65
                            }
                        ],
                        "text": "The first neural approaches to face detection were based on MLPs [10, 82, 147], where promising results where reported on fairly simple datasets."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 43374055,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "64b0c20b2737f71b995750b00578ae79e08724bf",
            "isKey": false,
            "numCitedBy": 113,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-hierarchical-neural-network-for-human-face-Juell-Marsh",
            "title": {
                "fragments": [],
                "text": "A hierarchical neural network for human face detection"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1727939"
                        ],
                        "name": "R. Herpers",
                        "slug": "R.-Herpers",
                        "structuredName": {
                            "firstName": "Rainer",
                            "lastName": "Herpers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Herpers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2501661"
                        ],
                        "name": "G. Verghese",
                        "slug": "G.-Verghese",
                        "structuredName": {
                            "firstName": "Gilbert",
                            "lastName": "Verghese",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Verghese"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3150825"
                        ],
                        "name": "K. Derpanis",
                        "slug": "K.-Derpanis",
                        "structuredName": {
                            "firstName": "Konstantinos",
                            "lastName": "Derpanis",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Derpanis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "21780068"
                        ],
                        "name": "R. McCready",
                        "slug": "R.-McCready",
                        "structuredName": {
                            "firstName": "Rob",
                            "lastName": "McCready",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. McCready"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1448470130"
                        ],
                        "name": "J. MacLean",
                        "slug": "J.-MacLean",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "MacLean",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. MacLean"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056116838"
                        ],
                        "name": "A. Levin",
                        "slug": "A.-Levin",
                        "structuredName": {
                            "firstName": "Alexandra",
                            "lastName": "Levin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Levin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "117576196"
                        ],
                        "name": "D. Topalovic",
                        "slug": "D.-Topalovic",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Topalovic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Topalovic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2058066106"
                        ],
                        "name": "L. Wood",
                        "slug": "L.-Wood",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Wood",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Wood"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49199809"
                        ],
                        "name": "A. Jepson",
                        "slug": "A.-Jepson",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Jepson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Jepson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "147167746"
                        ],
                        "name": "J. Tsotsos",
                        "slug": "J.-Tsotsos",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Tsotsos",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tsotsos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2863312,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e886a2280f172d408ac1ed9c57b45321b4887cdc",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "A stereo active vision interface is introduced which detects frontal faces in real world environments and performs particular active control tasks dependent on changes in the visual field. Firstly, connected skin colour regions in the visual scene are detected by applying a radial scanline algorithm. Secondly, facial features are searched for in the most salient skin colour region while the blob is tracked by the camera system. The facial features are evaluated and, based on the obtained results and the current state of the system, particular actions are performed. The SAVI system is thought of as a smart laser interface for teleconferencing, telemedicine, and distance learning. The system is designed as a Perception-Action-Cycle (PAC), processing sensory data of different kinds and qualities. Both the vision module and the head motion control module work at frame rate. Hence, the system is able to react instantaneously to changing conditions in the visual scene."
            },
            "slug": "Detection-and-tracking-of-faces-in-real-Herpers-Verghese",
            "title": {
                "fragments": [],
                "text": "Detection and tracking of faces in real environments"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A stereo active vision interface is introduced which detects frontal faces in real world environments and performs particular active control tasks dependent on changes in the visual field."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings International Workshop on Recognition, Analysis, and Tracking of Faces and Gestures in Real-Time Systems. In Conjunction with ICCV'99 (Cat. No.PR00378)"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715634"
                        ],
                        "name": "Ming-Hsuan Yang",
                        "slug": "Ming-Hsuan-Yang",
                        "structuredName": {
                            "firstName": "Ming-Hsuan",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming-Hsuan Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145237406"
                        ],
                        "name": "N. Ahuja",
                        "slug": "N.-Ahuja",
                        "structuredName": {
                            "firstName": "Narendra",
                            "lastName": "Ahuja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Ahuja"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1765887"
                        ],
                        "name": "D. Kriegman",
                        "slug": "D.-Kriegman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Kriegman",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Kriegman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "[217] make this decision based on the rule \u201c ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "[217] proposed two methods for face detection which also seek to represent the manifold of human faces as a set of subclasses."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16705499,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "29fe0a5209ad7c538aeaf9819644abac532bcce9",
            "isKey": false,
            "numCitedBy": 120,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "We present two methods using mixtures of linear sub-spaces for face detection in gray level images. One method uses a mixture of factor analyzers to concurrently perform clustering and, within each cluster, perform local dimensionality reduction. The parameters of the mixture model are estimated using an EM algorithm. A face is detected if the probability of an input sample is above a predefined threshold. The other mixture of subspaces method uses Kohonen's self-organizing map for clustering and Fisher linear discriminant to find the optimal projection for pattern classification, and a Gaussian distribution to model the class-conditioned density function of the projected samples for each class. The parameters of the class-conditioned density functions are maximum likelihood estimates and the decision rule is also based on maximum likelihood. A wide range of face images including ones in different poses, with different expressions and under different lighting conditions are used as the training set to capture the variations of human faces. Our methods have been tested on three sets of 225 images which contain 871 faces. Experimental results on the first two datasets show that our methods perform as well as the best methods in the literature, yet have fewer false detects."
            },
            "slug": "Face-detection-using-mixtures-of-linear-subspaces-Yang-Ahuja",
            "title": {
                "fragments": [],
                "text": "Face detection using mixtures of linear subspaces"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "Two methods using mixtures of linear sub-spaces for face detection in gray level images using Kohonen's self-organizing map for clustering and Fisher linear discriminant to find the optimal projection for pattern classification are presented."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39682833"
                        ],
                        "name": "H. Rowley",
                        "slug": "H.-Rowley",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Rowley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Rowley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1767244"
                        ],
                        "name": "S. Baluja",
                        "slug": "S.-Baluja",
                        "structuredName": {
                            "firstName": "Shumeet",
                            "lastName": "Baluja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Baluja"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733113"
                        ],
                        "name": "T. Kanade",
                        "slug": "T.-Kanade",
                        "structuredName": {
                            "firstName": "Takeo",
                            "lastName": "Kanade",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanade"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "[158] and at MIT by Sung and Poggio [182]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "[158] set the standards for research on this topic, and the performances of their algorithms are still comparable to more recent image-based approaches."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "[158] is used as a part of an image search engine for the World Wide Web in WebSeer [45]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 46
                            }
                        ],
                        "text": "is the number used in the reported results in [158], [140], and [113]) when included in the CMU dataset."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "[158] ( c \u00a9 1998/2001 IEEE) and Sung and Poggio."
                    },
                    "intents": []
                }
            ],
            "corpusId": 40120983,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3d76ef8e61395a6e9c32627f1f108772d084e2e9",
            "isKey": true,
            "numCitedBy": 4156,
            "numCiting": 134,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a neural network-based face detection system. A retinally connected neural network examines small windows of an image and decides whether each window contains a face. The system arbitrates between multiple networks to improve performance over a single network. We use a bootstrap algorithm for training the networks, which adds false detections into the training set as training progresses. This eliminates the difficult task of manually selecting non-face training examples, which must be chosen to span the entire space of non-face images. Comparisons with other state-of-the-art face detection systems are presented; our system has better performance in terms of detection and false-positive rates."
            },
            "slug": "Neural-network-based-face-detection-Rowley-Baluja",
            "title": {
                "fragments": [],
                "text": "Neural Network-Based Face Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "A neural network-based face detection system that arbitrates between multiple networks to improve performance over a single network using a bootstrap algorithm, which eliminates the difficult task of manually selecting non-face training examples."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144847940"
                        ],
                        "name": "K. Lam",
                        "slug": "K.-Lam",
                        "structuredName": {
                            "firstName": "Kin-Man",
                            "lastName": "Lam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Lam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152996923"
                        ],
                        "name": "Hong Yan",
                        "slug": "Hong-Yan",
                        "structuredName": {
                            "firstName": "Hong",
                            "lastName": "Yan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hong Yan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 52
                            }
                        ],
                        "text": "Several recent facial feature extraction algorithms [5, 53, 100] search for local gray minima within segmented facial regions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62437394,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "098b41571b6460bf8f2f5a801b1514e9c7c532fa",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Facial feature extraction is an important step in automated visual interpretation and human face recognition. Deformable template is a method used in extracting the eye boundaries. However, a weakness of the deformable template is the lengthy processing time. In this paper, the head boundary is first located in a head-and-shoulders image. The approximate positions of the eyes are estimated by means of average anthropometric measures. Corners, the salient features of the eyes, are detected and used to set the initial parameters of the eye templates. This scheme can accurately position the templates in relation to the eye images and greatly reduce the processing time for the templates. In order to illustrate this performance, a simplified eye template model is used in the experiment."
            },
            "slug": "Facial-Feature-Location-and-Extraction-for-Human-Lam-Yan",
            "title": {
                "fragments": [],
                "text": "Facial Feature Location and Extraction for Computerized Human Face Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The head boundary is first located in a head-and-shoulders image and Corners, the salient features of the eyes, are detected and used to set the initial parameters of the eye templates."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34854725"
                        ],
                        "name": "F. Marqu\u00e9s",
                        "slug": "F.-Marqu\u00e9s",
                        "structuredName": {
                            "firstName": "Ferran",
                            "lastName": "Marqu\u00e9s",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Marqu\u00e9s"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1798046"
                        ],
                        "name": "Ver\u00f3nica Vilaplana",
                        "slug": "Ver\u00f3nica-Vilaplana",
                        "structuredName": {
                            "firstName": "Ver\u00f3nica",
                            "lastName": "Vilaplana",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ver\u00f3nica Vilaplana"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 125
                            }
                        ],
                        "text": "Other color models applied to face detection include HSV [48, 60, 81, 216], YES [160], YCrCb [2, 48, 84, 130, 191, 200], YUV [1, 123], CIE-xyz [15], L\u2217 a\u2217 b\u2217 [13, 109], L\u2217 u\u2217 v \u2217 [63], CSN (a modified rg representation) [90, 91] and UCS/Farnsworth (a perceptually uniform color system was proposed by Farnsworth [210]) [208]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1829521,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "74d428fefa2858902aaa6ea097c6c0a91750b755",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "A new technique for segmenting and tracking human faces in video sequences is presented. The technique relies on morphological tools such as using connected operators to extract the connected component that more likely belongs to a face, and partition projection to track this component through the sequence. A binary partition tree (BPT) is used to implement the connected operator. The BPT is constructed based on the chrominance criteria and its nodes are analyzed so that the selected node maximizes an estimation of the likelihood of being part of a face. The tracking is performed using a partition projection approach. Images are divided into face and non-face parts, which are tracked through the sequence. The technique has been successfully assessed using several test sequences from the MPEG-4 (raw format) and the MPEG-7 databases (MPEG-1 format)."
            },
            "slug": "A-morphological-approach-for-segmentation-and-of-Marqu\u00e9s-Vilaplana",
            "title": {
                "fragments": [],
                "text": "A morphological approach for segmentation and tracking of human faces"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "A new technique for segmenting and tracking human faces in video sequences is presented, which relies on morphological tools such as using connected operators to extract the connected component that more likely belongs to a face, and partition projection to track this component through the sequence."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 15th International Conference on Pattern Recognition. ICPR-2000"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1893129"
                        ],
                        "name": "M. Bichsel",
                        "slug": "M.-Bichsel",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Bichsel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Bichsel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144994682"
                        ],
                        "name": "A. Pentland",
                        "slug": "A.-Pentland",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Pentland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pentland"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62911157,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b23f4919527264148762d476c58a7b21a21808dd",
            "isKey": false,
            "numCitedBy": 136,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract If we consider an n \u00d7 n image as an n2-dimensional vector, then images of faces can be considered as points in this n2-dimensional image space. Our previous studies of physical transformations of the face, including translation, small rotations, and illumination changes, showed that the set of face images consists of relatively simple connected subregions in image space. Consequently linear matching techniques can be used to obtain reliable face recognition. However, for more general transformations, such as large rotations or scale changes, the face subregions become highly non-convex. We have therefore developed a scale-space matching technique that allows us to take advantage of knowledge about important geometrical transformations and about the topology of the face subregion in image space. While recognition of faces is the focus of this paper, the algorithm is sufficiently general to be applicable to a large variety of object recognition tasks"
            },
            "slug": "Human-Face-Recognition-and-the-Face-Image-Set's-Bichsel-Pentland",
            "title": {
                "fragments": [],
                "text": "Human Face Recognition and the Face Image Set's Topology"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A scale-space matching technique that allows it to take advantage of knowledge about important geometrical transformations and about the topology of the face subregion in image space to be applicable to a large variety of object recognition tasks."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37846824"
                        ],
                        "name": "A. Jacquin",
                        "slug": "A.-Jacquin",
                        "structuredName": {
                            "firstName": "Arnaud",
                            "lastName": "Jacquin",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Jacquin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792224"
                        ],
                        "name": "A. Eleftheriadis",
                        "slug": "A.-Eleftheriadis",
                        "structuredName": {
                            "firstName": "Alexandros",
                            "lastName": "Eleftheriadis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Eleftheriadis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 83
                            }
                        ],
                        "text": "The Sobel operator was the most common filter among the techniques mentioned above [9, 16, 32, 76, 87, 108]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 121
                            }
                        ],
                        "text": "More recent examples of edge-based techniques can be found in [9, 16, 59, 115, 116] for facial feature extraction and in [32, 50, 58, 60, 70, 76, 108, 201, 222] for face detection."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15525945,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ee2a2c4515724ca66b3ee0ecb9720df914513924",
            "isKey": false,
            "numCitedBy": 41,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "The work reported in this paper addresses the issue of automatically tracking the faces and facial features of persons in head-and-shoulders video sequences. We propose two totally automatic algorithms which respectively perform the detection of head outlines and identify rectangular \\eyes-nose-mouth\" regions, both from downsampled binary thresholded edge images. Unlike ones that have been proposed recently, a priori assumptions regarding the nature and content of the sequences to code are minimal for our techniques, and the algorithms operate accurately and robustly, even in cases of signi cant head rotation or partial occlusion by moving objects."
            },
            "slug": "Automatic-Location-Tracking-of-Faces-and-Facial-in-Jacquin-Eleftheriadis",
            "title": {
                "fragments": [],
                "text": "Automatic Location Tracking of Faces and Facial Features in Video Sequences"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Two totally automatic algorithms which respectively perform the detection of head outlines and identify rectangular \"eyes-nose-mouth\" regions are proposed, both from downsampled binary thresholded edge images."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47814225"
                        ],
                        "name": "Shi-Hong Jeng",
                        "slug": "Shi-Hong-Jeng",
                        "structuredName": {
                            "firstName": "Shi-Hong",
                            "lastName": "Jeng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shi-Hong Jeng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704678"
                        ],
                        "name": "H. Liao",
                        "slug": "H.-Liao",
                        "structuredName": {
                            "firstName": "Hong-Yuan",
                            "lastName": "Liao",
                            "middleNames": [
                                "Mark"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Liao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3203056"
                        ],
                        "name": "Chin-Chuan Han",
                        "slug": "Chin-Chuan-Han",
                        "structuredName": {
                            "firstName": "Chin-Chuan",
                            "lastName": "Han",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chin-Chuan Han"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40037860"
                        ],
                        "name": "M. Chern",
                        "slug": "M.-Chern",
                        "structuredName": {
                            "firstName": "Ming-Yang",
                            "lastName": "Chern",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Chern"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108924004"
                        ],
                        "name": "Y. Liu",
                        "slug": "Y.-Liu",
                        "structuredName": {
                            "firstName": "Yao",
                            "lastName": "Liu",
                            "middleNames": [
                                "Tsorng"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Liu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[78] propose a system for face and facial feature detection which is also based on anthropometric measures."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 42126886,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b60030b334f9fae4f2c8744e024898ec83dc0e59",
            "isKey": false,
            "numCitedBy": 196,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Facial-feature-detection-using-geometrical-face-An-Jeng-Liao",
            "title": {
                "fragments": [],
                "text": "Facial feature detection using geometrical face model: An efficient approach"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145328018"
                        ],
                        "name": "M. Oren",
                        "slug": "M.-Oren",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Oren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Oren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145030811"
                        ],
                        "name": "C. Papageorgiou",
                        "slug": "C.-Papageorgiou",
                        "structuredName": {
                            "firstName": "Constantine",
                            "lastName": "Papageorgiou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Papageorgiou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46597039"
                        ],
                        "name": "P. Sinha",
                        "slug": "P.-Sinha",
                        "structuredName": {
                            "firstName": "Pawan",
                            "lastName": "Sinha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Sinha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781874"
                        ],
                        "name": "E. Osuna",
                        "slug": "E.-Osuna",
                        "structuredName": {
                            "firstName": "Edgar",
                            "lastName": "Osuna",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Osuna"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7967646,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bd0534a87e09b3d64b7e7462e2684c60c9aca1f5",
            "isKey": false,
            "numCitedBy": 837,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a trainable object detection architecture that is applied to detecting people in static images of cluttered scenes. This problem poses several challenges. People are highly non-rigid objects with a high degree of variability in size, shape, color, and texture. Unlike previous approaches, this system learns from examples and does not rely on any a priori (hand-crafted) models or on motion. The detection technique is based on the novel idea of the wavelet template that defines the shape of an object in terms of a subset of the wavelet coefficients of the image. It is invariant to changes in color and texture and can be used to robustly define a rich and complex class of objects such as people. We show how the invariant properties and computational efficiency of the wavelet template make it an effective tool for object detection."
            },
            "slug": "Pedestrian-detection-using-wavelet-templates-Oren-Papageorgiou",
            "title": {
                "fragments": [],
                "text": "Pedestrian detection using wavelet templates"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "This paper presents a trainable object detection architecture that is applied to detecting people in static images of cluttered scenes and shows how the invariant properties and computational efficiency of the wavelet template make it an effective tool for object detection."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2124866301"
                        ],
                        "name": "A. Mohan",
                        "slug": "A.-Mohan",
                        "structuredName": {
                            "firstName": "Anuj",
                            "lastName": "Mohan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Mohan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145030811"
                        ],
                        "name": "C. Papageorgiou",
                        "slug": "C.-Papageorgiou",
                        "structuredName": {
                            "firstName": "Constantine",
                            "lastName": "Papageorgiou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Papageorgiou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2559322,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "448bd4e124175ad358078a7b930ecad994c97812",
            "isKey": false,
            "numCitedBy": 1137,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a general example-based framework for detecting objects in static images by components. The technique is demonstrated by developing a system that locates people in cluttered scenes. The system is structured with four distinct example-based detectors that are trained to separately find the four components of the human body: the head, legs, left arm, and right arm. After ensuring that these components are present in the proper geometric configuration, a second example-based classifier combines the results of the component detectors to classify a pattern as either a \"person\" or a \"nonperson.\" We call this type of hierarchical architecture, in which learning occurs at multiple stages, an adaptive combination of classifiers (ACC). We present results that show that this system performs significantly better than a similar full-body person detector. This suggests that the improvement in performance is due to the component-based approach and the ACC data classification architecture. The algorithm is also more robust than the full-body person detection method in that it is capable of locating partially occluded views of people and people whose body parts have little contrast with the background."
            },
            "slug": "Example-Based-Object-Detection-in-Images-by-Mohan-Papageorgiou",
            "title": {
                "fragments": [],
                "text": "Example-Based Object Detection in Images by Components"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Results suggest that the improvement in performance is due to the component-based approach and the ACC data classification architecture, which is capable of locating partially occluded views of people and people whose body parts have little contrast with the background."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145558170"
                        ],
                        "name": "S. Dass",
                        "slug": "S.-Dass",
                        "structuredName": {
                            "firstName": "Sarat",
                            "lastName": "Dass",
                            "middleNames": [
                                "Chandra"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dass"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31598140"
                        ],
                        "name": "Xiaoguang Lu",
                        "slug": "Xiaoguang-Lu",
                        "structuredName": {
                            "firstName": "Xiaoguang",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaoguang Lu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 176930,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a534c4ab79520789830164c7dabfe6b4fd1f30d1",
            "isKey": false,
            "numCitedBy": 14,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Markov random fields (MRFs) are proposed as viable stochastic models for the spatial distribution of gray levels for images of human faces. These models are trained using data bases of face and non-face images. The trained MRF models are then used for detecting human faces in test images. We investigate the performance of the face detection algorithm for two classes of MRFs given by the first- and second-order neighborhood systems. From the cross validation results and from actual detection in real images, it is shown that the second-order model makes fewer false detections. We also investigate the possibility of increasing our training data base of faces by simulating face-like images from the trained MRFs. The performance of the re-trained MRFs based on added face-like images is compared to the original training data base."
            },
            "slug": "Face-detection-and-synthesis-using-Markov-random-Dass-Jain",
            "title": {
                "fragments": [],
                "text": "Face detection and synthesis using Markov random field models"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work investigates the performance of the face detection algorithm for two classes of MRFs given by the first- and second-order neighborhood systems and investigates the possibility of increasing the training data base of faces by simulating face-like images from the trained MRFs."
            },
            "venue": {
                "fragments": [],
                "text": "Object recognition supported by user interaction for service robots"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2568478"
                        ],
                        "name": "H. Hongo",
                        "slug": "H.-Hongo",
                        "structuredName": {
                            "firstName": "Hitoshi",
                            "lastName": "Hongo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hongo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3139527"
                        ],
                        "name": "M. Yasumoto",
                        "slug": "M.-Yasumoto",
                        "structuredName": {
                            "firstName": "Mamoru",
                            "lastName": "Yasumoto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Yasumoto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1778444"
                        ],
                        "name": "Y. Niwa",
                        "slug": "Y.-Niwa",
                        "structuredName": {
                            "firstName": "Yoshinori",
                            "lastName": "Niwa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Niwa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2585057"
                        ],
                        "name": "M. Ohya",
                        "slug": "M.-Ohya",
                        "structuredName": {
                            "firstName": "Mitsunori",
                            "lastName": "Ohya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Ohya"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143686714"
                        ],
                        "name": "Kazuhiko Yamamoto",
                        "slug": "Kazuhiko-Yamamoto",
                        "structuredName": {
                            "firstName": "Kazuhiko",
                            "lastName": "Yamamoto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kazuhiko Yamamoto"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 179
                            }
                        ],
                        "text": "Other color models applied to face detection include HSV [48, 60, 81, 216], YES [160], YCrCb [2, 48, 84, 130, 191, 200], YUV [1, 123], CIE-xyz [15], L\u2217 a\u2217 b\u2217 [13, 109], L\u2217 u\u2217 v \u2217 [63], CSN (a modified rg representation) [90, 91] and UCS/Farnsworth (a perceptually uniform color system was proposed by Farnsworth [210]) [208]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 143
                            }
                        ],
                        "text": "Color segmentation can basically be performed using appropriate skin color thresholds where skin color is modeled through histograms or charts [13, 63, 88, 118, 178, 207, 220]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 27755976,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5abf53b41988443c1279624932d1526144a5cd88",
            "isKey": false,
            "numCitedBy": 46,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a multi-camera system that can track multiple human faces and hands as well as focus on face and hand gestures for recognition. Our current system consists of four cameras. Two fixed cameras are used as a stereo system to estimate face and hand positions. The stereo camera detects faces and hands by a standard skin color method we propose. The distances of the targets are then estimated. Next to track multiple targets, we estimate the positions and sizes of targets between consecutive frames. The other two cameras perform tracking of such targets as faces and hands. If a target is not the appropriate size for recognition, the tracking cameras acquire its zoomed image. Since our system has two tracking cameras, it can track two targets at the same time. To recognize faces and hand gestures, we propose four directional features by using linear discriminant analysis. Using our system, we experimented on human position estimation, multiple face tracking, and face and hand gesture recognition. These experiments showed that our system could estimate human position with the stereo camera and track multiple targets by using target positions and sizes even if the persons overlapped with each other. In addition, our system could recognize faces and hand gestures by using the four directional features."
            },
            "slug": "Focus-of-attention-for-face-and-hand-gesture-using-Hongo-Yasumoto",
            "title": {
                "fragments": [],
                "text": "Focus of attention for face and hand gesture recognition using multiple cameras"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "A multi-camera system that can track multiple human faces and hands as well as focus on face and hand gestures for recognition and four directional features by using linear discriminant analysis are proposed."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3041256"
                        ],
                        "name": "F. Luthon",
                        "slug": "F.-Luthon",
                        "structuredName": {
                            "firstName": "Franck",
                            "lastName": "Luthon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Luthon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3254828"
                        ],
                        "name": "M. Li\u00e9vin",
                        "slug": "M.-Li\u00e9vin",
                        "structuredName": {
                            "firstName": "Marc",
                            "lastName": "Li\u00e9vin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Li\u00e9vin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14848229,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e8437d913419ccfa171e4bef8da58702fa8c36c3",
            "isKey": false,
            "numCitedBy": 10,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "An algorithm for speaker's lip motion detection is presented, based on the processing of a colour video sequence of speaker's face under natural lighting conditions and without any particular make-up. It is intended for applications in speech recognition, videoconferencing or speaker's face synthesis and animation. The algorithm is based on a statistical approach using Markov Random Field (MRF) modelling, with a spatiotemporal neighbourhood of the pixels in the image sequence. Two kinds of observations are used : the temporal difference between successive images (motion information) and the purity of red hue in the current and past images (spatial information about lip location). The field of hidden labels, relevant for lip motion detection, is obtained by energy minimisation and proves to be robust to lighting conditions (shadows). This label field is used to extract qualitative information (mouth opening and closing) but also quantitative information by measuring some geometrical features (horizontal and vertical lip spacing) directly on the label field."
            },
            "slug": "Lip-motion-automatic-detection-Luthon-Li\u00e9vin",
            "title": {
                "fragments": [],
                "text": "Lip motion automatic detection"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "An algorithm for speaker's lip motion detection is presented, based on the processing of a colour video sequence of speaker's face under natural lighting conditions and without any particular make-up, intended for applications in speech recognition, videoconferencing or speaker'sface synthesis and animation."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1830709"
                        ],
                        "name": "A. Lanitis",
                        "slug": "A.-Lanitis",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Lanitis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Lanitis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144482985"
                        ],
                        "name": "C. Taylor",
                        "slug": "C.-Taylor",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Taylor",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7205190"
                        ],
                        "name": "Tim Cootes",
                        "slug": "Tim-Cootes",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Cootes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tim Cootes"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 3
                            }
                        ],
                        "text": "In [105], it is implemented as a generic representation for several applications such as"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 699530,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6b7e722de154fa30da9d534a935ed7c5dc233ff9",
            "isKey": false,
            "numCitedBy": 724,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "Face images are difficult to interpret because they are highly variable. Sources of variability include individual appearance, 3D pose, facial expression, and lighting. We describe a compact parametrized model of facial appearance which takes into account all these sources of variability. The model represents both shape and gray-level appearance, and is created by performing a statistical analysis over a training set of face images. A robust multiresolution search algorithm is used to fit the model to faces in new images. This allows the main facial features to be located, and a set of shape, and gray-level appearance parameters to be recovered. A good approximation to a given face can be reconstructed using less than 100 of these parameters. This representation can be used for tasks such as image coding, person identification, 3D pose recovery, gender recognition, and expression recognition. Experimental results are presented for a database of 690 face images obtained under widely varying conditions of 3D pose, lighting, and facial expression. The system performs well on all the tasks listed above."
            },
            "slug": "Automatic-Interpretation-and-Coding-of-Face-Images-Lanitis-Taylor",
            "title": {
                "fragments": [],
                "text": "Automatic Interpretation and Coding of Face Images Using Flexible Models"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A compact parametrized model of facial appearance which takes into account all sources of variability and can be used for tasks such as image coding, person identification, 3D pose recovery, gender recognition, and expression recognition is described."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145235303"
                        ],
                        "name": "Jun Miao",
                        "slug": "Jun-Miao",
                        "structuredName": {
                            "firstName": "Jun",
                            "lastName": "Miao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jun Miao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714354"
                        ],
                        "name": "Baocai Yin",
                        "slug": "Baocai-Yin",
                        "structuredName": {
                            "firstName": "Baocai",
                            "lastName": "Yin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Baocai Yin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2641821"
                        ],
                        "name": "Kongqiao Wang",
                        "slug": "Kongqiao-Wang",
                        "structuredName": {
                            "firstName": "Kongqiao",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kongqiao Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1808618"
                        ],
                        "name": "Lansun Shen",
                        "slug": "Lansun-Shen",
                        "structuredName": {
                            "firstName": "Lansun",
                            "lastName": "Shen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lansun Shen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46772962"
                        ],
                        "name": "Xuecun Chen",
                        "slug": "Xuecun-Chen",
                        "structuredName": {
                            "firstName": "Xuecun",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xuecun Chen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6464488,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4db205eb15a72c177438275746eebeaabf7e6b1a",
            "isKey": false,
            "numCitedBy": 135,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-hierarchical-multiscale-and-multiangle-system-for-Miao-Yin",
            "title": {
                "fragments": [],
                "text": "A hierarchical multiscale and multiangle system for human face detection in a complex background using gravity-center template"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2122612050"
                        ],
                        "name": "B.D. Zarit",
                        "slug": "B.D.-Zarit",
                        "structuredName": {
                            "firstName": "B.D.",
                            "lastName": "Zarit",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B.D. Zarit"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3221174"
                        ],
                        "name": "B. Super",
                        "slug": "B.-Super",
                        "structuredName": {
                            "firstName": "Boaz",
                            "lastName": "Super",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Super"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69019774"
                        ],
                        "name": "Freddie Quek",
                        "slug": "Freddie-Quek",
                        "structuredName": {
                            "firstName": "Freddie",
                            "lastName": "Quek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Freddie Quek"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2773627,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "46ac61c176d736860dd6987a5bdae62f58bdfe25",
            "isKey": false,
            "numCitedBy": 433,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Detection of skin in video is an important component of systems for detecting, recognizing, and tracking faces and hands. Different skin detection methods have used different color spaces. This paper presents a comparative evaluation of pixel classification performance of two skin detection methods in five color spaces. The skin detection methods used in this paper are color-histogram based approaches that are intended to work with a wide variety of individuals, lighting conditions, and skin tones. One is the widely-used lookup table method, the other makes use of Bayesian decision theory. Two types of enhancements, based on spatial and texture analyses, are also evaluated."
            },
            "slug": "Comparison-of-five-color-models-in-skin-pixel-Zarit-Super",
            "title": {
                "fragments": [],
                "text": "Comparison of five color models in skin pixel classification"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A comparative evaluation of pixel classification performance of two skin detection methods in five color spaces using the widely-used lookup table method and the other makes use of Bayesian decision theory are presented."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings International Workshop on Recognition, Analysis, and Tracking of Faces and Gestures in Real-Time Systems. In Conjunction with ICCV'99 (Cat. No.PR00378)"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2780543"
                        ],
                        "name": "Lingmin Meng",
                        "slug": "Lingmin-Meng",
                        "structuredName": {
                            "firstName": "Lingmin",
                            "lastName": "Meng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lingmin Meng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143803197"
                        ],
                        "name": "Truong Q. Nguyen",
                        "slug": "Truong-Q.-Nguyen",
                        "structuredName": {
                            "firstName": "Truong",
                            "lastName": "Nguyen",
                            "middleNames": [
                                "Q."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Truong Q. Nguyen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752462"
                        ],
                        "name": "D. Casta\u00f1\u00f3n",
                        "slug": "D.-Casta\u00f1\u00f3n",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Casta\u00f1\u00f3n",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Casta\u00f1\u00f3n"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 65
                            }
                        ],
                        "text": "Hidden Markov models have also been applied to face detection in [121, 129]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18537457,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c7fc1a9dd3c0b2653b0c9ff668cafaff7670da92",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present a novel approach for frontal face detection in gray-scale images. We represent both faces and clutter by using two-dimensional wavelet decomposition. To characterize the statistical dependency between different levels of wavelet, we introduce a Hidden Markov Model (HMM), in which a number of discrete states at each level capture the diversity of faces as well as clutter. Our experiments indicate that the proposed algorithm outperforms conventional template-based methods such as matched filter and eigenface methods."
            },
            "slug": "An-image-based-Bayesian-framework-for-face-Meng-Nguyen",
            "title": {
                "fragments": [],
                "text": "An image-based Bayesian framework for face detection"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "A novel approach for frontal face detection in gray-scale images using a Hidden Markov Model (HMM), in which a number of discrete states at each level capture the diversity of faces as well as clutter."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings IEEE Conference on Computer Vision and Pattern Recognition. CVPR 2000 (Cat. No.PR00662)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792053"
                        ],
                        "name": "B. Scassellati",
                        "slug": "B.-Scassellati",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Scassellati",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Scassellati"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 36
                            }
                        ],
                        "text": "Contemporary research in this field [7, 35, 98, 168, 174] has mainly concentrated on issues such as execution time reductions, template modifications, and energy term considerations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7383841,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2699c31109e6ac262a4bf2f4d832646ed3834656",
            "isKey": false,
            "numCitedBy": 98,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Eye finding is the first step toward building a machine that can recognize social cues, like eye contact and gaze direction, in a natural context. In this paper, we present a real-time implementation of an eye finding algorithm for a foveated active vision system. The system uses a motion-based prefilter to identify potential face locations. These locations are analyzed for faces with a template-based algorithm developed by Sinha (1996). Detected faces are tracked in real time, and the active vision system saccades to the face using a learned sensorimotor mapping. Once gaze has been centered on the face, a high-resolution image of the eye can be captured from the foveal camera using a self-calibrated peripheral-ta-foveal mapping.We also present a performance analysis of Sinha's ratio template algorithm on a standard set of static face images. Although this algorithm performs relatively poorly on static images, this result is a poor indicator of real-time performance of the behaving system. We find that our system finds eyes in 94% of a set of behavioral trials. We suggest that alternate means of evaluating behavioral systems are necessary."
            },
            "slug": "Eye-Finding-via-Face-Detection-for-a-Foveated-Scassellati",
            "title": {
                "fragments": [],
                "text": "Eye Finding via Face Detection for a Foveated Active Vision System"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper presents a real-time implementation of an eye finding algorithm for a foveated active vision system, and finds that the system finds eyes in 94% of a set of behavioral trials, suggesting that alternate means of evaluating behavioral systems are necessary."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI/IAAI"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144097660"
                        ],
                        "name": "M. Turk",
                        "slug": "M.-Turk",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Turk",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Turk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144994682"
                        ],
                        "name": "A. Pentland",
                        "slug": "A.-Pentland",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Pentland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pentland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 26127529,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a6f1dfcc44277d4cfd8507284d994c9283dc3a2f",
            "isKey": false,
            "numCitedBy": 14955,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We have developed a near-real-time computer system that can locate and track a subject's head, and then recognize the person by comparing characteristics of the face to those of known individuals. The computational approach taken in this system is motivated by both physiology and information theory, as well as by the practical requirements of near-real-time performance and accuracy. Our approach treats the face recognition problem as an intrinsically two-dimensional (2-D) recognition problem rather than requiring recovery of three-dimensional geometry, taking advantage of the fact that faces are normally upright and thus may be described by a small set of 2-D characteristic views. The system functions by projecting face images onto a feature space that spans the significant variations among known face images. The significant features are known as \"eigenfaces,\" because they are the eigenvectors (principal components) of the set of faces; they do not necessarily correspond to features such as eyes, ears, and noses. The projection operation characterizes an individual face by a weighted sum of the eigenface features, and so to recognize a particular face it is necessary only to compare these weights to those of known individuals. Some particular advantages of our approach are that it provides for the ability to learn and later recognize new faces in an unsupervised manner, and that it is easy to implement using a neural network architecture."
            },
            "slug": "Eigenfaces-for-Recognition-Turk-Pentland",
            "title": {
                "fragments": [],
                "text": "Eigenfaces for Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "A near-real-time computer system that can locate and track a subject's head, and then recognize the person by comparing characteristics of the face to those of known individuals, and that is easy to implement using a neural network architecture."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Cognitive Neuroscience"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145691986"
                        ],
                        "name": "P. Phillips",
                        "slug": "P.-Phillips",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Phillips",
                            "middleNames": [
                                "Jonathon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Phillips"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143979395"
                        ],
                        "name": "H. Wechsler",
                        "slug": "H.-Wechsler",
                        "structuredName": {
                            "firstName": "Harry",
                            "lastName": "Wechsler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Wechsler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152292143"
                        ],
                        "name": "Jeffrey R. Huang",
                        "slug": "Jeffrey-R.-Huang",
                        "structuredName": {
                            "firstName": "Jeffrey R.",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeffrey R. Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3313513"
                        ],
                        "name": "Patrick J. Rauss",
                        "slug": "Patrick-J.-Rauss",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Rauss",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Patrick J. Rauss"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17779599,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dc8b25e35a3acb812beb499844734081722319b4",
            "isKey": false,
            "numCitedBy": 2398,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-FERET-database-and-evaluation-procedure-for-Phillips-Wechsler",
            "title": {
                "fragments": [],
                "text": "The FERET database and evaluation procedure for face-recognition algorithms"
            },
            "venue": {
                "fragments": [],
                "text": "Image Vis. Comput."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2060131247"
                        ],
                        "name": "N. Oliver",
                        "slug": "N.-Oliver",
                        "structuredName": {
                            "firstName": "N.",
                            "lastName": "Oliver",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Oliver"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144994682"
                        ],
                        "name": "A. Pentland",
                        "slug": "A.-Pentland",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Pentland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pentland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "146688280"
                        ],
                        "name": "F. B\u00e9rard",
                        "slug": "F.-B\u00e9rard",
                        "structuredName": {
                            "firstName": "Fran\u00e7ois",
                            "lastName": "B\u00e9rard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. B\u00e9rard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4610650,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b09a0d996ffb72b2529ac06333e600ea15c9d50d",
            "isKey": false,
            "numCitedBy": 159,
            "numCiting": 80,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "LAFTER:-a-real-time-face-and-lips-tracker-with-Oliver-Pentland",
            "title": {
                "fragments": [],
                "text": "LAFTER: a real-time face and lips tracker with facial expression recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118579343"
                        ],
                        "name": "Jie Yang",
                        "slug": "Jie-Yang",
                        "structuredName": {
                            "firstName": "Jie",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jie Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724972"
                        ],
                        "name": "A. Waibel",
                        "slug": "A.-Waibel",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Waibel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Waibel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 92
                            }
                        ],
                        "text": "Since the main variation in skin appearance is largely due to luminance change (brightness) [215], normalized RGB colors are generally preferred [27, 53, 75, 88, 92, 165, 181, 196, 202, 207, 213, 215], so that the effect of luminance can be filtered out."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 36
                            }
                        ],
                        "text": "For instance, Oliveret al.[139] and Yang and Waibel [215] employ a Gaussian distribution to represent a skin color cluster of thousands of skin color samples taken from difference races."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 73
                            }
                        ],
                        "text": "Examples of such a learning approach have been used by Oliver et al. and Yang and Waibel according to Eq."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 55
                            }
                        ],
                        "text": "P. Duchnowski, M. Hunke, D. Busching, U. Meier, and A. Waibel, Toward movement invariant automatic lip-reading and speech recognition, inProc."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 15
                            }
                        ],
                        "text": "J. Yang and A. Waibel, A real-time face tracker, inIEEE Proc. of the 3rd Workshop on Applications of Computer Vision, Florida, 1996."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 16
                            }
                        ],
                        "text": "M. Hunke and A. Waibel, Face locating and tracking for human-computer interaction, in28th Asilomar Conference on Signals, Systems and Computers, Monterey, CA, 1994."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 142
                            }
                        ],
                        "text": "It was found that different human skin color gives rise to a tight cluster in color spaces even when faces of difference races are considered [75, 125, 215]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 108
                            }
                        ],
                        "text": "More complex methods make use of statistical measures that model face variation within a wide user spectrum [3, 27, 75, 125, 139, 215]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 42
                            }
                        ],
                        "text": "occupies a small cluster in the histogram [215]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 26
                            }
                        ],
                        "text": "[139] and Yang and Waibel [215] employ a Gaussian distribution to represent a skin color cluster of thousands of skin color samples taken from difference races."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1154755,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "002aaf4412f91d0828b79511f35c0863a1a32c47",
            "isKey": true,
            "numCitedBy": 656,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors present a real-time face tracker. The system has achieved a rate of 30+ frames/second using an HP-9000 workstation with a frame grabber and a Canon VC-Cl camera. It can track a person's face while the person moves freely (e.g., walks, jumps, sits down and stands up) in a room. Three types of models have been employed in developing the system. First, they present a stochastic model to characterize skin color distributions of human faces. The information provided by the model is sufficient for tracking a human face in various poses and views. This model is adaptable to different people and different lighting conditions in real-time. Second, a motion model is used to estimate image motion and to predict the search window. Third, a camera model is used to predict and compensate for camera motion. The system can be applied to teleconferencing and many HCI applications including lip reading and gaze tracking. The principle in developing this system can be extended to other tracking problems such as tracking the human hand."
            },
            "slug": "A-real-time-face-tracker-Yang-Waibel",
            "title": {
                "fragments": [],
                "text": "A real-time face tracker"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "The authors present a real-time face tracker that can track a person's face while the person moves freely in a room and can be applied to teleconferencing and many HCI applications including lip reading and gaze tracking."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Third IEEE Workshop on Applications of Computer Vision. WACV'96"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110304062"
                        ],
                        "name": "Masaru Tanaka",
                        "slug": "Masaru-Tanaka",
                        "structuredName": {
                            "firstName": "Masaru",
                            "lastName": "Tanaka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Masaru Tanaka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2510362"
                        ],
                        "name": "K. Hotta",
                        "slug": "K.-Hotta",
                        "structuredName": {
                            "firstName": "Kazuhiro",
                            "lastName": "Hotta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Hotta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35797142"
                        ],
                        "name": "T. Mishima",
                        "slug": "T.-Mishima",
                        "structuredName": {
                            "firstName": "Taketoshi",
                            "lastName": "Mishima",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Mishima"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145375983"
                        ],
                        "name": "T. Kurita",
                        "slug": "T.-Kurita",
                        "structuredName": {
                            "firstName": "Takio",
                            "lastName": "Kurita",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kurita"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 59
                            }
                        ],
                        "text": "Other face detection approaches using LDA include [65] and [184], while SOMs have been used in [183]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14233869,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2996944354dcb1dea117b5b9d6938584dfcb1111",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method to narrow down the search space for scale-invariant human face detection, which uses a dynamic attention map implemented by Ising dynamics. Combining the proposed method and the scale-invariant face detection method which is based on both higher-order local autocorrelation (HLAC) features of a log-polar image and linear discriminant analysis for \"face\" and \"not face\" classification, it is shown that the \"face\" region in the image can be detected faster with some experiments."
            },
            "slug": "Dynamic-attention-map-by-Ising-model-for-human-face-Tanaka-Hotta",
            "title": {
                "fragments": [],
                "text": "Dynamic attention map by Ising model for human face detection"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "A method to narrow down the search space for scale-invariant human face detection is presented, which uses a dynamic attention map implemented by Ising dynamics and it is shown that the \"face\" region in the image can be detected faster with some experiments."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. Fourteenth International Conference on Pattern Recognition (Cat. No.98EX170)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731948"
                        ],
                        "name": "Paul A. Viola",
                        "slug": "Paul-A.-Viola",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Viola",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paul A. Viola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145319478"
                        ],
                        "name": "Michael J. Jones",
                        "slug": "Michael-J.-Jones",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jones",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Jones"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6141636,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b1a8b66a5ea2c3431911e220499c6481bb4231c3",
            "isKey": false,
            "numCitedBy": 567,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper develops a new approach for extremely fast detection in domains where the distribution of positive and negative examples is highly skewed (e.g. face detection or database retrieval). In such domains a cascade of simple classifiers each trained to achieve high detection rates and modest false positive rates can yield a final detector with many desirable features: including high detection rates, very low false positive rates, and fast performance. Achieving extremely high detection rates, rather than low error, is not a task typically addressed by machine learning algorithms. We propose a new variant of AdaBoost as a mechanism for training the simple classifiers used in the cascade. Experimental results in the domain of face detection show the training algorithm yields significant improvements in performance over conventional AdaBoost. The final face detection system can process 15 frames per second, achieves over 90% detection, and a false positive rate of 1 in a 1,000,000."
            },
            "slug": "Fast-and-Robust-Classification-using-Asymmetric-and-Viola-Jones",
            "title": {
                "fragments": [],
                "text": "Fast and Robust Classification using Asymmetric AdaBoost and a Detector Cascade"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "A new variant of AdaBoost is proposed as a mechanism for training the simple classifiers used in the cascade in domains where the distribution of positive and negative examples is highly skewed (e.g. face detection or database retrieval)."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33438590"
                        ],
                        "name": "D. Keren",
                        "slug": "D.-Keren",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Keren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Keren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700737"
                        ],
                        "name": "Margarita Osadchy",
                        "slug": "Margarita-Osadchy",
                        "structuredName": {
                            "firstName": "Margarita",
                            "lastName": "Osadchy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Margarita Osadchy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724072"
                        ],
                        "name": "C. Gotsman",
                        "slug": "C.-Gotsman",
                        "structuredName": {
                            "firstName": "Craig",
                            "lastName": "Gotsman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Gotsman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5736057,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "666118b4623eeb119f619f1cc77a23e8b0a87fcc",
            "isKey": false,
            "numCitedBy": 74,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper offers a novel detection method, which works well even in the case of a complicated image collection. It can also be applied to detect 3D objects under different views. The detection problem is solved by sequentially applying very simple filters (or detectors), which are designed to yield small results on the multitemplate (hence antifaces), and large results on \"random\" natural images. This is achieved by making use of a simple probabilistic assumption on the distribution of natural images, which is borne out well in practice. Only images which passed the threshold test imposed by the first detector are examined by the second detector, etc. The detectors are designed to act independently so that their false alarms are uncorrelated; this results in a false alarm rate which decreases exponentially in the number of detectors. The algorithm's performance compares favorably to the well-known eigenface and support vector machine based algorithms, but is substantially faster."
            },
            "slug": "Antifaces:-A-Novel,-Fast-Method-for-Image-Detection-Keren-Osadchy",
            "title": {
                "fragments": [],
                "text": "Antifaces: A Novel, Fast Method for Image Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A novel detection method, which works well even in the case of a complicated image collection, and can be applied to detect 3D objects under different views, which compares favorably to the well-known eigenface and support vector machine based algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "73728038"
                        ],
                        "name": "G. Holst",
                        "slug": "G.-Holst",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Holst",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Holst"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 62
                            }
                        ],
                        "text": "The simplest image-based approaches rely on template matching [62, 114], but these approaches do not perform as well as the more complex techniques presented in the following sections."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 987987,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2b8c048bf7becfe6321c652cdd387fb2773fd9fd",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "New techniques are needed to effectively search the image and feature space of larger and more complex domains. One such technique uses subfeatures and spatial models to represent a compound object, such as a face. From these compound models, hypothesis based search then combines bottom-up and top-down search processes to localize the search within the image and feature space. Detected subfeatures become evidence for facial hypotheses, which then guide local searches for the remaining subfeatures, based upon the expected facial configuration. We describe this compound technique and present a comparison of the compound templates technique with a single template technique in a mug shot style face domain. Attention is paid to performance, including both efficiency and accuracy. The results are complex, and the strengths, weaknesses, and various trade-offs of the two techniques are detailed."
            },
            "slug": "Face-detection-by-facets:-combined-bottom-up-and-Holst",
            "title": {
                "fragments": [],
                "text": "Face detection by facets: combined bottom-up and top-down search using compound templates"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work describes this compound technique and presents a comparison of the compound templates technique with a single template technique in a mug shot style face domain, and the strengths, weaknesses, and various trade-offs of the two techniques are detailed."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 2000 International Conference on Image Processing (Cat. No.00CH37101)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2127304"
                        ],
                        "name": "D. Pramadihanto",
                        "slug": "D.-Pramadihanto",
                        "structuredName": {
                            "firstName": "Dadet",
                            "lastName": "Pramadihanto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Pramadihanto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805337"
                        ],
                        "name": "Y. Iwai",
                        "slug": "Y.-Iwai",
                        "structuredName": {
                            "firstName": "Yoshio",
                            "lastName": "Iwai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Iwai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735941"
                        ],
                        "name": "M. Yachida",
                        "slug": "M.-Yachida",
                        "structuredName": {
                            "firstName": "Masahiko",
                            "lastName": "Yachida",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Yachida"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 79
                            }
                        ],
                        "text": "Gabor responses have also been applied to face and facial feature detection in [18, 56, 66, 146]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12364468,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aa8562df9e16d8e7ca2dd5915ed7886397f21460",
            "isKey": false,
            "numCitedBy": 10,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "An automatic face and facial feature points (FFPs) detection system is proposed. A face is represented as a graph where the nodes are placed at facial feature points (FFPs) labeled by their Gabor features and the edges describe their spatial relations. An innovative flexible feature matching is proposed to perform features correspondence between models and the input image. This matching model works like a random diffusion process in the image space by employing the locally competitive and globally corporative mechanism. The system works nicely on the face images under complicated background, pose variations and distorted by facial accessories. We demonstrate the benefits of our approach by its implementation on a face identification system."
            },
            "slug": "A-flexible-feature-matching-for-automatic-face-and-Pramadihanto-Iwai",
            "title": {
                "fragments": [],
                "text": "A flexible feature matching for automatic face and facial feature points detection"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "An automatic face and facial feature points (FFPs) detection system that works nicely on the face images under complicated background, pose variations and distorted by facial accessories is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. Fourteenth International Conference on Pattern Recognition (Cat. No.98EX170)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112413023"
                        ],
                        "name": "Xiao-guang Lv",
                        "slug": "Xiao-guang-Lv",
                        "structuredName": {
                            "firstName": "Xiao-guang",
                            "lastName": "Lv",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiao-guang Lv"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49640256"
                        ],
                        "name": "Jie Zhou",
                        "slug": "Jie-Zhou",
                        "structuredName": {
                            "firstName": "Jie",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jie Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "14966740"
                        ],
                        "name": "Changshui Zhang",
                        "slug": "Changshui-Zhang",
                        "structuredName": {
                            "firstName": "Changshui",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Changshui Zhang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "[119] and an extension of the algorithm is presented in Kotropoulos and Pitas [95]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 44304704,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "92b7b9f77bcd9eb8a57ba346f09ba6093cea2cd3",
            "isKey": false,
            "numCitedBy": 27,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a framework to detect human faces rotated within the image plane. A novel orientation histogram of an image is constructed using local orientation analysis. From the view of an oriented pattern, a histogram of the human face shows symmetry with respect to the orientation, /spl beta/, of the principal axis, and a local peak also occurs at the orientation orthogonal to /spl beta/. We analyze the histogram to find the orientation of the face principal axis, with which conventional upright face detectors can be used to verify the supposed rotated face. Experimental results show that the algorithm is effective."
            },
            "slug": "A-novel-algorithm-for-rotated-human-face-detection-Lv-Zhou",
            "title": {
                "fragments": [],
                "text": "A novel algorithm for rotated human face detection"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "A novel orientation histogram of an image is constructed using local orientation analysis to find the orientation of the face principal axis, with which conventional upright face detectors can be used to verify the supposed rotated face."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings IEEE Conference on Computer Vision and Pattern Recognition. CVPR 2000 (Cat. No.PR00662)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2510362"
                        ],
                        "name": "K. Hotta",
                        "slug": "K.-Hotta",
                        "structuredName": {
                            "firstName": "Kazuhiro",
                            "lastName": "Hotta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Hotta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145375983"
                        ],
                        "name": "T. Kurita",
                        "slug": "T.-Kurita",
                        "structuredName": {
                            "firstName": "Takio",
                            "lastName": "Kurita",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kurita"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35797142"
                        ],
                        "name": "T. Mishima",
                        "slug": "T.-Mishima",
                        "structuredName": {
                            "firstName": "Taketoshi",
                            "lastName": "Mishima",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Mishima"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 50
                            }
                        ],
                        "text": "Other face detection approaches using LDA include [65] and [184], while SOMs have been used in [183]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12233931,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "680ca8763560e93db88c9b8e74780ec2ccd78c0c",
            "isKey": false,
            "numCitedBy": 77,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a scale invariant face detection method which combines higher-order local autocorrelation (HLAC) features extracted from a log-polar transformed image with linear discriminant analysis for \"face\" and \"not face\" classification. Since HLAC features of log-polar images are sensitive to shifts of a face, we utilize this property and develop a face detection method. HLAC features extracted from a log-polar image become scale and rotation invariant because scalings and rotations of a face are expressed as shifts in a log-polar image (coordinate). By combining these features with the linear discriminant analysis which is extended to treat \"face\" and \"not face\" classes, a scale invariant face detection system can be realized."
            },
            "slug": "Scale-invariant-face-detection-method-using-local-Hotta-Kurita",
            "title": {
                "fragments": [],
                "text": "Scale invariant face detection method using higher-order local autocorrelation features extracted from log-polar image"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "A scale invariant face detection method which combines higher-order local autocorrelation (HLAC) features extracted from a log-polar transformed image with linear discriminant analysis for \"face\" and \"not face\" classification is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Third IEEE International Conference on Automatic Face and Gesture Recognition"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2372507"
                        ],
                        "name": "S. Clippingdale",
                        "slug": "S.-Clippingdale",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Clippingdale",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Clippingdale"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152883359"
                        ],
                        "name": "Takayuki Ito",
                        "slug": "Takayuki-Ito",
                        "structuredName": {
                            "firstName": "Takayuki",
                            "lastName": "Ito",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Takayuki Ito"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 79
                            }
                        ],
                        "text": "Gabor responses have also been applied to face and facial feature detection in [18, 56, 66, 146]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8513691,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7da809c3d4066d81f9487bfa031cbe09acd55769",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a prototype system which recognizes faces in video by tracking variable parameters such as face pose, size etc., and integrating information about identity over time. The detection, tracking and recognition processes are unified, using multi-resolution deformable template matching in a hypothesis generation and revision framework. We discuss test results and various possibilities for improving performance."
            },
            "slug": "A-unified-approach-to-video-face-detection,-and-Clippingdale-Ito",
            "title": {
                "fragments": [],
                "text": "A unified approach to video face detection, tracking and recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "A prototype system which recognizes faces in video by tracking variable parameters such as face pose, size etc., and integrating information about identity over time is described, using multi-resolution deformable template matching in a hypothesis generation and revision framework."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348)"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144097660"
                        ],
                        "name": "M. Turk",
                        "slug": "M.-Turk",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Turk",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Turk"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16643664,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "04d2cd8fb7ddffab3540b9d9b4804c067b745c2f",
            "isKey": false,
            "numCitedBy": 142,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "SUMMARY It has been over a decade since the \u201cEigenfaces\u201d approach to automatic face recognition, and other appearancebased methods, made an impression on the computer vision research community and helped spur interest in vision systems being used to support biometrics and human-computer interface. In this paper I give a personal viewof the original motivation for the work, some of the strengths and limitation of the approach, and progress in the years since. Appearance-based approaches to recognition complement feature- or shape-based approaches, and a practical face recognition system should have elements of both. Eigenfaces is not a general approach to recognition, but rather one tool out of many to be applied and evaluated in the appropriate context."
            },
            "slug": "A-Random-Walk-through-Eigenspace-Turk",
            "title": {
                "fragments": [],
                "text": "A Random Walk through Eigenspace"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "It has been over a decade since the Eigenfaces approach to automatic face recognition, and other appearancebased methods, made an impression on the computer vision research community and helped spur interest in vision systems being used to support biometrics and human-computer interface."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2228528"
                        ],
                        "name": "J. Terrillon",
                        "slug": "J.-Terrillon",
                        "structuredName": {
                            "firstName": "Jean-Christophe",
                            "lastName": "Terrillon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Terrillon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143906699"
                        ],
                        "name": "H. Fukamachi",
                        "slug": "H.-Fukamachi",
                        "structuredName": {
                            "firstName": "Hideo",
                            "lastName": "Fukamachi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Fukamachi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49052113"
                        ],
                        "name": "S. Akamatsu",
                        "slug": "S.-Akamatsu",
                        "structuredName": {
                            "firstName": "Shigeru",
                            "lastName": "Akamatsu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Akamatsu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707424"
                        ],
                        "name": "M. N. Shirazi",
                        "slug": "M.-N.-Shirazi",
                        "structuredName": {
                            "firstName": "Mahdad",
                            "lastName": "Shirazi",
                            "middleNames": [
                                "Nouri"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. N. Shirazi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 39824480,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "21522ac315abee52a50486ae4ff4c378f66fc337",
            "isKey": false,
            "numCitedBy": 474,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents an analysis of the performance of two different skin chrominance models and of nine different chrominance spaces for the color segmentation and subsequent detection of human faces in two-dimensional static images. For each space, we use the single Gaussian model based on the Mahalanobis metric and a Gaussian mixture density model to segment faces from scene backgrounds. In the case of the mixture density model, the skin chrominance distribution is estimated by use of the expectation-maximisation (EM) algorithm. Feature extraction is performed on the segmented images by use of invariant Fourier-Mellin moments. A multilayer perceptron neural network (NN), with the invariant moments as the input vector, is then applied to distinguish faces from distractors. With the single Gaussian model, normalized color spaces are shown to produce the best segmentation results, and subsequently the highest rate of face detection. The results are comparable to those obtained with the more sophisticated mixture density model. However, the mixture density model improves the segmentation and face detection results significantly for most of the un-normalized color spaces. Ultimately, we show that, for each chrominance space, the detection efficiency depends on the capacity of each model to estimate the skin chrominance distribution and, most importantly, on the discriminability between skin and \"non-skin\" distributions."
            },
            "slug": "Comparative-performance-of-different-skin-models-of-Terrillon-Fukamachi",
            "title": {
                "fragments": [],
                "text": "Comparative performance of different skin chrominance models and chrominance spaces for the automatic detection of human faces in color images"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "An analysis of the performance of two different skin chrominance models and of nine different chrominance spaces for the color segmentation and subsequent detection of human faces in two-dimensional static images shows that, for each chrominance space, the detection efficiency depends on the capacity of each model to estimate the skin Chrominance distribution and, most importantly, on the discriminability between skin and \"non-skin\" distributions."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715634"
                        ],
                        "name": "Ming-Hsuan Yang",
                        "slug": "Ming-Hsuan-Yang",
                        "structuredName": {
                            "firstName": "Ming-Hsuan",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming-Hsuan Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144590225"
                        ],
                        "name": "D. Roth",
                        "slug": "D.-Roth",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Roth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Roth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145237406"
                        ],
                        "name": "N. Ahuja",
                        "slug": "N.-Ahuja",
                        "structuredName": {
                            "firstName": "Narendra",
                            "lastName": "Ahuja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Ahuja"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1709452,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "23234a0f211a44d9706b2570d474427b8f899ec1",
            "isKey": false,
            "numCitedBy": 354,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "A novel learning approach for human face detection using a network of linear units is presented. The SNoW learning architecture is a sparse network of linear functions over a pre-defined or incrementally learned feature space and is specifically tailored for learning in the presence of a very large number of features. A wide range of face images in different poses, with different expressions and under different lighting conditions are used as a training set to capture the variations of human faces. Experimental results on commonly used benchmark data sets of a wide range of face images show that the SNoW-based approach outperforms methods that use neural networks, Bayesian methods, support vector machines and others. Furthermore, learning and evaluation using the SNoW-based method are significantly more efficient than with other methods."
            },
            "slug": "A-SNoW-Based-Face-Detector-Yang-Roth",
            "title": {
                "fragments": [],
                "text": "A SNoW-Based Face Detector"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Experimental results on commonly used benchmark data sets of a wide range of face images show that the SNoW-based approach outperforms methods that use neural networks, Bayesian methods, support vector machines and others."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143736991"
                        ],
                        "name": "L. Yin",
                        "slug": "L.-Yin",
                        "structuredName": {
                            "firstName": "Lijun",
                            "lastName": "Yin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Yin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1684311"
                        ],
                        "name": "A. Basu",
                        "slug": "A.-Basu",
                        "structuredName": {
                            "firstName": "Anup",
                            "lastName": "Basu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Basu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 33430125,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "b9574e90395ea721cd9f190b0c5873dc987cb499",
            "isKey": false,
            "numCitedBy": 46,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Integrating-active-face-tracking-with-model-based-Yin-Basu",
            "title": {
                "fragments": [],
                "text": "Integrating active face tracking with model based coding"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit. Lett."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143797420"
                        ],
                        "name": "Mariano Alvira",
                        "slug": "Mariano-Alvira",
                        "structuredName": {
                            "firstName": "Mariano",
                            "lastName": "Alvira",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mariano Alvira"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "11691394"
                        ],
                        "name": "R. Rifkin",
                        "slug": "R.-Rifkin",
                        "structuredName": {
                            "firstName": "Ryan",
                            "lastName": "Rifkin",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rifkin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7301416,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2c2d6eade1be7dc405eeb44b7f574f77ce5750fc",
            "isKey": false,
            "numCitedBy": 59,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "Impressive claims have been made for the performance of the SNoW algorithm on face detection tasks by Yang et. al. [7]. In particular, by looking at both their results and those of Heisele et. al. [3], one could infer that the SNoW system performed substantially better than an SVM-based system, even when the SVM used a polynomial kernel and the SNoW system used a particularly simplistic \\primitive\" linear representation. We evaluated the two approaches in a controlled experiment, looking directly at performance on a simple, xed-sized test set, isolating out \\infrastructure\" issues related to detecting faces at various scales in large images. We found that SNoW performed about as well as linear SVMs, and substantially worse than polynomial SVMs. Copyright c Massachusetts Institute of Technology, 2001 This report describes research done within the Center for Biological and Computational Learning in the Department of Brain and Cognitive Sciences and at the Arti cial Intelligence Laboratory at the Massachusetts Institute of Technology. This research is sponsored by a grant from the O\u00c6ce of Naval Research under contract No. N00014-93-13085. Additional support is provided by Eastman Kodak Company, Daimler-Benz AG, Siemens Corporate Research Inc., AT&T, Digital Equipment and Central Research Institute of Electric Power Industry."
            },
            "slug": "An-Empirical-Comparison-of-SNoW-and-SVMs-for-Face-Alvira-Rifkin",
            "title": {
                "fragments": [],
                "text": "An Empirical Comparison of SNoW and SVMs for Face Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Evaluating the two approaches to face detection algorithms in a controlled experiment found that SNoW performed about as well as linear SVMs, and substantially worse than polynomial SVMs."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1803182"
                        ],
                        "name": "A. Nefian",
                        "slug": "A.-Nefian",
                        "structuredName": {
                            "firstName": "Ara",
                            "lastName": "Nefian",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Nefian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144449603"
                        ],
                        "name": "M. Hayes",
                        "slug": "M.-Hayes",
                        "structuredName": {
                            "firstName": "Monson",
                            "lastName": "Hayes",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hayes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12576529,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dcdd358dd2e79da3fff78cae37417a251b0170ed",
            "isKey": false,
            "numCitedBy": 167,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "The work presented in this paper describes a hidden Markov model (HMM)-based framework for face recognition and face detection. The observation vectors used to characterize the states of the HMM are obtained using the coefficients of the Karhunen-Loeve transform (KLT). The face recognition method presented reduces significantly the computational complexity of previous HMM-based face recognition systems, while slightly improving the recognition rate. Consistent with the HMM model of the face, this paper introduces a novel HMM-based face detection approach using the same feature extraction techniques used for face recognition."
            },
            "slug": "Face-detection-and-recognition-using-hidden-Markov-Nefian-Hayes",
            "title": {
                "fragments": [],
                "text": "Face detection and recognition using hidden Markov models"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A novel HMM-based face detection approach using the same feature extraction techniques used for face recognition using the coefficients of the Karhunen-Loeve transform is introduced."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 1998 International Conference on Image Processing. ICIP98 (Cat. No.98CB36269)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794240"
                        ],
                        "name": "Haiyuan Wu",
                        "slug": "Haiyuan-Wu",
                        "structuredName": {
                            "firstName": "Haiyuan",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Haiyuan Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "122439944"
                        ],
                        "name": "Qian Chen",
                        "slug": "Qian-Chen",
                        "structuredName": {
                            "firstName": "Qian",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qian Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735941"
                        ],
                        "name": "M. Yachida",
                        "slug": "M.-Yachida",
                        "structuredName": {
                            "firstName": "Masahiko",
                            "lastName": "Yachida",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Yachida"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 324,
                                "start": 319
                            }
                        ],
                        "text": "Other color models applied to face detection include HSV [48, 60, 81, 216], YES [160], YCrCb [2, 48, 84, 130, 191, 200], YUV [1, 123], CIE-xyz [15], L\u2217 a\u2217 b\u2217 [13, 109], L\u2217 u\u2217 v \u2217 [63], CSN (a modified rg representation) [90, 91] and UCS/Farnsworth (a perceptually uniform color system was proposed by Farnsworth [210]) [208]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5869722,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "080f57ebfe8b6e5e26b3c9589c95ee84feeb2a71",
            "isKey": false,
            "numCitedBy": 297,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a new method to detect faces in color images based on the fuzzy theory. We make two fuzzy models to describe the skin color and hair color, respectively. In these models, we use a perceptually uniform color space to describe the color information to increase the accuracy and stableness. We use the two models to extract the skin color regions and the hair color regions, and then comparing them with the prebuilt head-shape models by using a fuzzy theory based pattern-matching method to detect face candidates."
            },
            "slug": "Face-Detection-From-Color-Images-Using-a-Fuzzy-Wu-Chen",
            "title": {
                "fragments": [],
                "text": "Face Detection From Color Images Using a Fuzzy Pattern Matching Method"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Two fuzzy models are made to describe the skin color and hair color and then compared with the prebuilt head-shape models by using a fuzzy theory based pattern-matching method to detect face candidates."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33642939"
                        ],
                        "name": "Jiebo Luo",
                        "slug": "Jiebo-Luo",
                        "structuredName": {
                            "firstName": "Jiebo",
                            "lastName": "Luo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiebo Luo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153246082"
                        ],
                        "name": "C. W. Chen",
                        "slug": "C.-W.-Chen",
                        "structuredName": {
                            "firstName": "Chang",
                            "lastName": "Chen",
                            "middleNames": [
                                "Wen"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. W. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2226863"
                        ],
                        "name": "K. Parker",
                        "slug": "K.-Parker",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Parker",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Parker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 56
                            }
                        ],
                        "text": "Various types of face constellations have been proposed [4, 11, 73, 110, 117, 131, 180, 197, 221]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7815769,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5802d1198c3d57bac9d3a0fca833a52e349672ff",
            "isKey": false,
            "numCitedBy": 25,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a human face location technique based on contour extraction within the framework of a wavelet-based video compression scheme for videoconferencing applications. In addition to an adaptive quantization in which spatial constraints are enforced to preserve perceptually important information at low bit rates, semantic information of the human face is incorporated to design a hybrid compression scheme for videoconferencing, since the face is often the most important part and should be coded with high fidelity. The human face is detected based on contour extraction and feature point analysis. An approximate face mask is then used in the quantization of the decomposed subbands. At the same total bit rate, coarser quantization of the background enables the face region to be quantized finer and coded with a higher quality. Moreover, the resultant larger quantization noise in the background can be suppressed using an edge-preserving enhancement algorithm. Experimental results have shown that the perceptual image quality is greatly improved using the proposed scheme."
            },
            "slug": "Face-location-in-wavelet-based-video-compression-Luo-Chen",
            "title": {
                "fragments": [],
                "text": "Face location in wavelet-based video compression for high perceptual quality videoconferencing"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "A human face location technique based on contour extraction within the framework of a wavelet-based video compression scheme for videoconferencing applications and results have shown that the perceptual image quality is greatly improved using the proposed scheme."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings., International Conference on Image Processing"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144994682"
                        ],
                        "name": "A. Pentland",
                        "slug": "A.-Pentland",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Pentland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pentland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780935"
                        ],
                        "name": "B. Moghaddam",
                        "slug": "B.-Moghaddam",
                        "structuredName": {
                            "firstName": "Baback",
                            "lastName": "Moghaddam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Moghaddam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738894"
                        ],
                        "name": "T. Starner",
                        "slug": "T.-Starner",
                        "structuredName": {
                            "firstName": "Thad",
                            "lastName": "Starner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Starner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 133
                            }
                        ],
                        "text": "Other biometric systems using face detection include template-based methods in the CSIRO PC-Check system [145] and eigenface methods [142, 192] in FaceID from Viisage Technologies."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 83
                            }
                        ],
                        "text": "The performance of the eye locations was reported to be 94% with 6% false positive [142] in a database of 7562 frontal face images on a plain background."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 0
                            }
                        ],
                        "text": "[133, 142] later proposed a facial feature detector using DFFS generated from eigenfeatures (eigeneyes, eigennose, eigenmouth) obtained from various facial feature templates in a training set."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 97
                            }
                        ],
                        "text": "A slightly reduced but still accurate performance for nose and mouth locations was also shown in [142]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 136280,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b0bf5d558220d39698ce96d59ee5772e8e1a0663",
            "isKey": true,
            "numCitedBy": 2234,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe experiments with eigenfaces for recognition and interactive search in a large-scale face database. Accurate visual recognition is demonstrated using a database of O(10/sup 3/) faces. The problem of recognition under general viewing orientation is also examined. A view-based multiple-observer eigenspace technique is proposed for use in face recognition under variable pose. In addition, a modular eigenspace description technique is used which incorporates salient features such as the eyes, nose and mouth, in an eigenfeature layer. This modular representation yields higher recognition rates as well as a more robust framework for face recognition. An automatic feature extraction technique using feature eigentemplates is also demonstrated.<<ETX>>"
            },
            "slug": "View-based-and-modular-eigenspaces-for-face-Pentland-Moghaddam",
            "title": {
                "fragments": [],
                "text": "View-based and modular eigenspaces for face recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A modular eigenspace description technique is used which incorporates salient features such as the eyes, nose and mouth, in an eigenfeature layer, which yields higher recognition rates as well as a more robust framework for face recognition."
            },
            "venue": {
                "fragments": [],
                "text": "1994 Proceedings of IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1830709"
                        ],
                        "name": "A. Lanitis",
                        "slug": "A.-Lanitis",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Lanitis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Lanitis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144482985"
                        ],
                        "name": "C. Taylor",
                        "slug": "C.-Taylor",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Taylor",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7205190"
                        ],
                        "name": "Tim Cootes",
                        "slug": "Tim-Cootes",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Cootes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tim Cootes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 122941737,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7b65b06ecdd9df916d8f688e73ac41a2bb0fd63f",
            "isKey": false,
            "numCitedBy": 253,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Automatic-face-identification-system-using-flexible-Lanitis-Taylor",
            "title": {
                "fragments": [],
                "text": "Automatic face identification system using flexible appearance models"
            },
            "venue": {
                "fragments": [],
                "text": "Image Vis. Comput."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34948290"
                        ],
                        "name": "B. Tak\u00e1cs",
                        "slug": "B.-Tak\u00e1cs",
                        "structuredName": {
                            "firstName": "Barnab\u00e1s",
                            "lastName": "Tak\u00e1cs",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Tak\u00e1cs"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143979395"
                        ],
                        "name": "H. Wechsler",
                        "slug": "H.-Wechsler",
                        "structuredName": {
                            "firstName": "Harry",
                            "lastName": "Wechsler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Wechsler"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 95
                            }
                        ],
                        "text": "Other face detection approaches using LDA include [65] and [184], while SOMs have been used in [183]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 45701080,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "83502340455872b8a3031e32fdb88f90c01f62b4",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Detection-of-faces-and-facial-landmarks-using-banks-Tak\u00e1cs-Wechsler",
            "title": {
                "fragments": [],
                "text": "Detection of faces and facial landmarks using iconic filter banks"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2709589"
                        ],
                        "name": "Veronika Vogelhuber",
                        "slug": "Veronika-Vogelhuber",
                        "structuredName": {
                            "firstName": "Veronika",
                            "lastName": "Vogelhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Veronika Vogelhuber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462253"
                        ],
                        "name": "C. Schmid",
                        "slug": "C.-Schmid",
                        "structuredName": {
                            "firstName": "Cordelia",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schmid"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 56
                            }
                        ],
                        "text": "Various types of face constellations have been proposed [4, 11, 73, 110, 117, 131, 180, 197, 221]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2159872,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ee2e0dc06c879547588cbdec48015983726d1e1a",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an algorithm for face detection that is based on generic local descriptors (e.g. eyes). A generic descriptor captures the distribution of individual descriptors over a set of samples (training images). This distribution is assumed to be a Gaussian mixture model and is learnt using the minimum description length principle. A descriptor of an unknown image may then be classified as one of the generic local descriptors. Robustness is achieved by using spatial constraints between locations of descriptors. Experiments show very promising results."
            },
            "slug": "Face-detection-based-on-generic-local-descriptors-Vogelhuber-Schmid",
            "title": {
                "fragments": [],
                "text": "Face detection based on generic local descriptors and spatial constraints"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "An algorithm for face detection that is based on generic local descriptors (e.g. eyes) that is achieved by using spatial constraints between locations of descriptors."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 15th International Conference on Pattern Recognition. ICPR-2000"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2121240618"
                        ],
                        "name": "J. Cai",
                        "slug": "J.-Cai",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Cai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2312891"
                        ],
                        "name": "A. Goshtasby",
                        "slug": "A.-Goshtasby",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Goshtasby",
                            "middleNames": [
                                "Ardeshir"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Goshtasby"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11775703,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0a406e97cf090b697575d1be813b3106cf2ef49d",
            "isKey": false,
            "numCitedBy": 277,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "A method is introduced that detects human faces in color images by first separating skin regions from non-skin regions and then locating faces within the skin regions. A chroma chart is prepared via a training process that shows likelihoods of different colors representing the skin. Using the chroma chart, a color image is transformed into a gray-scale image, with the gray value at a pixel showing the likelihood of the pixel representing the skin. By segmenting the gray-scale image, skin regions are separated from non-skin regions. Then, using the luminance component of the color image and by template matching, faces are located within skin regions."
            },
            "slug": "Detecting-human-faces-in-color-images-Cai-Goshtasby",
            "title": {
                "fragments": [],
                "text": "Detecting human faces in color images"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "A method is introduced that detects human faces in color images by first separating skin regions from non-skin regions and then locating faces within the skin regions by using the luminance component of the color image and by template matching."
            },
            "venue": {
                "fragments": [],
                "text": "Image Vis. Comput."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "121478544"
                        ],
                        "name": "M. Shackleton",
                        "slug": "M.-Shackleton",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Shackleton",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Shackleton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50748720"
                        ],
                        "name": "W. Welsh",
                        "slug": "W.-Welsh",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Welsh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Welsh"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 21
                            }
                        ],
                        "text": "Shackleton and Welsh [174] improved the eye template matching accuracy by adding extra parameters and an external energy term that is sensitive to the enhanced white area of eyes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 36
                            }
                        ],
                        "text": "Contemporary research in this field [7, 35, 98, 168, 174] has mainly concentrated on issues such as execution time reductions, template modifications, and energy term considerations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 24
                            }
                        ],
                        "text": "A. Shackleton and W. J. Welsh, Classification of facial features for recognition, inIEEE Proc. of Int."
                    },
                    "intents": []
                }
            ],
            "corpusId": 11500685,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a78236ff0815f2ccbc465f7fb23a7d827a670906",
            "isKey": false,
            "numCitedBy": 69,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "A facial feature classification technique that independently captures both the geometric configuration and the image detail of a particular feature is described. The geometric configuration is first extracted by fitting a deformable template to the shape of the feature (for example, an eye) in the image. This information is then used to geometrically normalize the image in such a way that the feature in the image attains a standard shape. The normalized image of the facial feature is then classified in terms of a set of principal components previously obtained from a representative set of training images of similar features. This classification stage yields a representation vector which can be used for recognition matching of the feature in terms of image detail alone without the complication of changes in facial expression. Implementation of the system is described and results are given for its application to a set of test faces. These results show that features can be reliably recognized using the representation vectors obtained.<<ETX>>"
            },
            "slug": "Classification-of-facial-features-for-recognition-Shackleton-Welsh",
            "title": {
                "fragments": [],
                "text": "Classification of facial features for recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "A facial feature classification technique that independently captures both the geometric configuration and the image detail of a particular feature is described and results show that features can be reliably recognized using the representation vectors obtained."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2960140"
                        ],
                        "name": "B. Martinkauppi",
                        "slug": "B.-Martinkauppi",
                        "structuredName": {
                            "firstName": "Birgitta",
                            "lastName": "Martinkauppi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Martinkauppi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 61301496,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f22c9094bae238bfcf040e015f9b9b703a8d4bce",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 160,
            "paperAbstract": {
                "fragments": [],
                "text": "The colours of objects perceived by a colour camera are dependent on the illumination conditions. For example, when the prevailing illumination condition does not correspond to the one used in the white balancing of the camera, the object colours can change their appearance due to the lack of colour constancy capabilities. Many methods for colour constancy have been suggested but so far their performance has been inadequate. Faces are common and important objects encountered in many applications. Therefore, this thesis is dedicated to studying face colours and their robust use under real world illumination conditions. The main thesis statement is \"knowledge about an object's colour, like skin colour changes under different illumination conditions, can be used to develop more robust techniques against illumination changes\". Many face databases exist, and in some cases they contain colour images and even videos. However, from the point of view of this thesis these databases have several limitations: unavailability of spectral data related to image acquisition, undefined illumination conditions of the acquisition, and if illumination change is present it often means only change in illumination direction. To overcome these limitations, two databases, a Physics-Based Face Database and a Face Video Database were created. In addition to the images, the Physics-Based Face Database consists of spectral data part including skin reflectances, channel responsivities of the camera and spectral power distribution of the illumination. The images of faces are taken under four known light sources with different white balancing illumination conditions for over 100 persons. In addition to videos, the Face Video Database has spectral reflectances of skin for selected persons and images taken with the same measurement arrangement as in the Physics-Based Face Database. The images and videos are taken with several cameras. The databases were used to gather information about skin chromaticities and to provide test material. The skin RGB from images were converted to different colour spaces and the result showed that the normalized colour coordinate was among the most usable colour spaces for skin chromaticity modelling. None of the colour spaces could eliminate the colour shifts in chromaticity. The obtained chromaticity constraint can be implemented as an adaptive skin colour modelling part of face tracking algorithms, like histogram backprojection or mean shift. The performances of these adaptive algorithms were superior compared to those using a fixed skin colour model or model adaptation based on spatial pixel selection. Of course, there are cases when the colour cue is not enough alone and use of other cues like motion or edge data would improve the result. It was also demonstrated that the skin colour model can be used to segment faces and the segmentation results depend on the background due to the method used. Also an application for colour correction using principal component analysis and a simplified dichromatic reflection model was shown to improve colour quality of seriously clipped images. The results of tracking, segmentation and colour correction experiments using the collected data validate the thesis statement."
            },
            "slug": "Face-colour-under-varying-illumination-analysis-and-Martinkauppi",
            "title": {
                "fragments": [],
                "text": "Face colour under varying illumination - analysis and applications"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It was demonstrated that the skin colour model can be used to segment faces and the segmentation results depend on the background due to the method used, and the performances of these adaptive algorithms were superior compared to those using a fixed skin color model or model adaptation based on spatial pixel selection."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1727939"
                        ],
                        "name": "R. Herpers",
                        "slug": "R.-Herpers",
                        "structuredName": {
                            "firstName": "Rainer",
                            "lastName": "Herpers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Herpers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "13393551"
                        ],
                        "name": "M. Michaelis",
                        "slug": "M.-Michaelis",
                        "structuredName": {
                            "firstName": "Markus",
                            "lastName": "Michaelis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Michaelis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47183366"
                        ],
                        "name": "K. Lichtenauer",
                        "slug": "K.-Lichtenauer",
                        "structuredName": {
                            "firstName": "K.-H.",
                            "lastName": "Lichtenauer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Lichtenauer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143921323"
                        ],
                        "name": "G. Sommer",
                        "slug": "G.-Sommer",
                        "structuredName": {
                            "firstName": "Gerald",
                            "lastName": "Sommer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Sommer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 27
                            }
                        ],
                        "text": "The steerable filtering in [59] consists of three sequential edge detection steps which include detection of edges, determination of the filter orientation of any detected edges, and stepwise tracking of neighboring edges using the orientation information."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 62
                            }
                        ],
                        "text": "More recent examples of edge-based techniques can be found in [9, 16, 59, 115, 116] for facial feature extraction and in [32, 50, 58, 60, 70, 76, 108, 201, 222] for face detection."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 136
                            }
                        ],
                        "text": "For instance, a Laplacian of large scale was used to obtain a line drawing in [162] and steerable and multiscale-orientation filters in [59] and [58], respectively."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5067596,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2f4531473ee74e5c470174c632f08b099bd4b88b",
            "isKey": false,
            "numCitedBy": 70,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors introduce a method for the automatic detection of facial features and characteristic anatomical keypoints. In the application they are aiming at the anatomical landmarks are used to accurately measure facial features. Their approach is essentially bused on a selective search and sequential tracking of characteristic edge and line structures of the facial object to be searched. It integrates model knowledge to guarantee a consistent interpretation of the abundance of local features. The search and the tracking is controlled in each step by interpreting the already derived edge and line information in the context of the whole considered region. For their application, the edge and line detection has to be very precise and flexible. Therefore, they apply a powerful filtering scheme based on steerable filters."
            },
            "slug": "Edge-and-keypoint-detection-in-facial-regions-Herpers-Michaelis",
            "title": {
                "fragments": [],
                "text": "Edge and keypoint detection in facial regions"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "The authors introduce a method for the automatic detection of facial features and characteristic anatomical keypoints based on steerable filters that integrates model knowledge to guarantee a consistent interpretation of the abundance of local features."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Second International Conference on Automatic Face and Gesture Recognition"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1775043"
                        ],
                        "name": "H. Graf",
                        "slug": "H.-Graf",
                        "structuredName": {
                            "firstName": "Hans",
                            "lastName": "Graf",
                            "middleNames": [
                                "Peter"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Graf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3165487"
                        ],
                        "name": "E. Cosatto",
                        "slug": "E.-Cosatto",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Cosatto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Cosatto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1932050"
                        ],
                        "name": "T. Ezzat",
                        "slug": "T.-Ezzat",
                        "structuredName": {
                            "firstName": "Tony",
                            "lastName": "Ezzat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Ezzat"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 135
                            }
                        ],
                        "text": "Since the representation strongly relates to human perception of color [106, 178], it is also widely used in face segmentation schemes [51, 83, 118, 125, 175, 178, 187, 220]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15138108,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "48271e6f546f32f9c2fb0cf70bd66e25bf632c1b",
            "isKey": false,
            "numCitedBy": 46,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes techniques for extracting bitmaps of facial parts from videos of a talking person. The goal is to synthesize photo-realistic talking heads of high quality that show picture-perfect appearance and realistic head movements with good lip-sound synchronization. For the synthesis of a talking head, bitmaps of facial parts are combined to form whole heads and then sequences of such images are integrated with audio from a text-to-speech synthesizer. For a seamless integration of facial parts into an animation, their shape and visual appearance must be known with high accuracy. The recognition system has to find not only the locations of facial features, but must also be able to determine the head's orientation and recognize the facial expressions. Our face recognition proceeds in multiple steps, each with an increased precision. Using motion, color and shape information, the head's position and the location of the main facial features are determined first. Then smaller areas are searched with matched filters, in order to identify specific facial features with high precision. From this information a head's 3D orientation is calculated. Facial parts are cut from the image and, using the head's orientation, are warped into bitmaps with 'normalized' orientation and scale."
            },
            "slug": "Face-analysis-for-the-synthesis-of-photo-realistic-Graf-Cosatto",
            "title": {
                "fragments": [],
                "text": "Face analysis for the synthesis of photo-realistic talking heads"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "Techniques for extracting bitmaps of facial parts from videos of a talking person to synthesize photo-realistic talking heads of high quality that show picture-perfect appearance and realistic head movements with good lip-sound synchronization are described."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34679741"
                        ],
                        "name": "S. Li",
                        "slug": "S.-Li",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054919232"
                        ],
                        "name": "Qian Gu",
                        "slug": "Qian-Gu",
                        "structuredName": {
                            "firstName": "Qian",
                            "lastName": "Gu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qian Gu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 10
                            }
                        ],
                        "text": "Gu and Li [54] present a variation of Sung and Poggio\u2019s approach where linear discriminant analysis is applied for feature selection before training the neural network classifier."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 46493366,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f4c48f70519580025183980b15f1f4abae84fd79",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "A system using feature optimization and neural learning techniques is presented for detection of upright frontal faces. The feature set is optimized in terms of the Fisher's linear discrimination (FLD) criterion. A neural network (NN) method is used to learn a complex mapping function for the classification, given the optimized feature set. The optimization of feature set reduces the burden of the subsequent NN classifier and improves its performance in learning speed and classification rates. Experimental results show that the feature set optimized by using the FLD transform significantly improves the detection rate while maintaining the same false alarms. Our system produces higher detection and lower missing rates than several existing state-of-the-art face detection systems, with an average false detection rate."
            },
            "slug": "Combining-feature-optimization-into-neural-network-Li-Gu",
            "title": {
                "fragments": [],
                "text": "Combining feature optimization into neural network based face detection"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Experimental results show that the feature set optimized by using the FLD transform significantly improves the detection rate while maintaining the same false alarms."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 15th International Conference on Pattern Recognition. ICPR-2000"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753908"
                        ],
                        "name": "Michael Elad",
                        "slug": "Michael-Elad",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Elad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Elad"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398184731"
                        ],
                        "name": "Y. Hel-Or",
                        "slug": "Y.-Hel-Or",
                        "structuredName": {
                            "firstName": "Yacov",
                            "lastName": "Hel-Or",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Hel-Or"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2851673"
                        ],
                        "name": "R. Keshet",
                        "slug": "R.-Keshet",
                        "structuredName": {
                            "firstName": "Renato",
                            "lastName": "Keshet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Keshet"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14110375,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "109085efa51df793a2fc2c2b51757510d8cafb42",
            "isKey": false,
            "numCitedBy": 54,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Rejection-based-classifier-for-face-detection-Elad-Hel-Or",
            "title": {
                "fragments": [],
                "text": "Rejection based classifier for face detection"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit. Lett."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760186"
                        ],
                        "name": "Xangdong Xie",
                        "slug": "Xangdong-Xie",
                        "structuredName": {
                            "firstName": "Xangdong",
                            "lastName": "Xie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xangdong Xie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "29038097"
                        ],
                        "name": "R. Sudhakar",
                        "slug": "R.-Sudhakar",
                        "structuredName": {
                            "firstName": "Raghavan",
                            "lastName": "Sudhakar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sudhakar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "14829648"
                        ],
                        "name": "H. Zhuang",
                        "slug": "H.-Zhuang",
                        "structuredName": {
                            "firstName": "Hanqi",
                            "lastName": "Zhuang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Zhuang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 8107101,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3fb90b4316b14837fc3c6ff24dc09673f723cf16",
            "isKey": false,
            "numCitedBy": 172,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-improving-eye-feature-extraction-using-templates-Xie-Sudhakar",
            "title": {
                "fragments": [],
                "text": "On improving eye feature extraction using deformable templates"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112702038"
                        ],
                        "name": "Q. Song",
                        "slug": "Q.-Song",
                        "structuredName": {
                            "firstName": "Qing",
                            "lastName": "Song",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Q. Song"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115466358"
                        ],
                        "name": "J. Robinson",
                        "slug": "J.-Robinson",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Robinson",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Robinson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 188,
                                "start": 178
                            }
                        ],
                        "text": "The eigenvector in the LDA transformation matrix with the largest eigenvalue is known as Fisher\u2019s linear discriminant [43], which by itself has also been used for face detection [179, 203]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8796072,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "77ab8aa4a644916b124c7580243803323ecbdc6b",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose criteria for a feature space for face image processing and a method for generating such a space. Beginning with many input dimensions, including deformation vectors (obtained through optical flow analysis between an input image and a neutral template) and deformation residues, we apply principal components analysis and Fisher's classification criterion to derive a feature space. We demonstrate classification in two important tasks-face detection and expression analysis-in each case using only one linear discriminant, thereby demonstrating that the feature space fulfils a restricted version of the criteria."
            },
            "slug": "A-feature-space-for-face-image-processing-Song-Robinson",
            "title": {
                "fragments": [],
                "text": "A feature space for face image processing"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work demonstrates classification in two important tasks-face detection and expression analysis-in each case using only one linear discriminant, thereby demonstrating that the feature space fulfils a restricted version of the criteria."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 15th International Conference on Pattern Recognition. ICPR-2000"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731570"
                        ],
                        "name": "M. Lew",
                        "slug": "M.-Lew",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Lew",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lew"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14363121,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1e3cb790313c793cebfe86d5bfdcd300b0d759e1",
            "isKey": false,
            "numCitedBy": 51,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes information theoretic methods for the determination of the optimal subset of pixels for the problem of face detection in complex backgrounds. A view-based method is described which has limitations due to misalignments. This motivates the modular feature based method which minimizes the misalignment problem. Empirical comparisons between the view-based, modular, and sum of squared difference methods are made using four databases from three universities."
            },
            "slug": "Information-theoretic-view-based-and-modular-face-Lew",
            "title": {
                "fragments": [],
                "text": "Information theoretic view-based and modular face detection"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "Information theoretic methods for the determination of the optimal subset of pixels for the problem of face detection in complex backgrounds are described and the modular feature based method which minimizes the misalignment problem is described."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Second International Conference on Automatic Face and Gesture Recognition"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1785111"
                        ],
                        "name": "J. Karlekar",
                        "slug": "J.-Karlekar",
                        "structuredName": {
                            "firstName": "Jayashree",
                            "lastName": "Karlekar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Karlekar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144907915"
                        ],
                        "name": "U. Desai",
                        "slug": "U.-Desai",
                        "structuredName": {
                            "firstName": "Uday",
                            "lastName": "Desai",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Desai"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 93
                            }
                        ],
                        "text": "Other color models applied to face detection include HSV [48, 60, 81, 216], YES [160], YCrCb [2, 48, 84, 130, 191, 200], YUV [1, 123], CIE-xyz [15], L\u2217 a\u2217 b\u2217 [13, 109], L\u2217 u\u2217 v \u2217 [63], CSN (a modified rg representation) [90, 91] and UCS/Farnsworth (a perceptually uniform color system was proposed by Farnsworth [210]) [208]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18661270,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ebb04c4d7489b24d4c786612195575aba5dc8f98",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, a new fast method for detecting human faces in color images using the wavelet transform is proposed. The face detection algorithm has three stages, where chrominance, shape and frequency information are used respectively. The algorithm starts at the lower resolution version of the image obtained from the wavelet transform, so that the amount of data to be processed is greatly reduced. Experimental results show that the human face in color images can be detected regardless of orientation."
            },
            "slug": "Finding-faces-in-color-images-using-wavelet-Karlekar-Desai",
            "title": {
                "fragments": [],
                "text": "Finding faces in color images using wavelet transform"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A new fast method for detecting human faces in color images using the wavelet transform, where chrominance, shape and frequency information are used respectively."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 10th International Conference on Image Analysis and Processing"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153054693"
                        ],
                        "name": "M. Zobel",
                        "slug": "M.-Zobel",
                        "structuredName": {
                            "firstName": "Matthias",
                            "lastName": "Zobel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Zobel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3155362"
                        ],
                        "name": "A. Gebhard",
                        "slug": "A.-Gebhard",
                        "structuredName": {
                            "firstName": "Arnd",
                            "lastName": "Gebhard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gebhard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153543875"
                        ],
                        "name": "D. Paulus",
                        "slug": "D.-Paulus",
                        "structuredName": {
                            "firstName": "Dietrich",
                            "lastName": "Paulus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Paulus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728382"
                        ],
                        "name": "Joachim Denzler",
                        "slug": "Joachim-Denzler",
                        "structuredName": {
                            "firstName": "Joachim",
                            "lastName": "Denzler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joachim Denzler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144290244"
                        ],
                        "name": "H. Niemann",
                        "slug": "H.-Niemann",
                        "structuredName": {
                            "firstName": "Heinrich",
                            "lastName": "Niemann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Niemann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 87
                            }
                        ],
                        "text": "Probabilistic face models based on multiple face appearance have also been proposed in [180, 204, 221, 224]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5875845,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "feb11358525b995167162b8b341d2998f59ad086",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of robust localization of faces and some of their facial features. The task arises, e.g., in the medical field of visual analysis of facial paresis. We detect faces and facial features by means of appropriate DCT coefficients that we obtain by neatly using the coding capabilities of a JPEG hardware compressor. Beside an anthropometric localization approach we focus on how spatial coupling of the facial features can be used to improve robustness of the localization. Because the presented approach is embedded in a completely probabilistic framework, it is not restricted to facial features, it can be generalized to multipart objects of any kind. Therefore the notion of a \"coupled structure\" is introduced. Finally, the approach is applied to the problem of localizing facial features in DCT-coded images and results from our experiments are shown."
            },
            "slug": "Robust-facial-feature-localization-by-coupled-Zobel-Gebhard",
            "title": {
                "fragments": [],
                "text": "Robust facial feature localization by coupled features"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The presented approach is embedded in a completely probabilistic framework, it is not restricted to facial features, it can be generalized to multipart objects of any kind, and the notion of a \"coupled structure\" is introduced."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1775043"
                        ],
                        "name": "H. Graf",
                        "slug": "H.-Graf",
                        "structuredName": {
                            "firstName": "Hans",
                            "lastName": "Graf",
                            "middleNames": [
                                "Peter"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Graf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3165487"
                        ],
                        "name": "E. Cosatto",
                        "slug": "E.-Cosatto",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Cosatto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Cosatto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2387879"
                        ],
                        "name": "D. Gibbon",
                        "slug": "D.-Gibbon",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Gibbon",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Gibbon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2535899"
                        ],
                        "name": "M. Kocheisen",
                        "slug": "M.-Kocheisen",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Kocheisen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kocheisen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2920582"
                        ],
                        "name": "E. Petajan",
                        "slug": "E.-Petajan",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Petajan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Petajan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1249176,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ff3faa698efae2c58ee1782654b2c4769dbb11be",
            "isKey": false,
            "numCitedBy": 152,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "We designed a modular system using a combination of shape analysis, color segmentation and motion information for locating reliably heads and faces of different sizes and orientations in complex images. The first of the system's three channels does a shape analysis on gray-level images to determine the location of individual facial features as well as the outlines of heads. In the second channel the color space is analyzed with a clustering algorithm to find areas of skin colors. The color space is first calibrated, using the results from the other channels. In the third channel motion information is extracted from frame differences. Head outlines are determined by analyzing the shapes of areas with large motion vectors. All three channels produce lists of shapes, each marking an area of the image where a facial feature or a part of the outline of a head may be present. Combinations of such shapes are evaluated with n-gram searches to produce a list of likely head positions and the locations of facial features. We tested the system for tracking faces of people sitting in front of terminals and video phones and used it to track people entering through a doorway."
            },
            "slug": "Multi-modal-system-for-locating-heads-and-faces-Graf-Cosatto",
            "title": {
                "fragments": [],
                "text": "Multi-modal system for locating heads and faces"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "A modular system using a combination of shape analysis, color segmentation and motion information for locating reliably heads and faces of different sizes and orientations in complex images for tracking faces of people sitting in front of terminals and video phones is designed."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Second International Conference on Automatic Face and Gesture Recognition"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704975"
                        ],
                        "name": "G. Burel",
                        "slug": "G.-Burel",
                        "structuredName": {
                            "firstName": "Gilles",
                            "lastName": "Burel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Burel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2081671334"
                        ],
                        "name": "Dominique Carel",
                        "slug": "Dominique-Carel",
                        "structuredName": {
                            "firstName": "Dominique",
                            "lastName": "Carel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dominique Carel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The first neural approaches to face detection were based on MLPs [10, 82, 147], where promising results where reported on fairly simple datasets."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Sung and Poggio suggested a training algorithm, known as \u201cboot-strap training,\u201d to partially deal with this problem (a more precise strategy than the one proposed in [10])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14746961,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1cdb510aed4f8b04b73240287da954f9eff39318",
            "isKey": false,
            "numCitedBy": 94,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Detection-and-localization-of-faces-on-digital-Burel-Carel",
            "title": {
                "fragments": [],
                "text": "Detection and localization of faces on digital images"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit. Lett."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681254"
                        ],
                        "name": "M. Reinders",
                        "slug": "M.-Reinders",
                        "structuredName": {
                            "firstName": "Marcel",
                            "lastName": "Reinders",
                            "middleNames": [
                                "J.",
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Reinders"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2608622"
                        ],
                        "name": "P. Beek",
                        "slug": "P.-Beek",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Beek",
                            "middleNames": [
                                "van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Beek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145940271"
                        ],
                        "name": "B. Sankur",
                        "slug": "B.-Sankur",
                        "structuredName": {
                            "firstName": "B\u00fclent",
                            "lastName": "Sankur",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sankur"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145601290"
                        ],
                        "name": "J. Lubbe",
                        "slug": "J.-Lubbe",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Lubbe",
                            "middleNames": [
                                "C.",
                                "A.",
                                "van",
                                "der"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lubbe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 3
                            }
                        ],
                        "text": "In [5, 53, 152, 192, 218], moving silhouettes that include face and body parts are extracted by thresholding accumulated frame difference."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14502909,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "946be3160b36d301f3df0e8626a6f0b07ec4a1f1",
            "isKey": false,
            "numCitedBy": 87,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Facial-feature-localization-and-adaptation-of-a-for-Reinders-Beek",
            "title": {
                "fragments": [],
                "text": "Facial feature localization and adaptation of a generic face model for model-based coding"
            },
            "venue": {
                "fragments": [],
                "text": "Signal Process. Image Commun."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781874"
                        ],
                        "name": "E. Osuna",
                        "slug": "E.-Osuna",
                        "structuredName": {
                            "firstName": "Edgar",
                            "lastName": "Osuna",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Osuna"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1771659"
                        ],
                        "name": "R. Freund",
                        "slug": "R.-Freund",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Freund",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "[140], a support vector machine (SVM) [194] is applied to face detection."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 53
                            }
                        ],
                        "text": "is the number used in the reported results in [158], [140], and [113]) when included in the CMU dataset."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2845602,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9008cdacbdcff8a218a6928e94fe7c6dfc237b24",
            "isKey": false,
            "numCitedBy": 2841,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate the application of Support Vector Machines (SVMs) in computer vision. SVM is a learning technique developed by V. Vapnik and his team (AT&T Bell Labs., 1985) that can be seen as a new method for training polynomial, neural network, or Radial Basis Functions classifiers. The decision surfaces are found by solving a linearly constrained quadratic programming problem. This optimization problem is challenging because the quadratic form is completely dense and the memory requirements grow with the square of the number of data points. We present a decomposition algorithm that guarantees global optimality, and can be used to train SVM's over very large data sets. The main idea behind the decomposition is the iterative solution of sub-problems and the evaluation of optimality conditions which are used both to generate improved iterative values, and also establish the stopping criteria for the algorithm. We present experimental results of our implementation of SVM, and demonstrate the feasibility of our approach on a face detection problem that involves a data set of 50,000 data points."
            },
            "slug": "Training-support-vector-machines:-an-application-to-Osuna-Freund",
            "title": {
                "fragments": [],
                "text": "Training support vector machines: an application to face detection"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A decomposition algorithm that guarantees global optimality, and can be used to train SVM's over very large data sets is presented, and the feasibility of the approach on a face detection problem that involves a data set of 50,000 data points is demonstrated."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143891066"
                        ],
                        "name": "A. Rajagopalan",
                        "slug": "A.-Rajagopalan",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Rajagopalan",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Rajagopalan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39219932"
                        ],
                        "name": "K. S. Kumar",
                        "slug": "K.-S.-Kumar",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Kumar",
                            "middleNames": [
                                "Sunil"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. S. Kumar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1785111"
                        ],
                        "name": "J. Karlekar",
                        "slug": "J.-Karlekar",
                        "structuredName": {
                            "firstName": "Jayashree",
                            "lastName": "Karlekar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Karlekar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2269939"
                        ],
                        "name": "R. Manivasakan",
                        "slug": "R.-Manivasakan",
                        "structuredName": {
                            "firstName": "Rathinam",
                            "lastName": "Manivasakan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Manivasakan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052261268"
                        ],
                        "name": "M. Patil",
                        "slug": "M.-Patil",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Patil",
                            "middleNames": [
                                "Milind"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Patil"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144907915"
                        ],
                        "name": "U. Desai",
                        "slug": "U.-Desai",
                        "structuredName": {
                            "firstName": "Uday",
                            "lastName": "Desai",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Desai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2772733"
                        ],
                        "name": "P. G. Poonacha",
                        "slug": "P.-G.-Poonacha",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Poonacha",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. G. Poonacha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144527832"
                        ],
                        "name": "S. Chaudhuri",
                        "slug": "S.-Chaudhuri",
                        "structuredName": {
                            "firstName": "Subhasis",
                            "lastName": "Chaudhuri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Chaudhuri"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[150] using higher order statistics to model the face and nonface clusters."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11398552,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5c610939c03a450617a2d5290f0a4fa731407333",
            "isKey": false,
            "numCitedBy": 110,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Two new schemes are presented for finding human faces in a photograph. The first scheme approximates the unknown distributions of the face and the face-like manifolds wing higher order statistics (HOS). An HOS-based data clustering algorithm is also proposed. In the second scheme, the face to non-face and non-face to face transitions are learnt using a hidden Markov model (HMM). The HMM parameters are estimated corresponding to a given photograph and the faces are located by examining the optimal state sequence of the HMM. Experimental results are presented on the performance of both the schemes."
            },
            "slug": "Finding-faces-in-photographs-Rajagopalan-Kumar",
            "title": {
                "fragments": [],
                "text": "Finding faces in photographs"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "Two new schemes for finding human faces in a photograph are presented, one of which approximates the unknown distributions of the face and the face-like manifolds wing higher order statistics (HOS)."
            },
            "venue": {
                "fragments": [],
                "text": "Sixth International Conference on Computer Vision (IEEE Cat. No.98CH36271)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2085375589"
                        ],
                        "name": "H. Schneiderman",
                        "slug": "H.-Schneiderman",
                        "structuredName": {
                            "firstName": "Henry",
                            "lastName": "Schneiderman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Schneiderman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733113"
                        ],
                        "name": "T. Kanade",
                        "slug": "T.-Kanade",
                        "structuredName": {
                            "firstName": "Takeo",
                            "lastName": "Kanade",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanade"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15424450,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3737d479a5764eecef0cee5081e64f5f884508b1",
            "isKey": false,
            "numCitedBy": 282,
            "numCiting": 96,
            "paperAbstract": {
                "fragments": [],
                "text": "In this thesis, we describe a statistical method for 3D object detection. In this method, we decompose the 3D geometry of each object into a small number of viewpoints. For each viewpoint, we construct a decision rule that determines if the object is present at that specific orientation. Each decision rule uses the statistics of both object appearance and \u201cnon-object\u201d visual appearance. We represent each set of statistics using a product of histograms. Each histogram represents the joint statistics of a subset of wavelet coefficients and their position on the object. Our approach is to use many such histograms representing a wide variety of visual attributes. Using this method, we have developed the first algorithm that can reliably detect faces that vary from frontal view to full profile view and the first algorithm that can reliably detect cars over a wide range of viewpoints."
            },
            "slug": "A-statistical-approach-to-3d-object-detection-to-Schneiderman-Kanade",
            "title": {
                "fragments": [],
                "text": "A statistical approach to 3d object detection applied to faces and cars"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "This thesis describes a statistical method for 3D object detection that has developed the first algorithm that can reliably detect faces that vary from frontal view to full profile view and the first algorithms thatCan reliably detect cars over a wide range of viewpoints."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783224"
                        ],
                        "name": "A. Colmenarez",
                        "slug": "A.-Colmenarez",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Colmenarez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Colmenarez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749650"
                        ],
                        "name": "B. Frey",
                        "slug": "B.-Frey",
                        "structuredName": {
                            "firstName": "Brendan",
                            "lastName": "Frey",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Frey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153652752"
                        ],
                        "name": "Thomas S. Huang",
                        "slug": "Thomas-S.-Huang",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Huang",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas S. Huang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 11
                            }
                        ],
                        "text": "2), and in [20] the technique was incorporated in a real-time face tracking system."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 46687870,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "906460995c15df5f1d8532e75fd64eedf51a3dad",
            "isKey": false,
            "numCitedBy": 48,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a real-time system for face and facial feature detection and tracking in continuous video. The core of this system consists of a set of novel facial feature detectors based on our previously proposed information-based maximum discrimination learning technique. These classifiers are very fast and allow us to implement a fully automatic, real-time system for detection and tracking multiple faces. In addition to locking onto up to four target faces, this system locates and tracks nine facial features as they move under facial expression changes."
            },
            "slug": "Detection-and-tracking-of-faces-and-facial-features-Colmenarez-Frey",
            "title": {
                "fragments": [],
                "text": "Detection and tracking of faces and facial features"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "A real-time system for face and facial feature detection and tracking in continuous video that locks onto up to four target faces and locates and tracks nine facial features as they move under facial expression changes."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348)"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49174781"
                        ],
                        "name": "B. Menser",
                        "slug": "B.-Menser",
                        "structuredName": {
                            "firstName": "Bernd",
                            "lastName": "Menser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Menser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1923124"
                        ],
                        "name": "M. Br\u00fcnig",
                        "slug": "M.-Br\u00fcnig",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Br\u00fcnig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Br\u00fcnig"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 93
                            }
                        ],
                        "text": "Other color models applied to face detection include HSV [48, 60, 81, 216], YES [160], YCrCb [2, 48, 84, 130, 191, 200], YUV [1, 123], CIE-xyz [15], L\u2217 a\u2217 b\u2217 [13, 109], L\u2217 u\u2217 v \u2217 [63], CSN (a modified rg representation) [90, 91] and UCS/Farnsworth (a perceptually uniform color system was proposed by Farnsworth [210]) [208]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 45517121,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "11f90bd0bdcef521cf3cf6bf28a05c90941d3348",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present a face segmentation algorithm for color images based on connected operators. Using a skin color model, we construct a skin probability image that indicates the probability of each pixel representing skin. Morphological filters are applied to this probability images instead of applying them to the original image. A hierarchy of operators with geometrical criteria (size, compactness, orientation) is employed to simplify the skin probability image and a gray level criterion based on principal components analysis is used for final classification. Using connected operators, regions with different probabilities of being skin are analyzed which leads to a more robust segmentation compared to a single threshold classification."
            },
            "slug": "Segmentation-of-human-faces-in-color-images-using-Menser-Br\u00fcnig",
            "title": {
                "fragments": [],
                "text": "Segmentation of human faces in color images using connected operators"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A face segmentation algorithm for color images based on connected operators using a skin color model that indicates the probability of each pixel representing skin leads to a more robust segmentation compared to a single threshold classification."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348)"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3262395"
                        ],
                        "name": "B. Raducanu",
                        "slug": "B.-Raducanu",
                        "structuredName": {
                            "firstName": "Bogdan",
                            "lastName": "Raducanu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Raducanu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144278339"
                        ],
                        "name": "M. Gra\u00f1a",
                        "slug": "M.-Gra\u00f1a",
                        "structuredName": {
                            "firstName": "Manuel",
                            "lastName": "Gra\u00f1a",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gra\u00f1a"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 109
                            }
                        ],
                        "text": "A comparison of DFFS with a morphological multiscale fingerprint algorithm is provided in Raducanu and Grana [149]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 19
                            }
                        ],
                        "text": "B. Raducanu and M. Grana, Face localization based on the morphological multiscale fingerprint, in Proceedings of the 15th International Conference on Pattern Recognition, 2000Vol. II, p. 4B.\n150."
                    },
                    "intents": []
                }
            ],
            "corpusId": 28990349,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b9872418637dab5338026c8ed7787ce0b3d9d236",
            "isKey": false,
            "numCitedBy": 4,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose the use of morphological multi-scale fingerprints (MMF) for face localization. The MMF is computed as the local maxima and minima preserved up to a certain scale in a multi-scale analysis based on morphological erosion and dilation. This approach belongs to a class of global image feature extraction approaches, that can be combined with others to ensure robust face localization. No structural relationships between face elements is taken into account. We compare this approach to the eigenface approach to face detection with clear superior results."
            },
            "slug": "Face-localization-based-on-the-morphological-Raducanu-Gra\u00f1a",
            "title": {
                "fragments": [],
                "text": "Face localization based on the morphological multi-scale fingerprint"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "The use of morphological multi-scale fingerprints (MMF) for face localization is proposed as the local maxima and minima preserved up to a certain scale in a multi- scale analysis based on morphological erosion and dilation."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 15th International Conference on Pattern Recognition. ICPR-2000"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782734"
                        ],
                        "name": "A. Tankus",
                        "slug": "A.-Tankus",
                        "structuredName": {
                            "firstName": "Ariel",
                            "lastName": "Tankus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Tankus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47620675"
                        ],
                        "name": "Y. Yeshurun",
                        "slug": "Y.-Yeshurun",
                        "structuredName": {
                            "firstName": "Yehezkel",
                            "lastName": "Yeshurun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Yeshurun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8909922"
                        ],
                        "name": "N. Intrator",
                        "slug": "N.-Intrator",
                        "structuredName": {
                            "firstName": "Nathan",
                            "lastName": "Intrator",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Intrator"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[185]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 5637848,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "c0521ff40923ef72c00b5677f26a1b2b043a5fa0",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Face-detection-by-direct-convexity-estimation-Tankus-Yeshurun",
            "title": {
                "fragments": [],
                "text": "Face Detection by Direct Convexity Estimation"
            },
            "venue": {
                "fragments": [],
                "text": "AVBPA"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2080791815"
                        ],
                        "name": "C. Burly",
                        "slug": "C.-Burly",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Burly",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burly"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2087868060"
                        ],
                        "name": "T. K. Leungz",
                        "slug": "T.-K.-Leungz",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Leungz",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. K. Leungz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1226614,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2052cab4162051486813b7ed0c2c5f6f4c0667ee",
            "isKey": false,
            "numCitedBy": 79,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, a face localization system is proposed in which local detectors are coupled with a statistical model of the spatial arrangement of facial features to yield robust performance. The outputs from the local detectors are treated as candidate locations and constellations are formed from these. The eeects of translation, rotation, and scale are eliminated by mapping to a set of shape variables. The constellations are then ranked according to the likelihood that the shape variables correspond to a face versus an alternative model. Incomplete constellations , which occur when some of the true features are missed, are handled in a principled way."
            },
            "slug": "Face-Localization-via-Shape-Statistics-Burly-Leungz",
            "title": {
                "fragments": [],
                "text": "Face Localization via Shape Statistics"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "A face localization system is proposed in which local detectors are coupled with a statistical model of the spatial arrangement of facial features to yield robust performance and incomplete constellations are handled in a principled way."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398659765"
                        ],
                        "name": "S. Marchand-Maillet",
                        "slug": "S.-Marchand-Maillet",
                        "structuredName": {
                            "firstName": "St\u00e9phane",
                            "lastName": "Marchand-Maillet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Marchand-Maillet"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 65
                            }
                        ],
                        "text": "Hidden Markov models have also been applied to face detection in [121, 129]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15145423,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0ccd410b6ae977a945a84bad1c2785cef4c73214",
            "isKey": false,
            "numCitedBy": 7,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper introduces the use of Hidden Markov Models (HMM) as an alternative to techniques classically used for face detection. Our aim is to locate faces in colour images of a video sequence in view to indexing. The use of HMM in pattern recognition is rst brie y reviewed and the mapping of these models onto our problem is presented. Pseudo two-dimensional HMM are presented and shown to be e cient and wellsuited tools for performing face detection in a context where no constraints on face orientation are given. Issues about e cient face modelling are discussed and illustrated with practical examples."
            },
            "slug": "Pseudo-two-dimensional-Hidden-Markov-Models-for-in-Marchand-Maillet",
            "title": {
                "fragments": [],
                "text": "Pseudo two-dimensional Hidden Markov Models for face detection in colour images"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Pseudo two-dimensional HMM are presented and shown to be well-suited tools for performing face detection in a context where no constraints on face orientation are given."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109453619"
                        ],
                        "name": "Ce Wang",
                        "slug": "Ce-Wang",
                        "structuredName": {
                            "firstName": "Ce",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ce Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3059077"
                        ],
                        "name": "S. Griebel",
                        "slug": "S.-Griebel",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Griebel",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Griebel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3016044"
                        ],
                        "name": "M. Brandstein",
                        "slug": "M.-Brandstein",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Brandstein",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Brandstein"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "[199], who propose an automatic video conferencing system consisting of multiple cameras, where decisions involving which camera to use are based on an estimate of the head\u2019s gazing angle."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5357633,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "3ab1e99e4e565cddc9fa44fb20393a31faacaaaa",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "An automatic video-conferencing system is proposed which employs acoustic source localization, video face tracking and pose estimation, and multi-channel speech enhancement. The video portion of the system tracks talkers by utilizing source motion, contour geometry, color data and simple facial features. Decisions involving which camera to use are based on an estimate of the head's gazing angle. This head pose estimation is achieved using a very general head model which employs hairline features and a learned network classification procedure. Finally, a wavelet microphone array technique is used to create an enhanced speech waveform to accompany the recorded video signal. The system presented in this paper is robust to both visual clutter (e.g. ovals in the scene of interest which are not faces) and audible noise (e.g. reverberations and background noise)."
            },
            "slug": "Robust-automatic-video-conferencing-with-multiple-Wang-Griebel",
            "title": {
                "fragments": [],
                "text": "Robust automatic video-conferencing with multiple cameras and microphones"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "An automatic video-conferencing system is proposed which employs acoustic source localization, video face tracking and pose estimation, and multi-channel speech enhancement."
            },
            "venue": {
                "fragments": [],
                "text": "2000 IEEE International Conference on Multimedia and Expo. ICME2000. Proceedings. Latest Advances in the Fast Changing World of Multimedia (Cat. No.00TH8532)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2085375589"
                        ],
                        "name": "H. Schneiderman",
                        "slug": "H.-Schneiderman",
                        "structuredName": {
                            "firstName": "Henry",
                            "lastName": "Schneiderman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Schneiderman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733113"
                        ],
                        "name": "T. Kanade",
                        "slug": "T.-Kanade",
                        "structuredName": {
                            "firstName": "Takeo",
                            "lastName": "Kanade",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanade"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 384,
                                "start": 378
                            }
                        ],
                        "text": "The number of labeled faces was originally 149 by Sung and Poggio (which is the number used in the reported results in [182] and [107], but it was changed to 155 (which\nTABLE 2 Results Reported in Terms of Percentage Correct Detection (CD) and Number of False Positives (FP), CD/FP, on the CMU and MIT Datasets\nFace detection system CMU-130 CMU-125 MIT-23 MIT-20\nSchneiderman & Kanade\u2014Ea [170] 94.4%/65 Schneiderman & Kanade\u2014Wb [170] 90.2%/110 Yanget al.\u2014FA [217] 92.3%/82 89.4%/3 Yanget al.\u2014LDA [217] 93.6%/74 91.5%/1 Rothet al. [157] 94.8%/78 94.1%/3 Rowleyet al. [158] 86.2%/23 84.5%/8 Feraudet al. [42] 86%/8 Colmenarez & Huang [22] 93.9%/8122 Sung & Poggio [182] 79.9%/5 Lew & Huijsmans [107] 94.1%/64 Osunaet al. [140] 74.2%/20 Lin et al. [113] 72.3%/6 Gu and Li [54] 87.1%/0\na Eigenvector coefficients. b Wavelet coefficients.\nis the number used in the reported results in [158], [140], and [113]) when included in the CMU dataset."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 18
                            }
                        ],
                        "text": "H. D. Wactlar, T. Kanade, M. A. Smith, and S. M. Stevens, Intelligent access to digital video: Informedia project,IEEE Comput.29(5), 1996, 46\u201352."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 23
                            }
                        ],
                        "text": "H. Schneiderman and T. Kanade, A statistical model for 3D object detection applied to faces and cars, in IEEE Conference on Computer Vision and Pattern Recognition, 2000."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 71
                            }
                        ],
                        "text": "In the first proposed face detection system of Schneiderman and Kanade [169], the posterior probability function is derived based on a set of modifications and simplifications (some of which are mentioned here):"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 47
                            }
                        ],
                        "text": "In the first proposed face detection system of Schneiderman and Kanade [169], the posterior probability function is derived based on a set of modifications and simplifications (some of which are mentioned here):\n\u2022 the resolution of a face image is normalized to 64\u00d7 64 pixels \u2022 the face images are decomposed into 16\u00d7 16 subregions and there is no modeling\nof statistical dependency among the subregions \u2022 the subregions are projected onto a 12-dimensional subspace (constructed by PCA) \u2022 the entire face region is normalized to have zero mean and unit variance."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 23
                            }
                        ],
                        "text": "H. Schneiderman and T. Kanade, Probabilistic modeling of local appearance and spatial relationships for object recognition, inIEEE Conference on Computer Vision and Pattern Recognition, 6, 1998."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 32
                            }
                        ],
                        "text": "H. A. Rowley, S. Baluja, and T. Kanade, Rotation invariant neural network-based face detection, inProc."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 32
                            }
                        ],
                        "text": "H. A. Rowley, S. Baluja, and T. Kanade, Neural network-based face detection,IEEE Trans."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 27
                            }
                        ],
                        "text": "T. Sakai, M. Nagao, and T. Kanade, Computer analysis and classification of photographs of human faces, in Proc."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 24
                            }
                        ],
                        "text": "Schneiderman and Kanade [169, 170] describe two face detectors based on Bayes\u2019 decision rule (presented as a likelihood ratio test in Eq."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 30
                            }
                        ],
                        "text": "S. Satoh, Y. Nakamura, and T. Kanade, Name-It: Naming and detecting faces in news videos,IEEE Multimedia6, 1999, 22\u201335."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1060186,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bbe488bb190d75f4b665d43e306bcab1ab228890",
            "isKey": true,
            "numCitedBy": 459,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we describe an algorithm for object recognition that explicitly models and estimated the posterior probability function, P(object/image). We have chosen a functional form of the posterior probability function that captures the joint statistics of local appearance and position on the object as well as the statistics of local appearance in the visual world at large. We use a discrete representation of local appearance consisting of approximately 10/sup 6/ patterns. We compute an estimate of P(object/image) in closed form by counting the frequency of occurrence of these patterns over various sets of training images. We have used this method for detecting human faces from frontal and profile views. The algorithm for frontal views has shown a detection rate of 93.0% with 88 false alarms on a set of 125 images containing 483 faces combining the MIT test set of Sung and Poggio with the CMU test sets of Rowley, Baluja, and Kanade. The algorithm for detection of profile views has also demonstrated promising results."
            },
            "slug": "Probabilistic-modeling-of-local-appearance-and-for-Schneiderman-Kanade",
            "title": {
                "fragments": [],
                "text": "Probabilistic modeling of local appearance and spatial relationships for object recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "An algorithm for object recognition that explicitly models and estimated the posterior probability function, P(object/image) in closed form is described, which captures the joint statistics of local appearance and position on the object as well as the statistics ofLocal appearance in the visual world at large."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 1998 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.98CB36231)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152292143"
                        ],
                        "name": "Jeffrey R. Huang",
                        "slug": "Jeffrey-R.-Huang",
                        "structuredName": {
                            "firstName": "Jeffrey R.",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeffrey R. Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1819046"
                        ],
                        "name": "Xuhui Shao",
                        "slug": "Xuhui-Shao",
                        "structuredName": {
                            "firstName": "Xuhui",
                            "lastName": "Shao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xuhui Shao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143979395"
                        ],
                        "name": "H. Wechsler",
                        "slug": "H.-Wechsler",
                        "structuredName": {
                            "firstName": "Harry",
                            "lastName": "Wechsler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Wechsler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1902304,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2c4c61440cfceddf1b1521ee7abf04ed31b79f37",
            "isKey": false,
            "numCitedBy": 225,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes an approach for the problem of face pose discrimination using support vector machines (SVM). Face pose discrimination means that one can label the face image as one of several known poses. Face images are drawn from the standard FERET database. The training set consists of 150 images equally distributed among frontal, approximately 33.75/spl deg/ rotated left and right poses, respectively, and the test set consists of 450 images again equally distributed among the three different types of poses. SVM achieved perfect accuracy-100%-discriminating between the three possible face poses on unseen test data, using either polynomials of degree 3 or radial basis functions (RBF) as kernel approximation functions."
            },
            "slug": "Face-pose-discrimination-using-support-vector-(SVM)-Huang-Shao",
            "title": {
                "fragments": [],
                "text": "Face pose discrimination using support vector machines (SVM)"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "This paper describes an approach for the problem of face pose discrimination using support vector machines (SVM), which achieved perfect accuracy-100%-discriminating between the three possible face poses on unseen test data, using either polynomials of degree 3 or radial basis functions (RBF) as kernel approximation functions."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. Fourteenth International Conference on Pattern Recognition (Cat. No.98EX170)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6435894"
                        ],
                        "name": "S. McKenna",
                        "slug": "S.-McKenna",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "McKenna",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. McKenna"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144784813"
                        ],
                        "name": "S. Gong",
                        "slug": "S.-Gong",
                        "structuredName": {
                            "firstName": "Shaogang",
                            "lastName": "Gong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Gong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115271255"
                        ],
                        "name": "J. Collins",
                        "slug": "J.-Collins",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Collins",
                            "middleNames": [
                                "James"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Collins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12570020,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5da7b0b7ef73e799e485f5fcb015360b11230998",
            "isKey": false,
            "numCitedBy": 65,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a dynamic face tracking system based on an integrated motion-based object tracking and model-based face detection framework. The motion-based tracker focuses attention for the face detector whilst the latter aids the tracking process. The system produces segmented face sequences from complex scenes with poor viewing conditions in surveillance applications. We also investigate a Gabor wavelet transform as a representation scheme for capturing head rotations in depth. Principal components analysis was used to visualise the mani-folds described by pose changes. Qualitative results are given."
            },
            "slug": "Face-Tracking-and-Pose-Representation-McKenna-Gong",
            "title": {
                "fragments": [],
                "text": "Face Tracking and Pose Representation"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "A dynamic face tracking system based on an integrated motion-based object tracking and model-based face detection framework that produces segmented face sequences from complex scenes with poor viewing conditions in surveillance applications is described."
            },
            "venue": {
                "fragments": [],
                "text": "BMVC"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35240264"
                        ],
                        "name": "S. Katahara",
                        "slug": "S.-Katahara",
                        "structuredName": {
                            "firstName": "Shunji",
                            "lastName": "Katahara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Katahara"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50459528"
                        ],
                        "name": "M. Aoki",
                        "slug": "M.-Aoki",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Aoki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Aoki"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 36258673,
            "fieldsOfStudy": [
                "Geology"
            ],
            "id": "d86cd304559cdb5bdecec5c7c83f13adf1083e12",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a simple algorithm to determine face parts extraction window in face image. We utilize bilateral symmetries between and within face parts. We also use knowledge about size and locationship of face parts. First, we examine bilateral symmetries around vertical orientation edge, then obtain symmetry measures. The symmetry measures are projected onto y-axis to produce histogram of the measures. We estimate height of face parts regions by frequency of the histogram. Face parts region, which contains maximum frequency of the histogram, becomes a candidate of face parts region that includes eyes and eyebrows. Secondly, the measures that exist within the height of the face parts region are projected onto x-axis to estimate width of face parts region. We determine face parts extraction windows by the estimated height and width. Finally, we detect irises in the candidate of face parts region that includes eyes and eyebrows, using circular mask."
            },
            "slug": "Face-Parts-Extraction-Windows-Based-on-Bilateral-of-Katahara-Aoki",
            "title": {
                "fragments": [],
                "text": "Face Parts Extraction Windows Based on Bilateral Symmetry of Gradient Direction"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A simple algorithm to determine face parts extraction window in face image using bilateral symmetries between and within face parts and detects irises in the candidate of face parts region that includes eyes and eyebrows, using circular mask."
            },
            "venue": {
                "fragments": [],
                "text": "CAIP"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "13930396"
                        ],
                        "name": "Tae-Woong Yoo",
                        "slug": "Tae-Woong-Yoo",
                        "structuredName": {
                            "firstName": "Tae-Woong",
                            "lastName": "Yoo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tae-Woong Yoo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40563148"
                        ],
                        "name": "Il-Seok Oh",
                        "slug": "Il-Seok-Oh",
                        "structuredName": {
                            "firstName": "Il-Seok",
                            "lastName": "Oh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Il-Seok Oh"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 206037919,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "42c8b2bfdfed27f2e5019b5b8ad8fcc426026a2e",
            "isKey": false,
            "numCitedBy": 103,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-fast-algorithm-for-tracking-human-faces-based-on-Yoo-Oh",
            "title": {
                "fragments": [],
                "text": "A fast algorithm for tracking human faces based on chromatic histograms"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit. Lett."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717217"
                        ],
                        "name": "R. J. Qian",
                        "slug": "R.-J.-Qian",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Qian",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. J. Qian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153652752"
                        ],
                        "name": "Thomas S. Huang",
                        "slug": "Thomas-S.-Huang",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Huang",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas S. Huang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 83
                            }
                        ],
                        "text": "Based on an earlier work of maximum likelihood face detection [21], Colmenarez and Huang [22] proposed a system based on Kullback relative information (Kullback divergence)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 26
                            }
                        ],
                        "text": "The technique of Yang and Huang was recently incorporated into a system for rotation invariant face detection by Lvet al. [119] and an extension of the algorithm is presented in Kotropoulos and Pitas [95]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 627,
                                "start": 622
                            }
                        ],
                        "text": "The number of labeled faces was originally 149 by Sung and Poggio (which is the number used in the reported results in [182] and [107], but it was changed to 155 (which\nTABLE 2 Results Reported in Terms of Percentage Correct Detection (CD) and Number of False Positives (FP), CD/FP, on the CMU and MIT Datasets\nFace detection system CMU-130 CMU-125 MIT-23 MIT-20\nSchneiderman & Kanade\u2014Ea [170] 94.4%/65 Schneiderman & Kanade\u2014Wb [170] 90.2%/110 Yanget al.\u2014FA [217] 92.3%/82 89.4%/3 Yanget al.\u2014LDA [217] 93.6%/74 91.5%/1 Rothet al. [157] 94.8%/78 94.1%/3 Rowleyet al. [158] 86.2%/23 84.5%/8 Feraudet al. [42] 86%/8 Colmenarez & Huang [22] 93.9%/8122 Sung & Poggio [182] 79.9%/5 Lew & Huijsmans [107] 94.1%/64 Osunaet al. [140] 74.2%/20 Lin et al. [113] 72.3%/6 Gu and Li [54] 87.1%/0\na Eigenvector coefficients. b Wavelet coefficients.\nis the number used in the reported results in [158], [140], and [113]) when included in the CMU dataset."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 27
                            }
                        ],
                        "text": "A. J. Colmenarez and T. S. Huang, Pattern detection with information-based maximum discrimination and error bootstrapping, inProc. of International Conference on Pattern Recognition, 1998."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 18
                            }
                        ],
                        "text": "G. Yang and T. S. Huang, Human face detection in a complex background,Pattern Recog.27, 1994, 53\u201363."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 15
                            }
                        ],
                        "text": "R. Qian and T. Huang, Object detection using hierarchical mrf and map estimation, inIEEE Proc. of Int."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "Huanget al.[74] also apply a Gaussian filter for pre-processing in a framework based on image feature analysis."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 32
                            }
                        ],
                        "text": "J. P. Phillips, H. Wechsler, J. Huang, and P. Rauss, The FERET database and evaluation procedure for face-recognition algorithms,Image Vision Comput."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 24
                            }
                        ],
                        "text": "In [23], Colmenarez and Huang further improved on this technique by including error bootstrapping (described in Section 4.2), and in [20] the technique was incorporated in a real-time face tracking system."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 27
                            }
                        ],
                        "text": "A. J. Colmenarez and T. S. Huang, Maximum likelihood face detection, inIEEE Proc. of 2nd Int."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 3
                            }
                        ],
                        "text": "J. Huang, S. Gutta, and H. Wechsler, Detection of human faces using decision trees, inIEEE Proc. of 2nd Int."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 37
                            }
                        ],
                        "text": "A. J. Colmenarez, B. Frey, and T. S. Huang, Detection and tracking of faces and facial features, inProceedings International Conference on Image Processing, 1999."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 9
                            }
                        ],
                        "text": "Yang and Huang [214], on the other hand, explore the gray-scale behavior of faces in mosaic (pyramid) images."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 3
                            }
                        ],
                        "text": "J. Huang and H. Wechsler, Eye detection using optimal wavelet packets and radial basis functions (rbfs),Int."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 71
                            }
                        ],
                        "text": "Due to the high computational requirement in the minimization process, Huang and Chen [69] and Lam and Yan [101] both employ fast iteration methods (greedy algorithms) for faster convergence."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 3
                            }
                        ],
                        "text": "J. Huang and H. Wechsler, Visual routines for eye location using learning and evolution, inIEEE Transactions on Evolutionary Computation, 1999."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 15
                            }
                        ],
                        "text": "Colmenarez and Huang use a large set of 11\u00d7 11 images of faces and nonfaces for training, and the training procedure results in a set of look-up tables with likelihood ratios."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 3
                            }
                        ],
                        "text": "W. Huang, Q. Sun, C. P. Lam, and J. K. Wu, A robust approach to face and eyes detection from images with cluttered background, inProc. of International Conference on Pattern Recognition, 1998."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 6
                            }
                        ],
                        "text": "C. L. Huang and C. W. Chen, Human facial feature extraction for face interpretation and recognition,Pat er Recog.25, 1992, 1435\u20131444."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 3
                            }
                        ],
                        "text": "W. Huang and R. Mariani, Face detection and precise eyes location, inProceedings of the 15th International Conference on Pattern Recognition, 2000, Vol. IV, p. 3B.\n74."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 17
                            }
                        ],
                        "text": "Q. B. Sun, W. M. Huang, and J. K. Wu, Face detection based on color and local symmetry information, in IEEE Proc. of 3rd Int."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 80
                            }
                        ],
                        "text": "Bayes\u2019 decision rule has also been applied for face detection by Qian and Huang [148]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 27
                            }
                        ],
                        "text": "A. J. Colmenarez and T. S. Huang, Face detection with information-based maximum discrimination, inIEEE Proc. of Int."
                    },
                    "intents": []
                }
            ],
            "corpusId": 9413838,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b54f72748b74fc815aa802aaef90c3685ca8f3d6",
            "isKey": true,
            "numCitedBy": 40,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a new scale, position and orientation invariant approach to object detection. The proposed method first chooses attention regions in an image based on the region detection result on the image. Within the attention regions, the method then detects targets using a novel object detection algorithm that combines template matching methods with feature-based methods via hierarchical MRF and MAP estimation. Hierarchical MRF and MAP estimation provide a flexible framework to incorporate various visual clues. The combination of template matching and feature detection helps to achieve robustness against complex backgrounds and partial occlusions in object detection. Experimental results are given in the paper."
            },
            "slug": "Object-detection-using-hierarchical-MRF-and-MAP-Qian-Huang",
            "title": {
                "fragments": [],
                "text": "Object detection using hierarchical MRF and MAP estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A novel object detection algorithm that combines template matching methods with feature-based methods via hierarchical MRF and MAP estimation is presented, which helps to achieve robustness against complex backgrounds and partial occlusions in object detection."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152179974"
                        ],
                        "name": "Jianming Hu",
                        "slug": "Jianming-Hu",
                        "structuredName": {
                            "firstName": "Jianming",
                            "lastName": "Hu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianming Hu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152996923"
                        ],
                        "name": "Hong Yan",
                        "slug": "Hong-Yan",
                        "structuredName": {
                            "firstName": "Hong",
                            "lastName": "Yan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hong Yan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40435157"
                        ],
                        "name": "M. Sakalli",
                        "slug": "M.-Sakalli",
                        "structuredName": {
                            "firstName": "Mustafa",
                            "lastName": "Sakalli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Sakalli"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16940781,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "958b737459ad41d828a720896030aa0ffc01f43e",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Locating-head-and-face-boundaries-for-head-shoulder-Hu-Yan",
            "title": {
                "fragments": [],
                "text": "Locating head and face boundaries for head-shoulder images"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2586567"
                        ],
                        "name": "M. Collobert",
                        "slug": "M.-Collobert",
                        "structuredName": {
                            "firstName": "Michel",
                            "lastName": "Collobert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collobert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3430625"
                        ],
                        "name": "R. F\u00e9raud",
                        "slug": "R.-F\u00e9raud",
                        "structuredName": {
                            "firstName": "Rapha\u00ebl",
                            "lastName": "F\u00e9raud",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. F\u00e9raud"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2084746211"
                        ],
                        "name": "G. Tourneur",
                        "slug": "G.-Tourneur",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Tourneur",
                            "middleNames": [
                                "Le"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Tourneur"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3118328"
                        ],
                        "name": "O. Bernier",
                        "slug": "O.-Bernier",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Bernier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Bernier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35094984"
                        ],
                        "name": "J. Viallet",
                        "slug": "J.-Viallet",
                        "structuredName": {
                            "firstName": "Jean-Emmanuel",
                            "lastName": "Viallet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Viallet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052697"
                        ],
                        "name": "Y. Mahieux",
                        "slug": "Y.-Mahieux",
                        "structuredName": {
                            "firstName": "Yannick",
                            "lastName": "Mahieux",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Mahieux"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2444622"
                        ],
                        "name": "D. Collobert",
                        "slug": "D.-Collobert",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Collobert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Collobert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 98
                            }
                        ],
                        "text": "M. Collobert, R. Feraud, G. Le Tourneur, O. Bernier, J. E. Viallet, Y. Mahieux, and D. Collobert, LISTEN: A system for locating and tracking individual speaker, inIEEE Proc. of 2nd Int."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 56
                            }
                        ],
                        "text": "Approaches which focus on single camera control include LISTEN [19],\nSAVI [60], LAFTER [139], Rits Eye [212], and the system of Morimoto and Flickner [135]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 63
                            }
                        ],
                        "text": "Approaches which focus on single camera control include LISTEN [19],"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17288542,
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "id": "3af7faa8548ff45dd64c530a12344725bf8a1c84",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Both visual and acoustical informations provide effective means of telecommunication between persons. In this context, the face is the most important part of the person both visually and acoustically. We describe how the cooperation of image and audio processing allows to track a person's face and to collect the audio information it produces. We present detection techniques of regions of interest (e.g. Moving regions of skin color), coupled with a neural network based face detector with a low false alarm rate, to locate and track faces. The system is connected to a nine microphone array adaptive beam forming which performs immediate beam forming. Visual and acoustical informations from the speaker face are thus obtained in real time."
            },
            "slug": "LISTEN:-a-system-for-locating-and-tracking-speakers-Collobert-F\u00e9raud",
            "title": {
                "fragments": [],
                "text": "LISTEN: a system for locating and tracking individual speakers"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work describes how the cooperation of image and audio processing allows to track a person's face and to collect the audio information it produces, coupled with a neural network based face detector with a low false alarm rate, to locate and track faces."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Second International Conference on Automatic Face and Gesture Recognition"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2072474421"
                        ],
                        "name": "E. Hjelm",
                        "slug": "E.-Hjelm",
                        "structuredName": {
                            "firstName": "Erik",
                            "lastName": "Hjelm",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Hjelm"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15832477,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "764ccc4353f17b807fdbc7bff1094f9a4cab94b3",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "The eyes are one of the most important facial features for recognizing human faces. Many face recognition systems today make use of local features (such as eyes) for identiication or veriica-tion of individuals, but no system to our knowledge has studied performance when the only available information is the eyes. In this paper we show that we can obtain 85% correct classiication on the popular ORL face database, when the features are extracted from the eye area only. We compare feature extraction from eigenfeatures and Gabor wavelets with features consisting of simple gray-level pixel values."
            },
            "slug": "Recognizing-Faces-from-the-Eyes-OnlyE-Hjelm",
            "title": {
                "fragments": [],
                "text": "Recognizing Faces from the Eyes OnlyE"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper shows that it can obtain 85% correct classiication on the popular ORL face database, when the features are extracted from the eye area only, and compares feature extraction from eigenfeatures and Gabor wavelets with features consisting of simple gray-level pixel values."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144022223"
                        ],
                        "name": "M. Kapfer",
                        "slug": "M.-Kapfer",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Kapfer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kapfer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1381345946"
                        ],
                        "name": "J. Benois-Pineau",
                        "slug": "J.-Benois-Pineau",
                        "structuredName": {
                            "firstName": "Jenny",
                            "lastName": "Benois-Pineau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Benois-Pineau"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 135
                            }
                        ],
                        "text": "Since the representation strongly relates to human perception of color [106, 178], it is also widely used in face segmentation schemes [51, 83, 118, 125, 175, 178, 187, 220]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12506779,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5d77a1fd6a2a94c0ae2e6ca02f4a519d724138a4",
            "isKey": false,
            "numCitedBy": 21,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Detection-of-human-faces-in-color-image-sequences-Kapfer-Benois-Pineau",
            "title": {
                "fragments": [],
                "text": "Detection of human faces in color image sequences with arbitrary motions for very low bit-rate videophone coding"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit. Lett."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50564384"
                        ],
                        "name": "G. Edwards",
                        "slug": "G.-Edwards",
                        "structuredName": {
                            "firstName": "Gareth",
                            "lastName": "Edwards",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Edwards"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144482985"
                        ],
                        "name": "C. Taylor",
                        "slug": "C.-Taylor",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Taylor",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7205190"
                        ],
                        "name": "Tim Cootes",
                        "slug": "Tim-Cootes",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Cootes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tim Cootes"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In [39], Edwards et al."
                    },
                    "intents": []
                }
            ],
            "corpusId": 5497564,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "98f4a96d0bb9112fffceb108cbbab5bf80407f84",
            "isKey": false,
            "numCitedBy": 82,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the problem of robust face identification in the presence of pose, lighting, and expression variation. Previous approaches to the problem have assumed similar models of variation for each individual, estimated from pooled training data. We describe a method of updating a first order global estimate of identity by learning the class-specific correlation between the estimate and the residual variation during a sequence. This is integrated with an optimal tracking scheme, in which identity variation is decoupled from pose, lighting and expression variation. The method results in robust tracking and a more stable estimate of facial identity under changing conditions."
            },
            "slug": "Learning-to-identify-and-track-faces-in-image-Edwards-Taylor",
            "title": {
                "fragments": [],
                "text": "Learning to identify and track faces in image sequences"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A method of updating a first order global estimate of identity by learning the class-specific correlation between the estimate and the residual variation during a sequence is described, integrated with an optimal tracking scheme, in which identity variation is decoupled from pose, lighting and expression variation."
            },
            "venue": {
                "fragments": [],
                "text": "Sixth International Conference on Computer Vision (IEEE Cat. No.98CH36271)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50107932"
                        ],
                        "name": "A. Schubert",
                        "slug": "A.-Schubert",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Schubert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Schubert"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 3
                            }
                        ],
                        "text": "A. Schubert, Detection and tracking of facial features in real time using a synergistic approach of spatiotemporal models and generalized hough-transform techniques, inProceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition, 2000."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 77
                            }
                        ],
                        "text": "The generalized Hough transform has also been used in the system of Schubert [171]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 30937058,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1852b7b76e767046437b52fc221679a1cc056cab",
            "isKey": false,
            "numCitedBy": 14,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "The proposed algorithm requires the description of the facial features as 3D-polygons (optionally extended by additional intensity information) which are assembled in a 3D-model of the head provided for in separate data files. Detection is achieved by using a special implementation of the generalized Hough transform (GHT) for which the forms are generated by projecting the 3D-model into the image plane. In the initialization phase a comparatively wide range of relative positions and attitudes between head and camera has to be tested for. Aiming for illumination-independence, only information about the sign of the difference between the expected intensities on both sides of the edge of the polygons may be additionally used in the GHT. Once a feature is found, further search for the remaining features can be restricted by the use of the 3D-model. The detection of a minimum number of features starts the tracking phase which is performed by using an extended Kalman filter (EKF) and assuming a first- or second-order dynamical model for the state variables describing the position and the attitude of the head. Synergistic advantages between GHT and EKF can be realized since the EKF and the projection into the image plane yield a rather good prediction of the forms to be detected by the GHT. This reduces considerably the search space in the image and in the parameter space. On the other hand the GHT offers a solution to the matching problem between image and object features. During the tracking phase the GHT can be further enhanced by monitoring the actual intensities along the edges of the polygons, their assignment to the corresponding 3D-object features, and their use for feature selection during the accumulation process. The algorithm runs on a dual Pentium II 333 MHz with a cycle time of 40 ms in real time."
            },
            "slug": "Detection-and-tracking-of-facial-features-in-real-a-Schubert",
            "title": {
                "fragments": [],
                "text": "Detection and tracking of facial features in real time using a synergistic approach of spatio-temporal models and generalized Hough-transform techniques"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "The proposed algorithm requires the description of the facial features as3D-polygons which are assembled in a 3D-model of the head provided for in separate data files by using a special implementation of the generalized Hough transform (GHT), a solution to the matching problem between image and object features."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145519965"
                        ],
                        "name": "Chun-Hung Lin",
                        "slug": "Chun-Hung-Lin",
                        "structuredName": {
                            "firstName": "Chun-Hung",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chun-Hung Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50738882"
                        ],
                        "name": "Ja-Ling Wu",
                        "slug": "Ja-Ling-Wu",
                        "structuredName": {
                            "firstName": "Ja-Ling",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ja-Ling Wu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 107
                            }
                        ],
                        "text": "Other proposed approaches for feature searching include radial basis functions [71] and genetic algorithms [72, 112, 144]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12248965,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9e208e82ea2b3b2a60db4e1d90628e791c9cad8a",
            "isKey": false,
            "numCitedBy": 59,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "An automatic facial feature extraction algorithm is presented in this paper. The algorithm is composed of two main stages: the face region estimation stage and the feature extraction stage. In the face region estimation stage, a second-chance region growing method is adopted to estimate the face region of a target image. In the feature extraction stage, genetic search algorithms are applied to extract the facial feature points within the face region. It is shown by simulation results that the proposed algorithm can automatically and exactly extract facial features with limited computational complexity."
            },
            "slug": "Automatic-facial-feature-extraction-by-genetic-Lin-Wu",
            "title": {
                "fragments": [],
                "text": "Automatic facial feature extraction by genetic algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown by simulation results that the proposed algorithm can automatically and exactly extract facial features with limited computational complexity."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Image Process."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2413438"
                        ],
                        "name": "K. Yachi",
                        "slug": "K.-Yachi",
                        "structuredName": {
                            "firstName": "Kiyotake",
                            "lastName": "Yachi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Yachi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702631"
                        ],
                        "name": "T. Wada",
                        "slug": "T.-Wada",
                        "structuredName": {
                            "firstName": "Toshikazu",
                            "lastName": "Wada",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Wada"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143658244"
                        ],
                        "name": "T. Matsuyama",
                        "slug": "T.-Matsuyama",
                        "structuredName": {
                            "firstName": "Takashi",
                            "lastName": "Matsuyama",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Matsuyama"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 32687090,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "76acf09c11fd52ce0660bef428ff887aa8ea9bd4",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a method for detecting and tracking a human head in real time from an image sequence. The proposed method has three advantages: (1) we employ a fixed-viewpoint pan-tilt-zoom camera to acquire image sequences; with the camera, we eliminate the variations in the head appearance due to camera rotations with respect to the viewpoint; (2) we prepare a variety of contour models of the head appearances and relate them to the camera parameters; this allows us to adaptively select the model to deal with the variations in the head appearance due to human activities; (3) we use the model parameters obtained by detecting the head in the previous image to estimate those to be fitted in the current image; this estimation facilitates computational time for the head detection. Accordingly, the accuracy of the detection and required computational time are both improved and, at the same time, the robust head detection and tracking are realized in almost real time. Experimental results in the real situation show the effectiveness of our method."
            },
            "slug": "Human-head-tracking-using-adaptive-appearance-with-Yachi-Wada",
            "title": {
                "fragments": [],
                "text": "Human head tracking using adaptive appearance models with a fixed-viewpoint pan-tilt-zoom camera"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The accuracy of the detection and required computational time are both improved and, at the same time, the robust head detection and tracking are realized in almost real time."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145687022"
                        ],
                        "name": "J. Crowley",
                        "slug": "J.-Crowley",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Crowley",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Crowley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "146688280"
                        ],
                        "name": "F. B\u00e9rard",
                        "slug": "F.-B\u00e9rard",
                        "structuredName": {
                            "firstName": "Fran\u00e7ois",
                            "lastName": "B\u00e9rard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. B\u00e9rard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 91
                            }
                        ],
                        "text": "Among the literature survey, a pair of eyes is the most commonly applied reference feature [5, 27, 52, 61, 207, 214] due to its distinct side-by-side appearance."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 200,
                                "start": 145
                            }
                        ],
                        "text": "Since the main variation in skin appearance is largely due to luminance change (brightness) [215], normalized RGB colors are generally preferred [27, 53, 75, 88, 92, 165, 181, 196, 202, 207, 213, 215], so that the effect of luminance can be filtered out."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 21
                            }
                        ],
                        "text": "J. L. Crowley and F. Berard, Multi-model tracking of faces for video communications, inIEEE Proc. of Int, Conf. on Computer Vision and Pattern Recognition, Puerto Rico, Jun. 1997."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 0
                            }
                        ],
                        "text": "B\u00b4erard, LAFTER: A real-time face and lips tracker with facial expression recognition,Pattern Recog.33, 2000, 1369\u20131382."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 65
                            }
                        ],
                        "text": "Besides face region, Luthon and Lievin [118], Crowley and Berard [27], and Low [115, 116] also employ frame difference to locate facial features."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "In [27], the existence of an eye-pair is hypothesized by measuring the horizontal and the vertical displacements between two adjacent candidate regions obtained from frame difference."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 108
                            }
                        ],
                        "text": "More complex methods make use of statistical measures that model face variation within a wide user spectrum [3, 27, 75, 125, 139, 215]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 257283,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1dc544677e8176a2043f7d5080eba3499a928316",
            "isKey": true,
            "numCitedBy": 253,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Visual processes to detect and track faces for video compression and transmission. The system is based on an architecture in which a supervisor selects and activates visual processes in cyclic manner. Control of visual processes is made possible by a confidence factor which accompanies each observation. Fusion of results into a unified estimation for tracking is made possible by estimating a covariance matrix with each observation. Visual processes for face tracking are described using blink detection, normalised color histogram matching, and cross correlation (SSD and NCC). Ensembles of visual processes are organised into processing states so as to provide robust tracking. Transition between states is determined by events detected by processes. The result of face detection is fed into recursive estimator (Kalman filter). The output from the estimator drives a PD controller for a pan/tilt/zoom camera. The resulting system provides robust and precise tracking which operates continuously at approximately 20 images per second on a 150 megahertz computer workstation."
            },
            "slug": "Multi-modal-tracking-of-faces-for-video-Crowley-B\u00e9rard",
            "title": {
                "fragments": [],
                "text": "Multi-modal tracking of faces for video communications"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "Visual processes to detect and track faces for video compression and transmission based on an architecture in which a supervisor selects and activates visual processes in cyclic manner provides robust and precise tracking."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1400601690"
                        ],
                        "name": "A. O'Toole",
                        "slug": "A.-O'Toole",
                        "structuredName": {
                            "firstName": "Alice",
                            "lastName": "O'Toole",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. O'Toole"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144319186"
                        ],
                        "name": "H. Abdi",
                        "slug": "H.-Abdi",
                        "structuredName": {
                            "firstName": "Herv\u00e9",
                            "lastName": "Abdi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Abdi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4666577"
                        ],
                        "name": "K. Deffenbacher",
                        "slug": "K.-Deffenbacher",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Deffenbacher",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Deffenbacher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145391215"
                        ],
                        "name": "D. Valentin",
                        "slug": "D.-Valentin",
                        "structuredName": {
                            "firstName": "Dominique",
                            "lastName": "Valentin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Valentin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121463712,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "af78b1232828f187a61cf144a4265ff98182af87",
            "isKey": false,
            "numCitedBy": 252,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Faces can be represented efficiently as a weighted linear combination of the eigenvectors of a covariance matrix of face images. It has also been shown [ J. Opt. Soc. Am.4, 519\u2013 524 ( 1987)] that identifiable faces can be made by using only a subset of the eigenvectors, i.e., those with the largest eigenvalues. This low-dimensional representation is optimal in that it minimizes the squared error between the representation of the face image and the original face image. The present study demonstrates that, whereas this low-dimensional representation is optimal for identifying the physical categories of face, like sex, it is not optimal for recognizing the faces (i.e., discriminating known from unknown faces). Various low-dimensional representations of the faces in the higher dimensions of the face space (i.e., the eigenvectors with smaller eigenvalues) provide better information for face recognition."
            },
            "slug": "Low-dimensional-representation-of-faces-in-higher-O'Toole-Abdi",
            "title": {
                "fragments": [],
                "text": "Low-dimensional representation of faces in higher dimensions of the face space"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Various low-dimensional representations of the faces in the higher dimensions of the face space (i.e., the eigenvectors with smaller eigenvalues) provide better information for face recognition."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118579343"
                        ],
                        "name": "Jie Yang",
                        "slug": "Jie-Yang",
                        "structuredName": {
                            "firstName": "Jie",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jie Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "23705199"
                        ],
                        "name": "Weier Lu",
                        "slug": "Weier-Lu",
                        "structuredName": {
                            "firstName": "Weier",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Weier Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724972"
                        ],
                        "name": "A. Waibel",
                        "slug": "A.-Waibel",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Waibel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Waibel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6119206,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "23c52e4c82d278ea6ff92e8de15e6458dc4af0b4",
            "isKey": false,
            "numCitedBy": 497,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper studies a statistical skin-color model and its adaptation. It is revealed that (1) human skin colors cluster in a small region in a color space; (2) the variance of a skin color cluster can be reduced by intensity normalization, and (3) under a certain lighting condition, a skin-color distribution can be characterized by a multivariate normal distribution in the normalized color space. We then propose an adaptive model to characterize human skin-color distributions for tracking human faces under different lighting conditions. The parameters of the model are adapted based on the maximum likelihood criterion. The model has been successfully applied to a real-time face tracker and other applications."
            },
            "slug": "Skin-Color-Modeling-and-Adaptation-Yang-Lu",
            "title": {
                "fragments": [],
                "text": "Skin-Color Modeling and Adaptation"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "An adaptive model is proposed to characterize human skin-color distributions for tracking human faces under different lighting conditions based on the maximum likelihood criterion and has been successfully applied to a real-time face tracker and other applications."
            },
            "venue": {
                "fragments": [],
                "text": "ACCV"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145266088"
                        ],
                        "name": "T. Leung",
                        "slug": "T.-Leung",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Leung",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Leung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2511655"
                        ],
                        "name": "M. Burl",
                        "slug": "M.-Burl",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Burl",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Burl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2904067,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0d50d0e2af0b45cc7ed25fe4aa97af900c9bd32a",
            "isKey": false,
            "numCitedBy": 389,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "An algorithm for locating quasi-frontal views of human faces in cluttered scenes is presented. The algorithm works by coupling a set of local feature detectors with a statistical model of the mutual distances between facial features it is invariant with respect to translation, rotation (in the plane), and scale and can handle partial occlusions of the face. On a challenging database with complicated and varied backgrounds, the algorithm achieved a correct localization rate of 95% in images where the face appeared quasi-frontally.<<ETX>>"
            },
            "slug": "Finding-faces-in-cluttered-scenes-using-random-Leung-Burl",
            "title": {
                "fragments": [],
                "text": "Finding faces in cluttered scenes using random labeled graph matching"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "An algorithm for locating quasi-frontal views of human faces in cluttered scenes is presented and it is found that it is invariant with respect to translation, rotation, and scale and can handle partial occlusions of the face."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE International Conference on Computer Vision"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144522420"
                        ],
                        "name": "T. Ojala",
                        "slug": "T.-Ojala",
                        "structuredName": {
                            "firstName": "Timo",
                            "lastName": "Ojala",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Ojala"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145962204"
                        ],
                        "name": "M. Pietik\u00e4inen",
                        "slug": "M.-Pietik\u00e4inen",
                        "structuredName": {
                            "firstName": "Matti",
                            "lastName": "Pietik\u00e4inen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pietik\u00e4inen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "26883091"
                        ],
                        "name": "Topi M\u00e4enp\u00e4\u00e4",
                        "slug": "Topi-M\u00e4enp\u00e4\u00e4",
                        "structuredName": {
                            "firstName": "Topi",
                            "lastName": "M\u00e4enp\u00e4\u00e4",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Topi M\u00e4enp\u00e4\u00e4"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14540685,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f11a7136b6b7854bd0998ef463ffa8e907c411a2",
            "isKey": false,
            "numCitedBy": 13066,
            "numCiting": 95,
            "paperAbstract": {
                "fragments": [],
                "text": "Presents a theoretically very simple, yet efficient, multiresolution approach to gray-scale and rotation invariant texture classification based on local binary patterns and nonparametric discrimination of sample and prototype distributions. The method is based on recognizing that certain local binary patterns, termed \"uniform,\" are fundamental properties of local image texture and their occurrence histogram is proven to be a very powerful texture feature. We derive a generalized gray-scale and rotation invariant operator presentation that allows for detecting the \"uniform\" patterns for any quantization of the angular space and for any spatial resolution and presents a method for combining multiple operators for multiresolution analysis. The proposed approach is very robust in terms of gray-scale variations since the operator is, by definition, invariant against any monotonic transformation of the gray scale. Another advantage is computational simplicity as the operator can be realized with a few operations in a small neighborhood and a lookup table. Experimental results demonstrate that good discrimination can be achieved with the occurrence statistics of simple rotation invariant local binary patterns."
            },
            "slug": "Multiresolution-Gray-Scale-and-Rotation-Invariant-Ojala-Pietik\u00e4inen",
            "title": {
                "fragments": [],
                "text": "Multiresolution Gray-Scale and Rotation Invariant Texture Classification with Local Binary Patterns"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A generalized gray-scale and rotation invariant operator presentation that allows for detecting the \"uniform\" patterns for any quantization of the angular space and for any spatial resolution and presents a method for combining multiple operators for multiresolution analysis."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153281777"
                        ],
                        "name": "D. Marr",
                        "slug": "D.-Marr",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Marr",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Marr"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31857045"
                        ],
                        "name": "E. Hildreth",
                        "slug": "E.-Hildreth",
                        "structuredName": {
                            "firstName": "Ellen",
                            "lastName": "Hildreth",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Hildreth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2150419,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "9009c9685754346deb93f316144a9da1f70ffcd8",
            "isKey": false,
            "numCitedBy": 7031,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "A theory of edge detection is presented. The analysis proceeds in two parts. (1) Intensity changes, which occur in a natural image over a wide range of scales, are detected separately at different scales. An appropriate filter for this purpose at a given scale is found to be the second derivative of a Gaussian, and it is shown that, provided some simple conditions are satisfied, these primary filters need not be orientation-dependent. Thus, intensity changes at a given scale are best detected by finding the zero values of \u22072G(x, y)* I(x, y) for image I, where G(x, y) is a two-dimensional Gaussian distribution and \u22072 is the Laplacian. The intensity changes thus discovered in each of the channels are then represented by oriented primitives called zero-crossing segments, and evidence is given that this representation is complete. (2) Intensity changes in images arise from surface discontinuities or from reflectance or illumination boundaries, and these all have the property that they are spatially localized. Because of this, the zero-crossing segments from the different channels are not independent, and rules are deduced for combining them into a description of the image. This description is called the raw primal sketch. The theory explains several basic psychophysical findings, and the operation of forming oriented zero-crossing segments from the output of centre-surround \u22072G filters acting on the image forms the basis for a physiological model of simple cells (see Marr & Ullman 1979)."
            },
            "slug": "Theory-of-edge-detection-Marr-Hildreth",
            "title": {
                "fragments": [],
                "text": "Theory of edge detection"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The theory of edge detection explains several basic psychophysical findings, and the operation of forming oriented zero-crossing segments from the output of centre-surround \u22072G filters acting on the image forms the basis for a physiological model of simple cells."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Royal Society of London. Series B. Biological Sciences"
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111778147"
                        ],
                        "name": "Zhong Jing",
                        "slug": "Zhong-Jing",
                        "structuredName": {
                            "firstName": "Zhong",
                            "lastName": "Jing",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhong Jing"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737713"
                        ],
                        "name": "R. Mariani",
                        "slug": "R.-Mariani",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Mariani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mariani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 83
                            }
                        ],
                        "text": "Edge-based techniques have also been applied to detecting glasses in facial images [79, 80]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 28563362,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3b7c4bd04ee6d97418e2f767a28cc7250bf77777",
            "isKey": false,
            "numCitedBy": 56,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "To achieve a face recognition system robust to the presence of spectacles, we have developed a spectacle detection and extraction algorithm. The detection is realized using edge information within a small area defined between the eyes. The extraction is achieved with a deformable contour, combining edge features such as strength and orientation and geometrical features such as convexity, symmetry, smoothness and continuity. The final position of the deformable contour is obtained using dynamic programming. Experimental results demonstrate the effectiveness of the method, and the robustness to the variation of spectacle shapes and colours."
            },
            "slug": "Glasses-detection-and-extraction-by-deformable-Jing-Mariani",
            "title": {
                "fragments": [],
                "text": "Glasses detection and extraction by deformable contour"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "To achieve a face recognition system robust to the presence of spectacles, a spectacle detection and extraction algorithm is developed, combining edge features such as strength and orientation and geometrical featuressuch as convexity, symmetry, smoothness and continuity."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 15th International Conference on Pattern Recognition. ICPR-2000"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681254"
                        ],
                        "name": "M. Reinders",
                        "slug": "M.-Reinders",
                        "structuredName": {
                            "firstName": "Marcel",
                            "lastName": "Reinders",
                            "middleNames": [
                                "J.",
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Reinders"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054275048"
                        ],
                        "name": "R. Koch",
                        "slug": "R.-Koch",
                        "structuredName": {
                            "firstName": "Reinhard",
                            "lastName": "Koch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Koch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3322989"
                        ],
                        "name": "J. J. Gerbrands",
                        "slug": "J.-J.-Gerbrands",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Gerbrands",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. J. Gerbrands"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12233377,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "45fc10c11bb842695443328208405240055e71f0",
            "isKey": false,
            "numCitedBy": 91,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper describes a method for the automatic location of facial features, such as eyes, nose and mouth, in image sequences using a neural network approach. It is shown that by modeling the feature sought as a structural assembly of micro-features, and by using a probabilistic interpretation of neural network outputs, it is possible to construct a location system that is more robust than a location system which uses the feature as a single entity. With this micro-feature approach, not only can the position of the features be found but shape of the features can be obtained as well."
            },
            "slug": "Locating-facial-features-in-image-sequences-using-Reinders-Koch",
            "title": {
                "fragments": [],
                "text": "Locating facial features in image sequences using neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "It is shown that by modeling the feature sought as a structural assembly of micro-features, and by using a probabilistic interpretation of neural network outputs, it is possible to construct a location system that is more robust than a locations system which uses the feature as a single entity."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Second International Conference on Automatic Face and Gesture Recognition"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054024143"
                        ],
                        "name": "B. K. Low",
                        "slug": "B.-K.-Low",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Low",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. K. Low"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47836515"
                        ],
                        "name": "M. Ibrahim",
                        "slug": "M.-Ibrahim",
                        "structuredName": {
                            "firstName": "Mohammad",
                            "lastName": "Ibrahim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Ibrahim"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 6
                            }
                        ],
                        "text": "B. K. Low,Computer Extraction of Human Faces,PhD thesis, Dept. of Electronic and Electrical Engineering, De Montfort University, 1998."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 6
                            }
                        ],
                        "text": "B. K. Low and M. K. Ibrahim, A fast and accurate algorithm for facial feature segmentation, inProceedings International Conference on Image Processing, 1997."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 62
                            }
                        ],
                        "text": "More recent examples of edge-based techniques can be found in [9, 16, 59, 115, 116] for facial feature extraction and in [32, 50, 58, 60, 70, 76, 108, 201, 222] for face detection."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 280,
                                "start": 277
                            }
                        ],
                        "text": "Computer Vision and Image Understanding83,236\u2013274 (2001) doi:10.1006/cviu.2001.0921, available online at http://www.idealibrary.com on\nFace Detection: A Survey\nErik Hjelma\u030as1\nDepartment of Informatics, University of Oslo, P.O. Box 1080 Blindern, N-0316 Oslo, Norway\nand\nBoon Kee Low\nDepartment of Meteorology, University of Edinburgh, JCMB, Kings Buildings, Mayfield Road, Edinburgh EH9 3JZ, Scotland, United Kingdom\nE-mail: erikh@hig.no; boon@met.ed.ac.uk\nReceived October 23, 2000; accepted April 17, 2001\nIn this paper we present a comprehensive and critical survey of face detection algorithms."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 79
                            }
                        ],
                        "text": "Besides face region, Luthon and Lievin [118], Crowley and Berard [27], and Low [115, 116] also employ frame difference to locate facial features."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 26
                            }
                        ],
                        "text": "L. Sirovich and M. Kirby, Low-dimensional procedure for the characterization of human faces,J. Opt."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "Low-Level Analysis\n3.1.1."
                    },
                    "intents": []
                }
            ],
            "corpusId": 16615943,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "251715a6154d2fbb362c17858df8773a722deb8b",
            "isKey": true,
            "numCitedBy": 15,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "A facial feature segmentation algorithm for head and shoulder sequences is proposed. The method is based on simple spatial and temporal heuristics techniques for the purpose of practical implementation. Facial features are segmented using two common cues (i) regular and high image intensity variation in the temporal domain due to various motion like talking and moving, and (ii) high edge density around the feature regions. A simple motion detection method and a generic spatial operator are applied to achieve the required segmentation. The paper addresses the issue of how to combine both information to obtain final results and also assesses the segmentation performance."
            },
            "slug": "A-fast-and-accurate-algorithm-for-facial-feature-Low-Ibrahim",
            "title": {
                "fragments": [],
                "text": "A fast and accurate algorithm for facial feature segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The paper addresses the issue of how to combine both information to obtain final results and also assesses the segmentation performance."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of International Conference on Image Processing"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103801794"
                        ],
                        "name": "Xiao-Yue Jiang",
                        "slug": "Xiao-Yue-Jiang",
                        "structuredName": {
                            "firstName": "Xiao-Yue",
                            "lastName": "Jiang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiao-Yue Jiang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38940654"
                        ],
                        "name": "M. Binkert",
                        "slug": "M.-Binkert",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Binkert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Binkert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38385597"
                        ],
                        "name": "B. Achermann",
                        "slug": "B.-Achermann",
                        "structuredName": {
                            "firstName": "Bernard",
                            "lastName": "Achermann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Achermann"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 83
                            }
                        ],
                        "text": "Edge-based techniques have also been applied to detecting glasses in facial images [79, 80]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 34423795,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "471185a3950844c0d5058a30cf6cff30ece4b381",
            "isKey": false,
            "numCitedBy": 43,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we consider the automatic detection of the presence of glasses. Using the eye locations detected in facial images, two regions of interest around the eyes are investigated where glasses are expected. We introduce six measures for the likelihood of glasses. An experimental evaluation on two image sets demonstrates reasonable results of separating facial images with and without glasses. Furthermore, a combination of measures turns out to consistently improve the performance of the individual measures. We also perform a sensitivity analysis by means of simulation to figure out the inaccuracy of eye detection that can be tolerated by our glasses detection method."
            },
            "slug": "Towards-Detection-of-Glasses-in-Facial-Images-Jiang-Binkert",
            "title": {
                "fragments": [],
                "text": "Towards Detection of Glasses in Facial Images"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "Using the eye locations detected in facial images, two regions of interest around the eyes are investigated where glasses are expected and a combination of measures turns out to consistently improve the performance of the individual measures."
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Analysis & Applications"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2510464"
                        ],
                        "name": "P. Duchnowski",
                        "slug": "P.-Duchnowski",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Duchnowski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Duchnowski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2566599"
                        ],
                        "name": "M. Hunke",
                        "slug": "M.-Hunke",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Hunke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hunke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2077372639"
                        ],
                        "name": "Dietrich B\u00fcsching",
                        "slug": "Dietrich-B\u00fcsching",
                        "structuredName": {
                            "firstName": "Dietrich",
                            "lastName": "B\u00fcsching",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dietrich B\u00fcsching"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145352356"
                        ],
                        "name": "U. Meier",
                        "slug": "U.-Meier",
                        "structuredName": {
                            "firstName": "Uwe",
                            "lastName": "Meier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Meier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724972"
                        ],
                        "name": "A. Waibel",
                        "slug": "A.-Waibel",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Waibel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Waibel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 106
                            }
                        ],
                        "text": "Apart from face classification, neural networks have also been applied for facial features classification [36, 125, 151, 196] and a method for improving detection time for MLPs is presented in [6]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1351369,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fa638eed7fc69eeb2d4773efb11c4cb49028b0ab",
            "isKey": false,
            "numCitedBy": 96,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We present the development of a modular system for flexible human-computer interaction via speech. The speech recognition component integrates acoustic and visual information (automatic lip-reading) improving overall recognition, especially in noisy environments. The image of the lips, constituting the visual input, is automatically extracted from the camera picture of the speaker's face by the lip locator module. Finally, the speaker's face is automatically acquired and followed by the face tracker sub-system. Integration of the three functions results in the first bi-modal speech recognizer allowing the speaker reasonable freedom of movement within a possibly noisy room while continuing to communicate with the computer via voice. Compared to audio-alone recognition, the combined system achieves a 20 to 50 percent error rate reduction for various signal/noise conditions."
            },
            "slug": "Toward-movement-invariant-automatic-lip-reading-and-Duchnowski-Hunke",
            "title": {
                "fragments": [],
                "text": "Toward movement-invariant automatic lip-reading and speech recognition"
            },
            "venue": {
                "fragments": [],
                "text": "1995 International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793389"
                        ],
                        "name": "Chung-Lin Huang",
                        "slug": "Chung-Lin-Huang",
                        "structuredName": {
                            "firstName": "Chung-Lin",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chung-Lin Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110287333"
                        ],
                        "name": "Ching-Wen Chen",
                        "slug": "Ching-Wen-Chen",
                        "structuredName": {
                            "firstName": "Ching-Wen",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ching-Wen Chen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 20479780,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d7be6e3d9f410c9ea1d72b51a7897b9063fc8623",
            "isKey": false,
            "numCitedBy": 165,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents facial features extraction algorithms which can be used for automated visual interpretation and recognition of human faces. It is possible to capture the contours of eye and mouth by deformable template model because of their analytically describable shapes. However, the shapes of eyebrow, nostril and face are difficult to model using a deformable template. They are extracted by using an active contour model, the snake.<<ETX>>"
            },
            "slug": "Human-facial-feature-extraction-for-face-and-Huang-Chen",
            "title": {
                "fragments": [],
                "text": "Human facial feature extraction for face interpretation and recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "Facial features extraction algorithms which can be used for automated visual interpretation and recognition of human faces and shapes of eyebrow, nostril and face are presented."
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2566599"
                        ],
                        "name": "M. Hunke",
                        "slug": "M.-Hunke",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Hunke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hunke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "74557912"
                        ],
                        "name": "A. Waibel",
                        "slug": "A.-Waibel",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Waibel",
                            "middleNames": [
                                "William"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Waibel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 108
                            }
                        ],
                        "text": "More complex methods make use of statistical measures that model face variation within a wide user spectrum [3, 27, 75, 125, 139, 215]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 200,
                                "start": 145
                            }
                        ],
                        "text": "Since the main variation in skin appearance is largely due to luminance change (brightness) [215], normalized RGB colors are generally preferred [27, 53, 75, 88, 92, 165, 181, 196, 202, 207, 213, 215], so that the effect of luminance can be filtered out."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 142
                            }
                        ],
                        "text": "It was found that different human skin color gives rise to a tight cluster in color spaces even when faces of difference races are considered [75, 125, 215]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 289616,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f0841c6c99853bc0d6f5968dd8b5066e6bb4cfb2",
            "isKey": false,
            "numCitedBy": 133,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Effective human-to-human communication involves both auditory and visual modalities, providing robustness and naturalness in realistic communication situations. Recent efforts at our lab are aimed at providing such multimodal capabilities for human-machine communication. Most of the visual modalities require a stable image of a speaker's face. We propose a connectionist face tracker that manipulates camera orientation and room, to keep a person's face located at all times. The system operates in real time and can adapt rapidly to different lighting conditions, cameras and faces, making it robust against environmental variability. Extensions and integration of the system with a multimodal interface are presented.<<ETX>>"
            },
            "slug": "Face-locating-and-tracking-for-human-computer-Hunke-Waibel",
            "title": {
                "fragments": [],
                "text": "Face locating and tracking for human-computer interaction"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A connectionist face tracker that manipulates camera orientation and room, to keep a person's face located at all times is proposed, which operates in real time and can adapt rapidly to different lighting conditions, cameras and faces, making it robust against environmental variability."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 1994 28th Asilomar Conference on Signals, Systems and Computers"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152292143"
                        ],
                        "name": "Jeffrey R. Huang",
                        "slug": "Jeffrey-R.-Huang",
                        "structuredName": {
                            "firstName": "Jeffrey R.",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeffrey R. Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143979395"
                        ],
                        "name": "H. Wechsler",
                        "slug": "H.-Wechsler",
                        "structuredName": {
                            "firstName": "Harry",
                            "lastName": "Wechsler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Wechsler"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 79
                            }
                        ],
                        "text": "Other proposed approaches for feature searching include radial basis functions [71] and genetic algorithms [72, 112, 144]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15885880,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e5f3722bfc116ca5d4c47da23e3826379ad942e6",
            "isKey": false,
            "numCitedBy": 120,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "The eyes are important facial landmarks, both for image normalization due to their relatively constant interocular distance, and for post processing due to the anchoring on model-based schemes. This paper introduces a novel approach for the eye detection task using optimal wavelet packets for eye representation and Radial Basis Functions (RBFs) for subsequent classification (\"labeling\") of facial areas as eye versus non-eye regions. Entropy minimization is the driving force behind the derivation of optimal wavelet packets. It decreases the degree of data dispersion and it thus facilitates clustering (\"prototyping\") and capturing the most significant characteristics of the underlying (eye regions) data. Entropy minimization is thus functionally compatible with the first operational stage of the RBF classifier, that of clustering, and this explains the improved RBF performance on eye detection. Our experiments on the eye detection task prove the merit of this approach as they show that eye images compressed using optimal wavelet packets lead to improved and robust performance of the RBF classifier compared to the case where original raw images are used by the RBF classifier."
            },
            "slug": "Eye-Detection-Using-Optimal-Wavelet-Packets-and-Huang-Wechsler",
            "title": {
                "fragments": [],
                "text": "Eye Detection Using Optimal Wavelet Packets and Radial Basis Functions (RBFs)"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper introduces a novel approach for the eye detection task using optimal wavelet packets for eye representation and Radial Basis Functions (RBFs) for subsequent classification (\"labeling\") of facial areas as eye versus non-eye regions."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Pattern Recognit. Artif. Intell."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2122697"
                        ],
                        "name": "A. Nikolaidis",
                        "slug": "A.-Nikolaidis",
                        "structuredName": {
                            "firstName": "Athanasios",
                            "lastName": "Nikolaidis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Nikolaidis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144064571"
                        ],
                        "name": "I. Pitas",
                        "slug": "I.-Pitas",
                        "structuredName": {
                            "firstName": "Ioannis",
                            "lastName": "Pitas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Pitas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Active contours, or snakes, are commonly used to locate a head boundary [55, 69, 102, 137, 209, 219]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15104103,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dd08ff666458396a0dee4d09f9e5424a725fddcb",
            "isKey": false,
            "numCitedBy": 131,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Facial-feature-extraction-and-pose-determination-Nikolaidis-Pitas",
            "title": {
                "fragments": [],
                "text": "Facial feature extraction and pose determination"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2885735"
                        ],
                        "name": "E. Viennet",
                        "slug": "E.-Viennet",
                        "structuredName": {
                            "firstName": "Emmanuel",
                            "lastName": "Viennet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Viennet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66890955"
                        ],
                        "name": "F. F. Souli\u00e9",
                        "slug": "F.-F.-Souli\u00e9",
                        "structuredName": {
                            "firstName": "Fran\u00e7oise",
                            "lastName": "Souli\u00e9",
                            "middleNames": [
                                "Fogelman"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. F. Souli\u00e9"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 127053967,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "faf63efec3c104a66f1b60719db6b59acba08f96",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "We show in this paper how Neural Networks can be used for Human Face Processing. In Part I, we show how Neural Networks can be viewed as a particular class of Statistical models. We introduce learning as an estimation problem 1, then describe Multi-Layer Perceptrons and Radial Basis Function networks 2, widely used Neural Networks which we will use in Part II, for face processing. We further present Vapnik\u2019s framework for learning 3, show the capacity/generalization dilemma and discuss its implications for Neural Network training and model selection. Vapnik\u2019s ideas lead to a new interesting class of classifier, Support Vector Machines, presented in section 3.2. We then discuss the combination of models 4 and give a formalism which allows to cooperatively train multi-modular Neural Networks architectures. Finally, we present a multi-modular architecture to perform \u201cSegmentation-Recognition in the loop\u201d 5."
            },
            "slug": "Connectionists-Methods-for-Human-Face-Processing-Viennet-Souli\u00e9",
            "title": {
                "fragments": [],
                "text": "Connectionists Methods for Human Face Processing"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "It is shown in this paper how Neural Networks can be used for Human Face Processing and Vapnik\u2019s framework for learning is presented, and a formalism which allows to cooperatively train multi-modular Neural Networks architectures is given."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9127836"
                        ],
                        "name": "M. L. Cascia",
                        "slug": "M.-L.-Cascia",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Cascia",
                            "middleNames": [
                                "La"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. L. Cascia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749590"
                        ],
                        "name": "S. Sclaroff",
                        "slug": "S.-Sclaroff",
                        "structuredName": {
                            "firstName": "Stan",
                            "lastName": "Sclaroff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sclaroff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720747"
                        ],
                        "name": "V. Athitsos",
                        "slug": "V.-Athitsos",
                        "structuredName": {
                            "firstName": "Vassilis",
                            "lastName": "Athitsos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Athitsos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5874123,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "436ad5af721c56b714b13d68a45ab8898d75514f",
            "isKey": false,
            "numCitedBy": 601,
            "numCiting": 92,
            "paperAbstract": {
                "fragments": [],
                "text": "An improved technique for 3D head tracking under varying illumination conditions is proposed. The head is modeled as a texture mapped cylinder. Tracking is formulated as an image registration problem in the cylinder's texture map image. The resulting dynamic texture map provides a stabilized view of the face that can be used as input to many existing 2D techniques for face recognition, facial expressions analysis, lip reading, and eye tracking. To solve the registration problem in the presence of lighting variation and head motion, the residual error of registration is modeled as a linear combination of texture warping templates and orthogonal illumination templates. Fast and stable on-line tracking is achieved via regularized, weighted least squares minimization of the registration error. The regularization term tends to limit potential ambiguities that arise in the warping and illumination templates. It enables stable tracking over extended sequences. Tracking does not require a precise initial fit of the model; the system is initialized automatically using a simple 2D face detector. The only assumption is that the target is facing the camera in the first frame of the sequence. The formulation is tailored to take advantage of texture mapping hardware available in many workstations, PC's, and game consoles. The non-optimized implementation runs at about 15 frames per second on a SGI O2 graphic workstation. Extensive experiments evaluating the effectiveness of the formulation are reported. The sensitivity of the technique to illumination, regularization parameters, errors in the initial positioning and internal camera parameters are analyzed. Examples and applications of tracking are reported."
            },
            "slug": "Fast,-Reliable-Head-Tracking-under-Varying-An-Based-Cascia-Sclaroff",
            "title": {
                "fragments": [],
                "text": "Fast, Reliable Head Tracking under Varying Illumination: An Approach Based on Registration of Texture-Mapped 3D Models"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "An improved technique for 3D head tracking under varying illumination conditions is proposed, modeled as a texture mapped cylinder, which provides a stabilized view of the face that can be used as input to many existing 2D techniques for face recognition, facial expressions analysis, lip reading, and eye tracking."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110604335"
                        ],
                        "name": "Markus Weber",
                        "slug": "Markus-Weber",
                        "structuredName": {
                            "firstName": "Markus",
                            "lastName": "Weber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Markus Weber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1917767"
                        ],
                        "name": "W. Einh\u00e4user",
                        "slug": "W.-Einh\u00e4user",
                        "structuredName": {
                            "firstName": "Wolfgang",
                            "lastName": "Einh\u00e4user",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Einh\u00e4user"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678311"
                        ],
                        "name": "M. Welling",
                        "slug": "M.-Welling",
                        "structuredName": {
                            "firstName": "Max",
                            "lastName": "Welling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Welling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 87
                            }
                        ],
                        "text": "Probabilistic face models based on multiple face appearance have also been proposed in [180, 204, 221, 224]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 413092,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "574bed38dc800a8bb8973369e4f8d0c1f32cb13a",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method to learn models of human heads for the purpose of detection from different viewing angles. We focus on a model where objects are represented as constellations of rigid features (parts). Variability is represented by a joint probability density function (PDF) on the shape of the constellation. In the first stage, the method automatically identifies distinctive features in the training set using an interest operator followed by vector quantization. The set of model parameters, including the shape PDF, is then learned using expectation maximization. Experiments show good generalization performance to novel viewpoints and unseen faces. Performance is above 90% correct with less than 1 s computation time per image."
            },
            "slug": "Viewpoint-invariant-learning-and-detection-of-human-Weber-Einh\u00e4user",
            "title": {
                "fragments": [],
                "text": "Viewpoint-invariant learning and detection of human heads"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "A method to learn models of human heads for the purpose of detection from different viewing angles using a model where objects are represented as constellations of rigid features (parts) using a joint probability density function on the shape of the constellation."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2128977"
                        ],
                        "name": "H. Demirel",
                        "slug": "H.-Demirel",
                        "structuredName": {
                            "firstName": "Hasan",
                            "lastName": "Demirel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Demirel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33162260"
                        ],
                        "name": "T. Clarke",
                        "slug": "T.-Clarke",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Clarke",
                            "middleNames": [
                                "J.",
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Clarke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144480916"
                        ],
                        "name": "P. Cheung",
                        "slug": "P.-Cheung",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Cheung",
                            "middleNames": [
                                "Y.",
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Cheung"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 68
                            }
                        ],
                        "text": "The DFFS measure has also been used for facial feature detection in [33] and in combination with Fisher\u2019s linear discriminant [43] for face and facial feature detection [173]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 11
                            }
                        ],
                        "text": "Image-based[33] representations of faces, for example in 2D intensity arrays, are directly classified into a face group using training algorithms without feature derivation and analysis."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1807797,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "05577c16f20c927015c67793621bcdf40d48d0b0",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Automatic facial feature detection is typically solved by using manually segmented images to train a feature detector In this paper we investigate whether it is possible to improve the detection performance of such a feature detector by using additional unsegmented images. We propose a new adaptive automatic facial feature segmentation algorithm which aims to do this. The experimental results using this algorithm demonstrate that it is possible to improve the detection performance obtained from a small segmented training set by using a larger number of additional unsegmented images."
            },
            "slug": "Adaptive-automatic-facial-feature-segmentation-Demirel-Clarke",
            "title": {
                "fragments": [],
                "text": "Adaptive automatic facial feature segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "A new adaptive automatic facial feature segmentation algorithm is proposed which demonstrates that it is possible to improve the detection performance obtained from a small segmented training set by using a larger number of additional unsegmented images."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Second International Conference on Automatic Face and Gesture Recognition"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143857160"
                        ],
                        "name": "Robert Frischholz",
                        "slug": "Robert-Frischholz",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Frischholz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robert Frischholz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32665167"
                        ],
                        "name": "U. Dieckmann",
                        "slug": "U.-Dieckmann",
                        "structuredName": {
                            "firstName": "Ulrich",
                            "lastName": "Dieckmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Dieckmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 9
                            }
                        ],
                        "text": "In BioID [46], a model-based face detection method based on edge information [28] is used as a preprocessor in a biometric systems which combines face, voice, and lip movement recognition."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 34
                            }
                        ],
                        "text": "R. W. Frischolz and U. Dieckmann, BioID: A multimodal biometric identification system,IEEE Comput."
                    },
                    "intents": []
                }
            ],
            "corpusId": 7044953,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "54cef2e3ad1c07a589c1b148da0b1d84de0097fb",
            "isKey": false,
            "numCitedBy": 413,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Biometric identification systems, which use physical features to check a person's identity, ensure much greater security than password and number systems. Biometric features such as the face or a fingerprint can be stored on a microchip in a credit card, for example. A single feature, however, sometimes fails to be exact enough for identification. Another disadvantage of using only one feature is that the chosen feature is not always readable. Dialog Communication Systems (DCS AG) developed BioID, a multimodal identification system that uses three different features-face, voice, and lip movement-to identify people. With its three modalities, BioID achieves much greater accuracy than single-feature systems. Even if one modality is somehow disturbed-for example, if a noisy environment drowns out the voice-the ether two modalities still lead to an accurate identification. This article goes into detail about the system functions, explaining the data acquisition and preprocessing techniques for voice, facial, and lip imagery data. The authors also explain the classification principles used for optical features and the sensor fusion options (the combinations of the three results-face, voice, lip movement-to obtain varying levels of security)."
            },
            "slug": "BioID:-A-Multimodal-Biometric-Identification-System-Frischholz-Dieckmann",
            "title": {
                "fragments": [],
                "text": "BioID: A Multimodal Biometric Identification System"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This article goes into detail about the BioID system functions, explaining the data acquisition and preprocessing techniques for voice, facial, and lip imagery data and the classification principles used for optical features and the sensor fusion options."
            },
            "venue": {
                "fragments": [],
                "text": "Computer"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2780543"
                        ],
                        "name": "Lingmin Meng",
                        "slug": "Lingmin-Meng",
                        "structuredName": {
                            "firstName": "Lingmin",
                            "lastName": "Meng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lingmin Meng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143803197"
                        ],
                        "name": "Truong Q. Nguyen",
                        "slug": "Truong-Q.-Nguyen",
                        "structuredName": {
                            "firstName": "Truong",
                            "lastName": "Nguyen",
                            "middleNames": [
                                "Q."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Truong Q. Nguyen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 19
                            }
                        ],
                        "text": "In Meng and Nguyen [128], PCA is used to model both faces and background clutter (an eigenface and an eigenclutter space)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 12
                            }
                        ],
                        "text": "L. Meng, T. Nguyen, and D. A. Casta\u02dcnon, An image-based bayesian framework for face detection, inIEEE Conference on Computer Vision and Pattern Recognition, 2000."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 15
                            }
                        ],
                        "text": "L. Meng and T. Nguyen, Two subspace methods to discriminate faces and clutters, inProceedings of the 2000 International Conference on Image Processing, 2000, p. TA07."
                    },
                    "intents": []
                }
            ],
            "corpusId": 38414844,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "302653d2ae912105c3d72f1c8934297d711383f6",
            "isKey": true,
            "numCitedBy": 7,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Dimension reduction via linear subspace is very important in image pattern detection and recognition. This paper presents two new methods of dimension reduction and develops algorithms to locate human faces in gray-scale still images. The first technique develops eigenface subspace and eigenclutter subspace which represent faces and clutters respectively. The second technique chooses a common subspace to maximize the Bhattacharyya distance of two Gaussian distributions. Compared with the first method, the second method is more computationally efficient with slightly higher error rate. Our simulation result indicates that both methods outperform conventional template-based methods such as matched filter and eigenface methods."
            },
            "slug": "Two-subspace-methods-to-discriminate-faces-and-Meng-Nguyen",
            "title": {
                "fragments": [],
                "text": "Two subspace methods to discriminate faces and clutters"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "Two new methods of dimension reduction and algorithms to locate human faces in gray-scale still images are presented and it is indicated that both methods outperform conventional template-based methods such as matched filter and eigenface methods."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 2000 International Conference on Image Processing (Cat. No.00CH37101)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40309692"
                        ],
                        "name": "C. G. Harris",
                        "slug": "C.-G.-Harris",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Harris",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. G. Harris"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40365651"
                        ],
                        "name": "M. Stephens",
                        "slug": "M.-Stephens",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Stephens",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Stephens"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1694378,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6818668fb895d95861a2eb9673ddc3a41e27b3b3",
            "isKey": false,
            "numCitedBy": 14113,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem we are addressing in Alvey Project MMI149 is that of using computer vision to understand the unconstrained 3D world, in which the viewed scenes will in general contain too wide a diversity of objects for topdown recognition techniques to work. For example, we desire to obtain an understanding of natural scenes, containing roads, buildings, trees, bushes, etc., as typified by the two frames from a sequence illustrated in Figure 1. The solution to this problem that we are pursuing is to use a computer vision system based upon motion analysis of a monocular image sequence from a mobile camera. By extraction and tracking of image features, representations of the 3D analogues of these features can be constructed."
            },
            "slug": "A-Combined-Corner-and-Edge-Detector-Harris-Stephens",
            "title": {
                "fragments": [],
                "text": "A Combined Corner and Edge Detector"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The problem the authors are addressing in Alvey Project MMI149 is that of using computer vision to understand the unconstrained 3D world, in which the viewed scenes will in general contain too wide a diversity of objects for topdown recognition techniques to work."
            },
            "venue": {
                "fragments": [],
                "text": "Alvey Vision Conference"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145391215"
                        ],
                        "name": "D. Valentin",
                        "slug": "D.-Valentin",
                        "structuredName": {
                            "firstName": "Dominique",
                            "lastName": "Valentin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Valentin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144319186"
                        ],
                        "name": "H. Abdi",
                        "slug": "H.-Abdi",
                        "structuredName": {
                            "firstName": "Herv\u00e9",
                            "lastName": "Abdi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Abdi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1400601690"
                        ],
                        "name": "A. O'Toole",
                        "slug": "A.-O'Toole",
                        "structuredName": {
                            "firstName": "Alice",
                            "lastName": "O'Toole",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. O'Toole"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48524582"
                        ],
                        "name": "G. Cottrell",
                        "slug": "G.-Cottrell",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Cottrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Cottrell"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 5315848,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dd05aa5ea9aa8d8e34342362bddd2fe56eb1ae72",
            "isKey": false,
            "numCitedBy": 449,
            "numCiting": 67,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Connectionist-models-of-face-processing:-A-survey-Valentin-Abdi",
            "title": {
                "fragments": [],
                "text": "Connectionist models of face processing: A survey"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783224"
                        ],
                        "name": "A. Colmenarez",
                        "slug": "A.-Colmenarez",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Colmenarez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Colmenarez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153652752"
                        ],
                        "name": "Thomas S. Huang",
                        "slug": "Thomas-S.-Huang",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Huang",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas S. Huang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "In [23], Colmenarez and Huang further improved on this technique by including error bootstrapping (described in Section 4."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8866853,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "057d780369a0fd1bce8cba83aa4cee2b91bedac2",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "We have previously (1996, 1997) introduced a visual learning technique based on information-theoretic entropy. In that approach, positive and negative examples of a class of visual patterns were analyzed to obtain the probability model that best discriminate the class among others. Such models were tested in the context of maximum likelihood detection of faces and facial features. In this paper we further improve on that technique by using other family of probability model and by extending the optimization criteria to allow for error bootstrapping. The results include a detail analysis of the improvements obtained and a comparison of these pattern recognition algorithms."
            },
            "slug": "Pattern-detection-with-information-based-maximum-Colmenarez-Huang",
            "title": {
                "fragments": [],
                "text": "Pattern detection with information-based maximum discrimination and error bootstrapping"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "This paper improves on a visual learning technique based on information-theoretic entropy by using other family of probability model and by extending the optimization criteria to allow for error bootstrapping."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. Fourteenth International Conference on Pattern Recognition (Cat. No.98EX170)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2674015"
                        ],
                        "name": "Jyh-Yuan Deng",
                        "slug": "Jyh-Yuan-Deng",
                        "structuredName": {
                            "firstName": "Jyh-Yuan",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jyh-Yuan Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145497821"
                        ],
                        "name": "F. Lai",
                        "slug": "F.-Lai",
                        "structuredName": {
                            "firstName": "Feipei",
                            "lastName": "Lai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Lai"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7640987,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e4119863233012d6e2ce95037e83e507835a4b08",
            "isKey": false,
            "numCitedBy": 93,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Region-based-template-deformation-and-masking-for-Deng-Lai",
            "title": {
                "fragments": [],
                "text": "Region-based template deformation and masking for eye-feature extraction and description"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145347688"
                        ],
                        "name": "S. Baker",
                        "slug": "S.-Baker",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Baker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Baker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1750470"
                        ],
                        "name": "S. Nayar",
                        "slug": "S.-Nayar",
                        "structuredName": {
                            "firstName": "Shree",
                            "lastName": "Nayar",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Nayar"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12057260,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "74f7565782a6e619ef6ac2022880225f9a80a056",
            "isKey": false,
            "numCitedBy": 75,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "The efficiency of pattern recognition is particularly crucial in two scenarios; whenever there are a large number of classes to discriminate, and, whenever recognition must be performed a large number of times. We propose a single technique, namely, pattern rejection, that greatly enhances efficiency in both cases. A rejector is a generalization of a classifier, that quickly eliminates a large fraction of the candidate classes or inputs. This allows a recognition algorithm to dedicate its efforts to a much smaller number of possibilities. Importantly, a collection of rejectors may be combined to form a composite rejector, which is shown to be far more effective than any of its individual components. A simple algorithm is proposed for the construction of each of the component rejectors. Its generality is established through close relationships with the Karhunen-Loeve expansion and Fisher's discriminant analysis. Composite rejectors were constructed for two representative applications, namely, appearance matching based object recognition and local feature detection. The results demonstrate substantial efficiency improvements over existing approaches, most notably Fisher's discriminant analysis."
            },
            "slug": "Pattern-rejection-Baker-Nayar",
            "title": {
                "fragments": [],
                "text": "Pattern rejection"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings CVPR IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710238"
                        ],
                        "name": "R. Haralick",
                        "slug": "R.-Haralick",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Haralick",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Haralick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3144171"
                        ],
                        "name": "K. Shanmugam",
                        "slug": "K.-Shanmugam",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Shanmugam",
                            "middleNames": [
                                "Sam"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Shanmugam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686153"
                        ],
                        "name": "I. Dinstein",
                        "slug": "I.-Dinstein",
                        "structuredName": {
                            "firstName": "Its'hak",
                            "lastName": "Dinstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Dinstein"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 206786900,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "1fdb62555eb650662dbe2a6f3985d390861597c2",
            "isKey": false,
            "numCitedBy": 19249,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Texture is one of the important characteristics used in identifying objects or regions of interest in an image, whether the image be a photomicrograph, an aerial photograph, or a satellite image. This paper describes some easily computable textural features based on gray-tone spatial dependancies, and illustrates their application in category-identification tasks of three different kinds of image data: photomicrographs of five kinds of sandstones, 1:20 000 panchromatic aerial photographs of eight land-use categories, and Earth Resources Technology Satellite (ERTS) multispecial imagery containing seven land-use categories. We use two kinds of decision rules: one for which the decision regions are convex polyhedra (a piecewise linear decision rule), and one for which the decision regions are rectangular parallelpipeds (a min-max decision rule). In each experiment the data set was divided into two parts, a training set and a test set. Test set identification accuracy is 89 percent for the photomicrographs, 82 percent for the aerial photographic imagery, and 83 percent for the satellite imagery. These results indicate that the easily computable textural features probably have a general applicability for a wide variety of image-classification applications."
            },
            "slug": "Textural-Features-for-Image-Classification-Haralick-Shanmugam",
            "title": {
                "fragments": [],
                "text": "Textural Features for Image Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "These results indicate that the easily computable textural features based on gray-tone spatial dependancies probably have a general applicability for a wide variety of image-classification applications."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Syst. Man Cybern."
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48160597"
                        ],
                        "name": "P. V. van Beek",
                        "slug": "P.-V.-van-Beek",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "van Beek",
                            "middleNames": [
                                "J.",
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. V. van Beek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681254"
                        ],
                        "name": "M. Reinders",
                        "slug": "M.-Reinders",
                        "structuredName": {
                            "firstName": "Marcel",
                            "lastName": "Reinders",
                            "middleNames": [
                                "J.",
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Reinders"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145940271"
                        ],
                        "name": "B. Sankur",
                        "slug": "B.-Sankur",
                        "structuredName": {
                            "firstName": "B\u00fclent",
                            "lastName": "Sankur",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sankur"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "134228284"
                        ],
                        "name": "J. V. D. van der Lubbe",
                        "slug": "J.-V.-D.-van-der-Lubbe",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "van der Lubbe",
                            "middleNames": [
                                "C.",
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. V. D. van der Lubbe"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62165068,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c8d1215e44ba15eb33a930f5c3c88e5bc69cd9cb",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "A system for segmentation of head-and-shoulder scenes into semantic regions, to be applied in a model-based coding scheme on video telephony, is described. The system is conceptually divided into three levels of processing and uses successive semantic regions of interest to locate the speaker, the face and the eyes automatically. Once candidate regions have been obtained by the low level segmentation modules, higher level modules perform measurements on these regions and compare these with expected values to extract the specific region searched for. Fuzzy membership functions are used to allow deviations from the expected values. The system is able to locate satisfactorily the facial region and the eye regions."
            },
            "slug": "Semantic-segmentation-of-videophone-image-sequences-Beek-Reinders",
            "title": {
                "fragments": [],
                "text": "Semantic segmentation of videophone image sequences"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "A system for segmentation of head-and-shoulder scenes into semantic regions, to be applied in a model-based coding scheme on video telephony, is described and is able to locate satisfactorily the facial region and the eye regions."
            },
            "venue": {
                "fragments": [],
                "text": "Other Conferences"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2085375589"
                        ],
                        "name": "H. Schneiderman",
                        "slug": "H.-Schneiderman",
                        "structuredName": {
                            "firstName": "Henry",
                            "lastName": "Schneiderman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Schneiderman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733113"
                        ],
                        "name": "T. Kanade",
                        "slug": "T.-Kanade",
                        "structuredName": {
                            "firstName": "Takeo",
                            "lastName": "Kanade",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanade"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 32
                            }
                        ],
                        "text": "4%/65 Schneiderman & Kanade\u2014W b [170] 90."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 384,
                                "start": 378
                            }
                        ],
                        "text": "The number of labeled faces was originally 149 by Sung and Poggio (which is the number used in the reported results in [182] and [107], but it was changed to 155 (which\nTABLE 2 Results Reported in Terms of Percentage Correct Detection (CD) and Number of False Positives (FP), CD/FP, on the CMU and MIT Datasets\nFace detection system CMU-130 CMU-125 MIT-23 MIT-20\nSchneiderman & Kanade\u2014Ea [170] 94.4%/65 Schneiderman & Kanade\u2014Wb [170] 90.2%/110 Yanget al.\u2014FA [217] 92.3%/82 89.4%/3 Yanget al.\u2014LDA [217] 93.6%/74 91.5%/1 Rothet al. [157] 94.8%/78 94.1%/3 Rowleyet al. [158] 86.2%/23 84.5%/8 Feraudet al. [42] 86%/8 Colmenarez & Huang [22] 93.9%/8122 Sung & Poggio [182] 79.9%/5 Lew & Huijsmans [107] 94.1%/64 Osunaet al. [140] 74.2%/20 Lin et al. [113] 72.3%/6 Gu and Li [54] 87.1%/0\na Eigenvector coefficients. b Wavelet coefficients.\nis the number used in the reported results in [158], [140], and [113]) when included in the CMU dataset."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 18
                            }
                        ],
                        "text": "H. D. Wactlar, T. Kanade, M. A. Smith, and S. M. Stevens, Intelligent access to digital video: Informedia project,IEEE Comput.29(5), 1996, 46\u201352."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 23
                            }
                        ],
                        "text": "H. Schneiderman and T. Kanade, A statistical model for 3D object detection applied to faces and cars, in IEEE Conference on Computer Vision and Pattern Recognition, 2000."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 47
                            }
                        ],
                        "text": "In the first proposed face detection system of Schneiderman and Kanade [169], the posterior probability function is derived based on a set of modifications and simplifications (some of which are mentioned here):\n\u2022 the resolution of a face image is normalized to 64\u00d7 64 pixels \u2022 the face images are decomposed into 16\u00d7 16 subregions and there is no modeling\nof statistical dependency among the subregions \u2022 the subregions are projected onto a 12-dimensional subspace (constructed by PCA) \u2022 the entire face region is normalized to have zero mean and unit variance."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 23
                            }
                        ],
                        "text": "H. Schneiderman and T. Kanade, Probabilistic modeling of local appearance and spatial relationships for object recognition, inIEEE Conference on Computer Vision and Pattern Recognition, 6, 1998."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 32
                            }
                        ],
                        "text": "H. A. Rowley, S. Baluja, and T. Kanade, Rotation invariant neural network-based face detection, inProc."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 53
                            }
                        ],
                        "text": "Face detection examples from Schneiderman and Kanade [170] ( c \u00a9 2000/2001 IEEE)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 32
                            }
                        ],
                        "text": "H. A. Rowley, S. Baluja, and T. Kanade, Neural network-based face detection,IEEE Trans."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 27
                            }
                        ],
                        "text": "T. Sakai, M. Nagao, and T. Kanade, Computer analysis and classification of photographs of human faces, in Proc."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 30
                            }
                        ],
                        "text": "In the second proposed system [170], the visual attributes of the image are not represented by local eigenvector coefficients (as in the first approach), but instead by a locally sampled wavelet transform."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 33
                            }
                        ],
                        "text": "The wavelet transform applied in [170] is a three-level decomposition using a 5 /3 linear phase filter-bank."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 24
                            }
                        ],
                        "text": "Schneiderman and Kanade [169, 170] describe two face detectors based on Bayes\u2019 decision rule (presented as a likelihood ratio test in Eq."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 30
                            }
                        ],
                        "text": "S. Satoh, Y. Nakamura, and T. Kanade, Name-It: Naming and detecting faces in news videos,IEEE Multimedia6, 1999, 22\u201335."
                    },
                    "intents": []
                }
            ],
            "corpusId": 12209481,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3565c5a65842f26091578b9d71d496cc1561239d",
            "isKey": true,
            "numCitedBy": 1292,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we describe a statistical method for 3D object detection. We represent the statistics of both object appearance and \"non-object\" appearance using a product of histograms. Each histogram represents the joint statistics of a subset of wavelet coefficients and their position on the object. Our approach is to use many such histograms representing a wide variety of visual attributes. Using this method, we have developed the first algorithm that can reliably detect human faces with out-of-plane rotation and the first algorithm that can reliably detect passenger cars over a wide range of viewpoints."
            },
            "slug": "A-statistical-method-for-3D-object-detection-to-and-Schneiderman-Kanade",
            "title": {
                "fragments": [],
                "text": "A statistical method for 3D object detection applied to faces and cars"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Using this method, this work has developed the first algorithm that can reliably detect human faces with out-of-plane rotation and the first algorithms thatCan reliably detect passenger cars over a wide range of viewpoints."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings IEEE Conference on Computer Vision and Pattern Recognition. CVPR 2000 (Cat. No.PR00662)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2397742"
                        ],
                        "name": "H. Moon",
                        "slug": "H.-Moon",
                        "structuredName": {
                            "firstName": "Hankyu",
                            "lastName": "Moon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Moon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9215658"
                        ],
                        "name": "R. Chellappa",
                        "slug": "R.-Chellappa",
                        "structuredName": {
                            "firstName": "Rama",
                            "lastName": "Chellappa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Chellappa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143766793"
                        ],
                        "name": "A. Rosenfeld",
                        "slug": "A.-Rosenfeld",
                        "structuredName": {
                            "firstName": "Azriel",
                            "lastName": "Rosenfeld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Rosenfeld"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17796293,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7e96760d4f2ae09e9f0905c907ec3ffa9a7c39bd",
            "isKey": false,
            "numCitedBy": 138,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose an approach to accurately detecting two-dimensional (2-D) shapes. The cross section of the shape boundary is modeled as a step function. We first derive a one-dimensional (1-D) optimal step edge operator, which minimizes both the noise power and the mean squared error between the input and the filter output. This operator is found to be the derivative of the double exponential (DODE) function, originally derived by Ben-Arie and Rao. We define an operator for shape detection by extending the DODE filter along the shape's boundary contour. The responses are accumulated at the centroid of the operator to estimate the likelihood of the presence of the given shape. This method of detecting a shape is in fact a natural extension of the task of edge detection at the pixel level to the problem of global contour detection. This simple filtering scheme also provides a tool for a systematic analysis of edge-based shape detection. We investigate how the error is propagated by the shape geometry. We have found that, under general assumptions, the operator is locally linear at the peak of the response. We compute the expected shape of the response and derive some of its statistical properties. This enables us to predict both its localization and detection performance and adjust its parameters according to imaging conditions and given performance specifications. Applications to the problem of vehicle detection in aerial images, human facial feature detection, and contour tracking in video are presented."
            },
            "slug": "Optimal-edge-based-shape-detection-Moon-Chellappa",
            "title": {
                "fragments": [],
                "text": "Optimal edge-based shape detection"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "An approach to accurately detecting two-dimensional (2-D) shapes by extending the DODE filter along the shape's boundary contour by compute the expected shape of the response and derive some of its statistical properties."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Image Process."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47246616"
                        ],
                        "name": "R. Brunelli",
                        "slug": "R.-Brunelli",
                        "structuredName": {
                            "firstName": "Roberto",
                            "lastName": "Brunelli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Brunelli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16859093,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "239beb3861ceceb4c7c7f229234d97198d5c7697",
            "isKey": false,
            "numCitedBy": 2828,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Two new algorithms for computer recognition of human faces, one based on the computation of a set of geometrical features, such as nose width and length, mouth position, and chin shape, and the second based on almost-gray-level template matching, are presented. The results obtained for the testing sets show about 90% correct recognition using geometrical features and perfect recognition using template matching. >"
            },
            "slug": "Face-Recognition:-Features-Versus-Templates-Brunelli-Poggio",
            "title": {
                "fragments": [],
                "text": "Face Recognition: Features Versus Templates"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Two new algorithms for computer recognition of human faces, one based on the computation of a set of geometrical features, such as nose width and length, mouth position, and chin shape, and the second based on almost-gray-level template matching are presented."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783224"
                        ],
                        "name": "A. Colmenarez",
                        "slug": "A.-Colmenarez",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Colmenarez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Colmenarez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153652752"
                        ],
                        "name": "Thomas S. Huang",
                        "slug": "Thomas-S.-Huang",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Huang",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas S. Huang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 62
                            }
                        ],
                        "text": "Based on an earlier work of maximum likelihood face detection [21], Colmenarez and Huang [22] proposed a system based on Kullback relative information (Kullback divergence)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2021467,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a8c961694fc45cbf06f4f46e3f0d0f9d3c03a897",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a visual learning approach that uses non-parametric probability estimators. We use entropy analysis over the training set in order to select the features that best represent the pattern class of faces, and set up discrete probability models. These models are tested in the context of maximum likelihood detection of faces. Excellent results are reported in terms of the correct-answer-false-alarm tradeoff as well as in terms of the computational requirements of the systems."
            },
            "slug": "Maximum-likelihood-face-detection-Colmenarez-Huang",
            "title": {
                "fragments": [],
                "text": "Maximum likelihood face detection"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "A visual learning approach that uses non-parametric probability estimators, which uses entropy analysis over the training set in order to select the features that best represent the pattern class of faces, and set up discrete probability models."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Second International Conference on Automatic Face and Gesture Recognition"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3276986"
                        ],
                        "name": "F. Smeraldi",
                        "slug": "F.-Smeraldi",
                        "structuredName": {
                            "firstName": "Fabrizio",
                            "lastName": "Smeraldi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Smeraldi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "121788895"
                        ],
                        "name": "O. Carmona",
                        "slug": "O.-Carmona",
                        "structuredName": {
                            "firstName": "O.",
                            "lastName": "Carmona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Carmona"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1775110"
                        ],
                        "name": "J. Big\u00fcn",
                        "slug": "J.-Big\u00fcn",
                        "structuredName": {
                            "firstName": "Josef",
                            "lastName": "Big\u00fcn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Big\u00fcn"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8176320,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "698714b7b640e14dfb0792181af27a1fac5e354f",
            "isKey": false,
            "numCitedBy": 76,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Saccadic-search-with-Gabor-features-applied-to-eye-Smeraldi-Carmona",
            "title": {
                "fragments": [],
                "text": "Saccadic search with Gabor features applied to eye detection and real-time head tracking"
            },
            "venue": {
                "fragments": [],
                "text": "Image Vis. Comput."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144986880"
                        ],
                        "name": "I. Craw",
                        "slug": "I.-Craw",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Craw",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Craw"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3263505"
                        ],
                        "name": "D. Tock",
                        "slug": "D.-Tock",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Tock",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Tock"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056963263"
                        ],
                        "name": "Alan Bennett",
                        "slug": "Alan-Bennett",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Bennett",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alan Bennett"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17481367,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e2e6bc4db498566a9f95f122970fb4488eaf3392",
            "isKey": false,
            "numCitedBy": 332,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a computer program which understands a greyscale image of a face well enough to locate individual face features such as eyes and mouth. The program has two distinct components: modules designed to locate particular face features, usually in a restricted area; and the overall control strategy which activates modules on the basis of the current solution state, and assesses and integrates the results of each module."
            },
            "slug": "Finding-Face-Features-Craw-Tock",
            "title": {
                "fragments": [],
                "text": "Finding Face Features"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "A computer program which understands a greyscale image of a face well enough to locate individual face features such as eyes and mouth is described."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1515331844"
                        ],
                        "name": "Yasuyuki Saito",
                        "slug": "Yasuyuki-Saito",
                        "structuredName": {
                            "firstName": "Yasuyuki",
                            "lastName": "Saito",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yasuyuki Saito"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743274"
                        ],
                        "name": "Y. Kenmochi",
                        "slug": "Y.-Kenmochi",
                        "structuredName": {
                            "firstName": "Yukiko",
                            "lastName": "Kenmochi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Kenmochi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791753"
                        ],
                        "name": "K. Kotani",
                        "slug": "K.-Kotani",
                        "structuredName": {
                            "firstName": "Kazunori",
                            "lastName": "Kotani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kotani"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 3203381,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2f1d854ef859438002a3236e7a48b8bbfdbb85ae",
            "isKey": false,
            "numCitedBy": 16,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper shows an extraction method of a symmetric object for eyeglass face analysis using an active contour model. A contour of eyeglasses hinders a face analysis and synthesis which treats contours of facial parts or partial regions of the face such as eyes, mouth, cheek, eyelid and so on. Methods of active contours extract object contour well. We show an active contour model which is adapted to extract a contour of symmetric object, especially we focus on extracting eyeglass frame using snakes. We study to obtain parameters of snakes by genetic algorithm. We show the good results of applying the snakes in an actual facial image, and synthesize an expressive face with eyeglasses as an application."
            },
            "slug": "Extraction-of-a-symmetric-object-for-eyeglass-face-Saito-Kenmochi",
            "title": {
                "fragments": [],
                "text": "Extraction of a symmetric object for eyeglass face analysis using active contour model"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "An active contour model is shown which is adapted to extract a contour of symmetric object for eyeglass face analysis using an active contours model and the good results of applying the snakes in an actual facial image are shown."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 2000 International Conference on Image Processing (Cat. No.00CH37101)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2037776"
                        ],
                        "name": "D. Reisfeld",
                        "slug": "D.-Reisfeld",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Reisfeld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Reisfeld"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47620675"
                        ],
                        "name": "Y. Yeshurun",
                        "slug": "Y.-Yeshurun",
                        "structuredName": {
                            "firstName": "Yehezkel",
                            "lastName": "Yeshurun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Yeshurun"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 57
                            }
                        ],
                        "text": "ACKNOWLEDGMENTS\nThe authors are grateful to Professor H. Yeshurun for providing Fig."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 22
                            }
                        ],
                        "text": "A more recent work by Reisfeld and Yeshurun is [155]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 14
                            }
                        ],
                        "text": "A. Tankus, H. Yeshurun, and N. Intrator, Face detection by direct convexity estimation, inProc. of the 1st Int."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 32
                            }
                        ],
                        "text": "D. Reisfeld, H. Wolfson, and Y. Yeshurun, Context-free attentional operators: The generalized symmetry transform,Int."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 45
                            }
                        ],
                        "text": "In their earlier work, Reisfeld and Yeshurun [154] introduced a generalized symmetry operator that is based on edge pixel operation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 19
                            }
                        ],
                        "text": "D. Reisfeld and Y. Yeshurun, Preprocessing of face images: Detection of features and pose normalization, Comput."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 151
                            }
                        ],
                        "text": ", K ,\u2207pk is the gradient of the intensity at point pk, and\u03b1i j is the counterclockwise angle between the line passing through pi andpj and the horizon [154]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 19
                            }
                        ],
                        "text": "D. Reisfeld and Y. Yeshurun, Robust detection of facial features by generalised symmetry, inProc. of 11th Int."
                    },
                    "intents": []
                }
            ],
            "corpusId": 62756592,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2118c6e85f68a08c3bfe08aa05006c5db0564ca8",
            "isKey": true,
            "numCitedBy": 160,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Locating facial features is crucial for various face recognition schemes. The authors suggest a robust facial feature detector based on a generalized symmetry interest operator. No special tuning is required if the face occupies 15-60% of the image. The operator was tested on a large face data base with a success rate of over 95%.<<ETX>>"
            },
            "slug": "Robust-detection-of-facial-features-by-generalized-Reisfeld-Yeshurun",
            "title": {
                "fragments": [],
                "text": "Robust detection of facial features by generalized symmetry"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A robust facial feature detector based on a generalized symmetry interest operator that was tested on a large face data base with a success rate of over 95%."
            },
            "venue": {
                "fragments": [],
                "text": "[1992] Proceedings. 11th IAPR International Conference on Pattern Recognition"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055592233"
                        ],
                        "name": "J. Ng",
                        "slug": "J.-Ng",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144784813"
                        ],
                        "name": "S. Gong",
                        "slug": "S.-Gong",
                        "structuredName": {
                            "firstName": "Shaogang",
                            "lastName": "Gong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Gong"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 123
                            }
                        ],
                        "text": "SVMs have also been used for multiview face detection by constructing separate SVMs for different parts of the view sphere [136]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18504340,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "61823c18e0065300add3611dd113748d765cddf5",
            "isKey": false,
            "numCitedBy": 85,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Support vector machines have shown great potential for learning classification functions that can be applied to object recognition. In this work, we extend SVMs to model the 2D appearance of human faces which undergo nonlinear change across the view sphere. The model enables simultaneous multi-view face detection and pose estimation at near-frame rate."
            },
            "slug": "Multi-view-face-detection-and-pose-estimation-using-Ng-Gong",
            "title": {
                "fragments": [],
                "text": "Multi-view face detection and pose estimation using a composite support vector machine across the view sphere"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This work extends SVMs to model the 2D appearance of human faces which undergo nonlinear change across the view sphere and enables simultaneous multi-view face detection and pose estimation at near-frame rate."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings International Workshop on Recognition, Analysis, and Tracking of Faces and Gestures in Real-Time Systems. In Conjunction with ICCV'99 (Cat. No.PR00378)"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4801654"
                        ],
                        "name": "Y. Amit",
                        "slug": "Y.-Amit",
                        "structuredName": {
                            "firstName": "Yali",
                            "lastName": "Amit",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Amit"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707642"
                        ],
                        "name": "D. Geman",
                        "slug": "D.-Geman",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Geman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Geman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7784637,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "61b2d0383b186c4d634c5f51421cab67c16d90a1",
            "isKey": false,
            "numCitedBy": 197,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a computational model for detecting and localizing instances from an object class in static gray-level images. We divide detection into visual selection and final classification, concentrating on the former: drastically reducing the number of candidate regions that require further, usually more intensive, processing, but with a minimum of computation and missed detections. Bottom-up processing is based on local groupings of edge fragments constrained by loose geometrical relationships. They have no a priori semantic or geometric interpretation. The role of training is to select special groupings that are moderately likely at certain places on the object but rare in the background. We show that the statistics in both populations are stable. The candidate regions are those that contain global arrangements of several local groupings. Whereas our model was not conceived to explain brain functions, it does cohere with evidence about the functions of neurons in V1 and V2, such as responses to coarse or incomplete patterns (e.g., illusory contours) and to scale and translation invariance in IT. Finally, the algorithm is applied to face and symbol detection."
            },
            "slug": "A-Computational-Model-for-Visual-Selection-Amit-Geman",
            "title": {
                "fragments": [],
                "text": "A Computational Model for Visual Selection"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The model was not conceived to explain brain functions, but it does cohere with evidence about the functions of neurons in V1 and V2, such as responses to coarse or incomplete patterns and to scale and translation invariance in IT."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753908"
                        ],
                        "name": "Michael Elad",
                        "slug": "Michael-Elad",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Elad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Elad"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398184731"
                        ],
                        "name": "Y. Hel-Or",
                        "slug": "Y.-Hel-Or",
                        "structuredName": {
                            "firstName": "Yacov",
                            "lastName": "Hel-Or",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Hel-Or"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2851673"
                        ],
                        "name": "R. Keshet",
                        "slug": "R.-Keshet",
                        "structuredName": {
                            "firstName": "Renato",
                            "lastName": "Keshet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Keshet"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1104536,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "54d90f14068410875830251caddf3c7f8e6cdb8a",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Summary form only given. In target detection applications, the aim is to detect occurrences of a specific target in a given signal. In general, the target is subjected to some particular type of transformation, hence we have a set of target signals to be detected. In this context, the set of non-target samples are referred to as clutter. In practice, the target detection problem can be characterized as designing a classifier C(z), which, given an input vector z, has to decide whether z belongs to the target class X or the clutter class Y. In example based classification, this classifier is designed using two training sets -X/spl circ/={xi}/sub i=1..Lx/ (target samples) and Y/spl circ/={y/sub i/}/sub i=1..Ly/ (clutter samples), drawn from the above two classes."
            },
            "slug": "Pattern-detection-using-a-maximal-rejection-Elad-Hel-Or",
            "title": {
                "fragments": [],
                "text": "Pattern detection using a maximal rejection classifier"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work designs a classifier C(z), which, given an input vector z, has to decide whether z belongs to the target class X or the clutter class Y, drawn from the above two classes."
            },
            "venue": {
                "fragments": [],
                "text": "21st IEEE Convention of the Electrical and Electronic Engineers in Israel. Proceedings (Cat. No.00EX377)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1821402"
                        ],
                        "name": "S. Kawato",
                        "slug": "S.-Kawato",
                        "structuredName": {
                            "firstName": "Shinjiro",
                            "lastName": "Kawato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kawato"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708785"
                        ],
                        "name": "J. Ohya",
                        "slug": "J.-Ohya",
                        "structuredName": {
                            "firstName": "Jun",
                            "lastName": "Ohya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ohya"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 200,
                                "start": 145
                            }
                        ],
                        "text": "Since the main variation in skin appearance is largely due to luminance change (brightness) [215], normalized RGB colors are generally preferred [27, 53, 75, 88, 92, 165, 181, 196, 202, 207, 213, 215], so that the effect of luminance can be filtered out."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 143
                            }
                        ],
                        "text": "Color segmentation can basically be performed using appropriate skin color thresholds where skin color is modeled through histograms or charts [13, 63, 88, 118, 178, 207, 220]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17502636,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ed53ead39a3628ce64469d1884b1da380cdf2ab5",
            "isKey": false,
            "numCitedBy": 137,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Among head gestures, nodding and head-shaking are very common and used often. Thus the detection of such gestures is basic to a visual understanding of human responses. However it is difficult to detect them in real-time, because nodding and head-shaking are fairly small and fast head movements. We propose an approach for detecting nodding and head-shaking in real time from a single color video stream by directly detecting and tracking a point between the eyes, or what we call the \"between-eyes\". Along a circle of a certain radius centered at the \"between-eyes\", the pixel value has two cycles of bright parts (forehead and nose bridge) and dark parts (eyes and brows). The output of the proposed circle-frequency filter has a local maximum at these characteristic points. To distinguish the true \"between-eyes\" from similar characteristic points in other face parts, we do a confirmation with eye detection. Once the \"between-eyes\" is detected, a small area around it is copied as a template and the system enters the tracking mode. Combining with the circle-frequency filtering and the template, the tracking is done not by searching around but by selecting candidates using the template; the template is then updated. Due to this special tracking algorithm, the system can track the \"between-eyes\" stably and accurately. It runs at 13 frames/s rate without special hardware. By analyzing the movement of the point, we can detect nodding and head-shaking. Some experimental results are shown."
            },
            "slug": "Real-time-detection-of-nodding-and-head-shaking-by-Kawato-Ohya",
            "title": {
                "fragments": [],
                "text": "Real-time detection of nodding and head-shaking by directly detecting and tracking the \"between-eyes\""
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work proposes an approach for detecting nodding and head-shaking in real time from a single color video stream by directly detecting and tracking a point between the eyes, or what it calls the \"between-eyes\"."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1803435"
                        ],
                        "name": "A. Albiol",
                        "slug": "A.-Albiol",
                        "structuredName": {
                            "firstName": "Alberto",
                            "lastName": "Albiol",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Albiol"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745655"
                        ],
                        "name": "C. Bouman",
                        "slug": "C.-Bouman",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Bouman",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Bouman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741483"
                        ],
                        "name": "E. Delp",
                        "slug": "E.-Delp",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Delp",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Delp"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 108
                            }
                        ],
                        "text": "More complex methods make use of statistical measures that model face variation within a wide user spectrum [3, 27, 75, 125, 139, 215]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10345198,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c08dff35094c0e50d9b48ec13c9e5b3820239c4d",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Pseudo-semantic labeling represents a novel approach for automatic content description of video. This information can be used in the context of a video database to improve browsing and searching. In this paper we describe our work on using face detection techniques for pseudo-semantic labeling. We present our results using a database of MPEG sequences."
            },
            "slug": "Face-detection-for-pseudo-semantic-labeling-in-Albiol-Bouman",
            "title": {
                "fragments": [],
                "text": "Face detection for pseudo-semantic labeling in video databases"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This paper's work on using face detection techniques for pseudo-semantic labeling using a database of MPEG sequences to improve browsing and searching is described."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348)"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49053294"
                        ],
                        "name": "K. Okada",
                        "slug": "K.-Okada",
                        "structuredName": {
                            "firstName": "Kazunori",
                            "lastName": "Okada",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Okada"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49072208"
                        ],
                        "name": "J. Steffens",
                        "slug": "J.-Steffens",
                        "structuredName": {
                            "firstName": "Johannes",
                            "lastName": "Steffens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Steffens"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49072208"
                        ],
                        "name": "J. Steffens",
                        "slug": "J.-Steffens",
                        "structuredName": {
                            "firstName": "Johannes",
                            "lastName": "Steffens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Steffens"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055501916"
                        ],
                        "name": "Thomas Maurer",
                        "slug": "Thomas-Maurer",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Maurer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Maurer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2093215944"
                        ],
                        "name": "Hai Hong",
                        "slug": "Hai-Hong",
                        "structuredName": {
                            "firstName": "Hai",
                            "lastName": "Hong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hai Hong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2889830"
                        ],
                        "name": "Egor Elagin",
                        "slug": "Egor-Elagin",
                        "structuredName": {
                            "firstName": "Egor",
                            "lastName": "Elagin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Egor Elagin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2665814"
                        ],
                        "name": "H. Neven",
                        "slug": "H.-Neven",
                        "structuredName": {
                            "firstName": "Hartmut",
                            "lastName": "Neven",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Neven"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704573"
                        ],
                        "name": "C. Malsburg",
                        "slug": "C.-Malsburg",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Malsburg",
                            "middleNames": [
                                "von",
                                "der"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Malsburg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704573"
                        ],
                        "name": "C. Malsburg",
                        "slug": "C.-Malsburg",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Malsburg",
                            "middleNames": [
                                "von",
                                "der"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Malsburg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 71
                            }
                        ],
                        "text": "Several recent graph matching systems perform automatic face detection [132, 138], but none have this task as the main goal of the system; thus few extensive quantitative results have been reported on the face detection task specifically."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9076139,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c9579768d142a7020d095090183805c98a2f78e5",
            "isKey": false,
            "numCitedBy": 177,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper summarizes the Bochum/USC face recognition system, our preparations for the FERET Phase III test, and test results as far as they have been made known to us. Our technology is based on Gabor wavelets and elastic bunch graph matching. We briefly discuss our technology in relation to biological and PCA based systems and indicate current activities in the lab and potential future applications."
            },
            "slug": "The-Bochum/USC-Face-Recognition-System-And-How-it-Okada-Steffens",
            "title": {
                "fragments": [],
                "text": "The Bochum/USC Face Recognition System And How it Fared in the FERET Phase III Test"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "This paper summarizes the Bochum/USC face recognition system, the preparations for the FERET Phase III test, and test results as far as they have been made known to us."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768120"
                        ],
                        "name": "T. Jebara",
                        "slug": "T.-Jebara",
                        "structuredName": {
                            "firstName": "Tony",
                            "lastName": "Jebara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jebara"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144994682"
                        ],
                        "name": "A. Pentland",
                        "slug": "A.-Pentland",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Pentland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pentland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "In [77], Jebara and Pentland included this technique in a system for tracking human faces, which was also based on color, 3D, and motion information."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 249000,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c775b46b6fc80b16cbd27da65eb6ddac30868645",
            "isKey": false,
            "numCitedBy": 307,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "A real-time system is described for automatically detecting, modeling and tracking faces in 3D. A closed loop approach is proposed which utilizes structure from motion to generate a 3D model of a face and then feed back the estimated structure to constrain feature tracking in the next frame. The system initializes by using skin classification, symmetry operations, 3D warping and eigenfaces to find a face. Feature trajectories are then computed by SSD or correlation-based tracking. The trajectories are simultaneously processed by an extended Kalman filter to stably recover 3D structure, camera geometry and facial pose. Adaptively weighted estimation is used in this filter by modeling the noise characteristics of the 2D image patch tracking technique. In addition, the structural estimate is constrained by using parametrized models of facial structure (eigen-heads). The Kalman filter's estimate of the 3D state and motion of the face predicts the trajectory of the features which constrains the search space for the next frame in the video sequence. The feature tracking and Kalman filtering closed loop system operates at 25 Hz."
            },
            "slug": "Parametrized-structure-from-motion-for-3D-adaptive-Jebara-Pentland",
            "title": {
                "fragments": [],
                "text": "Parametrized structure from motion for 3D adaptive feedback tracking of faces"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "A real-time system is described for automatically detecting, modeling and tracking faces in 3D, which utilizes structure from motion to generate a 3D model of a face and then feeds back the estimated structure to constrain feature tracking in the next frame."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144410963"
                        ],
                        "name": "S. Kung",
                        "slug": "S.-Kung",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Kung",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1399066120"
                        ],
                        "name": "J. Taur",
                        "slug": "J.-Taur",
                        "structuredName": {
                            "firstName": "J.S.",
                            "lastName": "Taur",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Taur"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 35
                            }
                        ],
                        "text": "The network is similar to the DBNN [97], but it has an additional probabilistic constraint."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 27
                            }
                        ],
                        "text": "Training is performed with DBNN learning rules, which means that the teacher only tells the correctness of the classification (no exact target values) and LUGS (locally unsupervised globally supervised) learning is applied."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 131
                            }
                        ],
                        "text": "In Lin et al. [113], a fully automatic face recognition system is proposed based on probabilistic decision-based neural networks (PDBNN)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "A PDBNN is a classification neural network with a hierarchical modular structure."
                    },
                    "intents": []
                }
            ],
            "corpusId": 12732573,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6d750b2d6c0ccf4f93fe3cd7f146b3ca3b8d18bc",
            "isKey": true,
            "numCitedBy": 176,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Supervised learning networks based on a decision-based formulation are explored. More specifically, a decision-based neural network (DBNN) is proposed, which combines the perceptron-like learning rule and hierarchical nonlinear network structure. The decision-based mutual training can be applied to both static and temporal pattern recognition problems. For static pattern recognition, two hierarchical structures are proposed: hidden-node and subcluster structures. The relationships between DBNN's and other models (linear perceptron, piecewise-linear perceptron, LVQ, and PNN) are discussed. As to temporal DBNN's, model-based discriminant functions may be chosen to compensate possible temporal variations, such as waveform warping and alignments. Typical examples include DTW distance, prediction error, or likelihood functions. For classification applications, DBNN's are very effective in computation time and performance. This is confirmed by simulations conducted for several applications, including texture classification, OCR, and ECG analysis."
            },
            "slug": "Decision-based-neural-networks-with-signal/image-Kung-Taur",
            "title": {
                "fragments": [],
                "text": "Decision-based neural networks with signal/image classification applications"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "A decision-based neural network is proposed, which combines the perceptron-like learning rule and hierarchical nonlinear network structure, which is confirmed by simulations conducted for several applications, including texture classification, OCR, and ECG analysis."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2511655"
                        ],
                        "name": "M. Burl",
                        "slug": "M.-Burl",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Burl",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Burl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 10
                            }
                        ],
                        "text": "Burlet al.[11, 12] make use of statistical shape theory on the features detected from a multi-scale Gaussian derivative filter."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9204636,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "044779db85dc83e2633951791b29bc311cfbae53",
            "isKey": false,
            "numCitedBy": 133,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new framework for recognizing planar object classes, which is based on local feature detectors and a probabilistic model of the spatial arrangement of the features. The allowed object deformations are represented through shape statistics, which are learned from examples. Instances of an object in an image are detected by finding the appropriate features in the correct spatial configuration. The algorithm is robust with respect to partial occlusion, detector false alarms, and missed features. A 94% success rate was achieved for the problem of locating quasi-frontal views of faces in cluttered scenes."
            },
            "slug": "Recognition-of-planar-object-classes-Burl-Perona",
            "title": {
                "fragments": [],
                "text": "Recognition of planar object classes"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "A new framework for recognizing planar object classes is presented, which is based on local feature detectors and a probabilistic model of the spatial arrangement of the features, and the allowed object deformations are represented through shape statistics, which are learned from examples."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings CVPR IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144784813"
                        ],
                        "name": "S. Gong",
                        "slug": "S.-Gong",
                        "structuredName": {
                            "firstName": "Shaogang",
                            "lastName": "Gong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Gong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6435894"
                        ],
                        "name": "S. McKenna",
                        "slug": "S.-McKenna",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "McKenna",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. McKenna"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115271255"
                        ],
                        "name": "J. Collins",
                        "slug": "J.-Collins",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Collins",
                            "middleNames": [
                                "James"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Collins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5831654,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c2cb0ef99710f1894f4a09f407c90d40f6521cbc",
            "isKey": false,
            "numCitedBy": 116,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": "Visual perception of faces is invariant under many transformations, perhaps the most problematic of which is pose change (face rotating in depth). We use a variation of Gabor wavelet transform (GWT) as a representation framework for investigating face pose measurement. Dimensionality reduction using principal components analysis (PCA) enables pose changes to be visualised as manifolds in low-dimensional subspaces and provides a useful mechanism for investigating these changes. The effectiveness of measuring face pose with GWT representations was examined using PCA. We discuss our experimental results and draw a few preliminary conclusions."
            },
            "slug": "An-investigation-into-face-pose-distributions-Gong-McKenna",
            "title": {
                "fragments": [],
                "text": "An investigation into face pose distributions"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "This work uses a variation of Gabor wavelet transform as a representation framework for investigating face pose measurement and Dimensionality reduction using principal components analysis (PCA) enables pose changes to be visualised as manifolds in low-dimensional subspaces and provides a useful mechanism for investigating these changes."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Second International Conference on Automatic Face and Gesture Recognition"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6435894"
                        ],
                        "name": "S. McKenna",
                        "slug": "S.-McKenna",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "McKenna",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. McKenna"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144784813"
                        ],
                        "name": "S. Gong",
                        "slug": "S.-Gong",
                        "structuredName": {
                            "firstName": "Shaogang",
                            "lastName": "Gong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Gong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120597446"
                        ],
                        "name": "Y. Raja",
                        "slug": "Y.-Raja",
                        "structuredName": {
                            "firstName": "Yogesh",
                            "lastName": "Raja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Raja"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 8447172,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ee461d060da58d6053d2f4988b54eff8655ecede",
            "isKey": false,
            "numCitedBy": 263,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Modelling-facial-colour-and-identity-with-Gaussian-McKenna-Gong",
            "title": {
                "fragments": [],
                "text": "Modelling facial colour and identity with Gaussian mixtures"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34679741"
                        ],
                        "name": "S. Li",
                        "slug": "S.-Li",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2651359"
                        ],
                        "name": "ZhenQiu Zhang",
                        "slug": "ZhenQiu-Zhang",
                        "structuredName": {
                            "firstName": "ZhenQiu",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "ZhenQiu Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144154486"
                        ],
                        "name": "H. Shum",
                        "slug": "H.-Shum",
                        "structuredName": {
                            "firstName": "Harry",
                            "lastName": "Shum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Shum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108698841"
                        ],
                        "name": "HongJiang Zhang",
                        "slug": "HongJiang-Zhang",
                        "structuredName": {
                            "firstName": "HongJiang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "HongJiang Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14145523,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8ccc5ef87eab96a4cae226750eba8322b30606ea",
            "isKey": false,
            "numCitedBy": 84,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "AdaBoost [3] minimizes an upper error bound which is an exponential function of the margin on the training set [14]. However, the ultimate goal in applications of pattern classification is always minimum error rate. On the other hand, AdaBoost needs an effective procedure for learning weak classifiers, which by itself is difficult especially for high dimensional data. In this paper, we present a novel procedure, called FloatBoost, for learning a better boosted classifier. FloatBoost uses a backtrack mechanism after each iteration of AdaBoost to remove weak classifiers which cause higher error rates. The resulting float-boosted classifier consists of fewer weak classifiers yet achieves lower error rates than AdaBoost in both training and test. We also propose a statistical model for learning weak classifiers, based on a stagewise approximation of the posterior using an overcomplete set of scalar features. Experimental comparisons of FloatBoost and AdaBoost are provided through a difficult classification problem, face detection, where the goal is to learn from training examples a highly nonlinear classifier to differentiate between face and nonface patterns in a high dimensional space. The results clearly demonstrate the promises made by FloatBoost over AdaBoost."
            },
            "slug": "FloatBoost-Learning-for-Classification-Li-Zhang",
            "title": {
                "fragments": [],
                "text": "FloatBoost Learning for Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "FloatBoost uses a backtrack mechanism after each iteration of AdaBoost to remove weak classifiers which cause higher error rates, and proposes a statistical model for learning weak classifier, based on a stagewise approximation of the posterior using an overcomplete set of scalar features."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49937479"
                        ],
                        "name": "D. E. Benn",
                        "slug": "D.-E.-Benn",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Benn",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. E. Benn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1727698"
                        ],
                        "name": "M. Nixon",
                        "slug": "M.-Nixon",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Nixon",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Nixon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144615864"
                        ],
                        "name": "J. Carter",
                        "slug": "J.-Carter",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Carter",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Carter"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 36
                            }
                        ],
                        "text": "Contemporary research in this field [7, 35, 98, 168, 174] has mainly concentrated on issues such as execution time reductions, template modifications, and energy term considerations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 107724662,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "7f6e5abbc2c0127be173f862bfbabd6fc9fa2505",
            "isKey": false,
            "numCitedBy": 6,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new two stage approach to extracting human eyes from face image by structure alone, without priming on likely position. First a concentricity map of the face is created; the eyes are usually amongst the highest peaks of this map. Next, the energy of a deformable eye template, initialised at concentricity peaks, is optimised using a genetic algorithm. The two highest energy locations are deemed to contain the eyes. We have improved the robustness of an earlier concentricity map and improved Yuille et al's eye template model by removing two parameters whilst also removing the template's dependency on internal energies. Our enhancements has been verified on a database of one thousand full frontal face images, yielding a success rate just exceeding 90% correct eye extraction."
            },
            "slug": "Extending-Concentricity-Analysis-by-Deformable-for-Benn-Nixon",
            "title": {
                "fragments": [],
                "text": "Extending Concentricity Analysis by Deformable Templates for Improved Eye Extraction"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3118328"
                        ],
                        "name": "O. Bernier",
                        "slug": "O.-Bernier",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Bernier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Bernier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2586567"
                        ],
                        "name": "M. Collobert",
                        "slug": "M.-Collobert",
                        "structuredName": {
                            "firstName": "Michel",
                            "lastName": "Collobert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collobert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3430625"
                        ],
                        "name": "R. F\u00e9raud",
                        "slug": "R.-F\u00e9raud",
                        "structuredName": {
                            "firstName": "Rapha\u00ebl",
                            "lastName": "F\u00e9raud",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. F\u00e9raud"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066206356"
                        ],
                        "name": "V. Lemaire",
                        "slug": "V.-Lemaire",
                        "structuredName": {
                            "firstName": "V",
                            "lastName": "Lemaire",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Lemaire"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35094984"
                        ],
                        "name": "J. Viallet",
                        "slug": "J.-Viallet",
                        "structuredName": {
                            "firstName": "Jean-Emmanuel",
                            "lastName": "Viallet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Viallet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2444622"
                        ],
                        "name": "D. Collobert",
                        "slug": "D.-Collobert",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Collobert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Collobert"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 8742282,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "242c7e54293575318f594d7808136c81d2be1994",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "A real-time system is described for automatic detection and tracking of multiple persons, in the context of video-conferencing systems. This system, called MULTRAK (multiperson locating and tracking automatic kernel) is able to continuously detect and track the position of faces in its field of view. The heart of the system as a modular neural network based face detector, giving fast and accurate face detection."
            },
            "slug": "MULTRAK:-a-system-for-automatic-multiperson-and-in-Bernier-Collobert",
            "title": {
                "fragments": [],
                "text": "MULTRAK: a system for automatic multiperson localization and tracking in real-time"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "A real-time system is described for automatic detection and tracking of multiple persons, in the context of video-conferencing systems, able to continuously detect and track the position of faces in its field of view."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 1998 International Conference on Image Processing. ICIP98 (Cat. No.98CB36269)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055712370"
                        ],
                        "name": "T. Yokoyama",
                        "slug": "T.-Yokoyama",
                        "structuredName": {
                            "firstName": "Taro",
                            "lastName": "Yokoyama",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Yokoyama"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715071"
                        ],
                        "name": "Y. Yagi",
                        "slug": "Y.-Yagi",
                        "structuredName": {
                            "firstName": "Yasushi",
                            "lastName": "Yagi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Yagi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735941"
                        ],
                        "name": "M. Yachida",
                        "slug": "M.-Yachida",
                        "structuredName": {
                            "firstName": "Masahiko",
                            "lastName": "Yachida",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Yachida"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 28019795,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "faa859a3246a0c48766ad56751aad5e1278c8e5c",
            "isKey": false,
            "numCitedBy": 11,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "The extraction of facial contours from a facial image is a challenging problem in computer vision. We propose a new active contour model for extracting facial outline contours. Our model has three characteristics: axis symmetry constraint, dual scale edge filtering and iterative initialization. We implemented this method in our Automatical Facial Contour Extraction System, and tested it on 60 facial images. We show the advantage of this method for detecting facial outline contours by a psychological experiment."
            },
            "slug": "Facial-contour-extraction-model-Yokoyama-Yagi",
            "title": {
                "fragments": [],
                "text": "Facial contour extraction model"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A new active contour model is proposed for extracting facial outline contours from a facial image with three characteristics: axis symmetry constraint, dual scale edge filtering and iterative initialization."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Third IEEE International Conference on Automatic Face and Gesture Recognition"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700567"
                        ],
                        "name": "S. Satoh",
                        "slug": "S.-Satoh",
                        "structuredName": {
                            "firstName": "Shin\u2019ichi",
                            "lastName": "Satoh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Satoh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "24265259"
                        ],
                        "name": "Yuichi Nakamura",
                        "slug": "Yuichi-Nakamura",
                        "structuredName": {
                            "firstName": "Yuichi",
                            "lastName": "Nakamura",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuichi Nakamura"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733113"
                        ],
                        "name": "T. Kanade",
                        "slug": "T.-Kanade",
                        "structuredName": {
                            "firstName": "Takeo",
                            "lastName": "Kanade",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanade"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 8
                            }
                        ],
                        "text": "Name-It [167] also processes news videos, but is focused on associating names with faces."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14592234,
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "id": "fad265c5e322a6740009a12edc173860f5f516b1",
            "isKey": false,
            "numCitedBy": 311,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We developed Name-It, a system that associates faces and names in news videos. It processes information from the videos and can infer possible name candidates for a given face or locate a face in news videos by name. To accomplish this task, the system takes a multimodal video analysis approach: face sequence extraction and similarity evaluation from videos, name extraction from transcripts, and video-caption recognition."
            },
            "slug": "Name-It:-Naming-and-Detecting-Faces-in-News-Videos-Satoh-Nakamura",
            "title": {
                "fragments": [],
                "text": "Name-It: Naming and Detecting Faces in News Videos"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "Name-It, a system that associates faces and names in news videos, takes a multimodal video analysis approach: face sequence extraction and similarity evaluation from videos, name extraction from transcripts, and video-caption recognition."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Multim."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2527920"
                        ],
                        "name": "Douglas E. Zongker",
                        "slug": "Douglas-E.-Zongker",
                        "structuredName": {
                            "firstName": "Douglas",
                            "lastName": "Zongker",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Douglas E. Zongker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1764288,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2a5b31aaefd943b8eb3334fda53311b06f0bdbf5",
            "isKey": false,
            "numCitedBy": 2259,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "A large number of algorithms have been proposed for feature subset selection. Our experimental results show that the sequential forward floating selection algorithm, proposed by Pudil et al. (1994), dominates the other algorithms tested. We study the problem of choosing an optimal feature set for land use classification based on SAR satellite images using four different texture models. Pooling features derived from different texture models, followed by a feature selection results in a substantial improvement in the classification accuracy. We also illustrate the dangers of using feature selection in small sample size situations."
            },
            "slug": "Feature-Selection:-Evaluation,-Application,-and-Jain-Zongker",
            "title": {
                "fragments": [],
                "text": "Feature Selection: Evaluation, Application, and Small Sample Performance"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work studies the problem of choosing an optimal feature set for land use classification based on SAR satellite images using four different texture models and shows that pooling features derived from different texture Models, followed by a feature selection results in a substantial improvement in the classification accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715634"
                        ],
                        "name": "Ming-Hsuan Yang",
                        "slug": "Ming-Hsuan-Yang",
                        "structuredName": {
                            "firstName": "Ming-Hsuan",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming-Hsuan Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145237406"
                        ],
                        "name": "N. Ahuja",
                        "slug": "N.-Ahuja",
                        "structuredName": {
                            "firstName": "Narendra",
                            "lastName": "Ahuja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Ahuja"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14053944,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4de2e3e2873571a7fa145c1a1febe1b28e472b8e",
            "isKey": false,
            "numCitedBy": 364,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper is concerned with estimating a probability density function of human skin color, using a finite Gaussian mixture model, whose parameters are estimated through the EM algorithm. Hawkins' statistical test on the normality and homoscedasticity (common covariance matrix) of the estimated Gaussian mixture models is performed and McLachlan's bootstrap method is used to test the number of components in a mixture. Experimental results show that the estimated Gaussian mixture model fits skin images from a large database. Applications of the estimated density function in image and video databases are presented."
            },
            "slug": "Gaussian-mixture-model-for-human-skin-color-and-its-Yang-Ahuja",
            "title": {
                "fragments": [],
                "text": "Gaussian mixture model for human skin color and its applications in image and video databases"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Experimental results show that the estimated Gaussian mixture model fits skin images from a large database and applications of the estimated density function in image and video databases are presented."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47953439"
                        ],
                        "name": "R\u00e9gis Vaillant",
                        "slug": "R\u00e9gis-Vaillant",
                        "structuredName": {
                            "firstName": "R\u00e9gis",
                            "lastName": "Vaillant",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R\u00e9gis Vaillant"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3208918"
                        ],
                        "name": "C. Monrocq",
                        "slug": "C.-Monrocq",
                        "structuredName": {
                            "firstName": "Christophe",
                            "lastName": "Monrocq",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Monrocq"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9326933"
                        ],
                        "name": "Y. L. Cun",
                        "slug": "Y.-L.-Cun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "Cun",
                            "middleNames": [
                                "le"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. L. Cun"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62763570,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "09ebd9ad4fa21c0d56433ac57a4cd69e94c72281",
            "isKey": false,
            "numCitedBy": 286,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "An original approach is presented for the localisation of objects in an image which approach is neuronal and has two steps. In the first step, a rough localisation is performed by presenting each pixel with its neighbourhood to a neural net which is able to indicate whether this pixel and its neighbourhood are the image of the search object. This first filter does not discriminate for position. From its result, areas which might contain an image of the object can be selected. In the second step, these areas are presented to another neural net which can determine the exact position of the object in each area. This algorithm is applied to the problem of localising faces in images."
            },
            "slug": "Original-approach-for-the-localisation-of-objects-Vaillant-Monrocq",
            "title": {
                "fragments": [],
                "text": "Original approach for the localisation of objects in images"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "An original approach is presented for the localisation of objects in an image which approach is neuronal and has two steps and is applied to the problem of localising faces in images."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713089"
                        ],
                        "name": "D. Huttenlocher",
                        "slug": "D.-Huttenlocher",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Huttenlocher",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Huttenlocher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2901136"
                        ],
                        "name": "G. Klanderman",
                        "slug": "G.-Klanderman",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Klanderman",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Klanderman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116003"
                        ],
                        "name": "W. Rucklidge",
                        "slug": "W.-Rucklidge",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Rucklidge",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Rucklidge"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8027136,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "85efeeb25d8e363606d94c8fadaa922ba9b93a37",
            "isKey": false,
            "numCitedBy": 3913,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "The Hausdorff distance measures the extent to which each point of a model set lies near some point of an image set and vice versa. Thus, this distance can be used to determine the degree of resemblance between two objects that are superimposed on one another. Efficient algorithms for computing the Hausdorff distance between all possible relative positions of a binary image and a model are presented. The focus is primarily on the case in which the model is only allowed to translate with respect to the image. The techniques are extended to rigid motion. The Hausdorff distance computation differs from many other shape comparison methods in that no correspondence between the model and the image is derived. The method is quite tolerant of small position errors such as those that occur with edge detectors and other feature extraction methods. It is shown that the method extends naturally to the problem of comparing a portion of a model against an image. >"
            },
            "slug": "Comparing-Images-Using-the-Hausdorff-Distance-Huttenlocher-Klanderman",
            "title": {
                "fragments": [],
                "text": "Comparing Images Using the Hausdorff Distance"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Efficient algorithms for computing the Hausdorff distance between all possible relative positions of a binary image and a model are presented and it is shown that the method extends naturally to the problem of comparing a portion of a model against an image."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780935"
                        ],
                        "name": "B. Moghaddam",
                        "slug": "B.-Moghaddam",
                        "structuredName": {
                            "firstName": "Baback",
                            "lastName": "Moghaddam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Moghaddam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144994682"
                        ],
                        "name": "A. Pentland",
                        "slug": "A.-Pentland",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Pentland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pentland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9242811,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e49ad8354bdd2fd6e8babd348df9e9a5b30bf3a6",
            "isKey": false,
            "numCitedBy": 461,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an unsupervised technique for visual learning which is based on density estimation in high-dimensional spaces using an eigenspace decomposition. Two types of density estimates are derived for modeling the training data: a multivariate Gaussian (for a unimodal distributions) and a multivariate Mixture-of-Gaussians model (for multimodal distributions). These probability densities are then used to formulate a maximum-likelihood estimation framework for visual search and target detection for automatic object recognition. This learning technique is tested in experiments with modeling and subsequent detection of human faces and non-rigid objects such as hands.<<ETX>>"
            },
            "slug": "Probabilistic-visual-learning-for-object-detection-Moghaddam-Pentland",
            "title": {
                "fragments": [],
                "text": "Probabilistic visual learning for object detection"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "An unsupervised technique for visual learning which is based on density estimation in high-dimensional spaces using an eigenspace decomposition and a multivariate Mixture-of-Gaussians model is presented."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE International Conference on Computer Vision"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144522420"
                        ],
                        "name": "T. Ojala",
                        "slug": "T.-Ojala",
                        "structuredName": {
                            "firstName": "Timo",
                            "lastName": "Ojala",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Ojala"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145962204"
                        ],
                        "name": "M. Pietik\u00e4inen",
                        "slug": "M.-Pietik\u00e4inen",
                        "structuredName": {
                            "firstName": "Matti",
                            "lastName": "Pietik\u00e4inen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pietik\u00e4inen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145298122"
                        ],
                        "name": "D. Harwood",
                        "slug": "D.-Harwood",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Harwood",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Harwood"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 26881819,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "5985014dda6d502469614aae17349b4d08f9f74c",
            "isKey": false,
            "numCitedBy": 6554,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-comparative-study-of-texture-measures-with-based-Ojala-Pietik\u00e4inen",
            "title": {
                "fragments": [],
                "text": "A comparative study of texture measures with classification based on featured distributions"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7205190"
                        ],
                        "name": "Tim Cootes",
                        "slug": "Tim-Cootes",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Cootes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tim Cootes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144482985"
                        ],
                        "name": "C. Taylor",
                        "slug": "C.-Taylor",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Taylor",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1830709"
                        ],
                        "name": "A. Lanitis",
                        "slug": "A.-Lanitis",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Lanitis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Lanitis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 40
                            }
                        ],
                        "text": "Then a local gray-scale search strategy [25] is employed to move each point toward its corresponding boundary point."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16726457,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "de2ac2bb3977b585c1221ab3f0eef7c0c9a542cd",
            "isKey": false,
            "numCitedBy": 220,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a multi-resolution technique for locating for variable structures in images. This is an extension of work on Active Shape Models (ASMs) - statistical models which iteratively deform to match image data. An ASM consists of a shape model controlling a set of landmark points, together with a statistical model of the grey-levels expected around each landmark. Both the shape model and the grey-level models are trained on sets of labelled example images. In order to apply a coarse-to-fine search strategy it is necessary to train a set of grey-level models for each landmark, one for every level of a multi-resolution image pyramid. During image search the model is started on the coarsest resolution image. As the search progresses it moves to finer and finer resolutions until no further improvement can be made. We describe an automatic technique for deciding when to :ss has converged. We demonstrate the ntitative experiments which show a sigi speed and quality of fit compared to previous methods."
            },
            "slug": "Active-Shape-Models:-Evaluation-of-a-Method-for-Cootes-Taylor",
            "title": {
                "fragments": [],
                "text": "Active Shape Models: Evaluation of a Multi-Resolution Method for Improving Image Search"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "An automatic technique for deciding when to :ss has converged and the ntitative experiments which show a sigi speed and quality of fit compared to previous methods are demonstrated."
            },
            "venue": {
                "fragments": [],
                "text": "BMVC"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2812486"
                        ],
                        "name": "P. Simard",
                        "slug": "P.-Simard",
                        "structuredName": {
                            "firstName": "Patrice",
                            "lastName": "Simard",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Simard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721248"
                        ],
                        "name": "P. Haffner",
                        "slug": "P.-Haffner",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Haffner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Haffner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14647160,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b2ef3d741a8d267b813781a73d4222b6dbba99fa",
            "isKey": false,
            "numCitedBy": 150,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "Signal processing and pattern recognition algorithms make extensive use of convolution. In many cases, computational accuracy is not as important as computational speed. In feature extraction, for instance, the features of interest in a signal are usually quite distorted. This form of noise justifies some level of quantization in order to achieve faster feature extraction. Our approach consists of approximating regions of the signal with low degree polynomials, and then differentiating the resulting signals in order to obtain impulse functions (or derivatives of impulse functions). With this representation, convolution becomes extremely simple and can be implemented quite effectively. The true convolution can be recovered by integrating the result of the convolution. This method yields substantial speed up in feature extraction and is applicable to convolutional neural networks."
            },
            "slug": "Boxlets:-A-Fast-Convolution-Algorithm-for-Signal-Simard-Bottou",
            "title": {
                "fragments": [],
                "text": "Boxlets: A Fast Convolution Algorithm for Signal Processing and Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The approach consists of approximating regions of the signal with low degree polynomials, and then differentiating the resulting signals in order to obtain impulse functions (or derivatives of impulse functions) and yields substantial speed up in feature extraction."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152292143"
                        ],
                        "name": "Jeffrey R. Huang",
                        "slug": "Jeffrey-R.-Huang",
                        "structuredName": {
                            "firstName": "Jeffrey R.",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeffrey R. Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143979395"
                        ],
                        "name": "H. Wechsler",
                        "slug": "H.-Wechsler",
                        "structuredName": {
                            "firstName": "Harry",
                            "lastName": "Wechsler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Wechsler"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 107
                            }
                        ],
                        "text": "Other proposed approaches for feature searching include radial basis functions [71] and genetic algorithms [72, 112, 144]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1455797,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "15dc50e3bd1063c3760e1c17177a0e898175c61e",
            "isKey": false,
            "numCitedBy": 69,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Eye location is used as a test bed for developing navigation routines implemented as visual routines within the framework of adaptive behavior-based AI. The adaptive eye location approach seeks first where salient objects are, and then what their identity is. Specifically, eye location involves: 1) the derivation of the saliency attention map, and 2) the possible classification of salient locations as eve regions. The saliency (\"where\") map is derived using a consensus between navigation routines encoded as finite-state automata exploring the facial landscape and evolved using genetic algorithms (GAs). The classification (\"what\") stage is concerned with the optimal selection of features, and the derivation of decision trees, using GAs, to possibly classify salient locations as eyes. The experimental results, using facial image data, show the feasibility of our method, and suggest a novel approach for the adaptive development of task-driven active perception and navigational mechanisms."
            },
            "slug": "Visual-routines-for-eye-location-using-learning-and-Huang-Wechsler",
            "title": {
                "fragments": [],
                "text": "Visual routines for eye location using learning and evolution"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The experimental results, using facial image data, show the feasibility of the adaptive eye location approach, and suggest a novel approach for the adaptive development of task-driven active perception and navigational mechanisms."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Evol. Comput."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780935"
                        ],
                        "name": "B. Moghaddam",
                        "slug": "B.-Moghaddam",
                        "structuredName": {
                            "firstName": "Baback",
                            "lastName": "Moghaddam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Moghaddam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144994682"
                        ],
                        "name": "A. Pentland",
                        "slug": "A.-Pentland",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Pentland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pentland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 109
                            }
                        ],
                        "text": "More recently, Moghaddam and Pentland have further developed this technique within a probabilistic framework [134]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 483975,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "74b312560b79929540734067e58de46966b96130",
            "isKey": false,
            "numCitedBy": 1684,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an unsupervised technique for visual learning, which is based on density estimation in high-dimensional spaces using an eigenspace decomposition. Two types of density estimates are derived for modeling the training data: a multivariate Gaussian (for unimodal distributions) and a mixture-of-Gaussians model (for multimodal distributions). Those probability densities are then used to formulate a maximum-likelihood estimation framework for visual search and target detection for automatic object recognition and coding. Our learning technique is applied to the probabilistic visual modeling, detection, recognition, and coding of human faces and nonrigid objects, such as hands."
            },
            "slug": "Probabilistic-Visual-Learning-for-Object-Moghaddam-Pentland",
            "title": {
                "fragments": [],
                "text": "Probabilistic Visual Learning for Object Representation"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "An unsupervised technique for visual learning is presented, which is based on density estimation in high-dimensional spaces using an eigenspace decomposition and is applied to the probabilistic visual modeling, detection, recognition, and coding of human faces and nonrigid objects."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50577973"
                        ],
                        "name": "J. Vincent",
                        "slug": "J.-Vincent",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Vincent",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Vincent"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145208455"
                        ],
                        "name": "J. Waite",
                        "slug": "J.-Waite",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Waite",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Waite"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46503163"
                        ],
                        "name": "D. Myers",
                        "slug": "D.-Myers",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Myers",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Myers"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 106
                            }
                        ],
                        "text": "Apart from face classification, neural networks have also been applied for facial features classification [36, 125, 151, 196] and a method for improving detection time for MLPs is presented in [6]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 200,
                                "start": 145
                            }
                        ],
                        "text": "Since the main variation in skin appearance is largely due to luminance change (brightness) [215], normalized RGB colors are generally preferred [27, 53, 75, 88, 92, 165, 181, 196, 202, 207, 213, 215], so that the effect of luminance can be filtered out."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 61726920,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b009a4c5eaeb55f3f7b1c74c3a1a0143ab2beed0",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "A two-stage neural vision system for locating facial features is described. The first stage generates search regions in an image at low spatial resolution, and the second pinpoints the features at high resolution. Both stages employ multilayered perceptrons trained to detect specific visual details, followed by sophisticated global postprocessing of their outputs. This work demonstrates the power of combining neural feature detection with knowledge-based context-sensitive methods."
            },
            "slug": "Automatic-location-of-visual-features-by-a-system-Vincent-Waite",
            "title": {
                "fragments": [],
                "text": "Automatic location of visual features by a system of multilayered perceptrons"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work demonstrates the power of combining neural feature detection with knowledge-based context-sensitive methods in a two-stage neural vision system for locating facial features."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4801654"
                        ],
                        "name": "Y. Amit",
                        "slug": "Y.-Amit",
                        "structuredName": {
                            "firstName": "Yali",
                            "lastName": "Amit",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Amit"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35651617"
                        ],
                        "name": "Kenneth Wilder",
                        "slug": "Kenneth-Wilder",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Wilder",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenneth Wilder"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2023689,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9f7c640ea1fe32e017c68005ef5e18969039b3f4",
            "isKey": false,
            "numCitedBy": 209,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a very large family of binary features for two-dimensional shapes. The salient ones for separating particular shapes are determined by inductive learning during the construction of classification trees. There is a feature for every possible geometric arrangement of local topographic codes. The arrangements express coarse constraints on relative angles and distances among the code locations and are nearly invariant to substantial affine and nonlinear deformations. They are also partially ordered, which makes it possible to narrow the search for informative ones at each node of the tree. Different trees correspond to different aspects of shape. They are statistically and weakly dependent due to randomization and are aggregated in a simple way. Adapting the algorithm to a shape family is then fully automatic once training samples are provided. As an illustration, we classified handwritten digits from the NIST database; the error rate was 0.7 percent."
            },
            "slug": "Joint-Induction-of-Shape-Features-and-Tree-Amit-Wilder",
            "title": {
                "fragments": [],
                "text": "Joint Induction of Shape Features and Tree Classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "A very large family of binary features for two-dimensional shapes determined by inductive learning during the construction of classification trees is introduced, which makes it possible to narrow the search for informative ones at each node of the tree."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "97129855"
                        ],
                        "name": "M. Hu",
                        "slug": "M.-Hu",
                        "structuredName": {
                            "firstName": "Ming-Kuei",
                            "lastName": "Hu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 13
                            }
                        ],
                        "text": "Hu\u2019s moments [68] are used as features and a multilayer perceptron neural network is trained to classify the face candidates."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6431165,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ce1e3528047cd01937f6a8aa760640f6b3c8d531",
            "isKey": false,
            "numCitedBy": 8012,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper a theory of two-dimensional moment invariants for planar geometric figures is presented. A fundamental theorem is established to relate such moment invariants to the well-known algebraic invariants. Complete systems of moment invariants under translation, similitude and orthogonal transformations are derived. Some moment invariants under general two-dimensional linear transformations are also included. Both theoretical formulation and practical models of visual pattern recognition based upon these moment invariants are discussed. A simple simulation program together with its performance are also presented. It is shown that recognition of geometrical patterns and alphabetical characters independently of position, size and orientation can be accomplished. It is also indicated that generalization is possible to include invariance with parallel projection."
            },
            "slug": "Visual-pattern-recognition-by-moment-invariants-Hu",
            "title": {
                "fragments": [],
                "text": "Visual pattern recognition by moment invariants"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is shown that recognition of geometrical patterns and alphabetical characters independently of position, size and orientation can be accomplished and it is indicated that generalization is possible to include invariance with parallel projection."
            },
            "venue": {
                "fragments": [],
                "text": "IRE Trans. Inf. Theory"
            },
            "year": 1962
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679880"
                        ],
                        "name": "H. Wactlar",
                        "slug": "H.-Wactlar",
                        "structuredName": {
                            "firstName": "Howard",
                            "lastName": "Wactlar",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Wactlar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733113"
                        ],
                        "name": "T. Kanade",
                        "slug": "T.-Kanade",
                        "structuredName": {
                            "firstName": "Takeo",
                            "lastName": "Kanade",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanade"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116645471"
                        ],
                        "name": "Michael A. Smith",
                        "slug": "Michael-A.-Smith",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Smith",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael A. Smith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48817314"
                        ],
                        "name": "S. Stevens",
                        "slug": "S.-Stevens",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Stevens",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Stevens"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "One such example is the Informedia project [198] which provides search and retrieval of TV news and documentary broadcasts."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8345108,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3b4d170b81ffc895e5b7960040a3f44a044d6bb8",
            "isKey": false,
            "numCitedBy": 372,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Carnegie Mellon's Informedia Digital Video Library project will establish a large, on-line digital video library featuring full-content and knowledge-based search and retrieval. Intelligent, automatic mechanisms will be developed to populate the library. Search and retrieval from digital video, audio, and text libraries will take place via desktop computer over local, metropolitan, and wide-area networks. The project's approach applies several techniques for content-based searching and video-sequence retrieval. Content is conveyed in both the narrative (speech and language) and the image. Only by the collaborative interaction of image, speech, and natural language understanding technology is it possible to successfully populate, segment, index, and search diverse video collections with satisfactory recall and precision. This collaborative interaction approach uniquely compensates for problems of interpretation and search in error-ridden and ambiguous data sets. The authors have focused the work on two corpuses. One is science documentaries and lectures, the other is broadcast news content with partial closed-captions. Further work will continue to improve the accuracy and performance of the underlying processing as well as explore performance issues related to Web-based access and interoperability with other digital video resources."
            },
            "slug": "Intelligent-Access-to-Digital-Video:-Informedia-Wactlar-Kanade",
            "title": {
                "fragments": [],
                "text": "Intelligent Access to Digital Video: Informedia Project"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "Carnegie Mellon's Informedia Digital Video Library project will establish a large, on-line digital video library featuring full-content and knowledge-based search and retrieval, and focused the work on two corpuses."
            },
            "venue": {
                "fragments": [],
                "text": "Computer"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49555086"
                        ],
                        "name": "L. Sirovich",
                        "slug": "L.-Sirovich",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Sirovich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Sirovich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053520352"
                        ],
                        "name": "M. Kirby",
                        "slug": "M.-Kirby",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Kirby",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kirby"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 19
                            }
                        ],
                        "text": "L. Sirovich and M. Kirby, Low-dimensional procedure for the characterization of human faces,J. Opt."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 38
                            }
                        ],
                        "text": "In the late 1980s, Sirovich and Kirby [176] developed a technique using PCA to efficiently represent human faces."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 982857,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2a62d0cca2fabf1d6f6ee15e4c14cef415b657d1",
            "isKey": false,
            "numCitedBy": 2592,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "A method is presented for the representation of (pictures of) faces. Within a specified framework the representation is ideal. This results in the characterization of a face, to within an error bound, by a relatively low-dimensional vector. The method is illustrated in detail by the use of an ensemble of pictures taken for this purpose."
            },
            "slug": "Low-dimensional-procedure-for-the-characterization-Sirovich-Kirby",
            "title": {
                "fragments": [],
                "text": "Low-dimensional procedure for the characterization of human faces."
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "A method is presented for the representation of faces that results in the characterization of a face, to within an error bound, by a relatively low-dimensional vector."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of the Optical Society of America. A, Optics and image science"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781325"
                        ],
                        "name": "J. Daugman",
                        "slug": "J.-Daugman",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Daugman",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Daugman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 29
                            }
                        ],
                        "text": "They use 2-D Gabor functions [31] of six orientations and five different frequencies for feature extraction."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1984348,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6513888c5ef473bdbb3167c7b52f0985be071f7a",
            "isKey": false,
            "numCitedBy": 1899,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "A three-layered neural network is described for transforming two-dimensional discrete signals into generalized nonorthogonal 2-D Gabor representations for image analysis, segmentation, and compression. These transforms are conjoint spatial/spectral representations, which provide a complete image description in terms of locally windowed 2-D spectral coordinates embedded within global 2-D spatial coordinates. In the present neural network approach, based on interlaminar interactions involving two layers with fixed weights and one layer with adjustable weights, the network finds coefficients for complete conjoint 2-D Gabor transforms without restrictive conditions. In wavelet expansions based on a biologically inspired log-polar ensemble of dilations, rotations, and translations of a single underlying 2-D Gabor wavelet template, image compression is illustrated with ratios up to 20:1. Also demonstrated is image segmentation based on the clustering of coefficients in the complete 2-D Gabor transform. >"
            },
            "slug": "Complete-discrete-2-D-Gabor-transforms-by-neural-Daugman",
            "title": {
                "fragments": [],
                "text": "Complete discrete 2-D Gabor transforms by neural networks for image analysis and compression"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "A three-layered neural network based on interlaminar interactions involving two layers with fixed weights and one layer with adjustable weights finds coefficients for complete conjoint 2-D Gabor transforms without restrictive conditions for image analysis, segmentation, and compression."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Acoust. Speech Signal Process."
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717435"
                        ],
                        "name": "B. G. Schunck",
                        "slug": "B.-G.-Schunck",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Schunck",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. G. Schunck"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 115
                            }
                        ],
                        "text": "Lee t al. proposed a line clustering algorithm which is a modified and faster version of the original algorithm by Schunck [172]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 6
                            }
                        ],
                        "text": "B. G. Schunck, Image flow segmentation and estimation by constraint line clustering,IEEE Trans."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 113
                            }
                        ],
                        "text": "proposed a line clustering algorithm which is a modified and faster version of the original algorithm by Schunck [172]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 81902,
            "fieldsOfStudy": [
                "Geology"
            ],
            "id": "7173228b2c4b98bcc8447768d925c91c55116de2",
            "isKey": true,
            "numCitedBy": 196,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "Image flow is the velocity field in the image plane caused by the motion of the observer, objects in the scene, or apparent motion, and can contain discontinuities due to object occlusion in the scene. An algorithm that can estimate the image flow velocity field when there are discontinuities due to occlusions is described. The constraint line clustering algorithm uses a statistical test to estimate the image flow velocity field in the presence of step discontinuities in the image irradiance or velocity field. Particular emphasis is placed on motion estimation and segmentation in situations such as random dot patterns where motion is the only cue to segmentation. Experimental results on a demanding synthetic test case and a real image are presented. A smoothing algorithm for improving the velocity field estimate is also described. The smoothing algorithm constructs a smooth estimate of the velocity field by approximating a surface between step discontinuities. It is noted that the velocity field estimate can be improved using surface reconstruction between velocity field boundaries. >"
            },
            "slug": "Image-Flow-Segmentation-and-Estimation-by-Line-Schunck",
            "title": {
                "fragments": [],
                "text": "Image Flow Segmentation and Estimation by Constraint Line Clustering"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The constraint line clustering algorithm uses a statistical test to estimate the image flow velocity field in the presence of step discontinuities in the image irradiance or velocity field, with particular emphasis on motion estimation and segmentation in situations where motion is the only cue to segmentation."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109558447"
                        ],
                        "name": "Carol Wong",
                        "slug": "Carol-Wong",
                        "structuredName": {
                            "firstName": "Carol",
                            "lastName": "Wong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carol Wong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689716"
                        ],
                        "name": "D. Kortenkamp",
                        "slug": "D.-Kortenkamp",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Kortenkamp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Kortenkamp"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50448047"
                        ],
                        "name": "Mark Speich",
                        "slug": "Mark-Speich",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Speich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Speich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18229298,
            "fieldsOfStudy": [
                "Computer Science",
                "Business"
            ],
            "id": "deab1f7467738790c95c400a587ae5fd85bf1148",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "In order for mobile robots to interact effectively with people they will have to recognize faces. We describe a robot system that finds people, approaches them and then recognizes them. The system uses a variety of techniques: color vision is used to find people; vision and sonar sensors are used to approach them; a template-based pattern recognition algorithm is used to isolate the face; and a neural network is used to recognize the face. All of these processes are controlled using an intelligent robot architecture that sequences and monitors the robot's actions. We present the results of many experimental runs using an actual mobile robot finding and recognizing up to six different people."
            },
            "slug": "A-mobile-robot-that-recognizes-people-Wong-Kortenkamp",
            "title": {
                "fragments": [],
                "text": "A mobile robot that recognizes people"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "A robot system that finds people, approaches them and then recognizes them is described, which uses a variety of techniques: color vision is used to find people; vision and sonar sensors are used to approach them; and a template-based pattern recognition algorithm is usedto isolate the face."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 7th IEEE International Conference on Tools with Artificial Intelligence"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781874"
                        ],
                        "name": "E. Osuna",
                        "slug": "E.-Osuna",
                        "structuredName": {
                            "firstName": "Edgar",
                            "lastName": "Osuna",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Osuna"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1771659"
                        ],
                        "name": "R. Freund",
                        "slug": "R.-Freund",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Freund",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15140283,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "68c4749d9d3f1724aa01778d69a3774c732ca44c",
            "isKey": false,
            "numCitedBy": 844,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "The Support Vector Machine (SVM) is a new and very promising classification technique developed by Vapnik and his group at AT\\&T Bell Labs. This new learning algorithm can be seen as an alternative training technique for Polynomial, Radial Basis Function and Multi-Layer Perceptron classifiers. An interesting property of this approach is that it is an approximate implementation of the Structural Risk Minimization (SRM) induction principle. The derivation of Support Vector Machines, its relationship with SRM, and its geometrical insight, are discussed in this paper. Training a SVM is equivalent to solve a quadratic programming problem with linear and box constraints in a number of variables equal to the number of data points. When the number of data points exceeds few thousands the problem is very challenging, because the quadratic form is completely dense, so the memory needed to store the problem grows with the square of the number of data points. Therefore, training problems arising in some real applications with large data sets are impossible to load into memory, and cannot be solved using standard non-linear constrained optimization algorithms. We present a decomposition algorithm that can be used to train SVM''s over large data sets. The main idea behind the decomposition is the iterative solution of sub-problems and the evaluation of, and also establish the stopping criteria for the algorithm. We present previous approaches, as well as results and important details of our implementation of the algorithm using a second-order variant of the Reduced Gradient Method as the solver of the sub-problems. As an application of SVM''s, we present preliminary results we obtained applying SVM to the problem of detecting frontal human faces in real images."
            },
            "slug": "Support-Vector-Machines:-Training-and-Applications-Osuna-Freund",
            "title": {
                "fragments": [],
                "text": "Support Vector Machines: Training and Applications"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Preliminary results are presented obtained applying SVM to the problem of detecting frontal human faces in real images, and the main idea behind the decomposition is the iterative solution of sub-problems and the evaluation of, and also establish the stopping criteria for the algorithm."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056361"
                        ],
                        "name": "J. Friedman",
                        "slug": "J.-Friedman",
                        "structuredName": {
                            "firstName": "Jerome",
                            "lastName": "Friedman",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Friedman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9913392,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6f4493eff2531536a7aeb3fc11d62c30a8f487f6",
            "isKey": false,
            "numCitedBy": 4830,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "Boosting is one of the most important recent developments in classification methodology. Boosting works by sequentially applying a classification algorithm to reweighted versions of the training data and then taking a weighted majority vote of the sequence of classifiers thus produced. For many classification algorithms, this simple strategy results in dramatic improvements in performance. We show that this seemingly mysterious phenomenon can be understood in terms of well-known statistical principles, namely additive modeling and maximum likelihood. For the two-class problem, boosting can be viewed as an approximation to additive modeling on the logistic scale using maximum Bernoulli likelihood as a criterion. We develop more direct approximations and show that they exhibit nearly identical results to boosting. Direct multiclass generalizations based on multinomial likelihood are derived that exhibit performance comparable to other recently proposed multiclass generalizations of boosting in most situations, and far superior in some. We suggest a minor modification to boosting that can reduce computation, often by factors of 10 to 50. Finally, we apply these insights to produce an alternative formulation of boosting decision trees. This approach, based on best-first truncated tree induction, often leads to better performance, and can provide interpretable descriptions of the aggregate decision rule. It is also much faster computationally, making it more suitable to large-scale data mining applications."
            },
            "slug": "Special-Invited-Paper-Additive-logistic-regression:-Friedman",
            "title": {
                "fragments": [],
                "text": "Special Invited Paper-Additive logistic regression: A statistical view of boosting"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work shows that this seemingly mysterious phenomenon of boosting can be understood in terms of well-known statistical principles, namely additive modeling and maximum likelihood, and develops more direct approximations and shows that they exhibit nearly identical results to boosting."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35106875"
                        ],
                        "name": "R. Duda",
                        "slug": "R.-Duda",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Duda",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Duda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3108177"
                        ],
                        "name": "P. Hart",
                        "slug": "P.-Hart",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Hart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hart"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12946615,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b07ce649d6f6eb636872527104b0209d3edc8188",
            "isKey": false,
            "numCitedBy": 16928,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Provides a unified, comprehensive and up-to-date treatment of both statistical and descriptive methods for pattern recognition. The topics treated include Bayesian decision theory, supervised and unsupervised learning, nonparametric techniques, discriminant analysis, clustering, preprosessing of pictorial data, spatial filtering, shape description techniques, perspective transformations, projective invariants, linguistic procedures, and artificial intelligence techniques for scene analysis."
            },
            "slug": "Pattern-classification-and-scene-analysis-Duda-Hart",
            "title": {
                "fragments": [],
                "text": "Pattern classification and scene analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "The topics treated include Bayesian decision theory, supervised and unsupervised learning, nonparametric techniques, discriminant analysis, clustering, preprosessing of pictorial data, spatial filtering, shape description techniques, perspective transformations, projective invariants, linguistic procedures, and artificial intelligence techniques for scene analysis."
            },
            "venue": {
                "fragments": [],
                "text": "A Wiley-Interscience publication"
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2502152"
                        ],
                        "name": "A. Treisman",
                        "slug": "A.-Treisman",
                        "structuredName": {
                            "firstName": "Anne",
                            "lastName": "Treisman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Treisman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 2368075,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "5188b8d12e1011e67c3b63eabca2df20c78b1c7b",
            "isKey": false,
            "numCitedBy": 753,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Preattentive-processing-in-vision-Treisman",
            "title": {
                "fragments": [],
                "text": "Preattentive processing in vision"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Vis. Graph. Image Process."
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107833293"
                        ],
                        "name": "M. U. R. S\u00e1nchez",
                        "slug": "M.-U.-R.-S\u00e1nchez",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "S\u00e1nchez",
                            "middleNames": [
                                "Ulises",
                                "Ramos"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. U. R. S\u00e1nchez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145564537"
                        ],
                        "name": "Jiri Matas",
                        "slug": "Jiri-Matas",
                        "structuredName": {
                            "firstName": "Jiri",
                            "lastName": "Matas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiri Matas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145801638"
                        ],
                        "name": "J. Kittler",
                        "slug": "J.-Kittler",
                        "structuredName": {
                            "firstName": "Josef",
                            "lastName": "Kittler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kittler"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15510253,
            "fieldsOfStudy": [
                "Physics",
                "Computer Science"
            ],
            "id": "0b1fbac88dc540d7bd4cc89c080adf5b17915654",
            "isKey": false,
            "numCitedBy": 43,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "A method for lip tracking intended to support personal verification is presented in this paper. Lip contours are represented by means of quadratic B-splines. The lips are automatically localised in the original image and an elliptic B-spline is generated to start up tracking. Lip localisation exploits grey-level gradient projections as well as chromaticity models to find the lips in an automatically segmented region corresponding to the face area. Tracking proceeds by estimating new lip contour positions according to a statistical chromaticity model for the lips. The current tracker implementation follows a deterministic second order model for the spline motion based on a Lagrangian formulation of contour dynamics. The method has been tested on the M2VTS database[1]. Lips were accurately tracked on sequences consisting of more than hundred frames. localisation"
            },
            "slug": "Statistical-Chromaticity-Models-for-Lip-Tracking-S\u00e1nchez-Matas",
            "title": {
                "fragments": [],
                "text": "Statistical Chromaticity Models for Lip Tracking with B-splines"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "A method for lip tracking intended to support personal verification that exploits grey-level gradient projections as well as chromaticity models to find the lips in an automatically segmented region corresponding to the face area."
            },
            "venue": {
                "fragments": [],
                "text": "AVBPA"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1830709"
                        ],
                        "name": "A. Lanitis",
                        "slug": "A.-Lanitis",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Lanitis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Lanitis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144482985"
                        ],
                        "name": "C. Taylor",
                        "slug": "C.-Taylor",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Taylor",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7205190"
                        ],
                        "name": "Tim Cootes",
                        "slug": "Tim-Cootes",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Cootes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tim Cootes"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 82
                            }
                        ],
                        "text": "(17)), the model can approximate up to 95% of the face shapes in the training set [104]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 13
                            }
                        ],
                        "text": "Cootes et al.[24, 104] later proposed the use of a new generic flexible model which they termed s art snakes and PDM to provide an efficient interpretation of the human face."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 123037313,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0b5846eac9410de967649b7f0a45aaff0cbb0c5a",
            "isKey": false,
            "numCitedBy": 51,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "The use of flexible shape and flexible grey-level models for representing variations in the appearance of human faces is described. These models are controlled by a small number of parameters which can be used for coding and reconstructing a face image."
            },
            "slug": "Automatic-tracking,-coding-and-reconstruction-of-Lanitis-Taylor",
            "title": {
                "fragments": [],
                "text": "Automatic tracking, coding and reconstruction of human faces, using flexible appearance models"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The use of flexible shape and flexible grey-level models for representing variations in the appearance of human faces is described."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403864737"
                        ],
                        "name": "Souheil Ben-Yacoub",
                        "slug": "Souheil-Ben-Yacoub",
                        "structuredName": {
                            "firstName": "Souheil",
                            "lastName": "Ben-Yacoub",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Souheil Ben-Yacoub"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8745904"
                        ],
                        "name": "B. Fasel",
                        "slug": "B.-Fasel",
                        "structuredName": {
                            "firstName": "Beat",
                            "lastName": "Fasel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Fasel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678373"
                        ],
                        "name": "J. Luettin",
                        "slug": "J.-Luettin",
                        "structuredName": {
                            "firstName": "Juergen",
                            "lastName": "Luettin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Luettin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 196,
                                "start": 193
                            }
                        ],
                        "text": "Apart from face classification, neural networks have also been applied for facial features classification [36, 125, 151, 196] and a method for improving detection time for MLPs is presented in [6]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15177233,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f64fb2e5cfa86fe68e9c239557f440c412d02274",
            "isKey": false,
            "numCitedBy": 70,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a new technique for a faster computation of the activities of the hidden layer units. This has been demonstrated on face detection examples."
            },
            "slug": "Fast-Face-Detection-using-MLP-and-FFT-Ben-Yacoub-Fasel",
            "title": {
                "fragments": [],
                "text": "Fast Face Detection using MLP and FFT"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "A new technique for a faster computation of the activities of the hidden layer units is proposed and has been demonstrated on face detection examples."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703537"
                        ],
                        "name": "Y. Freund",
                        "slug": "Y.-Freund",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Freund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2022386739"
                        ],
                        "name": "Peter Barlett",
                        "slug": "Peter-Barlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Barlett",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter Barlett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740222"
                        ],
                        "name": "Wee Sun Lee",
                        "slug": "Wee-Sun-Lee",
                        "structuredName": {
                            "firstName": "Wee",
                            "lastName": "Lee",
                            "middleNames": [
                                "Sun"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wee Sun Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 573509,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4d19272112b50547614479a0c409fca66e3b05f7",
            "isKey": false,
            "numCitedBy": 2844,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the surprising recurring phenomena observed in experiments with boosting is that the test error of the generated classifier usually does not increase as its size becomes very large, and often is observed to decrease even after the training error reaches zero. In this paper, we show that this phenomenon is related to the distribution of margins of the training examples with respect to the generated voting classification rule, where the margin of an example is simply the difference between the number of correct votes and the maximum number of votes received by any incorrect label. We show that techniques used in the analysis of Vapnik's support vector classifiers and of neural networks with small weights can be applied to voting methods to relate the margin distribution to the test error. We also show theoretically and experimentally that boosting is especially effective at increasing the margins of the training examples. Finally, we compare our explanation to those based on the bias-variance"
            },
            "slug": "Boosting-the-margin:-A-new-explanation-for-the-of-Schapire-Freund",
            "title": {
                "fragments": [],
                "text": "Boosting the margin: A new explanation for the effectiveness of voting methods"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown that techniques used in the analysis of Vapnik's support vector classifiers and of neural networks with small weights can be applied to voting methods to relate the margin distribution to the test error."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053520352"
                        ],
                        "name": "M. Kirby",
                        "slug": "M.-Kirby",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Kirby",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kirby"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49555086"
                        ],
                        "name": "L. Sirovich",
                        "slug": "L.-Sirovich",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Sirovich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Sirovich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 570648,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "66d75a5fe9e1b6511c5135d68e9ce8c0da5a7374",
            "isKey": false,
            "numCitedBy": 2853,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "The use of natural symmetries (mirror images) in a well-defined family of patterns (human faces) is discussed within the framework of the Karhunen-Loeve expansion. This results in an extension of the data and imposes even and odd symmetry on the eigenfunctions of the covariance matrix, without increasing the complexity of the calculation. The resulting approximation of faces projected from outside of the data set onto this optimal basis is improved on average. >"
            },
            "slug": "Application-of-the-Karhunen-Loeve-Procedure-for-the-Kirby-Sirovich",
            "title": {
                "fragments": [],
                "text": "Application of the Karhunen-Loeve Procedure for the Characterization of Human Faces"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "The use of natural symmetries (mirror images) in a well-defined family of patterns (human faces) is discussed within the framework of the Karhunen-Loeve expansion, which results in an extension of the data and imposes even and odd symmetry on the eigenfunctions of the covariance matrix."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1969431741"
                        ],
                        "name": "Yeuvo Jphonen",
                        "slug": "Yeuvo-Jphonen",
                        "structuredName": {
                            "firstName": "Yeuvo",
                            "lastName": "Jphonen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yeuvo Jphonen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 221941815,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4957f5fb5b62ece2eeb0dbfee30cf3cd26b0c997",
            "isKey": false,
            "numCitedBy": 394,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nThe Self-Organizing Map (SOM) algorithm was introduced by the author in 1981. Its theory and many applications form one of the major approaches to the contemporary artificial neural networks field, and new technologies have already been based on it. The most important practical applications are in exploratory data analysis, pattern recognition, speech analysis, robotics, industrial and medical diagnostics, instrumentation, and control, and literally hundreds of other tasks. In this monograph the mathematical preliminaries, background, basic ideas, and implications are expounded in a clear, well-organized form, accessible without prior expert knowledge. Still the contents are handled with theoretical rigor."
            },
            "slug": "Self-Organizing-Maps-Jphonen",
            "title": {
                "fragments": [],
                "text": "Self-Organizing Maps"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The mathematical preliminaries, background, basic ideas, and implications are expounded in a clear, well-organized form, accessible without prior expert knowledge, and the contents are handled with theoretical rigor."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1832462"
                        ],
                        "name": "F. Crow",
                        "slug": "F.-Crow",
                        "structuredName": {
                            "firstName": "Franklin",
                            "lastName": "Crow",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Crow"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2210332,
            "fieldsOfStudy": [
                "Environmental Science"
            ],
            "id": "8b22e1751f75be137b7b210981baccc1b9ab9222",
            "isKey": false,
            "numCitedBy": 1468,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Texture-map computations can be made tractable through use of precalculated tables which allow computational costs independent of the texture density. The first example of this technique, the \u201cmip\u201d map, uses a set of tables containing successively lower-resolution representations filtered down from the discrete texture function. An alternative method using a single table of values representing the integral over the texture function rather than the function itself may yield superior results at similar cost. The necessary algorithms to support the new technique are explained. Finally, the cost and performance of the new technique is compared to previous techniques."
            },
            "slug": "Summed-area-tables-for-texture-mapping-Crow",
            "title": {
                "fragments": [],
                "text": "Summed-area tables for texture mapping"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "Texture-map computations can be made tractable through use of precalculated tables which allow computational costs independent of the texture density, and the cost and performance of the new technique is compared to previous techniques."
            },
            "venue": {
                "fragments": [],
                "text": "SIGGRAPH"
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7205190"
                        ],
                        "name": "Tim Cootes",
                        "slug": "Tim-Cootes",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Cootes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tim Cootes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144482985"
                        ],
                        "name": "C. Taylor",
                        "slug": "C.-Taylor",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Taylor",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Taylor"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 18
                            }
                        ],
                        "text": "In order to fit a PDM to a face, the mean shape model (model with labeled points= x\u0304) is first placed near the face."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 106
                            }
                        ],
                        "text": "These models ranging fromsnakes, proposed in the late 1980s, to the more recent point distributed models (PDM) have been developed for the purpose of complex and nonrigid feature extraction such as eye pupil and lip tracking."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 110
                            }
                        ],
                        "text": "Cooteset al.[24, 104] later proposed the use of a new generic flexible model which they termeds art snakesand PDM to provide an efficient interpretation of the human face."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 4
                            }
                        ],
                        "text": "PDM [24] is a compact parameterized description of the shape based upon statistics."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 13
                            }
                        ],
                        "text": "Cootes et al.[24, 104] later proposed the use of a new generic flexible model which they termed s art snakes and PDM to provide an efficient interpretation of the human face."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 44
                            }
                        ],
                        "text": "The architecture and the fitting process of PDM is different from the other active shape models."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 83
                            }
                        ],
                        "text": "Using 152 manually planted control points (x) and 160 training face images, a face PDM is obtained."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 213,
                                "start": 210
                            }
                        ],
                        "text": "The model comprises the mean of all the features in the sets and the principle modes of variation for each point (for further PCA description, see Section 4.1)\nx = x\u0304 + Pv, (17)\nwherex represents a point on the PDM,x\u0304 is the mean feature in the training set for that point, P = [ p1 p2 . . . pt ] is the matrix of thet most significant variation vectors of the covariance of deviations, andv is the weight vector for each mode."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 15
                            }
                        ],
                        "text": "The contour of PDM is discretized into a set of labeled points."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 31
                            }
                        ],
                        "text": "The advantages of using a face PDM is that it provides a compact parameterized description."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 5
                            }
                        ],
                        "text": "Face PDM was first developed by Lanitiset al. [104] as a flexible model."
                    },
                    "intents": []
                }
            ],
            "corpusId": 11058546,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "4d79f7ebf833c549dd1429fd9440628d6c0824c3",
            "isKey": true,
            "numCitedBy": 346,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe 'Active Shape Models' which iteratively adapt to refine estimates of the pose, scale and shape of models of image objects. The method uses flexible models derived from sets of training examples. These models, known as Point Distribution Models, represent objects as sets of labelled points. An initial estimate of the location of the model points in an image is improved by attempting to move each point to a better position nearby. Adjustments to the pose variables and shape parameters are calculated. Limits are placed on the shape parameters ensuring that the example can only deform into shapes conforming to global constraints imposed by the training set. An iterative procedure deforms the model example to find the best fit to the image object. Results of applying the method are described. The technique is shown to be a powerful method for refining estimates of object shape and location."
            },
            "slug": "Active-Shape-Models-'smart-snakes'-Cootes-Taylor",
            "title": {
                "fragments": [],
                "text": "Active Shape Models - 'smart snakes'"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "'Active Shape Models' are described which iteratively adapt to refine estimates of the pose, scale and shape of models of image objects to be a powerful method for refining estimates of object shape and location."
            },
            "venue": {
                "fragments": [],
                "text": "BMVC"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "121941586"
                        ],
                        "name": "C. Frankel",
                        "slug": "C.-Frankel",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Frankel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Frankel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2987811"
                        ],
                        "name": "M. Swain",
                        "slug": "M.-Swain",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Swain",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Swain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720747"
                        ],
                        "name": "V. Athitsos",
                        "slug": "V.-Athitsos",
                        "structuredName": {
                            "firstName": "Vassilis",
                            "lastName": "Athitsos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Athitsos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[158] is used as a part of an image search engine for the World Wide Web in WebSeer [45]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7811959,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "105b158b73511030ff10ac0f0f1cbee65236e4a6",
            "isKey": false,
            "numCitedBy": 378,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Because of the size of the World Wide Web and its inherent lack of structure, finding what one is looking for can be a challenge. PC-Meters March, 1996, survey found that three of the five most visited Web sites were search engines. However, while Web pages typically contain both text and images, all the currently available search engines only index text. This paper describes WebSeer, a system for locating images on the Web. WebSeer uses image content in addition to associated text to index images, presenting the user with a selection that potentially fits her needs."
            },
            "slug": "WebSeer:-An-Image-Search-Engine-for-the-World-Wide-Frankel-Swain",
            "title": {
                "fragments": [],
                "text": "WebSeer: An Image Search Engine for the World Wide Web"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "WebSeer uses image content in addition to associated text to index images, presenting the user with a selection that potentially fits her needs on the Web."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47166877"
                        ],
                        "name": "T. Hamada",
                        "slug": "T.-Hamada",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Hamada",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Hamada"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110056387"
                        ],
                        "name": "Kazunori Kato",
                        "slug": "Kazunori-Kato",
                        "structuredName": {
                            "firstName": "Kazunori",
                            "lastName": "Kato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kazunori Kato"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30637720"
                        ],
                        "name": "Kowichiro Kawakami",
                        "slug": "Kowichiro-Kawakami",
                        "structuredName": {
                            "firstName": "Kowichiro",
                            "lastName": "Kawakami",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kowichiro Kawakami"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2445720,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "098fd9d8561e62c3fde89e2ac706eee1975bfcf9",
            "isKey": false,
            "numCitedBy": 5,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Extracting-facial-features-as-in-infants-Hamada-Kato",
            "title": {
                "fragments": [],
                "text": "Extracting facial features as in infants"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit. Lett."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "12568146"
                        ],
                        "name": "D. Noton",
                        "slug": "D.-Noton",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Noton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Noton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144145222"
                        ],
                        "name": "L. Stark",
                        "slug": "L.-Stark",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Stark",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Stark"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 6907731,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "2aea0b1d01e298052220461fe4d2c2686313504c",
            "isKey": false,
            "numCitedBy": 539,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Scanpaths-in-saccadic-eye-movements-while-viewing-Noton-Stark",
            "title": {
                "fragments": [],
                "text": "Scanpaths in saccadic eye movements while viewing and recognizing patterns."
            },
            "venue": {
                "fragments": [],
                "text": "Vision research"
            },
            "year": 1971
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3331580"
                        ],
                        "name": "O. Maron",
                        "slug": "O.-Maron",
                        "structuredName": {
                            "firstName": "Oded",
                            "lastName": "Maron",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Maron"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1388700951"
                        ],
                        "name": "Tomas Lozano-Perez",
                        "slug": "Tomas-Lozano-Perez",
                        "structuredName": {
                            "firstName": "Tomas",
                            "lastName": "Lozano-Perez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tomas Lozano-Perez"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8516600,
            "fieldsOfStudy": [
                "Computer Science",
                "Education"
            ],
            "id": "4d8340eae2c98ab5e0a3b1a7e071a7ddb9106cff",
            "isKey": false,
            "numCitedBy": 1227,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "Multiple-instance learning is a variation on supervised learning, where the task is to learn a concept given positive and negative bags of instances. Each bag may contain many instances, but a bag is labeled positive even if only one of the instances in it falls within the concept. A bag is labeled negative only if all the instances in it are negative. We describe a new general framework, called Diverse Density, for solving multiple-instance learning problems. We apply this framework to learn a simple description of a person from a series of images (bags) containing that person, to a stock selection problem, and to the drug activity prediction problem."
            },
            "slug": "A-Framework-for-Multiple-Instance-Learning-Maron-Lozano-Perez",
            "title": {
                "fragments": [],
                "text": "A Framework for Multiple-Instance Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A new general framework, called Diverse Density, is described, which is applied to learn a simple description of a person from a series of images containing that person, to a stock selection problem, and to the drug activity prediction problem."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144847940"
                        ],
                        "name": "K. Lam",
                        "slug": "K.-Lam",
                        "structuredName": {
                            "firstName": "Kin-Man",
                            "lastName": "Lam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Lam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "121686163"
                        ],
                        "name": "Heng Yan",
                        "slug": "Heng-Yan",
                        "structuredName": {
                            "firstName": "Heng",
                            "lastName": "Yan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Heng Yan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121953337,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0192be2a5ae549426682ddc0632b2f31c1f5d0d7",
            "isKey": false,
            "numCitedBy": 97,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The greedy algorithm is a fast iterative method for energy minimisation of snakes for image contour detection. In the Letter, an even faster algorithm is proposed. The new algorithm has the same performance as the conventional greedy algorithm, but hte reduces the computing time by 30% on average"
            },
            "slug": "Fast-greedy-algorithm-for-active-contours-Lam-Yan",
            "title": {
                "fragments": [],
                "text": "Fast greedy algorithm for active contours"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "The greedy algorithm is a fast iterative method for energy minimisation of snakes for image contour detection and an even faster algorithm is proposed, which reduces the computing time by 30% on average."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3176394"
                        ],
                        "name": "P. Somol",
                        "slug": "P.-Somol",
                        "structuredName": {
                            "firstName": "Petr",
                            "lastName": "Somol",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Somol"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701197"
                        ],
                        "name": "P. Pudil",
                        "slug": "P.-Pudil",
                        "structuredName": {
                            "firstName": "Pavel",
                            "lastName": "Pudil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Pudil"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687265"
                        ],
                        "name": "J. Novovicov\u00e1",
                        "slug": "J.-Novovicov\u00e1",
                        "structuredName": {
                            "firstName": "Jana",
                            "lastName": "Novovicov\u00e1",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Novovicov\u00e1"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1897550"
                        ],
                        "name": "P. Pacl\u00edk",
                        "slug": "P.-Pacl\u00edk",
                        "structuredName": {
                            "firstName": "Pavel",
                            "lastName": "Pacl\u00edk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Pacl\u00edk"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13216311,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d9aaa7dcee5657b8d6e73ba89b2ff9723b45f093",
            "isKey": false,
            "numCitedBy": 341,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Adaptive-floating-search-methods-in-feature-Somol-Pudil",
            "title": {
                "fragments": [],
                "text": "Adaptive floating search methods in feature selection"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit. Lett."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703537"
                        ],
                        "name": "Y. Freund",
                        "slug": "Y.-Freund",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Freund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9621074,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c834bddd5e75a64ca9bb80c195cf84345c38bb9b",
            "isKey": false,
            "numCitedBy": 3095,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "Boosting is a general method for improving the accuracy of any given learning algorithm. This short overview paper introduces the boosting algorithm AdaBoost, and explains the underlying theory of boosting, including an explanation of why boosting often does not suffer from overfitting as well as boosting\u2019s relationship to support-vector machines. Some examples of recent applications of boosting are also described."
            },
            "slug": "A-Short-Introduction-to-Boosting-Freund-Schapire",
            "title": {
                "fragments": [],
                "text": "A Short Introduction to Boosting"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "This short overview paper introduces the boosting algorithm AdaBoost, and explains the underlying theory of boosting, including an explanation of why boosting often does not suffer from overfitting as well as boosting\u2019s relationship to support-vector machines."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701197"
                        ],
                        "name": "P. Pudil",
                        "slug": "P.-Pudil",
                        "structuredName": {
                            "firstName": "Pavel",
                            "lastName": "Pudil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Pudil"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687265"
                        ],
                        "name": "J. Novovicov\u00e1",
                        "slug": "J.-Novovicov\u00e1",
                        "structuredName": {
                            "firstName": "Jana",
                            "lastName": "Novovicov\u00e1",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Novovicov\u00e1"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145801638"
                        ],
                        "name": "J. Kittler",
                        "slug": "J.-Kittler",
                        "structuredName": {
                            "firstName": "Josef",
                            "lastName": "Kittler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kittler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12795832,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "c47e6bc5ca7c90abd7b352b34529e33213f3c8ae",
            "isKey": false,
            "numCitedBy": 3059,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Floating-search-methods-in-feature-selection-Pudil-Novovicov\u00e1",
            "title": {
                "fragments": [],
                "text": "Floating search methods in feature selection"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit. Lett."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744700"
                        ],
                        "name": "Zoubin Ghahramani",
                        "slug": "Zoubin-Ghahramani",
                        "structuredName": {
                            "firstName": "Zoubin",
                            "lastName": "Ghahramani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zoubin Ghahramani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Given a set of training images, the EM algorithm [34] is used to estimate the parameters in the mixture model of factor analyzers [49]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18270595,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "09ef86868035bbfd4803a9e1c98640804bf8f4a4",
            "isKey": false,
            "numCitedBy": 700,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Factor analysis, a statistical method for modeling the covariance structure of high dimensional data using a small number of latent variables, can be extended by allowing di erent local factor models in di erent regions of the input space. This results in a model which concurrently performs clustering and dimensionality reduction, and can be thought of as a reduced dimension mixture of Gaussians. We present an exact Expectation{Maximization algorithm for tting the parameters of this mixture of factor analyzers."
            },
            "slug": "The-EM-algorithm-for-mixtures-of-factor-analyzers-Ghahramani-Hinton",
            "title": {
                "fragments": [],
                "text": "The EM algorithm for mixtures of factor analyzers"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work presents an exact Expectation{Maximization algorithm for determining the parameters of this mixture of factor analyzers which concurrently performs clustering and dimensionality reduction, and can be thought of as a reduced dimension mixture of Gaussians."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2295441"
                        ],
                        "name": "K. Fukunaga",
                        "slug": "K.-Fukunaga",
                        "structuredName": {
                            "firstName": "Keinosuke",
                            "lastName": "Fukunaga",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Fukunaga"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "13849278"
                        ],
                        "name": "W. Koontz",
                        "slug": "W.-Koontz",
                        "structuredName": {
                            "firstName": "W.L.G.",
                            "lastName": "Koontz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Koontz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 206617934,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "6ed4a2833ae3adaebaa73d24fd5f06bc9d9a02c9",
            "isKey": false,
            "numCitedBy": 540,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "The Karhunen-Lo6ve expansion has been used previously to extract important features for representing samples taken from a given distribution. A method is developed herein to use the Karhunen-Loeve expansion to extract features relevant to classification of a sample taken from one of two pattern classes. Numerical examples are presented to illustrate the technique."
            },
            "slug": "Application-of-the-Karhunen-Lo\u00e8ve-Expansion-to-and-Fukunaga-Koontz",
            "title": {
                "fragments": [],
                "text": "Application of the Karhunen-Lo\u00e8ve Expansion to Feature Selection and Ordering"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "A method is developed herein to use the Karhunen-Loeve expansion to extract features relevant to classification of a sample taken from one of two pattern classes."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Computers"
            },
            "year": 1970
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1821709"
                        ],
                        "name": "F. Werblin",
                        "slug": "F.-Werblin",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Werblin",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Werblin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47786156"
                        ],
                        "name": "A. Jacobs",
                        "slug": "A.-Jacobs",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Jacobs",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Jacobs"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40594372"
                        ],
                        "name": "J. Teeters",
                        "slug": "J.-Teeters",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Teeters",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Teeters"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 56592801,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e2cab983b42c7accd8602a7fd297a6f65986d686",
            "isKey": false,
            "numCitedBy": 80,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Specialized cells in the eye work in parallel for unequaled image processing and computation. Millionfold swings in light intensity from the outside world, transformed to electrical signals, are processed in space and time for contrast and edge enhancement, as well as to detect motion."
            },
            "slug": "The-computational-eye-Werblin-Jacobs",
            "title": {
                "fragments": [],
                "text": "The computational eye"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "Specialized cells in the eye work in parallel for unequaled image processing and computation for contrast and edge enhancement, as well as to detect motion."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3288397"
                        ],
                        "name": "A. Kuchinsky",
                        "slug": "A.-Kuchinsky",
                        "structuredName": {
                            "firstName": "Allan",
                            "lastName": "Kuchinsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kuchinsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761795"
                        ],
                        "name": "Celine Pering",
                        "slug": "Celine-Pering",
                        "structuredName": {
                            "firstName": "Celine",
                            "lastName": "Pering",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Celine Pering"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32150776"
                        ],
                        "name": "M. Creech",
                        "slug": "M.-Creech",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Creech",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Creech"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2049273"
                        ],
                        "name": "Dennis F. Freeze",
                        "slug": "Dennis-F.-Freeze",
                        "structuredName": {
                            "firstName": "Dennis",
                            "lastName": "Freeze",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dennis F. Freeze"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8978779"
                        ],
                        "name": "B. Serra",
                        "slug": "B.-Serra",
                        "structuredName": {
                            "firstName": "Bill",
                            "lastName": "Serra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Serra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2082319"
                        ],
                        "name": "J. Gwizdka",
                        "slug": "J.-Gwizdka",
                        "structuredName": {
                            "firstName": "Jacek",
                            "lastName": "Gwizdka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Gwizdka"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 16878029,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6047b726e69ca6b05ecea9084ce2df252d86ef4d",
            "isKey": false,
            "numCitedBy": 256,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "FotoFile is an experimental system for multimediaorganization and retrieval, based upon the design goal of makingmultimedia content accessible to non-expert users. Search andretrieval are done in terms that are natural to the task. Thesystem blends human and automatic annotation methods. It extendstextual search, browsing, and retrieval technologies to supportmultimedia data types."
            },
            "slug": "FotoFile:-a-consumer-multimedia-organization-and-Kuchinsky-Pering",
            "title": {
                "fragments": [],
                "text": "FotoFile: a consumer multimedia organization and retrieval system"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "FotoFile is an experimental system for multimedia organization and retrieval, based upon the design goal of making multimedia content accessible to non-expert users that blends human and automatic annotation methods."
            },
            "venue": {
                "fragments": [],
                "text": "CHI '99"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109349738"
                        ],
                        "name": "Xiao-Bai Li",
                        "slug": "Xiao-Bai-Li",
                        "structuredName": {
                            "firstName": "Xiao-Bai",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiao-Bai Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31792775"
                        ],
                        "name": "J. R. Sweigart",
                        "slug": "J.-R.-Sweigart",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Sweigart",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. R. Sweigart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054841308"
                        ],
                        "name": "James T. C. Teng",
                        "slug": "James-T.-C.-Teng",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Teng",
                            "middleNames": [
                                "T.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James T. C. Teng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2793764"
                        ],
                        "name": "Joan M. Donohue",
                        "slug": "Joan-M.-Donohue",
                        "structuredName": {
                            "firstName": "Joan",
                            "lastName": "Donohue",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joan M. Donohue"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "116922808"
                        ],
                        "name": "L. Thombs",
                        "slug": "L.-Thombs",
                        "structuredName": {
                            "firstName": "Lori",
                            "lastName": "Thombs",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Thombs"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 9493184,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "efd2e7e0813276aa8edbaf8db3ad518114d3bec0",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper concerns a decision-tree pruning method, a key issue in the development of decision trees. We propose a new method that applies the classical optimization technique, dynamic programming, to a decision-tree pruning procedure. We show that the proposed method generates a sequence of pruned trees that are optimal with respect to tree size. The dynamic-programming-based pruning (DPP) algorithm is then compared with cost-complexity pruning (CCP) in an experimental study. The results of our study indicate that DPP performs better than CCP in terms of classification accuracy."
            },
            "slug": "A-Dynamic-Programming-Based-Pruning-Method-for-Li-Sweigart",
            "title": {
                "fragments": [],
                "text": "A Dynamic Programming Based Pruning Method for Decision Trees"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "A new method that applies the classical optimization technique, dynamic programming, to a decision-tree pruning procedure is proposed and it is shown that the proposed method generates a sequence of pruned trees that are optimal with respect to tree size."
            },
            "venue": {
                "fragments": [],
                "text": "INFORMS J. Comput."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35043531"
                        ],
                        "name": "A. Dempster",
                        "slug": "A.-Dempster",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "Dempster",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dempster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7890796"
                        ],
                        "name": "N. Laird",
                        "slug": "N.-Laird",
                        "structuredName": {
                            "firstName": "Nan",
                            "lastName": "Laird",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Laird"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2235217"
                        ],
                        "name": "D. Rubin",
                        "slug": "D.-Rubin",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Rubin",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rubin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Given a set of training images, the EM algorithm [34] is used to estimate the parameters in the mixture model of factor analyzers [49]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 4193919,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "d36efb9ad91e00faa334b549ce989bfae7e2907a",
            "isKey": false,
            "numCitedBy": 48406,
            "numCiting": 134,
            "paperAbstract": {
                "fragments": [],
                "text": "Vibratory power unit for vibrating conveyers and screens comprising an asynchronous polyphase motor, at least one pair of associated unbalanced masses disposed on the shaft of said motor, with the first mass of a pair of said unbalanced masses being rigidly fastened to said shaft and with said second mass of said pair being movably arranged relative to said first mass, means for controlling and regulating the conveying rate during conveyer operation by varying the rotational speed of said motor between predetermined minimum and maximum values, said second mass being movably outwardly by centrifugal force against the pressure of spring means, said spring means being prestressed in such a manner that said second mass is, at rotational motor speeds lower than said minimum speed, held in its initial position, and at motor speeds between said lower and upper values in positions which are radially offset with respect to the axis of said motor to an extent depending on the value of said rotational motor speed."
            },
            "slug": "Maximum-likelihood-from-incomplete-data-via-the-EM-Dempster-Laird",
            "title": {
                "fragments": [],
                "text": "Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7138354,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8213dbed4db44e113af3ed17d6dad57471a0c048",
            "isKey": false,
            "numCitedBy": 38757,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "Setting of the learning problem consistency of learning processes bounds on the rate of convergence of learning processes controlling the generalization ability of learning processes constructing learning algorithms what is important in learning theory?."
            },
            "slug": "The-Nature-of-Statistical-Learning-Theory-Vapnik",
            "title": {
                "fragments": [],
                "text": "The Nature of Statistical Learning Theory"
            },
            "venue": {
                "fragments": [],
                "text": "Statistics for Engineering and Information Science"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 29
                            }
                        ],
                        "text": "For instance, Gunn and Nixon [55] make the term sensitive to the image gradient so that the contour is convergent toward edge locations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 15
                            }
                        ],
                        "text": "Elastic energy [55, 69, 209, 219] is used commonly as internal energy."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 0
                            }
                        ],
                        "text": "Gunn and Nixon addressed these problems by introducing a parameterized snake model for face and head boundary extraction."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 72
                            }
                        ],
                        "text": "Active contours, or snakes, are commonly used to locate a head boundary [55, 69, 102, 137, 209, 219]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 18
                            }
                        ],
                        "text": "D. E. Benn, M. S. Nixon, and J. N. Carter, Extending concentricity analysis by deformable templates for improved eye extraction, inProceedings Second International Conference on Audio- and Video-based Biometric Person Authentication (AVBPA), 1999."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 21
                            }
                        ],
                        "text": "S. R. Gunn and M. S. Nixon, A dual active contour for head and boundary extraction, inIEE Colloquium on Image Processing for Biometric Measurement, London, Apr. 1994, pp. 6/1."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A dual active contour for head and boundary extraction, in  IEE Colloquium on Image Processing for Biometric Measurement, London, Apr"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 91
                            }
                        ],
                        "text": "Among the literature survey, a pair of eyes is the most commonly applied reference feature [5, 27, 52, 61, 207, 214] due to its distinct side-by-side appearance."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 83
                            }
                        ],
                        "text": "Based on an earlier work of maximum likelihood face detection [21], Colmenarez and Huang [22] proposed a system based on Kullback relative information (Kullback divergence)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 17
                            }
                        ],
                        "text": "The technique of Yang and Huang was recently incorporated into a system for rotation invariant face detection by Lvet al. [119] and an extension of the algorithm is presented in Kotropoulos and Pitas [95]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 627,
                                "start": 622
                            }
                        ],
                        "text": "The number of labeled faces was originally 149 by Sung and Poggio (which is the number used in the reported results in [182] and [107], but it was changed to 155 (which\nTABLE 2 Results Reported in Terms of Percentage Correct Detection (CD) and Number of False Positives (FP), CD/FP, on the CMU and MIT Datasets\nFace detection system CMU-130 CMU-125 MIT-23 MIT-20\nSchneiderman & Kanade\u2014Ea [170] 94.4%/65 Schneiderman & Kanade\u2014Wb [170] 90.2%/110 Yanget al.\u2014FA [217] 92.3%/82 89.4%/3 Yanget al.\u2014LDA [217] 93.6%/74 91.5%/1 Rothet al. [157] 94.8%/78 94.1%/3 Rowleyet al. [158] 86.2%/23 84.5%/8 Feraudet al. [42] 86%/8 Colmenarez & Huang [22] 93.9%/8122 Sung & Poggio [182] 79.9%/5 Lew & Huijsmans [107] 94.1%/64 Osunaet al. [140] 74.2%/20 Lin et al. [113] 72.3%/6 Gu and Li [54] 87.1%/0\na Eigenvector coefficients. b Wavelet coefficients.\nis the number used in the reported results in [158], [140], and [113]) when included in the CMU dataset."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 27
                            }
                        ],
                        "text": "A. J. Colmenarez and T. S. Huang, Pattern detection with information-based maximum discrimination and error bootstrapping, inProc. of International Conference on Pattern Recognition, 1998."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 18
                            }
                        ],
                        "text": "G. Yang and T. S. Huang, Human face detection in a complex background,Pattern Recog.27, 1994, 53\u201363."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 15
                            }
                        ],
                        "text": "R. Qian and T. Huang, Object detection using hierarchical mrf and map estimation, inIEEE Proc. of Int."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "Huanget al.[74] also apply a Gaussian filter for pre-processing in a framework based on image feature analysis."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 32
                            }
                        ],
                        "text": "J. P. Phillips, H. Wechsler, J. Huang, and P. Rauss, The FERET database and evaluation procedure for face-recognition algorithms,Image Vision Comput."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 24
                            }
                        ],
                        "text": "In [23], Colmenarez and Huang further improved on this technique by including error bootstrapping (described in Section 4.2), and in [20] the technique was incorporated in a real-time face tracking system."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 27
                            }
                        ],
                        "text": "A. J. Colmenarez and T. S. Huang, Maximum likelihood face detection, inIEEE Proc. of 2nd Int."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 3
                            }
                        ],
                        "text": "J. Huang, S. Gutta, and H. Wechsler, Detection of human faces using decision trees, inIEEE Proc. of 2nd Int."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 37
                            }
                        ],
                        "text": "A. J. Colmenarez, B. Frey, and T. S. Huang, Detection and tracking of faces and facial features, inProceedings International Conference on Image Processing, 1999."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 15
                            }
                        ],
                        "text": "Yang and Huang [214], on the other hand, explore the gray-scale behavior of faces in mosaic (pyramid) images."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 3
                            }
                        ],
                        "text": "J. Huang and H. Wechsler, Eye detection using optimal wavelet packets and radial basis functions (rbfs),Int."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 71
                            }
                        ],
                        "text": "Due to the high computational requirement in the minimization process, Huang and Chen [69] and Lam and Yan [101] both employ fast iteration methods (greedy algorithms) for faster convergence."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 3
                            }
                        ],
                        "text": "J. Huang and H. Wechsler, Visual routines for eye location using learning and evolution, inIEEE Transactions on Evolutionary Computation, 1999."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 15
                            }
                        ],
                        "text": "Colmenarez and Huang use a large set of 11\u00d7 11 images of faces and nonfaces for training, and the training procedure results in a set of look-up tables with likelihood ratios."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 3
                            }
                        ],
                        "text": "W. Huang, Q. Sun, C. P. Lam, and J. K. Wu, A robust approach to face and eyes detection from images with cluttered background, inProc. of International Conference on Pattern Recognition, 1998."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 6
                            }
                        ],
                        "text": "C. L. Huang and C. W. Chen, Human facial feature extraction for face interpretation and recognition,Pat er Recog.25, 1992, 1435\u20131444."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 3
                            }
                        ],
                        "text": "W. Huang and R. Mariani, Face detection and precise eyes location, inProceedings of the 15th International Conference on Pattern Recognition, 2000, Vol. IV, p. 3B.\n74."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 17
                            }
                        ],
                        "text": "Q. B. Sun, W. M. Huang, and J. K. Wu, Face detection based on color and local symmetry information, in IEEE Proc. of 3rd Int."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 73
                            }
                        ],
                        "text": "Bayes\u2019 decision rule has also been applied for face detection by Qian and Huang [148]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 27
                            }
                        ],
                        "text": "A. J. Colmenarez and T. S. Huang, Face detection with information-based maximum discrimination, inIEEE Proc. of Int."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Human face detection in a complex background, Pattern Recog"
            },
            "venue": {
                "fragments": [],
                "text": "Human face detection in a complex background, Pattern Recog"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 17
                            }
                        ],
                        "text": "K. M. Lam and H. Yan, Facial feature location and extraction for computerised human face recognition, in Int."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 17
                            }
                        ],
                        "text": "The technique of Yang and Huang was recently incorporated into a system for rotation invariant face detection by Lvet al. [119] and an extension of the algorithm is presented in Kotropoulos and Pitas [95]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "Yanget al. [217] make this decision based on the rule \u201c . . .a detected face is a successful detect if the subimage contains eyes and mouth.\u201d"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 3
                            }
                        ],
                        "text": "L. Yang, Multiple-face tracking system for general region-of-interest video coding, inProceedings of the 2000 International Conference on Image Processing, 2000, p. MA09."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 445,
                                "start": 442
                            }
                        ],
                        "text": "The number of labeled faces was originally 149 by Sung and Poggio (which is the number used in the reported results in [182] and [107], but it was changed to 155 (which\nTABLE 2 Results Reported in Terms of Percentage Correct Detection (CD) and Number of False Positives (FP), CD/FP, on the CMU and MIT Datasets\nFace detection system CMU-130 CMU-125 MIT-23 MIT-20\nSchneiderman & Kanade\u2014Ea [170] 94.4%/65 Schneiderman & Kanade\u2014Wb [170] 90.2%/110 Yanget al.\u2014FA [217] 92.3%/82 89.4%/3 Yanget al.\u2014LDA [217] 93.6%/74 91.5%/1 Rothet al. [157] 94.8%/78 94.1%/3 Rowleyet al. [158] 86.2%/23 84.5%/8 Feraudet al. [42] 86%/8 Colmenarez & Huang [22] 93.9%/8122 Sung & Poggio [182] 79.9%/5 Lew & Huijsmans [107] 94.1%/64 Osunaet al. [140] 74.2%/20 Lin et al. [113] 72.3%/6 Gu and Li [54] 87.1%/0\na Eigenvector coefficients. b Wavelet coefficients.\nis the number used in the reported results in [158], [140], and [113]) when included in the CMU dataset."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 3
                            }
                        ],
                        "text": "G. Yang and T. S. Huang, Human face detection in a complex background,Pattern Recog.27, 1994, 53\u201363."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 36
                            }
                        ],
                        "text": "For instance, Oliveret al.[139] and Yang and Waibel [215] employ a Gaussian distribution to represent a skin color cluster of thousands of skin color samples taken from difference races."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 17
                            }
                        ],
                        "text": "K. M. Lam and H. Yan, Fast greedy algorithm for locating head boundaries,El ctron."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 73
                            }
                        ],
                        "text": "Examples of such a learning approach have been used by Oliver et al. and Yang and Waibel according to Eq."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 3
                            }
                        ],
                        "text": "J. Yang and A. Waibel, A real-time face tracker, inIEEE Proc. of the 3rd Workshop on Applications of Computer Vision, Florida, 1996."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 22
                            }
                        ],
                        "text": "In the second method, Yanget al.use Kohonen\u2019s self-organizing map (SOM) [93] to divide the training images into 25 face and 25 nonface classes (the number of classes chosen based on the size of the training set)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "Yang and Huang [214], on the other hand, explore the gray-scale behavior of faces in mosaic (pyramid) images."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 17
                            }
                        ],
                        "text": "K. M. Lam and H. Yan, Locating and extracting the eye in human face images,Pattern Recog.29, 1996, 771\u2013779."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 16
                            }
                        ],
                        "text": "T. Kondo and H. Yan, Automatic human face detection and recognition under nonuniform illumination, Pattern Recog.32, 1999, 1707\u20131718."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 107
                            }
                        ],
                        "text": "Due to the high computational requirement in the minimization process, Huang and Chen [69] and Lam and Yan [101] both employ fast iteration methods (greedy algorithms) for faster convergence."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 15
                            }
                        ],
                        "text": "D. Roth, M.-H. Yang, and N. Ahuja, A SNoW-based face detector, inAdvances in Neural Information Processing Systems 12 (NIPS 12), MIT Press, Cambridge, MA, 2000."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 6
                            }
                        ],
                        "text": "M.-H. Yang, N. Ahuja, and D. Kriegman, Face detection using mixtures of linear subspaces, inProceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition, 2000."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 27
                            }
                        ],
                        "text": "Based on this observation, Yang proposed a hierarchical face detection framework."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 22
                            }
                        ],
                        "text": "A. R. Mirhosseini, H. Yan, K.-M. Lam, and T. Pham, Human face image recognition: An evidence aggregation approach,Computer Vision and Image Understanding71, 1998, doi:10.1006/cviu.1998.0710."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "Yanget al.use a mixture of factor analyzers in their first method."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 28
                            }
                        ],
                        "text": "(23)\nBoth of the methods of Yanget al. use the window scanning technique with a 20\u00d7 20 window and scan the input images for 10 iterations with a subsampling factor of 1.2."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 10
                            }
                        ],
                        "text": "J. Hu, H. Yan, and M. Sakalli, Locating head and face boundaries for headshoulder images,Pattern Recog."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 30
                            }
                        ],
                        "text": "In a more recent development, Lam and Yan [102] make use of eye corner information to estimate the initial parameters of the eye template."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "Yanget al. [217] proposed two methods for face detection which also seek to represent the manifold of human faces as a set of subclasses."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Fast greedy algorithm for locating head boundaries,  El ctron"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 6
                            }
                        ],
                        "text": "B. K. Low,Computer Extraction of Human Faces,PhD thesis, Dept. of Electronic and Electrical Engineering, De Montfort University, 1998."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 6
                            }
                        ],
                        "text": "B. K. Low and M. K. Ibrahim, A fast and accurate algorithm for facial feature segmentation, inProceedings International Conference on Image Processing, 1997."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 62
                            }
                        ],
                        "text": "More recent examples of edge-based techniques can be found in [9, 16, 59, 115, 116] for facial feature extraction and in [32, 50, 58, 60, 70, 76, 108, 201, 222] for face detection."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 280,
                                "start": 277
                            }
                        ],
                        "text": "Computer Vision and Image Understanding83,236\u2013274 (2001) doi:10.1006/cviu.2001.0921, available online at http://www.idealibrary.com on\nFace Detection: A Survey\nErik Hjelma\u030as1\nDepartment of Informatics, University of Oslo, P.O. Box 1080 Blindern, N-0316 Oslo, Norway\nand\nBoon Kee Low\nDepartment of Meteorology, University of Edinburgh, JCMB, Kings Buildings, Mayfield Road, Edinburgh EH9 3JZ, Scotland, United Kingdom\nE-mail: erikh@hig.no; boon@met.ed.ac.uk\nReceived October 23, 2000; accepted April 17, 2001\nIn this paper we present a comprehensive and critical survey of face detection algorithms."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 79
                            }
                        ],
                        "text": "Besides face region, Luthon and Lievin [118], Crowley and Berard [27], and Low [115, 116] also employ frame difference to locate facial features."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 26
                            }
                        ],
                        "text": "L. Sirovich and M. Kirby, Low-dimensional procedure for the characterization of human faces,J. Opt."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "Low-Level Analysis\n3.1.1."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Extraction of Human Faces"
            },
            "venue": {
                "fragments": [],
                "text": "PhD thesis, Dept. of Electronic and Electrical Engineering,"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 0
                            }
                        ],
                        "text": "Maio and Maltoni report correct detection in 69 out of 70 test images with no false alarms, where the test images consist of single faces of varying sizes with complex backgrounds."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 46
                            }
                        ],
                        "text": "The face detection system of Maio and Maltoni [120]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 15
                            }
                        ],
                        "text": "D. Maio and D. Maltoni, Real-time face location on gray-scale static images,Pattern Recog.33, 2000, 1525\u20131539."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 34
                            }
                        ],
                        "text": "In the system of Maio and Maltoni [120], the input images are converted to a directional image using a gradient-type operator over local windows (7 \u00d7 7 pixels) (see also Fr \u0308 oba and K\u00fcblebeck [47])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 17
                            }
                        ],
                        "text": "In the system of Maio and Maltoni [120], the input images are converted to a directional image using a gradient-type operator over local windows (7\u00d7 7 pixels) (see also Fr\u00a8oba and Ku\u0308blebeck [47])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 10
                            }
                        ],
                        "text": "4, Dr. D. Maltoni for providing Fig."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 97
                            }
                        ],
                        "text": "Out of the feature-based approaches which perform on grayscale static images, Maio and Maltoni\u2019s [120] algorithm seems very promising, showing good detection results while still being computationally efficient."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Real-time face location on gray-scale static images,  Pattern Recog.33"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 91
                            }
                        ],
                        "text": "Among the literature survey, a pair of eyes is the most commonly applied reference feature [5, 27, 52, 61, 207, 214] due to its distinct side-by-side appearance."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 200,
                                "start": 145
                            }
                        ],
                        "text": "Since the main variation in skin appearance is largely due to luminance change (brightness) [215], normalized RGB colors are generally preferred [27, 53, 75, 88, 92, 165, 181, 196, 202, 207, 213, 215], so that the effect of luminance can be filtered out."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "[207] implement a robot that also looks for dark facial regions within face candidates obtained indirectly from color analysis."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 117
                            }
                        ],
                        "text": "Other features include a main face axis [26, 165], outline (top of the head) [26, 32, 162] and body (below the head) [192, 207]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 143
                            }
                        ],
                        "text": "Color segmentation can basically be performed using appropriate skin color thresholds where skin color is modeled through histograms or charts [13, 63, 88, 118, 178, 207, 220]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A mobile robot that recognises people"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Int. Conf. on Tools with Artificial Intelligence"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 121
                            }
                        ],
                        "text": "More recent examples of edge-based techniques can be found in [9, 16, 59, 115, 116] for facial feature extraction and in [32, 50, 58, 60, 70, 76, 108, 201, 222] for face detection."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 5
                            }
                        ],
                        "text": "SAVI [60], LAFTER [139], Rits Eye [212], and the system of Morimoto and Flickner [135]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 57
                            }
                        ],
                        "text": "Other color models applied to face detection include HSV [48, 60, 81, 216], YES [160], YCrCb [2, 48, 84, 130, 191, 200], YUV [1, 123], CIE-xyz [15], L\u2217 a\u2217 b\u2217 [13, 109], L\u2217 u\u2217 v \u2217 [63], CSN (a modified rg representation) [90, 91] and UCS/Farnsworth (a perceptually uniform color system was proposed by Farnsworth [210]) [208]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 69
                            }
                        ],
                        "text": "Approaches which focus on single camera control include LISTEN [19],\nSAVI [60], LAFTER [139], Rits Eye [212], and the system of Morimoto and Flickner [135]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 126
                            }
                        ],
                        "text": "In their study they compare normalized TSL (tint-saturation-luminance [1881]), rg and CIE-xy chrominance spaces, and CIE-DSH, HSV, YIQ, YES, CIE-L \u2217 u \u2217 v \u2217 , and CIE L\u2217 a \u2217 b \u2217 chrominance spaces by modeling skin color distributions with either a single Gaussian or a Gaussian mixture density model in each space."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Detection and tracking of faces in real environments, in  Proc"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE International Workshop on Recognition, Analysis, and Tracking of Faces and Gestures in Real-Time Systems,"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 121
                            }
                        ],
                        "text": "More recent examples of edge-based techniques can be found in [9, 16, 59, 115, 116] for facial feature extraction and in [32, 50, 58, 60, 70, 76, 108, 201, 222] for face detection."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[32] is a good example of feature searching."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 132
                            }
                        ],
                        "text": "The initial shape of the template is obtained by using anthropometric length with respect to the reference length (given in Table 1 [32]), obtained from the modeling of 42 frontal faces in a database."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 83
                            }
                        ],
                        "text": "The Sobel operator was the most common filter among the techniques mentioned above [9, 16, 32, 76, 87, 108]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 77
                            }
                        ],
                        "text": "Other features include a main face axis [26, 165], outline (top of the head) [26, 32, 162] and body (below the head) [192, 207]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Detection and tracking of facial features by using a facial feature model and deformable circular template"
            },
            "venue": {
                "fragments": [],
                "text": "IEICE Trans. Inform. Systems  E78\u2013D(9),"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 17
                            }
                        ],
                        "text": "F. Luthon and M. Lievin, Lip motion automatic detection, inScandinavian Conference on Image Analysis, Lappeenranta, Finland, 1997."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 143
                            }
                        ],
                        "text": "Color segmentation can basically be performed using appropriate skin color thresholds where skin color is modeled through histograms or charts [13, 63, 88, 118, 178, 207, 220]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 135
                            }
                        ],
                        "text": "Since the representation strongly relates to human perception of color [106, 178], it is also widely used in face segmentation schemes [51, 83, 118, 125, 175, 178, 187, 220]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 39
                            }
                        ],
                        "text": "Besides face region, Luthon and Lievin [118], Crowley and Berard [27], and Low [115, 116] also employ frame difference to locate facial features."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Lip motion automatic detection, in Scandinavian"
            },
            "venue": {
                "fragments": [],
                "text": "Conference on Image Analysis, Lappeenranta,"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 18
                            }
                        ],
                        "text": "A Samal and P. A. Iyengar, Automatic recognition and analysis of human faces and facial expressions: a survey,Pattern Recog.25, 1992, 65\u201377."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 18
                            }
                        ],
                        "text": "Samal and Iyengar [164] proposed a PCA face detection scheme based on face silhouettes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 19
                            }
                        ],
                        "text": "A. Samal and P. A. Iyengar, Human face detection using silhouttes,In ."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Human face detection using silhouttes"
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Pattern Recog. Artificial Intell"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 139
                            }
                        ],
                        "text": "Compared to frame difference, results generated from moving contours are always more reliable, especially when the motion is insignificant [126]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "[126] to detect moving boundaries of faces and human bodies."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 176
                            }
                        ],
                        "text": "The process involves convolution of gray image I (x, y) with the second order temporal edge operator m(x, y, t) which is defined from the Gaussian filter G(x, y, t) as follows [126],"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Real-time tracking for an integrated face recognition system, in 2nd Workshop on Parallel Modelling of Neural Operators"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 62
                            }
                        ],
                        "text": "(6) which updates the parameters of the Gaussian distribution [139] (a similar approach can be found in the face recognition"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 55
                            }
                        ],
                        "text": "Examples of such a learning approach have been used by Oliver et al. and Yang and Waibel according to Eq."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 18
                            }
                        ],
                        "text": "SAVI [60], LAFTER [139], Rits Eye [212], and the system of Morimoto and Flickner [135]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 8
                            }
                        ],
                        "text": "B\u00b4erard, LAFTER: A real-time face and lips tracker with facial expression recognition,Pattern Recog.33, 2000, 1369\u20131382."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 80
                            }
                        ],
                        "text": "Approaches which focus on single camera control include LISTEN [19],\nSAVI [60], LAFTER [139], Rits Eye [212], and the system of Morimoto and Flickner [135]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 27
                            }
                        ],
                        "text": "For instance, Oliver et al.[139] and Yang and Waibel [215] employ a Gaussian distribution to represent a skin color cluster of thousands of skin color samples taken from difference races."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 108
                            }
                        ],
                        "text": "More complex methods make use of statistical measures that model face variation within a wide user spectrum [3, 27, 75, 125, 139, 215]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "erard, LAFTER: A real-time face and lips tracker with facial expression recognition,Pattern Recog"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 106
                            }
                        ],
                        "text": "Apart from face classification, neural networks have also been applied for facial features classification [36, 125, 151, 196] and a method for improving detection time for MLPs is presented in [6]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 108
                            }
                        ],
                        "text": "More complex methods make use of statistical measures that model face variation within a wide user spectrum [3, 27, 75, 125, 139, 215]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 142
                            }
                        ],
                        "text": "It was found that different human skin color gives rise to a tight cluster in color spaces even when faces of difference races are considered [75, 125, 215]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 135
                            }
                        ],
                        "text": "Since the representation strongly relates to human perception of color [106, 178], it is also widely used in face segmentation schemes [51, 83, 118, 125, 175, 178, 187, 220]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Face tracking and pose representation, in  British Machine"
            },
            "venue": {
                "fragments": [],
                "text": "Vision Conference,"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46599154"
                        ],
                        "name": "T. Sakai",
                        "slug": "T.-Sakai",
                        "structuredName": {
                            "firstName": "Toshiyuki",
                            "lastName": "Sakai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sakai"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 77
                            }
                        ],
                        "text": "Other features include a main face axis [26, 165], outline (top of the head) [26, 32, 162] and body (below the head) [192, 207]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 78
                            }
                        ],
                        "text": "For instance, a Laplacian of large scale was used to obtain a line drawing in [162] and steerable and multiscale-orientation filters in [59] and [58], respectively."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 141
                            }
                        ],
                        "text": "Early efforts in face detection have dated back as early as the beginning of the 1970s, where simple heuristic and anthropometric techniques [162] were used."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 172431690,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "0fb70e696256866e3a28598bcd640ef1be9db500",
            "isKey": false,
            "numCitedBy": 156,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Computer-analysis-and-classification-of-photographs-Sakai",
            "title": {
                "fragments": [],
                "text": "Computer analysis and classification of photographs of human faces"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728478"
                        ],
                        "name": "D. Norman",
                        "slug": "D.-Norman",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Norman",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Norman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 143879819,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8dc2e471cc9994f712fea6b752935aee9dff477b",
            "isKey": false,
            "numCitedBy": 819,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Explorations-in-Cognition-Norman-Rumelhart",
            "title": {
                "fragments": [],
                "text": "Explorations in Cognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1975
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054024193"
                        ],
                        "name": "B. K. Low",
                        "slug": "B.-K.-Low",
                        "structuredName": {
                            "firstName": "Boon",
                            "lastName": "Low",
                            "middleNames": [
                                "Kee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. K. Low"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "More recent examples of edge-based techniques can be found in [9, 16, 59, 115, 116] for facial feature extraction and in [32, 50, 58, 60, 70, 76, 108, 201, 222] for face detection."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Besides face region, Luthon and Lievin [118], Crowley and Berard [27], and Low [115, 116] also employ frame difference to locate facial features."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 142287373,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "3bfa00e1ef2abbb996fd3c1fbbfdc3c3cf9b206d",
            "isKey": false,
            "numCitedBy": 2,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Computer-extraction-of-human-faces-Low",
            "title": {
                "fragments": [],
                "text": "Computer extraction of human faces"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4976431"
                        ],
                        "name": "L. Farkas",
                        "slug": "L.-Farkas",
                        "structuredName": {
                            "firstName": "Leslie",
                            "lastName": "Farkas",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Farkas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144478034"
                        ],
                        "name": "I. Munro",
                        "slug": "I.-Munro",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Munro",
                            "middleNames": [
                                "S.",
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Munro"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Govindaraju [50] accomplishes this by labeling edges as the left side, hairline, or right side of a front view face and matches these edges against a face model by using the golden ratio3 [40] for an ideal face:"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 70478435,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "621708b14595f51678dca4215b1fb28c5d946ca1",
            "isKey": false,
            "numCitedBy": 229,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Anthropometric-Facial-Proportions-in-Medicine-Farkas-Munro",
            "title": {
                "fragments": [],
                "text": "Anthropometric Facial Proportions in Medicine"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733113"
                        ],
                        "name": "T. Kanade",
                        "slug": "T.-Kanade",
                        "structuredName": {
                            "firstName": "Takeo",
                            "lastName": "Kanade",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanade"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62020702,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "52f869ade6ac2de7b86648f4cb72b5b3bb9862ac",
            "isKey": false,
            "numCitedBy": 414,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Picture-Processing-System-by-Computer-Complex-and-Kanade",
            "title": {
                "fragments": [],
                "text": "Picture Processing System by Computer Complex and Recognition of Human Faces"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145801638"
                        ],
                        "name": "J. Kittler",
                        "slug": "J.-Kittler",
                        "structuredName": {
                            "firstName": "Josef",
                            "lastName": "Kittler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kittler"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 208933412,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "39188e05b9394369788ad5bd8f9cbe451ff30c28",
            "isKey": false,
            "numCitedBy": 375,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Feature-Set-Search-Alborithms-Kittler",
            "title": {
                "fragments": [],
                "text": "Feature Set Search Alborithms"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2114755201"
                        ],
                        "name": "Michael D. Kelly",
                        "slug": "Michael-D.-Kelly",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Kelly",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael D. Kelly"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61019110,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ccee2d0459b7eed40796d65d79b900abe638748a",
            "isKey": false,
            "numCitedBy": 223,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Visual-identification-of-people-by-computer-Kelly",
            "title": {
                "fragments": [],
                "text": "Visual identification of people by computer"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1970
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "12047785"
                        ],
                        "name": "M. Propp",
                        "slug": "M.-Propp",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Propp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Propp"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144013393"
                        ],
                        "name": "A. Samal",
                        "slug": "A.-Samal",
                        "structuredName": {
                            "firstName": "Ashok",
                            "lastName": "Samal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Samal"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 172
                            }
                        ],
                        "text": "Apart from face classification, neural networks have also been applied for facial features classification [36, 125, 151, 196] and a method for improving detection time for MLPs is presented in [6]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 65
                            }
                        ],
                        "text": "The first neural approaches to face detection were based on MLPs [10, 82, 147], where promising results where reported on fairly simple datasets."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 59630685,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b0a39564e02ccd0fc15aa9756c4324639d5ec301",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Artificial-Neural-Network-architectures-for-human-Propp-Samal",
            "title": {
                "fragments": [],
                "text": "Artificial Neural Network architectures for human face detection"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[140], a support vector machine (SVM) [194] is applied to face detection."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 59752996,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "5451278e1a11cf3f1be28a05f38d36c8641e68f7",
            "isKey": false,
            "numCitedBy": 4580,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Nature-of-Statistical-Learning-Vapnik",
            "title": {
                "fragments": [],
                "text": "The Nature of Statistical Learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "corpusId": 30523165,
            "fieldsOfStudy": [],
            "id": "6f01963039d0a921b1930b4563d1601ff36b971f",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Face Recognition by Elastic Bunch Graph Matching"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153903103"
                        ],
                        "name": "C. Lin",
                        "slug": "C.-Lin",
                        "structuredName": {
                            "firstName": "Cheng",
                            "lastName": "Lin",
                            "middleNames": [
                                "Chung"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2262254"
                        ],
                        "name": "Wei-Chung Lin",
                        "slug": "Wei-Chung-Lin",
                        "structuredName": {
                            "firstName": "Wei-Chung",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei-Chung Lin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 31656339,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dcf049c58cb45f44ac30b32694518cdef56b5a8b",
            "isKey": false,
            "numCitedBy": 30,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "The extraction of facial features is a fundamental and crucial step in most face detection and recognition systems. Here a set of approaches are proposed for extracting internal and external facial features in a grey-level images containing single or multiple faces of various sizes at different locations without any restriction on the background. These approaches are distinctive in several aspects: the use of some rarely exploited photometric properties as the basis for facial features extraction, a novel metrics on radial symmetry of gradient orientations, an inhibitory mechanism for extracting internal facial features, and a simple mechanism for external ones."
            },
            "slug": "Extracting-facial-features-by-an-inhibitory-based-Lin-Lin",
            "title": {
                "fragments": [],
                "text": "Extracting facial features by an inhibitory mechanism based on gradient distributions"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A set of approaches are proposed for extracting internal and external facial features in a grey-level images containing single or multiple faces of various sizes at different locations without any restriction on the background."
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145710228"
                        ],
                        "name": "L. D. Silva",
                        "slug": "L.-D.-Silva",
                        "structuredName": {
                            "firstName": "Liyanage",
                            "lastName": "Silva",
                            "middleNames": [
                                "C.",
                                "De"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. D. Silva"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712839"
                        ],
                        "name": "K. Aizawa",
                        "slug": "K.-Aizawa",
                        "structuredName": {
                            "firstName": "Kiyoharu",
                            "lastName": "Aizawa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Aizawa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145748260"
                        ],
                        "name": "M. Hatori",
                        "slug": "M.-Hatori",
                        "structuredName": {
                            "firstName": "Mitsutoshi",
                            "lastName": "Hatori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hatori"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 40941519,
            "fieldsOfStudy": [
                "Environmental Science"
            ],
            "id": "59a327c1ada255ec1d1750eaa081f8703d328bda",
            "isKey": false,
            "numCitedBy": 49,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Detection-and-Tracking-of-Facial-Features-by-Using-Silva-Aizawa",
            "title": {
                "fragments": [],
                "text": "Detection and Tracking of Facial Features by Using Edge Pixel Counting and Deformable Circular Template Matching"
            },
            "venue": {
                "fragments": [],
                "text": "IEICE Trans. Inf. Syst."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49353544"
                        ],
                        "name": "R. Fisher",
                        "slug": "R.-Fisher",
                        "structuredName": {
                            "firstName": "Rory",
                            "lastName": "Fisher",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fisher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 118
                            }
                        ],
                        "text": "The eigenvector in the LDA transformation matrix with the largest eigenvalue is known as Fisher\u2019s linear discriminant [43], which by itself has also been used for face detection [179, 203]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 126
                            }
                        ],
                        "text": "The DFFS measure has also been used for facial feature detection in [33] and in combination with Fisher\u2019s linear discriminant [43] for face and facial feature detection [173]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 29084021,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "ab21376e43ac90a4eafd14f0f02a0c87502b6bbf",
            "isKey": false,
            "numCitedBy": 13266,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "THE-USE-OF-MULTIPLE-MEASUREMENTS-IN-TAXONOMIC-Fisher",
            "title": {
                "fragments": [],
                "text": "THE USE OF MULTIPLE MEASUREMENTS IN TAXONOMIC PROBLEMS"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1936
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47122463"
                        ],
                        "name": "Hiroaki Bessho",
                        "slug": "Hiroaki-Bessho",
                        "structuredName": {
                            "firstName": "Hiroaki",
                            "lastName": "Bessho",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hiroaki Bessho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805337"
                        ],
                        "name": "Y. Iwai",
                        "slug": "Y.-Iwai",
                        "structuredName": {
                            "firstName": "Yoshio",
                            "lastName": "Iwai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Iwai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735941"
                        ],
                        "name": "M. Yachida",
                        "slug": "M.-Yachida",
                        "structuredName": {
                            "firstName": "Masahiko",
                            "lastName": "Yachida",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Yachida"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 35048586,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "36c6f664df9bf81788efbfb0d4eb80890b8ed5b1",
            "isKey": false,
            "numCitedBy": 6,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper deals with the detection of human faces in an image and recognition of their facial expressions using the potential net. Previously, we proposed a method to recognize facial expression using the potential net and KL expansion. In this method, it was necessary to assign a face area manually. This manual process will affect the application of recognition of facial expression to man-machine interaction. In this paper, we propose a method to automatically detect human faces by pattern matching using the potential net and to recognize facial expressions of the detected face area."
            },
            "slug": "Detecting-human-face-and-recognizing-facial-using-Bessho-Iwai",
            "title": {
                "fragments": [],
                "text": "Detecting human face and recognizing facial expressions using potential net"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "A method to automatically detect human faces by pattern matching using the potential net and to recognize facial expressions of the detected face area is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 15th International Conference on Pattern Recognition. ICPR-2000"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3108177"
                        ],
                        "name": "P. Hart",
                        "slug": "P.-Hart",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Hart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35106875"
                        ],
                        "name": "R. Duda",
                        "slug": "R.-Duda",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Duda",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Duda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2586918"
                        ],
                        "name": "D. Stork",
                        "slug": "D.-Stork",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Stork",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Stork"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 196008710,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "78053512af13466c569e5946acfc3953bbfc9d36",
            "isKey": false,
            "numCitedBy": 18023,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Pattern-Classification-Hart-Duda",
            "title": {
                "fragments": [],
                "text": "Pattern Classification"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1830709"
                        ],
                        "name": "A. Lanitis",
                        "slug": "A.-Lanitis",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Lanitis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Lanitis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48195968"
                        ],
                        "name": "A. Hill",
                        "slug": "A.-Hill",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Hill",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hill"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7205190"
                        ],
                        "name": "Tim Cootes",
                        "slug": "Tim-Cootes",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Cootes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tim Cootes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144482985"
                        ],
                        "name": "C. Taylor",
                        "slug": "C.-Taylor",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Taylor",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Taylor"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15822571,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0f2e1b52fffb975f443304e1f3c86ff156d07757",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Locating-facial-features-using-genetic-algorithms.-Lanitis-Hill",
            "title": {
                "fragments": [],
                "text": "Locating facial features using genetic algorithms."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52125963"
                        ],
                        "name": "Boris E. Shpungin",
                        "slug": "Boris-E.-Shpungin",
                        "structuredName": {
                            "firstName": "Boris",
                            "lastName": "Shpungin",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Boris E. Shpungin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741200"
                        ],
                        "name": "J. Movellan",
                        "slug": "J.-Movellan",
                        "structuredName": {
                            "firstName": "Javier",
                            "lastName": "Movellan",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Movellan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 17495886,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9c0554905770b586771fa3859399c05515d7873a",
            "isKey": false,
            "numCitedBy": 4,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Multi-Threaded-Approach-to-Real-Time-Face-Shpungin-Movellan",
            "title": {
                "fragments": [],
                "text": "A Multi-Threaded Approach to Real Time Face Tracking"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33666044"
                        ],
                        "name": "A. Senior",
                        "slug": "A.-Senior",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Senior",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Senior"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 169
                            }
                        ],
                        "text": "The DFFS measure has also been used for facial feature detection in [33] and in combination with Fisher\u2019s linear discriminant [43] for face and facial feature detection [173]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 58193036,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "501f8dd266fd0fe75ec196d601610e52c7f716c2",
            "isKey": false,
            "numCitedBy": 62,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Face-and-Feature-Finding-for-a-Face-Recognition-Senior",
            "title": {
                "fragments": [],
                "text": "Face and Feature Finding for a Face Recognition System"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "corpusId": 12646365,
            "fieldsOfStudy": [],
            "id": "14e53403a0055dbe5faaf9f1f3be96ca0e692a4d",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Improved Boosting Algorithms using Confidence-Rated Predictions"
            },
            "venue": {
                "fragments": [],
                "text": "COLT"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 184,
                                "start": 174
                            }
                        ],
                        "text": "Visual features such as edges, color, and motion are derived in the early stage of the human visual system, shown by the various visual response patterns in our inner retina [190, 206]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The computational eye, in IEEE Spectrum: Toward an Artificial Eye"
            },
            "venue": {
                "fragments": [],
                "text": "The computational eye, in IEEE Spectrum: Toward an Artificial Eye"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Face recognition using view-based and modular eigenspaces, in Automatic Systems for the Identification of Humans"
            },
            "venue": {
                "fragments": [],
                "text": "Face recognition using view-based and modular eigenspaces, in Automatic Systems for the Identification of Humans"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 72
                            }
                        ],
                        "text": "Active contours, or snakes, are commonly used to locate a head boundary [55, 69, 102, 137, 209, 219]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Facial feature extraction and pose determination,  Pattern Recog.33"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 41
                            }
                        ],
                        "text": "information, the external energy term in [209, 219] includes a skin color function which attracts the contour to the face region."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 15
                            }
                        ],
                        "text": "Elastic energy [55, 69, 209, 219] is used commonly as internal energy."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 72
                            }
                        ],
                        "text": "Active contours, or snakes, are commonly used to locate a head boundary [55, 69, 102, 137, 209, 219]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Facial contour extraction"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Proc. of 3rd Int. Conf. on Automatic Face and Gesture Recognition,"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 49
                            }
                        ],
                        "text": "Given a set of training images, the EM algorithm [34] is used to estimate the parameters in the mixture model of factor analyzers [49]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Maximum likelihood from incomplete data via the EM algorithm,J"
            },
            "venue": {
                "fragments": [],
                "text": "Roy. Statist. Soc"
            },
            "year": 1977
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 35
                            }
                        ],
                        "text": "A similar approach is presented in [89] where PCA is applied for modeling both the class of faces and the class of pseudo-faces (nonfaces, but face-like patterns), together with matching criteria based on a generalized likelihood ratio."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Generalized likelihood ratio-based face detection and extraction of mouth features, Pattern Recog"
            },
            "venue": {
                "fragments": [],
                "text": "Generalized likelihood ratio-based face detection and extraction of mouth features, Pattern Recog"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Fast greedy algorithm for locating head boundaries"
            },
            "venue": {
                "fragments": [],
                "text": "Electron. Lett"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Robust detection of facial features by generalised symmetry The Hague, The Netherlands"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. of 11th Int. Conf. on Pattern Recognition"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Detection of human faces in color images, IEE Proc. Vision Image Signal Process"
            },
            "venue": {
                "fragments": [],
                "text": "Detection of human faces in color images, IEE Proc. Vision Image Signal Process"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A dual active contour for head and boundary extraction, in IEE Colloquium on Image Processing for Biometric Measurement"
            },
            "venue": {
                "fragments": [],
                "text": "A dual active contour for head and boundary extraction, in IEE Colloquium on Image Processing for Biometric Measurement"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 120
                            }
                        ],
                        "text": "An introduction to some basic neural network methods for face detection can be found in Viennet and Fougelman Souli \u0301 e [195]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Connectionist methods for human face processing, in Face Recognition: From Theory to Application"
            },
            "venue": {
                "fragments": [],
                "text": "Connectionist methods for human face processing, in Face Recognition: From Theory to Application"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 184,
                                "start": 174
                            }
                        ],
                        "text": "Visual features such as edges, color, and motion are derived in the early stage of the human visual system, shown by the various visual response patterns in our inner retina [190, 206]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Preattentive processing in vision,  Comput"
            },
            "venue": {
                "fragments": [],
                "text": "Vision, Graphics Image Process "
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Preattentive processing in vision, Comput. Vision, Graphics Image Process"
            },
            "venue": {
                "fragments": [],
                "text": "Preattentive processing in vision, Comput. Vision, Graphics Image Process"
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Locating head and face boundaries for headshoulder images, Pattern Recog"
            },
            "venue": {
                "fragments": [],
                "text": "Locating head and face boundaries for headshoulder images, Pattern Recog"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 56
                            }
                        ],
                        "text": "Various types of face constellations have been proposed [4, 11, 73, 110, 117, 131, 180, 197, 221]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 10
                            }
                        ],
                        "text": "Burlet al.[11, 12] make use of statistical shape theory on the features detected from a multi-scale Gaussian derivative filter."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Face localization via shape statistics, in  Int"
            },
            "venue": {
                "fragments": [],
                "text": "Workshop on Automatic Face and Gesture Recognition,"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 125
                            }
                        ],
                        "text": "Other color models applied to face detection include HSV [48, 60, 81, 216], YES [160], YCrCb [2, 48, 84, 130, 191, 200], YUV [1, 123], CIE-xyz [15], L\u2217 a\u2217 b\u2217 [13, 109], L\u2217 u\u2217 v \u2217 [63], CSN (a modified rg representation) [90, 91] and UCS/Farnsworth (a perceptually uniform color system was proposed by Farnsworth [210]) [208]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Face detection in complex environments"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings International Conference on Image Processing"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Active vision-based face authentication, Image Vision Comput"
            },
            "venue": {
                "fragments": [],
                "text": "Active vision-based face authentication, Image Vision Comput"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 200,
                                "start": 145
                            }
                        ],
                        "text": "Since the main variation in skin appearance is largely due to luminance change (brightness) [215], normalized RGB colors are generally preferred [27, 53, 75, 88, 92, 165, 181, 196, 202, 207, 213, 215], so that the effect of luminance can be filtered out."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Frontalview face detection and facial feature extraction using color and morphological operations, Pattern Recog"
            },
            "venue": {
                "fragments": [],
                "text": "Lett"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "[103] have incorporated a genetic algorithm (GA) and multiresolution approach to address the problem in multiple face candidates."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 190,
                                "start": 185
                            }
                        ],
                        "text": "Furthermore, it has been shown that occlusion of a particular feature does not pose a severe problem since other features in the model can still contribute to a global optimal solution [103]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Locating facial features using genetics"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. of Int. Conf. on Digital Signal Processing,"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 200,
                                "start": 145
                            }
                        ],
                        "text": "Since the main variation in skin appearance is largely due to luminance change (brightness) [215], normalized RGB colors are generally preferred [27, 53, 75, 88, 92, 165, 181, 196, 202, 207, 213, 215], so that the effect of luminance can be filtered out."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Venkatesth, An integrated face detection and recognition system"
            },
            "venue": {
                "fragments": [],
                "text": "inProceedings of the 10th International Conference on Image Analysis and Processing,"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 51
                            }
                        ],
                        "text": "Later development in generalized symmetry includes [86, 94] and the work by Lin and Lin et al."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Face parts extraction window based on bilateral symmetry of gradient information"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 8th International Conference on Computer Analysis of Images and Patterns (CAIP)"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 36
                            }
                        ],
                        "text": "Contemporary research in this field [7, 35, 98, 168, 174] has mainly concentrated on issues such as execution time reductions, template modifications, and energy term considerations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Recognition-based template deformation and masking for eye-feature extraction and description, Pattern Recog"
            },
            "venue": {
                "fragments": [],
                "text": "Recognition-based template deformation and masking for eye-feature extraction and description, Pattern Recog"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "in [42] to include color information and multiple views and applied to the problem of finding face images on the Web."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A fast and accurate face detector for indexation of face images, inProceedings"
            },
            "venue": {
                "fragments": [],
                "text": "Fourth IEEE International Conference on Automatic Face and Gesture Recognition,"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The CSIRO PC-check system"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Second International Conference on Audio-and Video-based Biometric Person Authentication (AVBPA)"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 110
                            }
                        ],
                        "text": "does not necessarily refer to the same 483 faces in all the reported results since at least one of the papers [157] indicate that they have labeled the set of faces themselves."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A SNoW-based face detector, in Advances in Neural Information Processing Systems 12 (NIPS 12)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 40
                            }
                        ],
                        "text": "use Kohonen\u2019s self-organizing map (SOM) [93] to divide the training images into 25 face and 25 nonface classes (the number of classes chosen based on the size of the training set)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 66
                            }
                        ],
                        "text": "In the second method, Yanget al.use Kohonen\u2019s self-organizing map (SOM) [93] to divide the training images into 25 face and 25 nonface classes (the number of classes chosen based on the size of the training set)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 72
                            }
                        ],
                        "text": "Other face detection approaches using LDA include [65] and [184], while SOMs have been used in [183]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Self-Organizing Maps,  Springer-Verlog"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A hierarchical multiscale and multiangle system for human face detection in a complex background using gravitycenter template, Pattern Recog"
            },
            "venue": {
                "fragments": [],
                "text": "A hierarchical multiscale and multiangle system for human face detection in a complex background using gravitycenter template, Pattern Recog"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 56
                            }
                        ],
                        "text": "Various types of face constellations have been proposed [4, 11, 73, 110, 117, 131, 180, 197, 221]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Efficient focusing and face detection, in  Face Recognition: From Theory to Application,Springer-Verlag"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 106
                            }
                        ],
                        "text": "Many of the current face recognition techniques assume the availability of frontal faces of similar sizes [14, 163]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 153
                            }
                        ],
                        "text": "For face recognition, this implies that neural approaches might be applied for all parts of the system, and this had indeed been shown in several papers [14, 113, 163]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Automatic recognition and analysis of human faces and facial expressions: a survey, Pattern Recog"
            },
            "venue": {
                "fragments": [],
                "text": "Automatic recognition and analysis of human faces and facial expressions: a survey, Pattern Recog"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 51
                            }
                        ],
                        "text": "Later development in generalized symmetry includes [86, 94] and the work by Lin and Lin et al."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Automatic human face detection and recognition under nonuniform illumination, Pattern Recog"
            },
            "venue": {
                "fragments": [],
                "text": "Automatic human face detection and recognition under nonuniform illumination, Pattern Recog"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 87
                            }
                        ],
                        "text": "Probabilistic face models based on multiple face appearance have also been proposed in [180, 204, 221, 224]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 56
                            }
                        ],
                        "text": "Various types of face constellations have been proposed [4, 11, 73, 110, 117, 131, 180, 197, 221]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 27
                            }
                        ],
                        "text": "In Yow and Cipolla\u2019s model [221], faces are classified into several partial facial appearance groups that are common under different viewpoints."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Feature-based human face detection,  Image Vision Comput"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Efficient focusing and face detection, in Face Recognition: From Theory to Application"
            },
            "venue": {
                "fragments": [],
                "text": "Efficient focusing and face detection, in Face Recognition: From Theory to Application"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "HJELMHJELM\u02daHJELM\u00c5S AND LOW"
            },
            "venue": {
                "fragments": [],
                "text": "HJELMHJELM\u02daHJELM\u00c5S AND LOW"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 68
                            }
                        ],
                        "text": "A new learning architecture called SNoW (sparse network of winnows) [156] is applied to face detection in Roth et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The SNoW Learning Architecture"
            },
            "venue": {
                "fragments": [],
                "text": "The SNoW Learning Architecture"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Face-texture model based on sgld and its application, Pattern Recog"
            },
            "venue": {
                "fragments": [],
                "text": "Face-texture model based on sgld and its application, Pattern Recog"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 83
                            }
                        ],
                        "text": "The Sobel operator was the most common filter among the techniques mentioned above [9, 16, 32, 76, 87, 108]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 62
                            }
                        ],
                        "text": "More recent examples of edge-based techniques can be found in [9, 16, 59, 115, 116] for facial feature extraction and in [32, 50, 58, 60, 70, 76, 108, 201, 222] for face detection."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 202,
                                "start": 194
                            }
                        ],
                        "text": "The techniques in the first category make explicit use of face knowledge and follow the classical detection methodology in which low level features are derived prior to knowledge-based analysis [9, 193]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Face recognition: Feature versus templates"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 91
                            }
                        ],
                        "text": "Among the literature survey, a pair of eyes is the most commonly applied reference feature [5, 27, 52, 61, 207, 214] due to its distinct side-by-side appearance."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Locating faces and facial parts"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Proc. of Int. Workshop on Automatic Face-and Gesture-Recognition"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 57
                            }
                        ],
                        "text": "Other color models applied to face detection include HSV [48, 60, 81, 216], YES [160], YCrCb [2, 48, 84, 130, 191, 200], YUV [1, 123], CIE-xyz [15], L\u2217 a\u2217 b\u2217 [13, 109], L\u2217 u\u2217 v \u2217 [63], CSN (a modified rg representation) [90, 91] and UCS/Farnsworth (a perceptually uniform color system was proposed by Farnsworth [210]) [208]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 126
                            }
                        ],
                        "text": "In their study they compare normalized TSL (tint-saturation-luminance [1881]), rg and CIE-xy chrominance spaces, and CIE-DSH, HSV, YIQ, YES, CIE-L \u2217 u \u2217 v \u2217 , and CIE L\u2217 a \u2217 b \u2217 chrominance spaces by modeling skin color distributions with either a single Gaussian or a Gaussian mixture density model in each space."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Face detection using quantized skin color regions, merging and wavelet packet analysis,IEEE"
            },
            "venue": {
                "fragments": [],
                "text": "Trans. Multimedia1,"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Automatic extraction of face-feature, Pattern Recog"
            },
            "venue": {
                "fragments": [],
                "text": "Lett. Feb"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Real-time face location on gray-scale static images, Pattern Recog"
            },
            "venue": {
                "fragments": [],
                "text": "Real-time face location on gray-scale static images, Pattern Recog"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Tracking facial features in image sequences using neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Proc. of 2nd Int. Conf. on Automatic Face and Gesture Recognition"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 172
                            }
                        ],
                        "text": "Apart from face classification, neural networks have also been applied for facial features classification [36, 125, 151, 196] and a method for improving detection time for MLPs is presented in [6]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 65
                            }
                        ],
                        "text": "The first neural approaches to face detection were based on MLPs [10, 82, 147], where promising results where reported on fairly simple datasets."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 166
                            }
                        ],
                        "text": "Sung and Poggio suggested a training algorithm, known as \u201cboot-strap training,\u201d to partially deal with this problem (a more precise strategy than the one proposed in [10])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Detection and localization of faces on digital images,  Pattern"
            },
            "venue": {
                "fragments": [],
                "text": "Recog. Lett.  15,"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 87
                            }
                        ],
                        "text": "Probabilistic face models based on multiple face appearance have also been proposed in [180, 204, 221, 224]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 56
                            }
                        ],
                        "text": "Various types of face constellations have been proposed [4, 11, 73, 110, 117, 131, 180, 197, 221]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "[180] also uses similar strategies in which various facial components are processed by concurrent agents in a distributed network."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Detection of face orientation and facial components using distributed appearance modelling"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Proc. of Int. Workshop on Automatic Face-and Gesture-Recognition"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 56
                            }
                        ],
                        "text": "Various types of face constellations have been proposed [4, 11, 73, 110, 117, 131, 180, 197, 221]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A hierarchical multiscale and multiangle system for human face detection in a complex background using gravitycenter template,  P tt rn Recog"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 186,
                                "start": 182
                            }
                        ],
                        "text": "By converting RGB colors into YIQ representation, it was found that the I-component, which includes color\u2019s ranging from orange to cyan, manages to enhance the skin region of Asians [29]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 58
                            }
                        ],
                        "text": "The YIQ color model has been applied to face detection in [29, 205]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Face-texture model based on sgld and its application,  Pattern Recog.29"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Detection and tracking of facial features by using a facial feature model and deformable circular template"
            },
            "venue": {
                "fragments": [],
                "text": "IEICE Trans. Inform. Systems E78\u2013D"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Facial feature extraction and pose determination, Pattern Recog"
            },
            "venue": {
                "fragments": [],
                "text": "Facial feature extraction and pose determination, Pattern Recog"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 93
                            }
                        ],
                        "text": "Other color models applied to face detection include HSV [48, 60, 81, 216], YES [160], YCrCb [2, 48, 84, 130, 191, 200], YUV [1, 123], CIE-xyz [15], L\u2217 a\u2217 b\u2217 [13, 109], L\u2217 u\u2217 v \u2217 [63], CSN (a modified rg representation) [90, 91] and UCS/Farnsworth (a perceptually uniform color system was proposed by Farnsworth [210]) [208]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A simple and efficient face detection algorithm for video database"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2000 International Conference on Image Processing,"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 0
                            }
                        ],
                        "text": "[133, 142] later proposed a facial feature detector using DFFS generated from eigenfeatures (eigeneyes, eigennose, eigenmouth) obtained from various facial feature templates in a training set."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Face recognition using view-based and modular eigenspaces, in Automatic Systems for the Identification of Humans, SPIE"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Automatic human face location in a complex background, Pattern Recog"
            },
            "venue": {
                "fragments": [],
                "text": "Automatic human face location in a complex background, Pattern Recog"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 83
                            }
                        ],
                        "text": "The Sobel operator was the most common filter among the techniques mentioned above [9, 16, 32, 76, 87, 108]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 62
                            }
                        ],
                        "text": "More recent examples of edge-based techniques can be found in [9, 16, 59, 115, 116] for facial feature extraction and in [32, 50, 58, 60, 70, 76, 108, 201, 222] for face detection."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Facial components segmentation for extracting facial feature"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Second International Conference on Audio-and Video-based Biometric Person Authentication (AVBPA)"
            },
            "year": 1999
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 90,
            "methodology": 112,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 339,
        "totalPages": 34
    },
    "page_url": "https://www.semanticscholar.org/paper/Face-Detection:-A-Survey-Hjelm\u00e5s-Low/887567782cb859ecd339693589056903b0071353?sort=total-citations"
}