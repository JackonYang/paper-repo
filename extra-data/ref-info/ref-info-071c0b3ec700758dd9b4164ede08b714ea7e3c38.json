{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2156632012"
                        ],
                        "name": "Tao Wang",
                        "slug": "Tao-Wang",
                        "structuredName": {
                            "firstName": "Tao",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tao Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "25629078"
                        ],
                        "name": "David J. Wu",
                        "slug": "David-J.-Wu",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Wu",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David J. Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144638694"
                        ],
                        "name": "Adam Coates",
                        "slug": "Adam-Coates",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Coates",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adam Coates"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 31
                            }
                        ],
                        "text": ", 2010), OCR in natural scenes (Jaderberg et al., 2015; 2016; Wang et al., 2012) and image caption generation (Karpathy & FeiFei, 2015; Vinyals et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 155
                            }
                        ],
                        "text": "For instance, advances have been made in the areas of handwriting recognition (Ciresan et al., 2010), OCR in natural scenes (Jaderberg et al., 2015; 2016; Wang et al., 2012) and image caption generation (Karpathy & FeiFei, 2015; Vinyals et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3126988,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "26cb14c9d22cf946314d685fe3541ef9f641e429",
            "isKey": false,
            "numCitedBy": 792,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Full end-to-end text recognition in natural images is a challenging problem that has received much attention recently. Traditional systems in this area have relied on elaborate models incorporating carefully hand-engineered features or large amounts of prior knowledge. In this paper, we take a different route and combine the representational power of large, multilayer neural networks together with recent developments in unsupervised feature learning, which allows us to use a common framework to train highly-accurate text detector and character recognizer modules. Then, using only simple off-the-shelf methods, we integrate these two modules into a full end-to-end, lexicon-driven, scene text recognition system that achieves state-of-the-art performance on standard benchmarks, namely Street View Text and ICDAR 2003."
            },
            "slug": "End-to-end-text-recognition-with-convolutional-Wang-Wu",
            "title": {
                "fragments": [],
                "text": "End-to-end text recognition with convolutional neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper combines the representational power of large, multilayer neural networks together with recent developments in unsupervised feature learning, which allows them to use a common framework to train highly-accurate text detector and character recognizer modules."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012)"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50521003"
                        ],
                        "name": "Chen-Yu Lee",
                        "slug": "Chen-Yu-Lee",
                        "structuredName": {
                            "firstName": "Chen-Yu",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chen-Yu Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2217144"
                        ],
                        "name": "Simon Osindero",
                        "slug": "Simon-Osindero",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Osindero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Simon Osindero"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8608310,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a8c9d85147039ca54b0439cde05ef8c33efecf00",
            "isKey": false,
            "numCitedBy": 322,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": "We present recursive recurrent neural networks with attention modeling (R2AM) for lexicon-free optical character recognition in natural scene images. The primary advantages of the proposed method are: (1) use of recursive convolutional neural networks (CNNs), which allow for parametrically efficient and effective image feature extraction, (2) an implicitly learned character-level language model, embodied in a recurrent neural network which avoids the need to use N-grams, and (3) the use of a soft-attention mechanism, allowing the model to selectively exploit image features in a coordinated way, and allowing for end-to-end training within a standard backpropagation framework. We validate our method with state-of-the-art performance on challenging benchmark datasets: Street View Text, IIIT5k, ICDAR and Synth90k."
            },
            "slug": "Recursive-Recurrent-Nets-with-Attention-Modeling-in-Lee-Osindero",
            "title": {
                "fragments": [],
                "text": "Recursive Recurrent Nets with Attention Modeling for OCR in the Wild"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "This work presents recursive recurrent neural networks with attention modeling (R2AM) for lexicon-free optical character recognition in natural scene images and validates the method with state-of-the-art performance on challenging benchmark datasets."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33344744"
                        ],
                        "name": "Hongyuan Mei",
                        "slug": "Hongyuan-Mei",
                        "structuredName": {
                            "firstName": "Hongyuan",
                            "lastName": "Mei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hongyuan Mei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143977268"
                        ],
                        "name": "Mohit Bansal",
                        "slug": "Mohit-Bansal",
                        "structuredName": {
                            "firstName": "Mohit",
                            "lastName": "Bansal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mohit Bansal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733702"
                        ],
                        "name": "Matthew R. Walter",
                        "slug": "Matthew-R.-Walter",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Walter",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew R. Walter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 71
                            }
                        ],
                        "text": "Note that ideas with the same name have been proposed in previous work (Mei et al., 2016), albeit in a different formulation without the goal of reducing computation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 141
                            }
                        ],
                        "text": "Even with only a small in-domain training set, the model is able to\n1Note that ideas with the same name have been proposed in previous work (Mei et al., 2016), albeit in a different formulation without the goal of reducing computation."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1354459,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f273adbfe0e6ba39a583b9669d94cc8d828d8c25",
            "isKey": false,
            "numCitedBy": 254,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose an end-to-end, domain-independent neural encoder-aligner-decoder model for selective generation, i.e., the joint task of content selection and surface realization. Our model first encodes a full set of over-determined database event records via an LSTM-based recurrent neural network, then utilizes a novel coarse-to-fine aligner to identify the small subset of salient records to talk about, and finally employs a decoder to generate free-form descriptions of the aligned, selected records. Our model achieves the best selection and generation results reported to-date (with 59% relative improvement in generation) on the benchmark WeatherGov dataset, despite using no specialized features or linguistic resources. Using an improved k-nearest neighbor beam filter helps further. We also perform a series of ablations and visualizations to elucidate the contributions of our key model components. Lastly, we evaluate the generalizability of our model on the RoboCup dataset, and get results that are competitive with or better than the state-of-the-art, despite being severely data-starved."
            },
            "slug": "What-to-talk-about-and-how-Selective-Generation-Mei-Bansal",
            "title": {
                "fragments": [],
                "text": "What to talk about and how? Selective Generation using LSTMs with Coarse-to-Fine Alignment"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "An end-to-end, domain-independent neural encoder-aligner-decoder model for selective generation, i.e., the joint task of content selection and surface realization, achieves the best selection and generation results reported to-date on the benchmark WeatherGov dataset, despite using no specialized features or linguistic resources."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117101253"
                        ],
                        "name": "Ke Xu",
                        "slug": "Ke-Xu",
                        "structuredName": {
                            "firstName": "Ke",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ke Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2503659"
                        ],
                        "name": "Jimmy Ba",
                        "slug": "Jimmy-Ba",
                        "structuredName": {
                            "firstName": "Jimmy",
                            "lastName": "Ba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jimmy Ba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3450996"
                        ],
                        "name": "Ryan Kiros",
                        "slug": "Ryan-Kiros",
                        "structuredName": {
                            "firstName": "Ryan",
                            "lastName": "Kiros",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ryan Kiros"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1979489"
                        ],
                        "name": "Kyunghyun Cho",
                        "slug": "Kyunghyun-Cho",
                        "structuredName": {
                            "firstName": "Kyunghyun",
                            "lastName": "Cho",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kyunghyun Cho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760871"
                        ],
                        "name": "Aaron C. Courville",
                        "slug": "Aaron-C.-Courville",
                        "structuredName": {
                            "firstName": "Aaron",
                            "lastName": "Courville",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aaron C. Courville"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804104"
                        ],
                        "name": "R. Zemel",
                        "slug": "R.-Zemel",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Zemel",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Zemel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": " a Tex generation mode (Wen 2002). However we found that this system performed too poorly on this task to compare. For a neural model, a natural comparison is to standard image captioning approaches (Xu et al. 2015). As our model is based on this approach, we compare by removing the encoder RNN, i.e. replacing V~ with V, and increasing the number of CNN such that the number of parameters is approximately the sam"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "what you see], is a simple extension of the attention-based encoder-decoder model (Bahdanau, Cho, and Bengio 2014), which is now standard for machine translation. Similar to work in image captioning (Xu et al. 2015), the model incorporates a multi-layer convolutional network over the image with an attention-based recurrent neural network decoder. To adapt this model to the OCR problem and capture the document\u2019s "
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 46
                            }
                        ],
                        "text": "The base model is adapted from the encoder of Xu et al. (2015) developed for image captioning."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 84
                            }
                        ],
                        "text": "For neural models, a natural comparison is to standard image captioning approaches (Xu et al., 2015), and CTCbased approaches (Shi et al., 2016)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 119
                            }
                        ],
                        "text": "For the second approach we use \u201chard\u201d attention for z\u2032t, an approach which has been shown to work in several image tasks (Xu et al., 2015; Mnih et al., 2014; Ba et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 10
                            }
                        ],
                        "text": "Following Xu et al. (2015), we include a moving average reward baseline for each timestep t that we update as bt \u2190 \u03b2bt+(1\u2212\u03b2)r\u0303t, where \u03b2 is a tunable learning rate."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 73
                            }
                        ],
                        "text": "In this work, we explore the use of attention-based imageto-text models (Xu et al., 2015) for the problem of generating structured markup."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "he raw input RH W and produces a feature grid V of size D 0H W0, where cdenotes the number of channels and H0and W0are the reduced sizes from pooling. Row Encoder In attention-based image captioning (Xu et al. 2015), the image feature grid can be directly fed into the decoder. For OCR, the visual features fed in to the decoder contain signi\ufb01cant relative sequential order information. Therefore we experiment with"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1055111,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4d8f2d14af5991d4f0d050d22216825cac3157bd",
            "isKey": true,
            "numCitedBy": 7252,
            "numCiting": 64,
            "paperAbstract": {
                "fragments": [],
                "text": "Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr9k, Flickr30k and MS COCO."
            },
            "slug": "Show,-Attend-and-Tell:-Neural-Image-Caption-with-Xu-Ba",
            "title": {
                "fragments": [],
                "text": "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "An attention based model that automatically learns to describe the content of images is introduced that can be trained in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3093886"
                        ],
                        "name": "Max Jaderberg",
                        "slug": "Max-Jaderberg",
                        "structuredName": {
                            "firstName": "Max",
                            "lastName": "Jaderberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Max Jaderberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34838386"
                        ],
                        "name": "K. Simonyan",
                        "slug": "K.-Simonyan",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Simonyan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Simonyan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687524"
                        ],
                        "name": "A. Vedaldi",
                        "slug": "A.-Vedaldi",
                        "structuredName": {
                            "firstName": "Andrea",
                            "lastName": "Vedaldi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Vedaldi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 125
                            }
                        ],
                        "text": "For instance, advances have been made in the areas of handwriting recognition (Ciresan et al., 2010), OCR in natural scenes (Jaderberg et al., 2015; 2016; Wang et al., 2012) and image caption generation (Karpathy & FeiFei, 2015; Vinyals et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 207252329,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a5ae7436b5946bd37d17fc1ed26374389a86deff",
            "isKey": false,
            "numCitedBy": 887,
            "numCiting": 78,
            "paperAbstract": {
                "fragments": [],
                "text": "In this work we present an end-to-end system for text spotting\u2014localising and recognising text in natural scene images\u2014and text based image retrieval. This system is based on a region proposal mechanism for detection and deep convolutional neural networks for recognition. Our pipeline uses a novel combination of complementary proposal generation techniques to ensure high recall, and a fast subsequent filtering stage for improving precision. For the recognition and ranking of proposals, we train very large convolutional neural networks to perform word recognition on the whole proposal region at the same time, departing from the character classifier based systems of the past. These networks are trained solely on data produced by a synthetic text generation engine, requiring no human labelled data. Analysing the stages of our pipeline, we show state-of-the-art performance throughout. We perform rigorous experiments across a number of standard end-to-end text spotting benchmarks and text-based image retrieval datasets, showing a large improvement over all previous methods. Finally, we demonstrate a real-world application of our text spotting system to allow thousands of hours of news footage to be instantly searchable via a text query."
            },
            "slug": "Reading-Text-in-the-Wild-with-Convolutional-Neural-Jaderberg-Simonyan",
            "title": {
                "fragments": [],
                "text": "Reading Text in the Wild with Convolutional Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "An end-to-end system for text spotting\u2014localising and recognising text in natural scene images\u2014and text based image retrieval and a real-world application to allow thousands of hours of news footage to be instantly searchable via a text query is demonstrated."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39557762"
                        ],
                        "name": "Jian-shu Zhang",
                        "slug": "Jian-shu-Zhang",
                        "structuredName": {
                            "firstName": "Jian-shu",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian-shu Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145419855"
                        ],
                        "name": "Jun Du",
                        "slug": "Jun-Du",
                        "structuredName": {
                            "firstName": "Jun",
                            "lastName": "Du",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jun Du"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1776581"
                        ],
                        "name": "Shiliang Zhang",
                        "slug": "Shiliang-Zhang",
                        "structuredName": {
                            "firstName": "Shiliang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shiliang Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144645397"
                        ],
                        "name": "Dan Liu",
                        "slug": "Dan-Liu",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dan Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112262091"
                        ],
                        "name": "Yulong Hu",
                        "slug": "Yulong-Hu",
                        "structuredName": {
                            "firstName": "Yulong",
                            "lastName": "Hu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yulong Hu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3185652"
                        ],
                        "name": "Jin-Shui Hu",
                        "slug": "Jin-Shui-Hu",
                        "structuredName": {
                            "firstName": "Jin-Shui",
                            "lastName": "Hu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jin-Shui Hu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144572674"
                        ],
                        "name": "Si Wei",
                        "slug": "Si-Wei",
                        "structuredName": {
                            "firstName": "Si",
                            "lastName": "Wei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Si Wei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1860774"
                        ],
                        "name": "Lirong Dai",
                        "slug": "Lirong-Dai",
                        "structuredName": {
                            "firstName": "Lirong",
                            "lastName": "Dai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lirong Dai"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 32241266,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c40905ca8608d28ef767f1f3f12eb3f7a288c017",
            "isKey": false,
            "numCitedBy": 111,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Watch,-attend-and-parse:-An-end-to-end-neural-based-Zhang-Du",
            "title": {
                "fragments": [],
                "text": "Watch, attend and parse: An end-to-end neural network based approach to handwritten mathematical expression recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689108"
                        ],
                        "name": "Oriol Vinyals",
                        "slug": "Oriol-Vinyals",
                        "structuredName": {
                            "firstName": "Oriol",
                            "lastName": "Vinyals",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oriol Vinyals"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726415"
                        ],
                        "name": "Alexander Toshev",
                        "slug": "Alexander-Toshev",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Toshev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander Toshev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751569"
                        ],
                        "name": "Samy Bengio",
                        "slug": "Samy-Bengio",
                        "structuredName": {
                            "firstName": "Samy",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Samy Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761978"
                        ],
                        "name": "D. Erhan",
                        "slug": "D.-Erhan",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Erhan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Erhan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 249,
                                "start": 229
                            }
                        ],
                        "text": "For instance, advances have been made in the areas of handwriting recognition (Ciresan et al., 2010), OCR in natural scenes (Jaderberg et al., 2015; 2016; Wang et al., 2012) and image caption generation (Karpathy & FeiFei, 2015; Vinyals et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 37
                            }
                        ],
                        "text": ", 2012) and image caption generation (Karpathy & FeiFei, 2015; Vinyals et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1169492,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0",
            "isKey": false,
            "numCitedBy": 4510,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. For instance, while the current state-of-the-art BLEU-1 score (the higher the better) on the Pascal dataset is 25, our approach yields 59, to be compared to human performance around 69. We also show BLEU-1 score improvements on Flickr30k, from 56 to 66, and on SBU, from 19 to 28. Lastly, on the newly released COCO dataset, we achieve a BLEU-4 of 27.7, which is the current state-of-the-art."
            },
            "slug": "Show-and-tell:-A-neural-image-caption-generator-Vinyals-Toshev",
            "title": {
                "fragments": [],
                "text": "Show and tell: A neural image caption generator"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This paper presents a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276155"
                        ],
                        "name": "Baoguang Shi",
                        "slug": "Baoguang-Shi",
                        "structuredName": {
                            "firstName": "Baoguang",
                            "lastName": "Shi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Baoguang Shi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145905113"
                        ],
                        "name": "X. Bai",
                        "slug": "X.-Bai",
                        "structuredName": {
                            "firstName": "Xiang",
                            "lastName": "Bai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Bai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2146721"
                        ],
                        "name": "C. Yao",
                        "slug": "C.-Yao",
                        "structuredName": {
                            "firstName": "Cong",
                            "lastName": "Yao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Yao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 24139,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8e9149ab00236d04db23394774e716c4f1d89231",
            "isKey": false,
            "numCitedBy": 1383,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "Image-based sequence recognition has been a long-standing research topic in computer vision. In this paper, we investigate the problem of scene text recognition, which is among the most important and challenging tasks in image-based sequence recognition. A novel neural network architecture, which integrates feature extraction, sequence modeling and transcription into a unified framework, is proposed. Compared with previous systems for scene text recognition, the proposed architecture possesses four distinctive properties: (1) It is end-to-end trainable, in contrast to most of the existing algorithms whose components are separately trained and tuned. (2) It naturally handles sequences in arbitrary lengths, involving no character segmentation or horizontal scale normalization. (3) It is not confined to any predefined lexicon and achieves remarkable performances in both lexicon-free and lexicon-based scene text recognition tasks. (4) It generates an effective yet much smaller model, which is more practical for real-world application scenarios. The experiments on standard benchmarks, including the IIIT-5K, Street View Text and ICDAR datasets, demonstrate the superiority of the proposed algorithm over the prior arts. Moreover, the proposed algorithm performs well in the task of image-based music score recognition, which evidently verifies the generality of it."
            },
            "slug": "An-End-to-End-Trainable-Neural-Network-for-Sequence-Shi-Bai",
            "title": {
                "fragments": [],
                "text": "An End-to-End Trainable Neural Network for Image-Based Sequence Recognition and Its Application to Scene Text Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A novel neural network architecture, which integrates feature extraction, sequence modeling and transcription into a unified framework, is proposed, which generates an effective yet much smaller model, which is more practical for real-world application scenarios."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3093886"
                        ],
                        "name": "Max Jaderberg",
                        "slug": "Max-Jaderberg",
                        "structuredName": {
                            "firstName": "Max",
                            "lastName": "Jaderberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Max Jaderberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34838386"
                        ],
                        "name": "K. Simonyan",
                        "slug": "K.-Simonyan",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Simonyan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Simonyan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687524"
                        ],
                        "name": "A. Vedaldi",
                        "slug": "A.-Vedaldi",
                        "structuredName": {
                            "firstName": "Andrea",
                            "lastName": "Vedaldi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Vedaldi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 31
                            }
                        ],
                        "text": ", 2010), OCR in natural scenes (Jaderberg et al., 2015; 2016; Wang et al., 2012) and image caption generation (Karpathy & FeiFei, 2015; Vinyals et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 125
                            }
                        ],
                        "text": "For instance, advances have been made in the areas of handwriting recognition (Ciresan et al., 2010), OCR in natural scenes (Jaderberg et al., 2015; 2016; Wang et al., 2012) and image caption generation (Karpathy & FeiFei, 2015; Vinyals et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 29
                            }
                        ],
                        "text": "Unlike some recent OCR work (Jaderberg et al., 2015; Lee & Osindero, 2016), we do not use final fully-connected layers (Ioffe & Szegedy, 2015), since we want to preserve the locality of CNN features in order to use visual attention."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16734174,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "21da448e7c31e1ff6cc3b7155a9c9c49a0138060",
            "isKey": false,
            "numCitedBy": 198,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop a representation suitable for the unconstrained recognition of words in natural images: the general case of no fixed lexicon and unknown length. \nTo this end we propose a convolutional neural network (CNN) based architecture which incorporates a Conditional Random Field (CRF) graphical model, taking the whole word image as a single input. The unaries of the CRF are provided by a CNN that predicts characters at each position of the output, while higher order terms are provided by another CNN that detects the presence of N-grams. We show that this entire model (CRF, character predictor, N-gram predictor) can be jointly optimised by back-propagating the structured output loss, essentially requiring the system to perform multi-task learning, and training uses purely synthetically generated data. The resulting model is a more accurate system on standard real-world text recognition benchmarks than character prediction alone, setting a benchmark for systems that have not been trained on a particular lexicon. In addition, our model achieves state-of-the-art accuracy in lexicon-constrained scenarios, without being specifically modelled for constrained recognition. To test the generalisation of our model, we also perform experiments with random alpha-numeric strings to evaluate the method when no visual language model is applicable."
            },
            "slug": "Deep-Structured-Output-Learning-for-Unconstrained-Jaderberg-Simonyan",
            "title": {
                "fragments": [],
                "text": "Deep Structured Output Learning for Unconstrained Text Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A convolutional neural network based architecture which incorporates a Conditional Random Field graphical model, taking the whole word image as a single input, which achieves state-of-the-art accuracy in lexicon-constrained scenarios, without being specifically modelled for constrained recognition."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1821711"
                        ],
                        "name": "Thang Luong",
                        "slug": "Thang-Luong",
                        "structuredName": {
                            "firstName": "Thang",
                            "lastName": "Luong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thang Luong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143950636"
                        ],
                        "name": "Hieu Pham",
                        "slug": "Hieu-Pham",
                        "structuredName": {
                            "firstName": "Hieu",
                            "lastName": "Pham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hieu Pham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 108
                            }
                        ],
                        "text": "Note there are different choices for a \u2013 we follow past empirical work and use at,h,w = \u03b2 tanh(W1ht +W2Vhw) (Luong et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 107
                            }
                        ],
                        "text": "Note there are different choices for a \u2013 we follow past empirical work and use at,h,w = \u03b2T tanh(W1ht +W2Vhw) (Luong et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1998416,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "93499a7c7f699b6630a86fad964536f9423bb6d0",
            "isKey": false,
            "numCitedBy": 5892,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches on the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems that already incorporate known techniques such as dropout. Our ensemble model using different attention architectures yields a new state-of-the-art result in the WMT\u201915 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker. 1"
            },
            "slug": "Effective-Approaches-to-Attention-based-Neural-Luong-Pham",
            "title": {
                "fragments": [],
                "text": "Effective Approaches to Attention-based Neural Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A global approach which always attends to all source words and a local one that only looks at a subset of source words at a time are examined, demonstrating the effectiveness of both approaches on the WMT translation tasks between English and German in both directions."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2206490"
                        ],
                        "name": "Marcin Andrychowicz",
                        "slug": "Marcin-Andrychowicz",
                        "structuredName": {
                            "firstName": "Marcin",
                            "lastName": "Andrychowicz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marcin Andrychowicz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2006889"
                        ],
                        "name": "Karol Kurach",
                        "slug": "Karol-Kurach",
                        "structuredName": {
                            "firstName": "Karol",
                            "lastName": "Kurach",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karol Kurach"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7780492,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "941462c74c550f60a838911858b054de1a1406a9",
            "isKey": false,
            "numCitedBy": 52,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose and investigate a novel memory architecture for neural networks called Hierarchical Attentive Memory (HAM). It is based on a binary tree with leaves corresponding to memory cells. This allows HAM to perform memory access in O(log n) complexity, which is a significant improvement over the standard attention mechanism that requires O(n) operations, where n is the size of the memory. \nWe show that an LSTM network augmented with HAM can learn algorithms for problems like merging, sorting or binary searching from pure input-output examples. In particular, it learns to sort n numbers in time O(n log n) and generalizes well to input sequences much longer than the ones seen during the training. We also show that HAM can be trained to act like classic data structures: a stack, a FIFO queue and a priority queue."
            },
            "slug": "Learning-Efficient-Algorithms-with-Hierarchical-Andrychowicz-Kurach",
            "title": {
                "fragments": [],
                "text": "Learning Efficient Algorithms with Hierarchical Attentive Memory"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is shown that an LSTM network augmented with Hierarchical Attentive Memory can learn algorithms for problems like merging, sorting or binary searching from pure input-output examples and generalizes well to input sequences much longer than the ones seen during the training."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3255983"
                        ],
                        "name": "Volodymyr Mnih",
                        "slug": "Volodymyr-Mnih",
                        "structuredName": {
                            "firstName": "Volodymyr",
                            "lastName": "Mnih",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Volodymyr Mnih"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2801204"
                        ],
                        "name": "N. Heess",
                        "slug": "N.-Heess",
                        "structuredName": {
                            "firstName": "Nicolas",
                            "lastName": "Heess",
                            "middleNames": [
                                "Manfred",
                                "Otto"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Heess"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753223"
                        ],
                        "name": "A. Graves",
                        "slug": "A.-Graves",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Graves",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Graves"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2645384"
                        ],
                        "name": "K. Kavukcuoglu",
                        "slug": "K.-Kavukcuoglu",
                        "structuredName": {
                            "firstName": "Koray",
                            "lastName": "Kavukcuoglu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kavukcuoglu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 136
                            }
                        ],
                        "text": "For the second approach we use \u201chard\u201d attention for z\u2032t, an approach which has been shown to work in several image tasks (Xu et al., 2015; Mnih et al., 2014; Ba et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 122
                            }
                        ],
                        "text": "For the second approach we use \u201chard\u201d attention for z\u2032 t, an approach which has been shown to work in several image tasks (Xu et al., 2015; Mnih et al., 2014; Ba et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17195923,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8a756d4d25511d92a45d0f4545fa819de993851d",
            "isKey": false,
            "numCitedBy": 2410,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Applying convolutional neural networks to large images is computationally expensive because the amount of computation scales linearly with the number of image pixels. We present a novel recurrent neural network model that is capable of extracting information from an image or video by adaptively selecting a sequence of regions or locations and only processing the selected regions at high resolution. Like convolutional neural networks, the proposed model has a degree of translation invariance built-in, but the amount of computation it performs can be controlled independently of the input image size. While the model is non-differentiable, it can be trained using reinforcement learning methods to learn task-specific policies. We evaluate our model on several image classification tasks, where it significantly outperforms a convolutional neural network baseline on cluttered images, and on a dynamic visual control problem, where it learns to track a simple object without an explicit training signal for doing so."
            },
            "slug": "Recurrent-Models-of-Visual-Attention-Mnih-Heess",
            "title": {
                "fragments": [],
                "text": "Recurrent Models of Visual Attention"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A novel recurrent neural network model that is capable of extracting information from an image or video by adaptively selecting a sequence of regions or locations and only processing the selected regions at high resolution is presented."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39719398"
                        ],
                        "name": "Anand Mishra",
                        "slug": "Anand-Mishra",
                        "structuredName": {
                            "firstName": "Anand",
                            "lastName": "Mishra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anand Mishra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "72492981"
                        ],
                        "name": "Alahari Karteek",
                        "slug": "Alahari-Karteek",
                        "structuredName": {
                            "firstName": "Alahari",
                            "lastName": "Karteek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alahari Karteek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694502"
                        ],
                        "name": "C. Jawahar",
                        "slug": "C.-Jawahar",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Jawahar",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Jawahar"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9695967,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bb5b2df137a4d54c3a9145fa363e66531b491580",
            "isKey": false,
            "numCitedBy": 550,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of recognizing text in images taken in the wild has gained significant attention from the computer vision community in recent years. Contrary to recognition of printed documents, recognizing scene text is a challenging problem. We focus on the problem of recognizing text extracted from natural scene images and the web. Significant attempts have been made to address this problem in the recent past. However, many of these works benefit from the availability of strong context, which naturally limits their applicability. In this work we present a framework that uses a higher order prior computed from an English dictionary to recognize a word, which may or may not be a part of the dictionary. We show experimental results on publicly available datasets. Furthermore, we introduce a large challenging word dataset with five thousand words to evaluate various steps of our method exhaustively. The main contributions of this work are: (1) We present a framework, which incorporates higher order statistical language models to recognize words in an unconstrained manner (i.e. we overcome the need for restricted word lists, and instead use an English dictionary to compute the priors). (2) We achieve significant improvement (more than 20%) in word recognition accuracies without using a restricted word list. (3) We introduce a large word recognition dataset (atleast 5 times larger than other public datasets) with character level annotation and benchmark it."
            },
            "slug": "Scene-Text-Recognition-using-Higher-Order-Language-Mishra-Karteek",
            "title": {
                "fragments": [],
                "text": "Scene Text Recognition using Higher Order Language Priors"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A framework is presented that uses a higher order prior computed from an English dictionary to recognize a word, which may or may not be a part of the dictionary, and achieves significant improvement in word recognition accuracies without using a restricted word list."
            },
            "venue": {
                "fragments": [],
                "text": "BMVC"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2354728"
                        ],
                        "name": "A. Karpathy",
                        "slug": "A.-Karpathy",
                        "structuredName": {
                            "firstName": "Andrej",
                            "lastName": "Karpathy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Karpathy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "al Intelligence (www.aaai.org). All rights reserved. handwriting recognition (Ciresan et al. 2010), OCR in natural scenes (Jaderberg et al. 2015; 2016; Wang et al. 2012) and image caption generation (Karpathy and Fei-Fei 2015; Vinyals et al. 2015b). At a high-level, each of these systems learns an abstract encoded representation of the input image which is then decoded to generate a textual output. In addition to performi"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8517067,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "55e022fb7581bb9e1fce678d21fb25ffbb3fbb88",
            "isKey": false,
            "numCitedBy": 2575,
            "numCiting": 102,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a model that generates natural language descriptions of images and their regions. Our approach leverages datasets of images and their sentence descriptions to learn about the inter-modal correspondences between language and visual data. Our alignment model is based on a novel combination of Convolutional Neural Networks over image regions, bidirectional Recurrent Neural Networks (RNN) over sentences, and a structured objective that aligns the two modalities through a multimodal embedding. We then describe a Multimodal Recurrent Neural Network architecture that uses the inferred alignments to learn to generate novel descriptions of image regions. We demonstrate that our alignment model produces state of the art results in retrieval experiments on Flickr8K, Flickr30K and MSCOCO datasets. We then show that the generated descriptions outperform retrieval baselines on both full images and on a new dataset of region-level annotations. Finally, we conduct large-scale analysis of our RNN language model on the Visual Genome dataset of 4.1 million captions and highlight the differences between image and region-level caption statistics."
            },
            "slug": "Deep-Visual-Semantic-Alignments-for-Generating-Karpathy-Fei-Fei",
            "title": {
                "fragments": [],
                "text": "Deep Visual-Semantic Alignments for Generating Image Descriptions"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "A model that generates natural language descriptions of images and their regions based on a novel combination of Convolutional Neural Networks over image regions, bidirectional Recurrent Neural networks over sentences, and a structured objective that aligns the two modalities through a multimodal embedding is presented."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1846258"
                        ],
                        "name": "Noam M. Shazeer",
                        "slug": "Noam-M.-Shazeer",
                        "structuredName": {
                            "firstName": "Noam",
                            "lastName": "Shazeer",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noam M. Shazeer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1861312"
                        ],
                        "name": "Azalia Mirhoseini",
                        "slug": "Azalia-Mirhoseini",
                        "structuredName": {
                            "firstName": "Azalia",
                            "lastName": "Mirhoseini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Azalia Mirhoseini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50351613"
                        ],
                        "name": "Krzysztof Maziarz",
                        "slug": "Krzysztof-Maziarz",
                        "structuredName": {
                            "firstName": "Krzysztof",
                            "lastName": "Maziarz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Krzysztof Maziarz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36347083"
                        ],
                        "name": "Andy Davis",
                        "slug": "Andy-Davis",
                        "structuredName": {
                            "firstName": "Andy",
                            "lastName": "Davis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andy Davis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2827616"
                        ],
                        "name": "Quoc V. Le",
                        "slug": "Quoc-V.-Le",
                        "structuredName": {
                            "firstName": "Quoc",
                            "lastName": "Le",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Quoc V. Le"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48448318"
                        ],
                        "name": "J. Dean",
                        "slug": "J.-Dean",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Dean",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Dean"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 237,
                                "start": 217
                            }
                        ],
                        "text": "\u2026inference (Raphael, 2001) from graphical models.1 Sparse memory and conditional computation with neural networks have also been explored with various levels of success in several previous works (Bengio et al., 2015; Shazeer et al., 2017; Rae et al., 2016; Andrychowicz & Kurach, 2016)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12462234,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "510e26733aaff585d65701b9f1be7ca9d5afc586",
            "isKey": false,
            "numCitedBy": 862,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost."
            },
            "slug": "Outrageously-Large-Neural-Networks:-The-Layer-Shazeer-Mirhoseini",
            "title": {
                "fragments": [],
                "text": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work introduces a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks, and applies the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2503659"
                        ],
                        "name": "Jimmy Ba",
                        "slug": "Jimmy-Ba",
                        "structuredName": {
                            "firstName": "Jimmy",
                            "lastName": "Ba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jimmy Ba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3255983"
                        ],
                        "name": "Volodymyr Mnih",
                        "slug": "Volodymyr-Mnih",
                        "structuredName": {
                            "firstName": "Volodymyr",
                            "lastName": "Mnih",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Volodymyr Mnih"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2645384"
                        ],
                        "name": "K. Kavukcuoglu",
                        "slug": "K.-Kavukcuoglu",
                        "structuredName": {
                            "firstName": "Koray",
                            "lastName": "Kavukcuoglu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kavukcuoglu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 155
                            }
                        ],
                        "text": "For the second approach we use \u201chard\u201d attention for z\u2032t, an approach which has been shown to work in several image tasks (Xu et al., 2015; Mnih et al., 2014; Ba et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 122
                            }
                        ],
                        "text": "For the second approach we use \u201chard\u201d attention for z\u2032 t, an approach which has been shown to work in several image tasks (Xu et al., 2015; Mnih et al., 2014; Ba et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14814581,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7845f1d3e796b5704d4bd37a945e0cf3fb8bbf1f",
            "isKey": false,
            "numCitedBy": 848,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an attention-based model for recognizing multiple objects in images. The proposed model is a deep recurrent neural network trained with reinforcement learning to attend to the most relevant regions of the input image. We show that the model learns to both localize and recognize multiple objects despite being given only class labels during training. We evaluate the model on the challenging task of transcribing house number sequences from Google Street View images and show that it is both more accurate than the state-of-the-art convolutional networks and uses fewer parameters and less computation."
            },
            "slug": "Multiple-Object-Recognition-with-Visual-Attention-Ba-Mnih",
            "title": {
                "fragments": [],
                "text": "Multiple Object Recognition with Visual Attention"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "The model is a deep recurrent neural network trained with reinforcement learning to attend to the most relevant regions of the input image and it is shown that the model learns to both localize and recognize multiple objects despite being given only class labels during training."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34269227"
                        ],
                        "name": "Jack W. Rae",
                        "slug": "Jack-W.-Rae",
                        "structuredName": {
                            "firstName": "Jack",
                            "lastName": "Rae",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jack W. Rae"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2323922"
                        ],
                        "name": "Jonathan J. Hunt",
                        "slug": "Jonathan-J.-Hunt",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Hunt",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan J. Hunt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1841008"
                        ],
                        "name": "Ivo Danihelka",
                        "slug": "Ivo-Danihelka",
                        "structuredName": {
                            "firstName": "Ivo",
                            "lastName": "Danihelka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ivo Danihelka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3367786"
                        ],
                        "name": "Tim Harley",
                        "slug": "Tim-Harley",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Harley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tim Harley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33666044"
                        ],
                        "name": "A. Senior",
                        "slug": "A.-Senior",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Senior",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Senior"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "89504302"
                        ],
                        "name": "Greg Wayne",
                        "slug": "Greg-Wayne",
                        "structuredName": {
                            "firstName": "Greg",
                            "lastName": "Wayne",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Greg Wayne"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753223"
                        ],
                        "name": "A. Graves",
                        "slug": "A.-Graves",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Graves",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Graves"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2542999"
                        ],
                        "name": "T. Lillicrap",
                        "slug": "T.-Lillicrap",
                        "structuredName": {
                            "firstName": "Timothy",
                            "lastName": "Lillicrap",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Lillicrap"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 255,
                                "start": 239
                            }
                        ],
                        "text": "\u2026inference (Raphael, 2001) from graphical models.1 Sparse memory and conditional computation with neural networks have also been explored with various levels of success in several previous works (Bengio et al., 2015; Shazeer et al., 2017; Rae et al., 2016; Andrychowicz & Kurach, 2016)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11792642,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "be8c6c69f3e357bfad2987e45b62cff7e7474378",
            "isKey": false,
            "numCitedBy": 126,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Neural networks augmented with external memory have the ability to learn algorithmic solutions to complex tasks. These models appear promising for applications such as language modeling and machine translation. However, they scale poorly in both space and time as the amount of memory grows \u2014 limiting their applicability to real-world domains. Here, we present an end-to-end differentiable memory access scheme, which we call Sparse Access Memory (SAM), that retains the representational power of the original approaches whilst training efficiently with very large memories. We show that SAM achieves asymptotic lower bounds in space and time complexity, and find that an implementation runs 1,000 x faster and with 3,000x less physical memory than non-sparse models. SAM learns with comparable data efficiency to existing models on a range of synthetic tasks and one-shot Omniglot character recognition, and can scale to tasks requiring 100,000s of time steps and memories. As well, we show how our approach can be adapted for models that maintain temporal associations between memories, as with the recently introduced Differentiable Neural Computer."
            },
            "slug": "Scaling-Memory-Augmented-Neural-Networks-with-Reads-Rae-Hunt",
            "title": {
                "fragments": [],
                "text": "Scaling Memory-Augmented Neural Networks with Sparse Reads and Writes"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work presents an end-to-end differentiable memory access scheme, which they call Sparse Access Memory (SAM), that retains the representational power of the original approaches whilst training efficiently with very large memories, and achieves asymptotic lower bounds in space and time complexity."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3335364"
                        ],
                        "name": "Dzmitry Bahdanau",
                        "slug": "Dzmitry-Bahdanau",
                        "structuredName": {
                            "firstName": "Dzmitry",
                            "lastName": "Bahdanau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dzmitry Bahdanau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1979489"
                        ],
                        "name": "Kyunghyun Cho",
                        "slug": "Kyunghyun-Cho",
                        "structuredName": {
                            "firstName": "Kyunghyun",
                            "lastName": "Cho",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kyunghyun Cho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 41
                            }
                        ],
                        "text": "Standard Attention In standard attention (Bahdanau et al., 2014), we use a neural network to approximate the attention distribution p(zt):"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 42
                            }
                        ],
                        "text": "Standard Attention In standard attention (Bahdanau et al., 2014), we use a neural network to approximate the attention distribution p(zt):\np(zt) = softmax(a(ht, {Vhw}))\nwhere a(\u00b7) is a neural network to produce unnormalized attention weights."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11212020,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5",
            "isKey": false,
            "numCitedBy": 19339,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition."
            },
            "slug": "Neural-Machine-Translation-by-Jointly-Learning-to-Bahdanau-Cho",
            "title": {
                "fragments": [],
                "text": "Neural Machine Translation by Jointly Learning to Align and Translate"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and it is proposed to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1750931"
                        ],
                        "name": "P. Nagabhushan",
                        "slug": "P.-Nagabhushan",
                        "structuredName": {
                            "firstName": "Panduranga",
                            "lastName": "Nagabhushan",
                            "middleNames": [
                                "Naidu"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Nagabhushan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153917971"
                        ],
                        "name": "A. Alaei",
                        "slug": "A.-Alaei",
                        "structuredName": {
                            "firstName": "Ali",
                            "lastName": "Alaei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Alaei"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14778337,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "60a88f65ebdbdb316ca8c07fd491e75a77b38a10",
            "isKey": false,
            "numCitedBy": 27,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "In this research work, we propose to identify an imaginary line called baseline threading through the entire stretch of text-line, with reference to which the location of vertical extents of Persian characters could be accurately interpreted. Depending upon the curvedness of the handwritten Persian text-line the baseline also would be curved. In this research a novel piece-wise painting scheme is proposed to prepare patches of black and white blocks all along the text- line, identify some candidate points, regress a curve through these candidate points to trace the baseline which is subsequently stretched straight horizontally and subsequently we de-tilt the characters to align the text-line with the horizontal imaginary baseline properly. The proposed algorithm is evaluated with 108 Persian handwritten text-lines containing 3612 subwords. Experimental analysis showed that 91.2% of the subwords are accurately aligned. Further, the proposed scheme is tested with another dataset containing 600 text-lines (13) and more accurate results are achieved when compared with the results reported in state of the art for the same dataset. The effectiveness of aligning text-lines linearly is demonstrated through OCRing for readability of tilted printed English text-lines and corresponding transformed text-lines, which are obtained using the proposed procedure."
            },
            "slug": "Tracing-and-straightening-the-baseline-in-A-new-on-Nagabhushan-Alaei",
            "title": {
                "fragments": [],
                "text": "Tracing and straightening the baseline in handwritten persian/arabic text-line: A new approach based on painting-technique"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "In this research a novel piece-wise painting scheme is proposed to prepare patches of black and white blocks all along the text- line, identify some candidate points, regress a curve through these candidate points to trace the baseline which is subsequently stretched straight horizontally and subsequently the authors de-tilt the characters to align thetext-line with the horizontal imaginary baseline properly."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145644643"
                        ],
                        "name": "Andr\u00e9 F. T. Martins",
                        "slug": "Andr\u00e9-F.-T.-Martins",
                        "structuredName": {
                            "firstName": "Andr\u00e9",
                            "lastName": "Martins",
                            "middleNames": [
                                "F.",
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andr\u00e9 F. T. Martins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3394760"
                        ],
                        "name": "Ram\u00f3n Fern\u00e1ndez Astudillo",
                        "slug": "Ram\u00f3n-Fern\u00e1ndez-Astudillo",
                        "structuredName": {
                            "firstName": "Ram\u00f3n",
                            "lastName": "Astudillo",
                            "middleNames": [
                                "Fern\u00e1ndez"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ram\u00f3n Fern\u00e1ndez Astudillo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 0
                            }
                        ],
                        "text": "Sparsemax achieves higher accuracy, at the cost of selecting multiple coarse cells."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 112
                            }
                        ],
                        "text": ", 2015] to select a single coarse cell, the presented model SPARSEMAX: use sparse activation function Sparsemax [Martins and Astudillo, 2016] instead of Softmax to select multiple coarse cells Y Deng, A Kanervisto, J Ling, A Rush Image-to-Markup Generation 12 / 20"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16432551,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c1e3a26fb88c6720f4e84b7118e6f2df7dc8efa3",
            "isKey": false,
            "numCitedBy": 374,
            "numCiting": 69,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose sparsemax, a new activation function similar to the traditional softmax, but able to output sparse probabilities. After deriving its properties, we show how its Jacobian can be efficiently computed, enabling its use in a network trained with backpropagation. Then, we propose a new smooth and convex loss function which is the sparsemax analogue of the logistic loss. We reveal an unexpected connection between this new loss and the Huber classification loss. We obtain promising empirical results in multi-label classification problems and in attention-based neural networks for natural language inference. For the latter, we achieve a similar performance as the traditional softmax, but with a selective, more compact, attention focus."
            },
            "slug": "From-Softmax-to-Sparsemax:-A-Sparse-Model-of-and-Martins-Astudillo",
            "title": {
                "fragments": [],
                "text": "From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "Sparsemax, a new activation function similar to the traditional softmax, but able to output sparse probabilities, is proposed, and an unexpected connection between this new loss and the Huber classification loss is revealed."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054165706"
                        ],
                        "name": "S. Ioffe",
                        "slug": "S.-Ioffe",
                        "structuredName": {
                            "firstName": "Sergey",
                            "lastName": "Ioffe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ioffe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2574060"
                        ],
                        "name": "Christian Szegedy",
                        "slug": "Christian-Szegedy",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Szegedy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian Szegedy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "sed by Shi et al. (2015) for OCR from images (speci\ufb01cation is given in Table 2.) Unlike some recent OCR work (Jaderberg et al. 2015; Lee and Osindero 2016), we do not use \ufb01nal fully-connected layers (Ioffe and Szegedy 2015), since we want to preserve the locality of CNN features in order to use visual attention. The CNN takes the raw input RH W and produces a feature grid V of size D 0H W0, where cdenotes the number of "
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5808102,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4d376d6978dad0374edfa6709c9556b42d3594d3",
            "isKey": false,
            "numCitedBy": 29233,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters."
            },
            "slug": "Batch-Normalization:-Accelerating-Deep-Network-by-Ioffe-Szegedy",
            "title": {
                "fragments": [],
                "text": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790706"
                        ],
                        "name": "H. Mouch\u00e8re",
                        "slug": "H.-Mouch\u00e8re",
                        "structuredName": {
                            "firstName": "Harold",
                            "lastName": "Mouch\u00e8re",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Mouch\u00e8re"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398288037"
                        ],
                        "name": "C. Viard-Gaudin",
                        "slug": "C.-Viard-Gaudin",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Viard-Gaudin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Viard-Gaudin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793699"
                        ],
                        "name": "R. Zanibbi",
                        "slug": "R.-Zanibbi",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Zanibbi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Zanibbi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804457"
                        ],
                        "name": "Utpal Garain",
                        "slug": "Utpal-Garain",
                        "structuredName": {
                            "firstName": "Utpal",
                            "lastName": "Garain",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Utpal Garain"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 123
                            }
                        ],
                        "text": "Copyright 2017 by the author(s).\ntems, have competed on this task as part of the CROHME handwritten mathematics challenge (Mouchere et al., 2013; 2014)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17092998,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6dd017e25f627b57ea27c935d3deb3c4b0c7f5f9",
            "isKey": false,
            "numCitedBy": 67,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We present the outcome of the latest edition of the CROHME competition, dedicated to on-line handwritten mathematical expression recognition. In addition to the standard full expression recognition task from previous competitions, CROHME 2014 features two new tasks. The first is dedicated to isolated symbol recognition including a reject option for invalid symbol hypotheses, and the second concerns recognizing expressions that contain matrices. System performance is improving relative to previous competitions. Data and evaluation tools used for the competition are publicly available."
            },
            "slug": "ICFHR-2014-Competition-on-Recognition-of-On-Line-Mouch\u00e8re-Viard-Gaudin",
            "title": {
                "fragments": [],
                "text": "ICFHR 2014 Competition on Recognition of On-Line Handwritten Mathematical Expressions (CROHME 2014)"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "The outcome of the latest edition of the CROHME competition, dedicated to on-line handwritten mathematical expression recognition, features two new tasks, one dedicated to isolated symbol recognition including a reject option for invalid symbol hypotheses, and the second concerns recognizing expressions that contain matrices."
            },
            "venue": {
                "fragments": [],
                "text": "2014 14th International Conference on Frontiers in Handwriting Recognition"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753223"
                        ],
                        "name": "A. Graves",
                        "slug": "A.-Graves",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Graves",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Graves"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143913738"
                        ],
                        "name": "Santiago Fern\u00e1ndez",
                        "slug": "Santiago-Fern\u00e1ndez",
                        "structuredName": {
                            "firstName": "Santiago",
                            "lastName": "Fern\u00e1ndez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Santiago Fern\u00e1ndez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145842938"
                        ],
                        "name": "Faustino J. Gomez",
                        "slug": "Faustino-J.-Gomez",
                        "structuredName": {
                            "firstName": "Faustino",
                            "lastName": "Gomez",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Faustino J. Gomez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9901844,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "96494e722f58705fa20302fe6179d483f52705b4",
            "isKey": false,
            "numCitedBy": 3476,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Many real-world sequence learning tasks require the prediction of sequences of labels from noisy, unsegmented input data. In speech recognition, for example, an acoustic signal is transcribed into words or sub-word units. Recurrent neural networks (RNNs) are powerful sequence learners that would seem well suited to such tasks. However, because they require pre-segmented training data, and post-processing to transform their outputs into label sequences, their applicability has so far been limited. This paper presents a novel method for training RNNs to label unsegmented sequences directly, thereby solving both problems. An experiment on the TIMIT speech corpus demonstrates its advantages over both a baseline HMM and a hybrid HMM-RNN."
            },
            "slug": "Connectionist-temporal-classification:-labelling-Graves-Fern\u00e1ndez",
            "title": {
                "fragments": [],
                "text": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper presents a novel method for training RNNs to label unsegmented sequences directly, thereby solving both problems of sequence learning and post-processing."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2416433"
                        ],
                        "name": "Emmanuel Bengio",
                        "slug": "Emmanuel-Bengio",
                        "structuredName": {
                            "firstName": "Emmanuel",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Emmanuel Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145180695"
                        ],
                        "name": "Pierre-Luc Bacon",
                        "slug": "Pierre-Luc-Bacon",
                        "structuredName": {
                            "firstName": "Pierre-Luc",
                            "lastName": "Bacon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pierre-Luc Bacon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145134886"
                        ],
                        "name": "Joelle Pineau",
                        "slug": "Joelle-Pineau",
                        "structuredName": {
                            "firstName": "Joelle",
                            "lastName": "Pineau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joelle Pineau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144368601"
                        ],
                        "name": "Doina Precup",
                        "slug": "Doina-Precup",
                        "structuredName": {
                            "firstName": "Doina",
                            "lastName": "Precup",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Doina Precup"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 146
                            }
                        ],
                        "text": "1 Sparse memory and conditional computation with neural networks have also been explored with various levels of success in several previous works (Bengio et al., 2015; Shazeer et al., 2017; Rae et al., 2016; Andrychowicz & Kurach, 2016)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 215,
                                "start": 196
                            }
                        ],
                        "text": "\u2026inference (Raphael, 2001) from graphical models.1 Sparse memory and conditional computation with neural networks have also been explored with various levels of success in several previous works (Bengio et al., 2015; Shazeer et al., 2017; Rae et al., 2016; Andrychowicz & Kurach, 2016)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16049527,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fba71eefd060e30f3516fdd46df9a191cd0aaaf7",
            "isKey": false,
            "numCitedBy": 198,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Deep learning has become the state-of-art tool in many applications, but the evaluation and training of deep models can be time-consuming and computationally expensive. The conditional computation approach has been proposed to tackle this problem (Bengio et al., 2013; Davis & Arel, 2013). It operates by selectively activating only parts of the network at a time. In this paper, we use reinforcement learning as a tool to optimize conditional computation policies. More specifically, we cast the problem of learning activation-dependent policies for dropping out blocks of units as a reinforcement learning problem. We propose a learning scheme motivated by computation speed, capturing the idea of wanting to have parsimonious activations while maintaining prediction accuracy. We apply a policy gradient algorithm for learning policies that optimize this loss function and propose a regularization mechanism that encourages diversification of the dropout policy. We present encouraging empirical results showing that this approach improves the speed of computation without impacting the quality of the approximation."
            },
            "slug": "Conditional-Computation-in-Neural-Networks-for-Bengio-Bacon",
            "title": {
                "fragments": [],
                "text": "Conditional Computation in Neural Networks for faster models"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper applies a policy gradient algorithm for learning policies that optimize this loss function and proposes a regularization mechanism that encourages diversification of the dropout policy and presents encouraging empirical results showing that this approach improves the speed of computation without impacting the quality of the approximation."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2354728"
                        ],
                        "name": "A. Karpathy",
                        "slug": "A.-Karpathy",
                        "structuredName": {
                            "firstName": "Andrej",
                            "lastName": "Karpathy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Karpathy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115231104"
                        ],
                        "name": "Justin Johnson",
                        "slug": "Justin-Johnson",
                        "structuredName": {
                            "firstName": "Justin",
                            "lastName": "Johnson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Justin Johnson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 988348,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "40be3888daa5c2e5af4d36ae22f690bcc8caf600",
            "isKey": false,
            "numCitedBy": 911,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "Recurrent Neural Networks (RNNs), and specifically a variant with Long Short-Term Memory (LSTM), are enjoying renewed interest as a result of successful applications in a wide range of machine learning problems that involve sequential data. However, while LSTMs provide exceptional results in practice, the source of their performance and their limitations remain rather poorly understood. Using character-level language models as an interpretable testbed, we aim to bridge this gap by providing an analysis of their representations, predictions and error types. In particular, our experiments reveal the existence of interpretable cells that keep track of long-range dependencies such as line lengths, quotes and brackets. Moreover, our comparative analysis with finite horizon n-gram models traces the source of the LSTM improvements to long-range structural dependencies. Finally, we provide analysis of the remaining errors and suggests areas for further study."
            },
            "slug": "Visualizing-and-Understanding-Recurrent-Networks-Karpathy-Johnson",
            "title": {
                "fragments": [],
                "text": "Visualizing and Understanding Recurrent Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work uses character-level language models as an interpretable testbed to provide an analysis of LSTM representations, predictions and error types, and reveals the existence of interpretable cells that keep track of long-range dependencies such as line lengths, quotes and brackets."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3308557"
                        ],
                        "name": "S. Hochreiter",
                        "slug": "S.-Hochreiter",
                        "structuredName": {
                            "firstName": "Sepp",
                            "lastName": "Hochreiter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Hochreiter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 104
                            }
                        ],
                        "text": "In practice there are many different variants of RNN; however, long short-term memory networks (LSTMs) (Hochreiter & Schmidhuber, 1997) have been shown to be very effective for most NLP tasks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1915014,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "44d2abe2175df8153f465f6c39b68b76a0d40ab9",
            "isKey": false,
            "numCitedBy": 51691,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms."
            },
            "slug": "Long-Short-Term-Memory-Hochreiter-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Long Short-Term Memory"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A novel, efficient, gradient based method called long short-term memory (LSTM) is introduced, which can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689108"
                        ],
                        "name": "Oriol Vinyals",
                        "slug": "Oriol-Vinyals",
                        "structuredName": {
                            "firstName": "Oriol",
                            "lastName": "Vinyals",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oriol Vinyals"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40527594"
                        ],
                        "name": "Lukasz Kaiser",
                        "slug": "Lukasz-Kaiser",
                        "structuredName": {
                            "firstName": "Lukasz",
                            "lastName": "Kaiser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lukasz Kaiser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2060101052"
                        ],
                        "name": "Terry Koo",
                        "slug": "Terry-Koo",
                        "structuredName": {
                            "firstName": "Terry",
                            "lastName": "Koo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Terry Koo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1754497"
                        ],
                        "name": "Slav Petrov",
                        "slug": "Slav-Petrov",
                        "structuredName": {
                            "firstName": "Slav",
                            "lastName": "Petrov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Slav Petrov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701686"
                        ],
                        "name": "Ilya Sutskever",
                        "slug": "Ilya-Sutskever",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Sutskever",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilya Sutskever"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 249,
                                "start": 229
                            }
                        ],
                        "text": "For instance, advances have been made in the areas of handwriting recognition (Ciresan et al., 2010), OCR in natural scenes (Jaderberg et al., 2015; 2016; Wang et al., 2012) and image caption generation (Karpathy & FeiFei, 2015; Vinyals et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "sual grammar of the underlying markup language. While results from language modeling suggest that neural models can consistently generate syntactically correct markup (Karpathy, Johnson, and Li 2015; Vinyals et al. 2015a), it is unclear whether the full solution can be learned from markup-image pairs. Our model, WYGIWYS [What you get is what you see], is a simple extension of the attention-based encoder-decoder mode"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "rg). All rights reserved. handwriting recognition (Ciresan et al. 2010), OCR in natural scenes (Jaderberg et al. 2015; 2016; Wang et al. 2012) and image caption generation (Karpathy and Fei-Fei 2015; Vinyals et al. 2015b). At a high-level, each of these systems learns an abstract encoded representation of the input image which is then decoded to generate a textual output. In addition to performing quite well on stan"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14223,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "47570e7f63e296f224a0e7f9a0d08b0de3cbaf40",
            "isKey": false,
            "numCitedBy": 845,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Syntactic constituency parsing is a fundamental problem in natural language processing and has been the subject of intensive research and engineering for decades. As a result, the most accurate parsers are domain specific, complex, and inefficient. In this paper we show that the domain agnostic attention-enhanced sequence-to-sequence model achieves state-of-the-art results on the most widely used syntactic constituency parsing dataset, when trained on a large synthetic corpus that was annotated using existing parsers. It also matches the performance of standard parsers when trained only on a small human-annotated dataset, which shows that this model is highly data-efficient, in contrast to sequence-to-sequence models without the attention mechanism. Our parser is also fast, processing over a hundred sentences per second with an unoptimized CPU implementation."
            },
            "slug": "Grammar-as-a-Foreign-Language-Vinyals-Kaiser",
            "title": {
                "fragments": [],
                "text": "Grammar as a Foreign Language"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The domain agnostic attention-enhanced sequence-to-sequence model achieves state-of-the-art results on the most widely used syntactic constituency parsing dataset, when trained on a large synthetic corpus that was annotated using existing parsers."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "41224113"
                        ],
                        "name": "E. Miller",
                        "slug": "E.-Miller",
                        "structuredName": {
                            "firstName": "Erik",
                            "lastName": "Miller",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Miller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731948"
                        ],
                        "name": "Paul A. Viola",
                        "slug": "Paul-A.-Viola",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Viola",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paul A. Viola"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": " and nested fractions (Belaid and Haton 1984; Chan and Yeung 2000). The most effective systems combine specialized character segmentation with grammars of the underlying mathematical layout language (Miller and Viola 1998). A prime example of this approach is the INFTY system that is used to convert printed mathematical expressions to LaTeX and other markup formats (Suzuki et al. 2003). Problems like OCR that require j"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5357113,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "99b0b8f71c93aa316ef5014c015eddda269565bc",
            "isKey": false,
            "numCitedBy": 98,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of recognizing mathematical expressions differs significantly from the recognition of standard prose. While in prose significant constraints can be put on the interpretation of a character by the characters immediately preceding and following it, few such simple constraints are present in a mathematical expression. In order to make the problem tractable, effective methods of recognizing mathematical expressions will need to put intelligent constraints on the possible interpretations. The authors present preliminary results on a system for the recognition of both handwritten and typeset mathematical expressions. While previous systems perform character recognition out of context, the current system maintains ambiguity of the characters until context can be used to disambiguate the interpretation. In addition, the system limits the number of potentially valid interpretations by decomposing the expressions into a sequence of compatible convex regions. The system uses A-star to search for the best possible interpretation of an expression. We provide a new lower bound estimate on the cost to goal that improves performance significantly."
            },
            "slug": "Ambiguity-and-Constraint-in-Mathematical-Expression-Miller-Viola",
            "title": {
                "fragments": [],
                "text": "Ambiguity and Constraint in Mathematical Expression Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A new lower bound estimate on the cost to goal that improves performance significantly is provided and the system limits the number of potentially valid interpretations by decomposing the expressions into a sequence of compatible convex regions."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI/IAAI"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144301565"
                        ],
                        "name": "R. Dale",
                        "slug": "R.-Dale",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Dale",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Dale"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745798"
                        ],
                        "name": "Barbara Maria Di Eugenio",
                        "slug": "Barbara-Maria-Di-Eugenio",
                        "structuredName": {
                            "firstName": "Barbara",
                            "lastName": "Di Eugenio",
                            "middleNames": [
                                "Maria"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Barbara Maria Di Eugenio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145817168"
                        ],
                        "name": "D. Scott",
                        "slug": "D.-Scott",
                        "structuredName": {
                            "firstName": "Donia",
                            "lastName": "Scott",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Scott"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1891955,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "b0fc5112498cb718ecc5790d919a90320b908242",
            "isKey": false,
            "numCitedBy": 27,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "There are two sides to natural language processing. On the one hand, work in natural language understanding is concerned with the mapping from some surface representation of linguistic material expressed as speech or text--to an underlying representation of the meaning carried by that surface representation. But there is also the question of how one maps from some underlying representation of meaning into text or speech: this is the domain of natural language generation. Whether our end-goal is the construction of artifacts that use natural languages intelligently, the formal characterization of phenomena in human languages, or the computational modeling of the human language processing mechanism, we cannot ignore the fact that language is both spoken (or written) and heard (or read). Both are equally large and important problems, but the literature contains much less work on natural language generation (NLG) than it does on natural language understanding (NLU). There are many reasons why this might be so, although clearly an important one is that researchers in natural language understanding in some sense start out with a more well-defined task: the input is known, and there is a lot of it around. This is not the case in natural language generation: there, it is the desired output that is known, but the input is an unknown; and while the world is awash with text waiting to be processed, there are fewer instances of what we might consider appropriate inputs for the process of natural language generation. For researchers in the field, this highlights the fundamental question that always has to be asked: What do we generate from? Despite this problem, the natural language generation community is a thriving one, with a research base that has been developing steadily--although perhaps at a slower pace because of the smaller size of the community--for just as long as work in natural language understanding. It should not be forgotten that much of NLP has its origins in the early work on machine translation in the 1950s; and that to carry out machine translation, one has to not only analyze existing texts but also to generate new ones. The early machine translation experiments, however, did not recognize the problems that give modern work in NLG its particular character. The first significant pieces of work in the field appeared during the 1970s; in particular, Goldman's work on the problem of lexicalizing underlying conceptual material (Goldman 1974) and"
            },
            "slug": "Introduction-to-the-Special-Issue-on-Natural-Dale-Eugenio",
            "title": {
                "fragments": [],
                "text": "Introduction to the Special Issue on Natural Language Generation"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The natural language generation community is a thriving one, with a research base that has been developing steadily--although perhaps at a slower pace because of the smaller size of the community--for just as long as work in natural language understanding."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790706"
                        ],
                        "name": "H. Mouch\u00e8re",
                        "slug": "H.-Mouch\u00e8re",
                        "structuredName": {
                            "firstName": "Harold",
                            "lastName": "Mouch\u00e8re",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Mouch\u00e8re"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398288037"
                        ],
                        "name": "C. Viard-Gaudin",
                        "slug": "C.-Viard-Gaudin",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Viard-Gaudin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Viard-Gaudin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793699"
                        ],
                        "name": "R. Zanibbi",
                        "slug": "R.-Zanibbi",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Zanibbi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Zanibbi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804457"
                        ],
                        "name": "Utpal Garain",
                        "slug": "Utpal-Garain",
                        "structuredName": {
                            "firstName": "Utpal",
                            "lastName": "Garain",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Utpal Garain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111889389"
                        ],
                        "name": "Dae Hwan Kim",
                        "slug": "Dae-Hwan-Kim",
                        "structuredName": {
                            "firstName": "Dae",
                            "lastName": "Kim",
                            "middleNames": [
                                "Hwan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dae Hwan Kim"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 123
                            }
                        ],
                        "text": "Copyright 2017 by the author(s).\ntems, have competed on this task as part of the CROHME handwritten mathematics challenge (Mouchere et al., 2013; 2014)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 89
                            }
                        ],
                        "text": "tems, have competed on this task as part of the CROHME handwritten mathematics challenge (Mouchere et al., 2013; 2014)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8469758,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3e476b49665b7414736c371c307b4eda7466902e",
            "isKey": false,
            "numCitedBy": 44,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We report on the third international Competition on Handwritten Mathematical Expression Recognition (CROHME), in which eight teams from academia and industry took part. For the third CROHME, the training dataset was expanded to over 8000 expressions, and new tools were developed for evaluating performance at the level of strokes as well as expressions and symbols. As an informal measure of progress, the performance of the participating systems on the CROHME 2012 data set is also reported. Data and tools used for the competition will be made publicly available."
            },
            "slug": "ICDAR-2013-CROHME:-Third-International-Competition-Mouch\u00e8re-Viard-Gaudin",
            "title": {
                "fragments": [],
                "text": "ICDAR 2013 CROHME: Third International Competition on Recognition of Online Handwritten Mathematical Expressions"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "For the third CROHME, the training dataset was expanded to over 8000 expressions, and new tools were developed for evaluating performance at the level of strokes as well as expressions and symbols."
            },
            "venue": {
                "fragments": [],
                "text": "2013 12th International Conference on Document Analysis and Recognition"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790706"
                        ],
                        "name": "H. Mouch\u00e8re",
                        "slug": "H.-Mouch\u00e8re",
                        "structuredName": {
                            "firstName": "Harold",
                            "lastName": "Mouch\u00e8re",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Mouch\u00e8re"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398288037"
                        ],
                        "name": "C. Viard-Gaudin",
                        "slug": "C.-Viard-Gaudin",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Viard-Gaudin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Viard-Gaudin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111889389"
                        ],
                        "name": "Dae Hwan Kim",
                        "slug": "Dae-Hwan-Kim",
                        "structuredName": {
                            "firstName": "Dae",
                            "lastName": "Kim",
                            "middleNames": [
                                "Hwan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dae Hwan Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152672892"
                        ],
                        "name": "J. H. Kim",
                        "slug": "J.-H.-Kim",
                        "structuredName": {
                            "firstName": "Jin",
                            "lastName": "Kim",
                            "middleNames": [
                                "Hyung"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. H. Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804457"
                        ],
                        "name": "Utpal Garain",
                        "slug": "Utpal-Garain",
                        "structuredName": {
                            "firstName": "Utpal",
                            "lastName": "Garain",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Utpal Garain"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "To generate markup from unseen images, we simply use beam search at test time with no further hard constraints. Data While there are some datasets available for the image-tomarkup generation problem (Mouchere et al. 2012; 2013; Figure 2: Network structure of WYGIWYS. Given an input image, a CNN is applied to extract visual features, then for each row in the \ufb01nal feature map we employ an RNN encoder. The encoded featu"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16296389,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f0d7f19c5eebecb005641f753fbb3beb1f57a7aa",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents an overview of the second Competition on Recognition of Online Handwritten Mathematical Expressions, CROHME 2012. The objective of the contest is to identify current advances in mathematical expression recognition using common evaluation performance measures and datasets. This paper describes the contest details including the evaluation measures used as well as the performance of the 7 submitted systems along with a short description of each system. Progress as compared to the 1st version of CROHME is also documented."
            },
            "slug": "ICFHR-2012-Competition-on-Recognition-of-On-Line-Mouch\u00e8re-Viard-Gaudin",
            "title": {
                "fragments": [],
                "text": "ICFHR 2012 Competition on Recognition of On-Line Mathematical Expressions (CROHME 2012)"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "The contest details including the evaluation measures used as well as the performance of the 7 submitted systems are described along with a short description of each system."
            },
            "venue": {
                "fragments": [],
                "text": "2012 International Conference on Frontiers in Handwriting Recognition"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "122800532"
                        ],
                        "name": "Christopher Raphael",
                        "slug": "Christopher-Raphael",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Raphael",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher Raphael"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16596302,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bf2451245b9d4c1c8051fbb547f9de811e4a54b6",
            "isKey": false,
            "numCitedBy": 50,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce an extension of dynamic programming, we call \"coarse-to-fine dynamic programming\" (CFDP), ideally suited to DP problems with large state space. CFDP uses dynamic programming to solve a sequence of coarse approximations which are lower bounds to the original DP problem. These approximations are developed by merging states in the original graph into \"superstates\" in a coarser graph which uses an optimistic arc cost between superstates. The approximations are designed so that CFDP terminates when the optimal path through the original state graph has been found. CFDP leads to significant decreases in the amount of computation necessary to solve many DP problems and can, in some instances, make otherwise infeasible computations possible. CFDP generalizes to DP problems with continuous state space and we offer a convergence result for this extension. We demonstrate applications of this technique to optimization of functions and boundary estimation in mine recognition."
            },
            "slug": "Coarse-to-Fine-Dynamic-Programming-Raphael",
            "title": {
                "fragments": [],
                "text": "Coarse-to-Fine Dynamic Programming"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "Coarse-to-fine dynamic programming (CFDP), ideally suited to DP problems with large state space, is introduced and applications of this technique to optimization of functions and boundary estimation in mine recognition are demonstrated."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2372756"
                        ],
                        "name": "A. Belaid",
                        "slug": "A.-Belaid",
                        "structuredName": {
                            "firstName": "Abdel",
                            "lastName": "Belaid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Belaid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693362"
                        ],
                        "name": "J. Haton",
                        "slug": "J.-Haton",
                        "structuredName": {
                            "firstName": "Jean",
                            "lastName": "Haton",
                            "middleNames": [
                                "Paul"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Haton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "al semantics. The primary focus of this work is OCR for mathematical expressions, and how to handle presentational aspects such as sub and superscript notation, special symbols, and nested fractions (Belaid and Haton 1984; Chan and Yeung 2000). The most effective systems combine specialized character segmentation with grammars of the underlying mathematical layout language (Miller and Viola 1998). A prime example of t"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6701404,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "83efd22c08e048ef47b04b9e35e0c1b35838ed9d",
            "isKey": false,
            "numCitedBy": 128,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Mathematical formulas are good examples of two-dimensional patterns as well as pictures or graphics. The use of syntactic methods is useful for interpreting such complex patterns. In this paper we propose a system for the interpretation of 2-D mathematic formulas based on a syntactic parser. This system is able to recognize a large class of 2-D mathematic formulas written on a graphic tablet. It starts the parsing by localization of the ``principal'' operator in the formula and attempts to partition it into subexpressions which are similarly analyzed by looking for a starting character. The generalized parser used in the system has been developed in our group for continuous speech recognition and picture interpretation."
            },
            "slug": "A-Syntactic-Approach-for-Handwritten-Mathematical-Belaid-Haton",
            "title": {
                "fragments": [],
                "text": "A Syntactic Approach for Handwritten Mathematical Formula Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This system is able to recognize a large class of 2-D mathematic formulas written on a graphic tablet and starts the parsing by localization of the ``principal'' operator in the formula and attempts to partition it into subexpressions which are similarly analyzed by looking for a starting character."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145568856"
                        ],
                        "name": "Robert H. Anderson",
                        "slug": "Robert-H.-Anderson",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Anderson",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robert H. Anderson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 32254067,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5b689c57ae22b0a9835f9a550c84b724e7f76038",
            "isKey": false,
            "numCitedBy": 160,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Research in the real-time recognition of hand-printed characters [1--5] offers the possibility of drawing mathematical expressions on a RAND Tablet [6] or similar input device, and obtaining a list of the characters and their positions in an x-, y-coordinate system. This paper discusses the use of a set of replacement rules to recognize, or \"parse,\" such two-dimensional configurations of characters. The replacement rules might be considered to be a generalization of the context-free Backus Normal Form rules used to describe a class of syntaxes for character strings."
            },
            "slug": "Syntax-directed-recognition-of-hand-printed-Anderson",
            "title": {
                "fragments": [],
                "text": "Syntax-directed recognition of hand-printed two-dimensional mathematics"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The use of a set of replacement rules to recognize, or \"parse,\" such two-dimensional configurations of characters in an x-, y-coordinate system is discussed."
            },
            "venue": {
                "fragments": [],
                "text": "Symposium on Interactive Systems for Experimental Applied Mathematics"
            },
            "year": 1967
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47971768"
                        ],
                        "name": "J. Schulman",
                        "slug": "J.-Schulman",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Schulman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schulman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2801204"
                        ],
                        "name": "N. Heess",
                        "slug": "N.-Heess",
                        "structuredName": {
                            "firstName": "Nicolas",
                            "lastName": "Heess",
                            "middleNames": [
                                "Manfred",
                                "Otto"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Heess"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143947744"
                        ],
                        "name": "T. Weber",
                        "slug": "T.-Weber",
                        "structuredName": {
                            "firstName": "Th\u00e9ophane",
                            "lastName": "Weber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Weber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689992"
                        ],
                        "name": "P. Abbeel",
                        "slug": "P.-Abbeel",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Abbeel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Abbeel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 66
                            }
                        ],
                        "text": "This gives us an unbiased estimate of the loss function gradient (Schulman et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7125379,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "438bb3d46e72b177ed1c9b7cd2c11a045644a1f4",
            "isKey": false,
            "numCitedBy": 296,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "In a variety of problems originating in supervised, unsupervised, and reinforcement learning, the loss function is defined by an expectation over a collection of random variables, which might be part of a probabilistic model or the external world. Estimating the gradient of this loss function, using samples, lies at the core of gradient-based learning algorithms for these problems. We introduce the formalism of stochastic computation graphs\u2014directed acyclic graphs that include both deterministic functions and conditional probability distributions\u2014and describe how to easily and automatically derive an unbiased estimator of the loss function's gradient. The resulting algorithm for computing the gradient estimator is a simple modification of the standard backpropagation algorithm. The generic scheme we propose unifies estimators derived in variety of prior work, along with variance-reduction techniques therein. It could assist researchers in developing intricate models involving a combination of stochastic and deterministic operations, enabling, for example, attention, memory, and control actions."
            },
            "slug": "Gradient-Estimation-Using-Stochastic-Computation-Schulman-Heess",
            "title": {
                "fragments": [],
                "text": "Gradient Estimation Using Stochastic Computation Graphs"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work introduces the formalism of stochastic computation graphs\u2014directed acyclic graphs that include both deterministic functions and conditional probability distributions\u2014and describes how to easily and automatically derive an unbiased estimator of the loss function's gradient."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39861444"
                        ],
                        "name": "G. Klein",
                        "slug": "G.-Klein",
                        "structuredName": {
                            "firstName": "Guillaume",
                            "lastName": "Klein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Klein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152847918"
                        ],
                        "name": "Yoon Kim",
                        "slug": "Yoon-Kim",
                        "structuredName": {
                            "firstName": "Yoon",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoon Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2505751"
                        ],
                        "name": "Yuntian Deng",
                        "slug": "Yuntian-Deng",
                        "structuredName": {
                            "firstName": "Yuntian",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuntian Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3053934"
                        ],
                        "name": "Jean Senellart",
                        "slug": "Jean-Senellart",
                        "structuredName": {
                            "firstName": "Jean",
                            "lastName": "Senellart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jean Senellart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2531268"
                        ],
                        "name": "Alexander M. Rush",
                        "slug": "Alexander-M.-Rush",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Rush",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander M. Rush"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 86
                            }
                        ],
                        "text": "The system is built using Torch (Collobert et al., 2011) based on the OpenNMT system (Klein et al., 2017)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 36
                            }
                        ],
                        "text": ", 2011) based on the OpenNMT system (Klein et al., 2017)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16538528,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aab5002a22b9b4244a8329b140bd0a86021aa2d1",
            "isKey": false,
            "numCitedBy": 1397,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe an open-source toolkit for neural machine translation (NMT). The toolkit prioritizes efficiency, modularity, and extensibility with the goal of supporting NMT research into model architectures, feature representations, and source modalities, while maintaining competitive performance and reasonable training requirements. The toolkit consists of modeling and translation support, as well as detailed pedagogical documentation about the underlying techniques."
            },
            "slug": "OpenNMT:-Open-Source-Toolkit-for-Neural-Machine-Klein-Kim",
            "title": {
                "fragments": [],
                "text": "OpenNMT: Open-Source Toolkit for Neural Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "The toolkit prioritizes efficiency, modularity, and extensibility with the goal of supporting NMT research into model architectures, feature representations, and source modalities, while maintaining competitive performance and reasonable training requirements."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3285362"
                        ],
                        "name": "Kam-Fai Chan",
                        "slug": "Kam-Fai-Chan",
                        "structuredName": {
                            "firstName": "Kam-Fai",
                            "lastName": "Chan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kam-Fai Chan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739816"
                        ],
                        "name": "D. Yeung",
                        "slug": "D.-Yeung",
                        "structuredName": {
                            "firstName": "Dit-Yan",
                            "lastName": "Yeung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Yeung"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "ry focus of this work is OCR for mathematical expressions, and how to handle presentational aspects such as sub and superscript notation, special symbols, and nested fractions (Belaid and Haton 1984; Chan and Yeung 2000). The most effective systems combine specialized character segmentation with grammars of the underlying mathematical layout language (Miller and Viola 1998). A prime example of this approach is the IN"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16156397,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a6112762f45f6b277a4d084561bc3bb498b562ed",
            "isKey": false,
            "numCitedBy": 301,
            "numCiting": 80,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract. Automatic recognition of mathematical expressions is one of the key vehicles in the drive towards transcribing documents in scientific and engineering disciplines into electronic form. This problem typically consists of two major stages, namely, symbol recognition and structural analysis. In this survey paper, we will review most of the existing work with respect to each of the two major stages of the recognition process. In particular, we try to put emphasis on the similarities and differences between systems. Moreover, some important issues in mathematical expression recognition will be addressed in depth. All these together serve to provide a clear overall picture of how this research area has been developed to date."
            },
            "slug": "Mathematical-expression-recognition:-a-survey-Chan-Yeung",
            "title": {
                "fragments": [],
                "text": "Mathematical expression recognition: a survey"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This survey paper will review most of the existing work with respect to each of the two major stages of the recognition process, and tries to put emphasis on the similarities and differences between systems."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal on Document Analysis and Recognition"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2939803"
                        ],
                        "name": "Ronan Collobert",
                        "slug": "Ronan-Collobert",
                        "structuredName": {
                            "firstName": "Ronan",
                            "lastName": "Collobert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronan Collobert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2645384"
                        ],
                        "name": "K. Kavukcuoglu",
                        "slug": "K.-Kavukcuoglu",
                        "structuredName": {
                            "firstName": "Koray",
                            "lastName": "Kavukcuoglu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kavukcuoglu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2256269"
                        ],
                        "name": "C. Farabet",
                        "slug": "C.-Farabet",
                        "structuredName": {
                            "firstName": "Cl\u00e9ment",
                            "lastName": "Farabet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Farabet"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 33
                            }
                        ],
                        "text": "The system is built using Torch (Collobert et al., 2011) based on the OpenNMT system (Klein et al., 2017)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 32
                            }
                        ],
                        "text": "The system is built using Torch (Collobert et al., 2011) based on the OpenNMT system (Klein et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14365368,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3449b65008b27f6e60a73d80c1fd990f0481126b",
            "isKey": false,
            "numCitedBy": 1490,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "Torch7 is a versatile numeric computing framework and machine learning library that extends Lua. Its goal is to provide a flexible environment to design and train learning machines. Flexibility is obtained via Lua, an extremely lightweight scripting language. High performance is obtained via efficient OpenMP/SSE and CUDA implementations of low-level numeric routines. Torch7 can easily be interfaced to third-party software thanks to Lua\u2019s light interface."
            },
            "slug": "Torch7:-A-Matlab-like-Environment-for-Machine-Collobert-Kavukcuoglu",
            "title": {
                "fragments": [],
                "text": "Torch7: A Matlab-like Environment for Machine Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "Torch7 is a versatile numeric computing framework and machine learning library that extends Lua that can easily be interfaced to third-party software thanks to Lua\u2019s light interface."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS 2011"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3323275"
                        ],
                        "name": "Kishore Papineni",
                        "slug": "Kishore-Papineni",
                        "structuredName": {
                            "firstName": "Kishore",
                            "lastName": "Papineni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kishore Papineni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781292"
                        ],
                        "name": "S. Roukos",
                        "slug": "S.-Roukos",
                        "structuredName": {
                            "firstName": "Salim",
                            "lastName": "Roukos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roukos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144582029"
                        ],
                        "name": "T. Ward",
                        "slug": "T.-Ward",
                        "structuredName": {
                            "firstName": "Todd",
                            "lastName": "Ward",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Ward"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2587983"
                        ],
                        "name": "Wei-Jing Zhu",
                        "slug": "Wei-Jing-Zhu",
                        "structuredName": {
                            "firstName": "Wei-Jing",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei-Jing Zhu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "y with the original image as well as the value after eliminating whitespace columns.6 We also include standard intrinsic text generation metrics, conditional language model perplexity and BLEU score (Papineni et al. 2002). Note that both of these metrics are sensitive to the fact that the markup languages have spurious ambiguity, so a deterministic perplexity of 1 would be impossible. Implementation Details The same m"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 251,
                                "start": 230
                            }
                        ],
                        "text": "\u2026check the exact match accuracy with the original image as well as the value after eliminating whitespace columns.7 We also include standard intrinsic text generation metrics, conditional language model perplexity and BLEU score (Papineni et al., 2002), on both tokenized and normalized gold data."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11080756,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d7da009f457917aa381619facfa5ffae9329a6e9",
            "isKey": false,
            "numCitedBy": 16616,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations."
            },
            "slug": "Bleu:-a-Method-for-Automatic-Evaluation-of-Machine-Papineni-Roukos",
            "title": {
                "fragments": [],
                "text": "Bleu: a Method for Automatic Evaluation of Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work proposes a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1387492844"
                        ],
                        "name": "Masakazu Suzuki",
                        "slug": "Masakazu-Suzuki",
                        "structuredName": {
                            "firstName": "Masakazu",
                            "lastName": "Suzuki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Masakazu Suzuki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34636599"
                        ],
                        "name": "F. Tamari",
                        "slug": "F.-Tamari",
                        "structuredName": {
                            "firstName": "Fumikazu",
                            "lastName": "Tamari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Tamari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34071929"
                        ],
                        "name": "Ryoji Fukuda",
                        "slug": "Ryoji-Fukuda",
                        "structuredName": {
                            "firstName": "Ryoji",
                            "lastName": "Fukuda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ryoji Fukuda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1809705"
                        ],
                        "name": "S. Uchida",
                        "slug": "S.-Uchida",
                        "structuredName": {
                            "firstName": "Seiichi",
                            "lastName": "Uchida",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Uchida"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1771418"
                        ],
                        "name": "T. Kanahori",
                        "slug": "T.-Kanahori",
                        "structuredName": {
                            "firstName": "Toshihiro",
                            "lastName": "Kanahori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanahori"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 57
                            }
                        ],
                        "text": "InftyReader is an implementation of the INFTY system of (Suzuki et al., 2003), combining symbol recognition and structural analysis phases."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 145
                            }
                        ],
                        "text": "A prime example of this approach is the INFTY system that is used to convert printed mathematical expressions to LaTeX and other markup formats (Suzuki et al., 2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5545146,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "680857bf67150e867ded7240d8889256ea29abbb",
            "isKey": false,
            "numCitedBy": 187,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "An integrated OCR system for mathematical documents, called INFTY, is presented. INFTY consists of four procedures, i.e., layout analysis, character recognition, structure analysis of mathematical expressions, and manual error correction. In those procedures, several novel techniques are utilized for better recognition performance. Experimental results on about 500 pages of mathematical documents showed high character recognition rates on both mathematical expressions and ordinary texts, and sufficient performance on the structure analysis of the mathematical expressions."
            },
            "slug": "INFTY:-an-integrated-OCR-system-for-mathematical-Suzuki-Tamari",
            "title": {
                "fragments": [],
                "text": "INFTY: an integrated OCR system for mathematical documents"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "An integrated OCR system for mathematical documents, called INFTY, is presented, which shows high character recognition rates on both mathematical expressions and ordinary texts, and sufficient performance on the structure analysis of the mathematical expressions."
            },
            "venue": {
                "fragments": [],
                "text": "DocEng '03"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1895356"
                        ],
                        "name": "D. Ciresan",
                        "slug": "D.-Ciresan",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Ciresan",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ciresan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2514691"
                        ],
                        "name": "U. Meier",
                        "slug": "U.-Meier",
                        "structuredName": {
                            "firstName": "Ueli",
                            "lastName": "Meier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Meier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6803671"
                        ],
                        "name": "L. Gambardella",
                        "slug": "L.-Gambardella",
                        "structuredName": {
                            "firstName": "Luca",
                            "lastName": "Gambardella",
                            "middleNames": [
                                "Maria"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gambardella"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 79
                            }
                        ],
                        "text": "For instance, advances have been made in the areas of handwriting recognition (Ciresan et al., 2010), OCR in natural scenes (Jaderberg et al., 2015; 2016; Wang et al., 2012) and image caption generation (Karpathy & FeiFei, 2015; Vinyals et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 78
                            }
                        ],
                        "text": "For instance, advances have been made in the areas of handwriting recognition (Ciresan et al., 2010), OCR in natural scenes (Jaderberg et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1918673,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b98cd08b75ebf2bd1d1ec47c51ef75777a7e64bd",
            "isKey": false,
            "numCitedBy": 875,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Good old online backpropagation for plain multilayer perceptrons yields a very low 0.35 error rate on the MNIST handwritten digits benchmark. All we need to achieve this best result so far are many hidden layers, many neurons per layer, numerous deformed training images to avoid overfitting, and graphics cards to greatly speed up learning."
            },
            "slug": "Deep,-Big,-Simple-Neural-Nets-for-Handwritten-Digit-Ciresan-Meier",
            "title": {
                "fragments": [],
                "text": "Deep, Big, Simple Neural Nets for Handwritten Digit Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Good old online backpropagation for plain multilayer perceptrons yields a very low 0.35 error rate on the MNIST handwritten digits benchmark."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2332513,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4c915c1eecb217c123a36dc6d3ce52d12c742614",
            "isKey": false,
            "numCitedBy": 5180,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms."
            },
            "slug": "Simple-statistical-gradient-following-algorithms-Williams",
            "title": {
                "fragments": [],
                "text": "Simple statistical gradient-following algorithms for connectionist reinforcement learning"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units that are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reInforcement tasks, and they do this without explicitly computing gradient estimates."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143614516"
                        ],
                        "name": "J. Gehrke",
                        "slug": "J.-Gehrke",
                        "structuredName": {
                            "firstName": "Johannes",
                            "lastName": "Gehrke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Gehrke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3183325"
                        ],
                        "name": "P. Ginsparg",
                        "slug": "P.-Ginsparg",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Ginsparg",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Ginsparg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3371403"
                        ],
                        "name": "J. Kleinberg",
                        "slug": "J.-Kleinberg",
                        "structuredName": {
                            "firstName": "Jon",
                            "lastName": "Kleinberg",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kleinberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 96
                            }
                        ],
                        "text": "We extract formulas by parsing LaTeX sources of papers from tasks I and II of the 2003 KDD cup (Gehrke et al., 2003), which contain over 60,000 papers."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 829034,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "05d5a28fd29fdbd405743cd282888e463c8cb26a",
            "isKey": false,
            "numCitedBy": 273,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper surveys the 2003 KDD Cup, a competition held in conjunction with the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD) in August 2003. The competition focused on mining the complex real-life social network inherent in the e-print arXiv (arXiv.org). We describe the four KDD Cup tasks: citation prediction, download prediction, data cleaning, and an open task."
            },
            "slug": "Overview-of-the-2003-KDD-Cup-Gehrke-Ginsparg",
            "title": {
                "fragments": [],
                "text": "Overview of the 2003 KDD Cup"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "This paper surveys the 2003 KDD Cup, a competition focused on mining the complex real-life social network inherent in the e-print arXiv (arXiv.org), and describes the four K DD Cup tasks: citation prediction, download prediction, data cleaning, and an open task."
            },
            "venue": {
                "fragments": [],
                "text": "SKDD"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 84
                            }
                        ],
                        "text": "Handwritten Formulas Synthetic handwritten formulas by using handwritten characters [Kirsch, 2010] as font, used for pretraining Finetune and evaluate on CROHME 13 and 14 (8K training set) CROHME 14"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 84
                            }
                        ],
                        "text": "Handwritten Formulas Synthetic handwritten formulas by using handwritten characters [Kirsch, 2010] as font, used for pretraining Finetune and evaluate on CROHME 13 and 14 (8K training set) CROHME 13 (*uses private in-domain handwritten training data)"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 84
                            }
                        ],
                        "text": "Handwritten Formulas Synthetic handwritten formulas by using handwritten characters [Kirsch, 2010] as font, used for pretraining Finetune and evaluate on CROHME 13 and 14 (8K training set)"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 84
                            }
                        ],
                        "text": "Handwritten Formulas Synthetic handwritten formulas by using handwritten characters [Kirsch, 2010] as font, used for pretraining Finetune and evaluate on CROHME 13 and 14 (8K training set) CROHME 13"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Detexify: Erkennung handgemalter LaTeX-symbole"
            },
            "venue": {
                "fragments": [],
                "text": "PhD thesis, Diploma thesis, Westfa\u0308lische Wilhelms-Universita\u0308t Mu\u0308nster,"
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": ", and Farabet , Cl\u00e9ment . Torch 7 : A matlab - like environment for machine learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Long shortterm memory"
            },
            "venue": {
                "fragments": [],
                "text": "Neural computation,"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 125
                            }
                        ],
                        "text": "For instance, advances have been made in the areas of handwriting recognition (Ciresan et al., 2010), OCR in natural scenes (Jaderberg et al., 2015; 2016; Wang et al., 2012) and image caption generation (Karpathy & FeiFei, 2015; Vinyals et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 29
                            }
                        ],
                        "text": "Unlike some recent OCR work (Jaderberg et al., 2015; Lee & Osindero, 2016), we do not use final fully-connected layers (Ioffe & Szegedy, 2015), since we want to preserve the locality of CNN features in order to use visual attention."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Hochreiter , Sepp and Schmidhuber , J\u00fcrgen . Long short - term memory"
            },
            "venue": {
                "fragments": [],
                "text": "Neural computation"
            },
            "year": 2015
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 127
                            }
                        ],
                        "text": "For neural models, a natural comparison is to standard image captioning approaches (Xu et al., 2015), and CTCbased approaches (Shi et al., 2016)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 37
                            }
                        ],
                        "text": "For CTC we use the implementation of Shi et al. (2016), designed for natural image OCR."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An end-toend trainable neural network for image-based sequence recognition and its application to scene text recognition"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence,"
            },
            "year": 2016
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 19,
            "methodology": 19
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 48,
        "totalPages": 5
    },
    "page_url": "https://www.semanticscholar.org/paper/Image-to-Markup-Generation-with-Coarse-to-Fine-Deng-Kanervisto/071c0b3ec700758dd9b4164ede08b714ea7e3c38?sort=total-citations"
}