{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144456145"
                        ],
                        "name": "W. Bruce Croft",
                        "slug": "W.-Bruce-Croft",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Croft",
                            "middleNames": [
                                "Bruce"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Bruce Croft"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "29436283"
                        ],
                        "name": "Stephen M. Harding",
                        "slug": "Stephen-M.-Harding",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Harding",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephen M. Harding"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731110"
                        ],
                        "name": "K. Taghva",
                        "slug": "K.-Taghva",
                        "structuredName": {
                            "firstName": "Kazem",
                            "lastName": "Taghva",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Taghva"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1921859"
                        ],
                        "name": "J. Borsack",
                        "slug": "J.-Borsack",
                        "structuredName": {
                            "firstName": "Julie",
                            "lastName": "Borsack",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Borsack"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 16480769,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "392a7bc77a39e2b8a5b5eef385803df920562e1a",
            "isKey": false,
            "numCitedBy": 43,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Optical Character Recognition (OCR) is a critical part of many text-based applications. Although some commercial systems use the output from OCR devices to index documents without editing, there is very little quantitative data on the impact of OCR errors on the accuracy of a text retrieval system. Because of the difficulty of constructing test collections to obtain this data, we have carried out evaluations using simulated OCR output on a variety of databases. The results show that high quality OCR devices have little effect on the accuracy of retrieval, but low quality devices used with databases of short documents can result in significant degradation."
            },
            "slug": "An-Evaluation-of-Information-Retrieval-Accuracy-OCR-Croft-Harding",
            "title": {
                "fragments": [],
                "text": "An Evaluation of Information Retrieval Accuracy with Simulated OCR Output"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117760"
                        ],
                        "name": "W. B. Cavnar",
                        "slug": "W.-B.-Cavnar",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Cavnar",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. B. Cavnar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31419242"
                        ],
                        "name": "J. Trenkle",
                        "slug": "J.-Trenkle",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Trenkle",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Trenkle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 170740,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "49af572ef8f7ea89db06d5e7b66e9369c22d7607",
            "isKey": false,
            "numCitedBy": 1816,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Text categorization is a fundamental task in document processing, allowing the automated handling of enormous streams of documents in electronic form. One difficulty in handling some classes of documents is the presence of different kinds of textual errors, such as spelling and grammatical errors in email, and character recognition errors in documents that come through OCR. Text categorization must work reliably on all input, and thus must tolerate some level of these kinds of problems. We describe here an N-gram-based approach to text categorization that is tolerant of textual errors. The system is small, fast and robust. This system worked very well for language classification, achieving in one test a 99.8% correct classification rate on Usenet newsgroup articles written in different languages. The system also worked reasonably well for classifying articles from a number of different computer-oriented newsgroups according to subject, achieving as high as an 80% correct classification rate. There are also several obvious directions for improving the system`s classification performance in those cases where it did not do as well. The system is based on calculating and comparing profiles of N-gram frequencies. First, we use the system to compute profiles on training set data that represent the variousmore\u00a0\u00bb categories, e.g., language samples or newsgroup content samples. Then the system computes a profile for a particular document that is to be classified. Finally, the system computes a distance measure between the document`s profile and each of the category profiles. The system selects the category whose profile has the smallest distance to the document`s profile. The profiles involved are quite small, typically 10K bytes for a category training set, and less than 4K bytes for an individual document. Using N-gram frequency profiles provides a simple and reliable way to categorize documents in a wide range of classification tasks.\u00ab\u00a0less"
            },
            "slug": "N-gram-based-text-categorization-Cavnar-Trenkle",
            "title": {
                "fragments": [],
                "text": "N-gram-based text categorization"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "An N-gram-based approach to text categorization that is tolerant of textual errors is described, which worked very well for language classification and worked reasonably well for classifying articles from a number of different computer-oriented newsgroups according to subject."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3314752"
                        ],
                        "name": "Penelope Sibun",
                        "slug": "Penelope-Sibun",
                        "structuredName": {
                            "firstName": "Penelope",
                            "lastName": "Sibun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Penelope Sibun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145358278"
                        ],
                        "name": "A. Spitz",
                        "slug": "A.-Spitz",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Spitz",
                            "middleNames": [
                                "Lawrence"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Spitz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13045218,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2e6e282c1c05103d13f17dd9cf4640660256409d",
            "isKey": false,
            "numCitedBy": 66,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Many documents are available to a computer only as images from paper. However, most natural language processing systems expect their input as character-coded text, which may be difficult or expensive to extract accurately from the page. We describe a method for converting a document image into character shape codes and word shape tokens. We believe that this representation, which is both cheap and robust, is sufficient for many NLP tasks. In this paper, we show that the representation is sufficient for determining which of 23 languages the document is written in, using only a small number of features, with greater than 90% accuracy overall."
            },
            "slug": "Language-Determination:-Natural-Language-Processing-Sibun-Spitz",
            "title": {
                "fragments": [],
                "text": "Language Determination: Natural Language Processing from Scanned Document Images"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper describes a method for converting a document image into character shape codes and word shape tokens, which it is shown is sufficient for determining which of 23 languages the document is written in, using only a small number of features."
            },
            "venue": {
                "fragments": [],
                "text": "ANLP"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731110"
                        ],
                        "name": "K. Taghva",
                        "slug": "K.-Taghva",
                        "structuredName": {
                            "firstName": "Kazem",
                            "lastName": "Taghva",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Taghva"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1921859"
                        ],
                        "name": "J. Borsack",
                        "slug": "J.-Borsack",
                        "structuredName": {
                            "firstName": "Julie",
                            "lastName": "Borsack",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Borsack"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3118330"
                        ],
                        "name": "A. Condit",
                        "slug": "A.-Condit",
                        "structuredName": {
                            "firstName": "Allen",
                            "lastName": "Condit",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Condit"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 1772383,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a8353197eee5470737b73ac3540c22d98c942f30",
            "isKey": false,
            "numCitedBy": 97,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Character accuracy of optically recognized text is considered a basic measure for evaluating OCR devices. In the broader sense, another fundamental measure of an OCR\u2019s goodness is whether its generated text is usable for retrieving information. In this study, we evaluate retrieval effectiveness from OCR text databases using a probabilistic IR system. We compare these retrieval results to their manually corrected equivalent. We show there is no statistical difference in precision and recall using graded accuracy levels from three OCR devices. However, characteristics of the OCR data have side effects that could cause unstable results with this IR model. In particular, we found individual queries can be greatly affected. Knowing the qualities of OCR text, we compensate for them by applying an automatic post-processing system that improves effectiveness."
            },
            "slug": "Results-of-applying-probabilistic-IR-to-OCR-text-Taghva-Borsack",
            "title": {
                "fragments": [],
                "text": "Results of applying probabilistic IR to OCR text"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This study evaluates retrieval effectiveness from OCR text databases using a probabilistic IR system and shows there is no statistical difference in precision and recall using graded accuracy levels from three OCR devices."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '94"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731110"
                        ],
                        "name": "K. Taghva",
                        "slug": "K.-Taghva",
                        "structuredName": {
                            "firstName": "Kazem",
                            "lastName": "Taghva",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Taghva"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1921859"
                        ],
                        "name": "J. Borsack",
                        "slug": "J.-Borsack",
                        "structuredName": {
                            "firstName": "Julie",
                            "lastName": "Borsack",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Borsack"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3118330"
                        ],
                        "name": "A. Condit",
                        "slug": "A.-Condit",
                        "structuredName": {
                            "firstName": "Allen",
                            "lastName": "Condit",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Condit"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3028499"
                        ],
                        "name": "Srinivas Erva",
                        "slug": "Srinivas-Erva",
                        "structuredName": {
                            "firstName": "Srinivas",
                            "lastName": "Erva",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Srinivas Erva"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 18518235,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f3df3e9963961a19d7ee574fc95fdefbc6a11e30",
            "isKey": false,
            "numCitedBy": 86,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "We report on the results of our experiments on query evaluation in the presence of noisy data. In particular, an OCR\u2010generated database and its corresponding 99.8% correct version are used to process a set of queries to determine the effect the degraded version will have on retrieval. It is shown that, with the set of scientific documents we use in our testing, the effect is insignificant. We further improve the result by applying an automatic postprocessing system designed to correct the kinds of errors generated by recognition devices. \u00a9 1994 John Wiley & Sons, Inc."
            },
            "slug": "The-Effects-of-Noisy-Data-on-Text-Retrieval-Taghva-Borsack",
            "title": {
                "fragments": [],
                "text": "The Effects of Noisy Data on Text Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "An OCR\u2010generated database and its corresponding 99.8% correct version are used to process a set of queries to determine the effect the degraded version will have on retrieval, and it is shown that the effect is insignificant."
            },
            "venue": {
                "fragments": [],
                "text": "J. Am. Soc. Inf. Sci."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36422303"
                        ],
                        "name": "R. Hoch",
                        "slug": "R.-Hoch",
                        "structuredName": {
                            "firstName": "Rainer",
                            "lastName": "Hoch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Hoch"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7152367,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3645407c6928b65a7a8ee026545f29c66783c1c4",
            "isKey": false,
            "numCitedBy": 58,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents the INFOCLAS system applying statistical methods of information retrieval for the classification of German business letters into corresponding message types such as order, offer, enclosure, etc. INFOCLAS is a first step towards the understanding of documents proceeding to a classification-driven extraction of information. The system is composed of two main modules: the central indexer (extraction and weighting of indexing terms) and the classifier (classification of business letters into given types). The system employs several knowledge sources including a letter database, word frequency statistics for German, lists of message type specific words, morphological knowledge as well as the underlying document structure. As output, the system evaluates a set of weighted hypotheses about the type of the actual letter. Classification of documents allow the automatic distribution or archiving of letters and is also an excellent starting point for higher-level document analysis.1"
            },
            "slug": "Using-IR-techniques-for-text-classification-in-Hoch",
            "title": {
                "fragments": [],
                "text": "Using IR techniques for text classification in document analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "The INFOCLAS system applying statistical methods of information retrieval for the classification of German business letters into corresponding message types such as order, offer, enclosure, etc is presented."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '94"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35153517"
                        ],
                        "name": "D. Lewis",
                        "slug": "D.-Lewis",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Lewis",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lewis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34938639"
                        ],
                        "name": "W. Gale",
                        "slug": "W.-Gale",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Gale",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Gale"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 915058,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5194b668c67aa83c037e71599a087f63c98eb713",
            "isKey": false,
            "numCitedBy": 2404,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "The ability to cheaply train text classifiers is critical to their use in information retrieval, content analysis, natural language processing, and other tasks involving data which is partly or fully textual. An algorithm for sequential sampling during machine learning of statistical classifiers was developed and tested on a newswire text categorization task. This method, which we call uncertainty sampling, reduced by as much as 500-fold the amount of training data that would have to be manually classified to achieve a given level of effectiveness."
            },
            "slug": "A-sequential-algorithm-for-training-text-Lewis-Gale",
            "title": {
                "fragments": [],
                "text": "A sequential algorithm for training text classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "An algorithm for sequential sampling during machine learning of statistical classifiers was developed and tested on a newswire text categorization task and reduced by as much as 500-fold the amount of training data that would have to be manually classified to achieve a given level of effectiveness."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '94"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3055160"
                        ],
                        "name": "David J. Ittner",
                        "slug": "David-J.-Ittner",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Ittner",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David J. Ittner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723766"
                        ],
                        "name": "H. Baird",
                        "slug": "H.-Baird",
                        "structuredName": {
                            "firstName": "Henry",
                            "lastName": "Baird",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Baird"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 5360061,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4bc5bf7115142885992f99516169ea24bf18b529",
            "isKey": false,
            "numCitedBy": 86,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "A system for isolating blocks, lines, words, and symbols within images of machine-printed textual documents that is, to a large existent, independent of language and writing system is described. This is achieved by exploiting a small number of nearly universal typesetting and layout conventions. The system does not require prior knowledge of page orientation (module 90/spl deg/), and copes well with nonzero skew and shear angles (within 10/spl deg/). Also it locates blocks of text without reliance on detailed a priori layout models, and in spite of unknown or mixed horizontal and vertical text-line orientations. Within blocks, it infers text-line orientation and isolates lines, without knowledge of the language, symbol set, text sizes, or the number of text lines. Segmentation into words and symbols, and determination of reading order, normally require some knowledge of the language: this is held to minimum by relying on shape-driven algorithms. The underlying algorithms are based on Fourier theory, digital signal processing, computational geometry, and statistical decision theory. Most of the computation occurs within algorithms that possess unambiguous semantics (that is, heuristics are kept to a minimum). The effectiveness of the method on English, Japanese, Hebrew, Thai, and Korean documents is discussed.<<ETX>>"
            },
            "slug": "Language-free-layout-analysis-Ittner-Baird",
            "title": {
                "fragments": [],
                "text": "Language-free layout analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "A system for isolating blocks, lines, words, and symbols within images of machine-printed textual documents that is, to a large existent, independent of language and writing system is described, achieved by exploiting a small number of nearly universal typesetting and layout conventions."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 2nd International Conference on Document Analysis and Recognition (ICDAR '93)"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35729970"
                        ],
                        "name": "Yiming Yang",
                        "slug": "Yiming-Yang",
                        "structuredName": {
                            "firstName": "Yiming",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yiming Yang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 16041292,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bc8e59e4c7c2cbb6695ee5488aa569780449b212",
            "isKey": false,
            "numCitedBy": 485,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Expert Network (ExpNet) is our new approach to automatic categorization and retrieval of natural language texts. We use a training set of texts with expert-assigned categories to construct a network which approximately reflects the conditional probabilities of categories given a text. The input nodes of the network are words in the training texts, the nodes on the intermediate level are the training texts, and the output nodes are categories. The links between nodes are computed based on statistics of the word distribution and the category distribution over the training set. ExpNet is used for relevance ranking of candidate categories of an arbitrary text in the case of text categorization, and for relevance ranking of documents via categories in the case of text retrieval. We have evaluated ExpNet in categorization and retrieval on a document collection of the MEDLINE database, and observed a performance in recall and precision comparable to the Linear Least Squares Fit (LLSF) mapping method, and significantly better than other methods tested. Computationally, ExpNet has an O(N 1og N) time complexity which is much more efficient than the cubic complexity of the LLSF method. The simplicity of the model, the high recall-precision rates, and the efficient computation together make ExpNet preferable as a practical solution for real-world applications."
            },
            "slug": "Expert-network:-effective-and-efficient-learning-in-Yang",
            "title": {
                "fragments": [],
                "text": "Expert network: effective and efficient learning from human decisions in text categorization and retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The simplicity of the model, the high recall-precision rates, and the efficient computation together make ExpNet preferable as a practical solution for real-world applications."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '94"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35153517"
                        ],
                        "name": "D. Lewis",
                        "slug": "D.-Lewis",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Lewis",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lewis"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 17260485,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "91cbbe24c807473b7b935d39b63df5b15da9bb32",
            "isKey": false,
            "numCitedBy": 392,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Text retrieval systems typically produce a ranking of documents and let a user decide how far down that ranking to go. In contrast, programs that filter text streams, software that categorizes documents, agents which alert users, and many other IR systems must make decisions without human input or supervision. It is important to define what constitutes good effectiveness for these autonomous systems, tune the systems to achieve the highest possible effectiveness, and estimate how the effectiveness changes as new data is processed. We show how to do this for binary text classification systems, emphasizing that different goals for the system le ad to different optimal behaviors. Optimizing and estimating effectiveness is greatly aided if classifiers that explicitly estimate the probability of class membership are used. Ranked retrieval is the information retrieval (IR) researc her\u2019s favorite tool for dealing with information overload. Ranked retrieval systems display documents in order of probability of releva nce or some similar measure. Users see the best documents first, anddecide how far down the ranking to go in examining the available information. The central role played by ranking in this appr oach has led researchers to evaluate IR systems primarily, often exclusively, on the quality of their rankings. (See, for instance , the TREC evaluations [1].) In some IR applications, however, ranking is not enough: A company provides an SDI (selective dissemination of information) service which filters newswire feeds. Relevant articles are faxed each morning to clients. Interaction between customer and system takes place infrequently. The cost of resources (tying up phone lines, fax machine paper, etc.) is a factor to consider in operating the system. A text categorization system assigns controlled vocabulary categories to incoming documents as they are stored in a text database. Cost cutting has eliminated manual checking of category assignments."
            },
            "slug": "Evaluating-and-optimizing-autonomous-text-systems-Lewis",
            "title": {
                "fragments": [],
                "text": "Evaluating and optimizing autonomous text classification systems"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This work shows how to define what constitutes good effectiveness for binary text classification systems, tune the systems to achieve the highest possible effectiveness, and estimate how the effectiveness changes as new data is processed."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '95"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37090109"
                        ],
                        "name": "T. Tokunaga",
                        "slug": "T.-Tokunaga",
                        "structuredName": {
                            "firstName": "Takenobu",
                            "lastName": "Tokunaga",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Tokunaga"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144702"
                        ],
                        "name": "Makoto Iwayama",
                        "slug": "Makoto-Iwayama",
                        "structuredName": {
                            "firstName": "Makoto",
                            "lastName": "Iwayama",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Makoto Iwayama"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 18257943,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "588d05c88887649544a204bb944912f1b3766bca",
            "isKey": false,
            "numCitedBy": 103,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "c The author(s) of this report reserves all the rights. Abstract This paper proposes a new term weighting method called weighted inverse document frequency (WIDF). As its name indicates, WIDF is an extension of IDF (inverse document frequency) to incorporate the term frequency over the collection of texts. WIDF of a term in a text is given by dividing the frequency of the term in the text by the sum of the frequency of the term over the collection of texts. WIDF is applied to the text categorization task and proved to be superior to the other methods. The improvement of accuracy on IDF is 7.4% at the maximum."
            },
            "slug": "Text-Categorization-based-on-Weighted-Inverse-Tokunaga-Iwayama",
            "title": {
                "fragments": [],
                "text": "Text Categorization based on Weighted Inverse Document Frequency"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "WIDF is applied to the text categorization task and proved to be superior to the other methods and the improvement of accuracy on IDF is 7.4% at the maximum."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723766"
                        ],
                        "name": "H. Baird",
                        "slug": "H.-Baird",
                        "structuredName": {
                            "firstName": "Henry",
                            "lastName": "Baird",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Baird"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61890978,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "9b0ac5a41c2742de9695810dadab2c156028f730",
            "isKey": false,
            "numCitedBy": 94,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "An experimental printed-page reader that is easy to adapt to various languages is described. Changing the target language may involve simultaneous changes in symbol sets, typefaces, sizes of text, page layouts, linguistic contexts, and imaging defects. The strategy has been to isolate the effects of these sources of variation within separate, independent engineering subsystems. In this way, it has been possible to construct, with a minimum of manual effort, classifiers for arbitrary combinations of symbols, typefaces, sizes, and imaging defects. An attempt has been made to rid the algorithms of all language-specific rules, relying instead on automatic learning from examples and generalized table-driven methods. For some tasks it has been feasible to avoid language dependency altogether. Linguistic context can be exploited through data-directed filtering algorithms in a uniform and modular manner, so that preexisting tools developed by computational linguistics can readily be applied. These principles are illustrated by trials on English, Swedish, Tibetan, and special technical texts. >"
            },
            "slug": "Anatomy-of-a-versatile-page-reader-Baird",
            "title": {
                "fragments": [],
                "text": "Anatomy of a versatile page reader"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "An experimental printed-page reader that is easy to adapt to various languages is described, and an attempt has been made to rid the algorithms of all language-specific rules, relying instead on automatic learning from examples and generalized table-driven methods."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32440409"
                        ],
                        "name": "T. Rose",
                        "slug": "T.-Rose",
                        "structuredName": {
                            "firstName": "Tony",
                            "lastName": "Rose",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Rose"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2123289"
                        ],
                        "name": "L. Evett",
                        "slug": "L.-Evett",
                        "structuredName": {
                            "firstName": "Lindsay",
                            "lastName": "Evett",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Evett"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6906987,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "99e5c20008bee2f7c637cfd0b3af6d79f65be8e2",
            "isKey": false,
            "numCitedBy": 11,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Text recognition systems require the use of contextual information in order to maximise the accuracy of their output. However, the acquisition of such knowledge for a realistically sized vocabulary presents a major problem. This paper describes methods for extracting contextual knowledge from text corpora, and demonstrates its contribution to the performance of handwriting recognition systems."
            },
            "slug": "Text-Recognition-and-Collocations-and-Domain-Codes-Rose-Evett",
            "title": {
                "fragments": [],
                "text": "Text Recognition and Collocations and Domain Codes"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Methods for extracting contextual knowledge from text corpora are described, and its contribution to the performance of handwriting recognition systems is demonstrated."
            },
            "venue": {
                "fragments": [],
                "text": "VLC@ACL"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2094165137"
                        ],
                        "name": "Re.",
                        "slug": "Re.",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Re.",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Re."
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5721022,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cb20f8d39237b40acb2eedf3404a73456e8c3ef4",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "\u2022 The goal in speech-message information retrieval is to categorize an input speech utterance according to a predefined notion ofa topic, or message class. The components of a speech-message information-retrieval system include an acoustic front end that provides an incomplete transcription of a spoken message, and a message classifier that interprets the incomplete transcription and classifies the message according to message category. The techniques and experiments described in this paper concern the integration ofthese components, and represent the first demonstration of a complete system that accepts speech messages as input and produces an estimated message class as output. The promising results obtained in information retrieval on conversational speech messages demonstrate the feasibility ofthe technology. T HE GOAL IN SPEECH-.MESSAGE information retrieval is similar to that of the more well-known. problem ofinformation retrieval from text documents. Text-based information-retrieval systems sort large collections of documents according to predefined relevance classes. This discipline is a mature area of research with a number of well-known document-retrieval systems already in existence. Speech-message information retrieval is a relatively new area, and work in this area is motivated by the rapid proliferation ofspeech-messaging and speech-storage technology in the home and office. A good example is the widespread application of large speech-mail and speech-messaging systems that can be accessed over telephone lines. The potential length and number of speech messages in these systems make exhaustive user review of all messages in a speech mailbox difficult. In such a system, speech-message information retrieval could automatically categorize speech messages by context to facilitate user review. Another application would be to classifY incoming customer telephone calls automatically and route them to the appropriate customer service areas [1]. Unlike information-retrieval systems designed for text messages, the speech-message information-retrieval system illustrated in Figure 1 relies on a limited-vocabulary acoustic front end that provides only an incomplete transcription of a spoken message. The second stage of the system is a message classifier that must interpret the incomplete transcription and classifY the message according to message category. In our system the acoustic front end is based on a hidden-Markov-model (HMM) word spotter [2]. The techniques described in this paper concern the design and training of the second-stage message classifier and the integration of the message classifier with the acoustic front end [3]. The major result described in the paper is the demonstration and evaluation ofan experimental system for speech-message information retrieval in which speech messages \u2026"
            },
            "slug": "Techniques-for-Information-Retrieval-from-Speech-Re.",
            "title": {
                "fragments": [],
                "text": "Techniques for Information Retrieval from Speech Messages"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "The first demonstration of a complete system that accepts speech messages as input and produces an estimated message class as output is presented, and the promising results obtained in information retrieval on conversational speech messages demonstrate the feasibility of the technology."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744200"
                        ],
                        "name": "I. T. Phillips",
                        "slug": "I.-T.-Phillips",
                        "structuredName": {
                            "firstName": "Ihsin",
                            "lastName": "Phillips",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. T. Phillips"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111237291"
                        ],
                        "name": "Su S. Chen",
                        "slug": "Su-S.-Chen",
                        "structuredName": {
                            "firstName": "Su",
                            "lastName": "Chen",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Su S. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710238"
                        ],
                        "name": "R. Haralick",
                        "slug": "R.-Haralick",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Haralick",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Haralick"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 3258255,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "de1580d4cc0c47e4985c8ce52d106c6a7432ff70",
            "isKey": false,
            "numCitedBy": 127,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "The design of a comprehensive standard document database for machine-printed documents is presented. The effort to produce a series of carefully ground-truthed document databases to be issued on CD-ROMs is described in detail. The databases can be utilized by the OCR and document understanding community as a common platform to develop, test, and evaluate their algorithms.<<ETX>>"
            },
            "slug": "CD-ROM-document-database-standard-Phillips-Chen",
            "title": {
                "fragments": [],
                "text": "CD-ROM document database standard"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "The design of a comprehensive standard document database for machine-printed documents is presented and can be utilized by the OCR and document understanding community as a common platform to develop, test, and evaluate their algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 2nd International Conference on Document Analysis and Recognition (ICDAR '93)"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35153517"
                        ],
                        "name": "D. Lewis",
                        "slug": "D.-Lewis",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Lewis",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lewis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724728"
                        ],
                        "name": "R. Tong",
                        "slug": "R.-Tong",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Tong",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Tong"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16878719,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "24fa715e09917dfafb77aca5aaa65bc788ac10f8",
            "isKey": false,
            "numCitedBy": 25,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the changes from the Third (MUC-3) to the Fourth (MUC-4) Message Understanding Conference was the emergence of text filtering as an explicit topic of discussion. In this paper we examine text filtering in MUC systems with three goals in mind. First, we clarify the difference between two uses of the term \"text filtering\" in the context of data extraction systems, and put these phenomena in the context of prior research on information retrieval (IR)."
            },
            "slug": "Text-filtering-in-MUC-3-and-MUC-4-Lewis-Tong",
            "title": {
                "fragments": [],
                "text": "Text filtering in MUC-3 and MUC-4"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The difference between two uses of the term \"text filtering\" in the context of data extraction systems is clarified, and these phenomena are put in thecontext of prior research on information retrieval (IR)."
            },
            "venue": {
                "fragments": [],
                "text": "MUC"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145428001"
                        ],
                        "name": "P. Hayes",
                        "slug": "P.-Hayes",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Hayes",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hayes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053034118"
                        ],
                        "name": "S. P. Weinstein",
                        "slug": "S.-P.-Weinstein",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Weinstein",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. P. Weinstein"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18312939,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "01d6d53fce6fac2a33d92ddf096290d6b99c2d13",
            "isKey": false,
            "numCitedBy": 230,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "The Construe news story categorization system assigns indexing terms to news stories according to their content using knowledge-based techniques. An initial deployment of Construe in Reuters Ltd. topic identification system (TIS) has replaced human indexing for Reuters Country Reports, an online information service based on news stories indexed by country and type of news. TIS indexing is comparable to human indexing in overall accuracy but costs much less, is more consistent, and is available much more rapidly. TIS can be justified in terms of cost savings alone, but Reuters also expects the speed and consistency of TIS to provide significant competitive advantage and, hence, an increased market share for Country Reports and other products from Reuters Historical Information Products Division."
            },
            "slug": "CONSTRUE/TIS:-A-System-for-Content-Based-Indexing-a-Hayes-Weinstein",
            "title": {
                "fragments": [],
                "text": "CONSTRUE/TIS: A System for Content-Based Indexing of a Database of News Stories"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "The Construe news story categorization system assigns indexing terms to news stories according to their content using knowledge-based techniques and Reuters expects the speed and consistency of TIS to provide significant competitive advantage and, hence, an increased market share for Country Reports and other products from Reuters Historical Information Products Division."
            },
            "venue": {
                "fragments": [],
                "text": "IAAI"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66629494"
                        ],
                        "name": "J. Tague",
                        "slug": "J.-Tague",
                        "structuredName": {
                            "firstName": "Jean",
                            "lastName": "Tague",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tague"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17126228,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "03d792ec97ff9d42791e8ea44227f2681e100c57",
            "isKey": false,
            "numCitedBy": 80,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "The novice information scientist, though he or she may have thoroughly studied the design and results of previous information retrieval tests and clearly described the purpose of his/her own test, may still, when faced with its implementation, have great difficulty in proceeding. Early information retrieval experiments were of necessity ad hoc, and it is only in recent years that a body of practice, based on the experiences of Cleverdon and later investigators, has made possible a few recommendations on the pragmatics of conducting information retrieval experiments. The following remarks, though based to some extent on a study of the major tests, including those described in later chapters of this book, are heavily dependent on the author's own trials, tribulations, and mistakes. If there is one lesson to be learned from experience, it is that the theoretically optimum design can never be achieved, and the art of information retrieval experimentation is to make the compromises that will least detract from the usefulness of the results. In determining experimental procedures, three aspects must be kept in mind:"
            },
            "slug": "The-pragmatics-of-information-retrieval-Tague",
            "title": {
                "fragments": [],
                "text": "The pragmatics of information retrieval experimentation"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "If there is one lesson to be learned from experience, it is that the theoretically optimum design can never be achieved, and the art of information retrieval experimentation is to make the compromises that will least detract from the usefulness of the results."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34938639"
                        ],
                        "name": "W. Gale",
                        "slug": "W.-Gale",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Gale",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Gale"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2244184"
                        ],
                        "name": "Kenneth Ward Church",
                        "slug": "Kenneth-Ward-Church",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Church",
                            "middleNames": [
                                "Ward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenneth Ward Church"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693517"
                        ],
                        "name": "David Yarowsky",
                        "slug": "David-Yarowsky",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Yarowsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Yarowsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17567112,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bf35ed0864ff6cf524a24f0a65aa6951f9d6f214",
            "isKey": false,
            "numCitedBy": 657,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Word sense disambiguation has been recognized as a major problem in natural language processing research for over forty years. Both quantitive and qualitative methods have been tried, but much of this work has been stymied by difficulties in acquiring appropriate lexical resources. The availability of this testing and training material has enabled us to develop quantitative disambiguation methods that achieve 92% accuracy in discriminating between two very distinct senses of a noun. In the training phase, we collect a number of instances of each sense of the polysemous noun. Then in the testing phase, we are given a new instance of the noun, and are asked to assign the instance to one of the senses. We attempt to answer this question by comparing the context of the unknown instance with contexts of known instances using a Bayesian argument that has been applied successfully in related tasks such as author identification and information retrieval. The proposed method is probably most appropriate for those aspects of sense disambiguation that are closest to the information retrieval task. In particular, the proposed method was designed to disambiguate senses that are usually associated with different topics."
            },
            "slug": "A-method-for-disambiguating-word-senses-in-a-large-Gale-Church",
            "title": {
                "fragments": [],
                "text": "A method for disambiguating word senses in a large corpus"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The proposed method was designed to disambiguate senses that are usually associated with different topics using a Bayesian argument that has been applied successfully in related tasks such as author identification and information retrieval."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Humanit."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144009691"
                        ],
                        "name": "C. Buckley",
                        "slug": "C.-Buckley",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Buckley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Buckley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797808"
                        ],
                        "name": "G. Salton",
                        "slug": "G.-Salton",
                        "structuredName": {
                            "firstName": "Gerard",
                            "lastName": "Salton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Salton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144890574"
                        ],
                        "name": "James Allan",
                        "slug": "James-Allan",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Allan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Allan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 17522959,
            "fieldsOfStudy": [
                "Computer Science",
                "Business"
            ],
            "id": "c972982771aeaaafdbdfbbcc7fe205bdecb3cf24",
            "isKey": false,
            "numCitedBy": 372,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "The effects of adding information from relevant documents are examined in the TREC routing environment. A modified Rocchio relevance feedback approach is used, with a varying number of relevant documents retrieved by an initial SMART search, and a varying number of terms from those relevant documents used to expand the initial query. Recall-precision evaluation reveals that as the amount of expansion of the query due to adding terms from relevant documents increases, so does the effectiveness. There appears to be a linear relationship between the log of the number of terms added and the recall-precision effectiveness. There also appears to be a linear relationship between the log of the number of known relevant documents and the recall-precision effectiveness."
            },
            "slug": "The-effect-of-adding-relevance-information-in-a-Buckley-Salton",
            "title": {
                "fragments": [],
                "text": "The effect of adding relevance information in a relevance feedback environment"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Recall-precision evaluation reveals that as the amount of expansion of the query due to adding terms from relevant documents increases, so does the effectiveness, and there appears to be a linear relationship between the log of the number of terms added and the recall- Precision effectiveness."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '94"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144009691"
                        ],
                        "name": "C. Buckley",
                        "slug": "C.-Buckley",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Buckley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Buckley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797808"
                        ],
                        "name": "G. Salton",
                        "slug": "G.-Salton",
                        "structuredName": {
                            "firstName": "Gerard",
                            "lastName": "Salton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Salton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144890574"
                        ],
                        "name": "James Allan",
                        "slug": "James-Allan",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Allan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Allan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 28256885,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ade20fdc9d35df1ed9f0864b4061a712135f0d92",
            "isKey": false,
            "numCitedBy": 169,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The Smart project at Cornell University, using a completely automatic approach for both routing and ad-hoc experiments, performed extremely well in the first Text Retrieval Conference. The basic ad-hoc approach uses local/global matching to achieve its results. A global match ensures that each retrieved document uses the same vocabulary as the query; a local match then attempts to guarantee some local part of the document (eg, a paragraph or sentence) focuses on the query algorithm is used for routing experiments"
            },
            "slug": "Automatic-Retrieval-With-Locality-Information-Using-Buckley-Salton",
            "title": {
                "fragments": [],
                "text": "Automatic Retrieval With Locality Information Using SMART"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The Smart project at Cornell University, using a completely automatic approach for both routing and ad-hoc experiments, performed extremely well in the first Text Retrieval Conference."
            },
            "venue": {
                "fragments": [],
                "text": "TREC"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35140996"
                        ],
                        "name": "Michael de la Maza",
                        "slug": "Michael-de-la-Maza",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Maza",
                            "middleNames": [
                                "de",
                                "la"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael de la Maza"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 5590134,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3e3e574e4eab1a4e4a7e13ce0794674c242dbe9f",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Prototype-Based-Symbolic-Concept-Learning-System-Maza",
            "title": {
                "fragments": [],
                "text": "A Prototype Based Symbolic Concept Learning System"
            },
            "venue": {
                "fragments": [],
                "text": "ML"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3055160"
                        ],
                        "name": "David J. Ittner",
                        "slug": "David-J.-Ittner",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Ittner",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David J. Ittner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723766"
                        ],
                        "name": "H. Baird",
                        "slug": "H.-Baird",
                        "structuredName": {
                            "firstName": "Henry",
                            "lastName": "Baird",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Baird"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13929669,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a4542d98c28f31264ac3c2ab5dabbacf9067a05c",
            "isKey": false,
            "numCitedBy": 3,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a method for rapid prototyping of contextual analysis algorithms within an experimental page reader. Due to the great variety of such algorithms and their dependency on details of the page-reader\u2019s internal data structures, the state of the art today is that each new application requires custom low-level programming. This is undesirable since it impedes experimentation and restricts cost-effective applications to high-volume problems. To make contextual analysis more easily retargetable, we have designed a high-level language with primitives for traversing the document hierarchy and generating, scoring, sorting, and pruning interpretations. It is based on Ousterhout\u2019s interpreted language tcl which provides constructs such as variables, decisions, looping, etc, and is easily extended by adding functions. Some functions are table-driven or built-in: for example, character typing, typographical morphology analysis, and regular expression matching. Other functions are normally implemented as separately executing UNIX processes communicating with the page reader via pipes. These may be pre-existing software tools imported from other research fields such as computational linguistics, information retrieval, and string matching. We illustrate the expressive power of the language in applications to English text using a spell-checker, Japanese text using character n-grams, and mixed Russian-English text using two lexicons with automatic context-switching."
            },
            "slug": "PROGRAMMABLE-CONTEXTUAL-ANALYSIS-Ittner-Baird",
            "title": {
                "fragments": [],
                "text": "PROGRAMMABLE CONTEXTUAL ANALYSIS"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A high-level language with primitives for traversing the document hierarchy and generating, scoring, sorting, and pruning interpretations, based on Ousterhout\u2019s interpreted language tcl which provides constructs such as variables, decisions, looping, etc, and is easily extended by adding functions."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "134211067"
                        ],
                        "name": "J. Rocchio",
                        "slug": "J.-Rocchio",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Rocchio",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Rocchio"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61859400,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4083ad1066cfa2ff0d65866ef4b011399d6873d1",
            "isKey": false,
            "numCitedBy": 3242,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Relevance-feedback-in-information-retrieval-Rocchio",
            "title": {
                "fragments": [],
                "text": "Relevance feedback in information retrieval"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1971
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2244184"
                        ],
                        "name": "Kenneth Ward Church",
                        "slug": "Kenneth-Ward-Church",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Church",
                            "middleNames": [
                                "Ward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenneth Ward Church"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34938639"
                        ],
                        "name": "W. Gale",
                        "slug": "W.-Gale",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Gale",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Gale"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145461274"
                        ],
                        "name": "J. Helfman",
                        "slug": "J.-Helfman",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Helfman",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Helfman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35153517"
                        ],
                        "name": "D. Lewis",
                        "slug": "D.-Lewis",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Lewis",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lewis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7692118,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e24114c9a06e5aa14b8cb2f25e50edc06c3a7eeb",
            "isKey": false,
            "numCitedBy": 5,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We have argued elsewhere (Church and Mercer, 1993) that text is more available than ever before, and that the availability of massive quantities of data has been responsible for much of the recent interest in text analysis. Ideally, we wotdd hope that this data would be distributed in a convenient format such as SGML (Goldfarb, 1990), but in practice, we usually have to work with the data in whatever format it happens to be in, since we usually aren't in much of a position to tell tim data providers how to do their business. Recently, we have been working with a collection of 15,000 AT&T internal documents (500,000 pages or 100 million words). Unfortunately, this data is stored in a particularly inconvenient format: fax."
            },
            "slug": "Fax:-An-Alternative-to-SGML-Church-Gale",
            "title": {
                "fragments": [],
                "text": "Fax: An Alternative to SGML"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "It is argued elsewhere (Church and Mercer, 1993) that text is more available than ever before, and that the availability of massive quantities of data has been responsible for much of the recent interest in text analysis."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2094660110"
                        ],
                        "name": "Peter Biebricher",
                        "slug": "Peter-Biebricher",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Biebricher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter Biebricher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703148"
                        ],
                        "name": "N. Fuhr",
                        "slug": "N.-Fuhr",
                        "structuredName": {
                            "firstName": "Norbert",
                            "lastName": "Fuhr",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Fuhr"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2084356644"
                        ],
                        "name": "G. Lustig",
                        "slug": "G.-Lustig",
                        "structuredName": {
                            "firstName": "Gerhard",
                            "lastName": "Lustig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Lustig"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1959019"
                        ],
                        "name": "M. Schwantner",
                        "slug": "M.-Schwantner",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Schwantner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Schwantner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2084310078"
                        ],
                        "name": "Gerhard Knorz",
                        "slug": "Gerhard-Knorz",
                        "structuredName": {
                            "firstName": "Gerhard",
                            "lastName": "Knorz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gerhard Knorz"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 16543804,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "dacc3d6d45ec9bec3d2eedccd9dbb35881f0a225",
            "isKey": false,
            "numCitedBy": 78,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Since October 1985, the automatic indexing system AIR/PHYS has been used in the input production of the physics data base of the Fachinformationsentrum Karlsruhe/West Germany. The texts to be indexed are abstracts written in English. The system of descriptors is prescribed. For the application of the AIR/PHYS system a large-scale dictionary containing more than 600 000 word-descriptor relations reap. phrase-descriptor relations has been developed. Most of these relations have been obtained by means of statistical and heuristical methods. In consequence, the relation system is rather imperfect. Therefore, the indexing system needs some fault- tolerating features. An appropriate indexing approach and the corresponding structure of the AIR/PHYS system are described. Finally, the conditions of the application as well as problems of further development are discussed."
            },
            "slug": "The-automatic-indexing-system-AIR/PHYS-from-to-Biebricher-Fuhr",
            "title": {
                "fragments": [],
                "text": "The automatic indexing system AIR/PHYS - from research to applications"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "An appropriate indexing approach and the corresponding structure of the AIR/PHYS system are described, and the conditions of the application as well as problems of further development are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '88"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144541931"
                        ],
                        "name": "Edward E. Smith",
                        "slug": "Edward-E.-Smith",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Smith",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Edward E. Smith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3023259"
                        ],
                        "name": "D. Medin",
                        "slug": "D.-Medin",
                        "structuredName": {
                            "firstName": "Douglas",
                            "lastName": "Medin",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Medin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 144952337,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "f9f79457703ec65a8c21b2017365bd9e176be800",
            "isKey": false,
            "numCitedBy": 1740,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Categories-and-concepts-Smith-Medin",
            "title": {
                "fragments": [],
                "text": "Categories and concepts"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "corpusId": 125668557,
            "fieldsOfStudy": [],
            "id": "e7f9254f8e9678dedd8c2ccee99624c271ceeb14",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Categorical Data Analysis."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144169170"
                        ],
                        "name": "D. Harman",
                        "slug": "D.-Harman",
                        "structuredName": {
                            "firstName": "Donna",
                            "lastName": "Harman",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Harman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 46426807,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5fa83eb09ab77bbfbb9543790c2bc7557bea717a",
            "isKey": false,
            "numCitedBy": 202,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Relevance-Feedback-and-Other-Query-Modification-Harman",
            "title": {
                "fragments": [],
                "text": "Relevance Feedback and Other Query Modification Techniques"
            },
            "venue": {
                "fragments": [],
                "text": "Information Retrieval: Data Structures & Algorithms"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797808"
                        ],
                        "name": "G. Salton",
                        "slug": "G.-Salton",
                        "structuredName": {
                            "firstName": "Gerard",
                            "lastName": "Salton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Salton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 34382228,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3f2f6772d96d972e3b2da5aaa8a0f2feefdf827f",
            "isKey": false,
            "numCitedBy": 3884,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Automatic-Text-Processing:-The-Transformation,-and-Salton",
            "title": {
                "fragments": [],
                "text": "Automatic Text Processing: The Transformation, Analysis, and Retrieval of Information by Computer"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\The EEect of Noise on Concept Learning"
            },
            "venue": {
                "fragments": [],
                "text": "\\Machine Learning. An Artiicial Intelligence Approach. Volume II"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "36] \\Ulrich's International Periodicals Directory"
            },
            "venue": {
                "fragments": [],
                "text": "36] \\Ulrich's International Periodicals Directory"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\An Analysis of the Eects of Data Corruption on Text Retrieval Performance"
            },
            "venue": {
                "fragments": [],
                "text": "Thinking Machines Corporation"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\The Third Annual Text of OCR Accuracy"
            },
            "venue": {
                "fragments": [],
                "text": "UNLV Information Science R esearch Institute Annual Report"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\A 100-Font Classiier\", 1st Int'l Conference o n Document Analysis and Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "\\A 100-Font Classiier\", 1st Int'l Conference o n Document Analysis and Recognition"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\Experiments with Representation in a Document Retrieval System"
            },
            "venue": {
                "fragments": [],
                "text": "Information Technology: Research and Development"
            },
            "year": 1983
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {},
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 36,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/Text-categorization-of-low-quality-images-Ittner-Lewis/72fe75228c198854d4c43cc70a381643a28deca6?sort=total-citations"
}