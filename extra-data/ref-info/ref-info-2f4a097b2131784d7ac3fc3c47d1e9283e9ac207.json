{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145468098"
                        ],
                        "name": "M. M\u00f8ller",
                        "slug": "M.-M\u00f8ller",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "M\u00f8ller",
                            "middleNames": [
                                "Fodslette"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. M\u00f8ller"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 34583843,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6eda41640964af6f4faf3029688acb40aec79a15",
            "isKey": false,
            "numCitedBy": 13,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "A learning algorithm (CG) with superlinear convergence rate is introduced. The algorithm is based upon a class of optimization techniques well known in numerical analysis as the Conjugate Gradient Methods. CG uses second order information from the neural network but requires only O(N) memory usage, where N is the number of minimization variables; in our case all the weights in the network. The performance of CG is benchmarked against the performance of the ordinary backpropagation algorithm (BP). We find that CG is considerably faster than BP and that CG is able to perform the learning task with fewer hidden units."
            },
            "slug": "Learning-by-Conjugate-Gradients-M\u00f8ller",
            "title": {
                "fragments": [],
                "text": "Learning by Conjugate Gradients"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The performance of CG is benchmarked against the performance of the ordinary backpropagation algorithm (BP) and it is found that CG is considerably faster than BP and thatCG is able to perform the learning task with fewer hidden units."
            },
            "venue": {
                "fragments": [],
                "text": "IMYCS"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1822055"
                        ],
                        "name": "Raymond L. Watrous",
                        "slug": "Raymond-L.-Watrous",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Watrous",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Raymond L. Watrous"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "While BP is inefficient on these ravine phenomena, it is shown that SCG handles them effectively."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15329984,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "934e49dac717a924bfda841bf6e54c32e900f0d1",
            "isKey": false,
            "numCitedBy": 294,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of learning using connectionist networks, in which network connection strengths are modified systematically so that the response of the network increasingly approximates the desired response can be structured as an optimization problem. The widely used back propagation method of connectionist learning [19, 21, 18] is set in the context of nonlinear optimization. In this framework, the issues of stability, convergence and parallelism are considered. As a form of gradient descent with fixed step size, back propagation is known to be unstable, which is illustrated using Rosenbrock's function. This is contrasted with stable methods which involve a line search in the gradient direction. The convergence criterion for connectionist problems involving binary functions is discussed relative to the behavior of gradient descent in the vicinity of local minima. A minimax criterion is compared with the least squares criterion. The contribution of the momentum term [19, 18] to more rapid convergence is interpreted relative to the geometry of the weight space. It is shown that in plateau regions of relatively constant gradient, the momentum term acts to increase the step size by a factor of 1/1-\u03bc, where \u03bc is the momentum term. In valley regions with steep sides, the momentum constant acts to focus the search direction toward the local minimum by averaging oscillations in the gradient. Comments University of Pennsylvania Department of Computer and Information Science Technical Report No. MSCIS-88-62. This technical report is available at ScholarlyCommons: http://repository.upenn.edu/cis_reports/597 LEARNING ALGORITHMS FOR CONNECTIONIST NETWORKS: APPLIED GRADIENT METHODS OF NONLINEAR OPTIMIZATION"
            },
            "slug": "Learning-Algorithms-for-Connectionist-Networks:-of-Watrous",
            "title": {
                "fragments": [],
                "text": "Learning Algorithms for Connectionist Networks: Applied Gradient Methods of Nonlinear Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is shown that in plateau regions of relatively constant gradient, the momentum term acts to increase the step size by a factor of 1/1-\u03bc, where \u03bc is the momentumTerm, and in valley regions with steep sides,The momentum constant acts to focus the search direction toward the local minimum by averaging oscillations in the gradient."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700994"
                        ],
                        "name": "R. Battiti",
                        "slug": "R.-Battiti",
                        "structuredName": {
                            "firstName": "Roberto",
                            "lastName": "Battiti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Battiti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144493859"
                        ],
                        "name": "F. Masulli",
                        "slug": "F.-Masulli",
                        "structuredName": {
                            "firstName": "Francesco",
                            "lastName": "Masulli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Masulli"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 165
                            }
                        ],
                        "text": "Battiti and Masulli has used a variation of the standard conjugate gradient method, the one-step Broyden-Fletcher-Goldfarb-Shanno memoryless quasi-Newton algorithm (BFGS), as an alternative learning algorithm."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 246,
                                "start": 243
                            }
                        ],
                        "text": "It is well known from the optimization literature that gradient descent methods, like BP, are very inefficient, when the weight space contains long ravines that are characterized by sharp curvature across the ravine and a gently sloping floor [1], [13]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 88
                            }
                        ],
                        "text": "A line-search could also be used in the above algorithm, but this would like in CGB and BFGS raise the calculation complexity per iteration considerable."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 110
                            }
                        ],
                        "text": "Compared to BP, the algorithm yields a speed-up of 100-500 relative to the amount of learning iterations used [1]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 0
                            }
                        ],
                        "text": "Battiti and Masulli obtained a speed-up of 500 compared to\n15\na variation of BP, called the Bold Driver Method (BD), where the learning rate is raised or lowered depending on whether the global error is increasing or decreasing [8], [16], [1]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 258,
                                "start": 255
                            }
                        ],
                        "text": "The performance of SCG is benchmarked against the performance of the standard backpropagation algorithm (BP) [13], the conjugate gradient backpropagation (CGB) [6] and the one-step Broyden-Fletcher-Goldfarb-Shanno memoryless quasi-Newton algorithm (BFGS) [1]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 142
                            }
                        ],
                        "text": "In order to compare SCG with BFGS, SCG was tested on a problem original introduced by Lapedes and Farber [8], but used by Battiti and Masulli [1] in testing BFGS."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 104
                            }
                        ],
                        "text": "At a convergence criterion of 0.01 SCG obtains a speed-up of about 540 which is better than reported by Battiti and Masulli even though their speed-up not takes the calculation complexity per learning iteration into account."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 38
                            }
                        ],
                        "text": "The calculation complexity of CGB and BFGS is about O(2-15N2) since the line-search in average involves 2-15 calls of E(w) or E'(w) per iteration [3]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 69
                            }
                        ],
                        "text": "The recurrence\nrelation is\n(7) xn+1 = r xn(1 \u2013 xn) 0   xn   1\nAccording to Battiti and Masulli relation (7) is an ergodic, chaotic system, when r = 4."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 117
                            }
                        ],
                        "text": "SCG is fully automated including no user dependent parameters and avoids a time consuming line-search, which CGB and BFGS uses in each iteration in order to determine an appropriate step size."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 21
                            }
                        ],
                        "text": "Both the CGB and the BFGS algorithm solves the problem by using a line-search per iteration in order to\n10\ndetermine a better step size."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 13
                            }
                        ],
                        "text": "Both CGB and BFGS raise the calculation complexity per learning iteration considerable, because they have to perform a line-search in order to determine an appropriate step size."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 177
                            }
                        ],
                        "text": "a variation of BP, called the Bold Driver Method (BD), where the learning rate is raised or lowered depending on whether the global error is increasing or decreasing [8], [16], [1]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 115
                            }
                        ],
                        "text": "Several conjugate gradient algorithms have recently been introduced as learning algorithms in neural networks [6], [1], [11]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 65
                            }
                        ],
                        "text": "The logistic map is a discrete-time, non-linear dynamical system [1]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 14
                            }
                        ],
                        "text": "Unfortunately Battiti and Masulli do not take the calculation complexity per learning iteration into account when calculating this impressive speed-up."
                    },
                    "intents": []
                }
            ],
            "corpusId": 60014158,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "90c4cd3e7f74cb937ebe592ca38c21886e7ed44e",
            "isKey": true,
            "numCitedBy": 130,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Standard back-propagation learning (BP) is known to have slow convergence properties. Furthermore no general prescription is given for selecting the appropriate learning rate, so success is dependent on a trial and error process. In this work a well known optimization technique (the Broyden-Fletcher-Goldfarb-Shanno memoryless quasi-Newton method) is employed to speed up convergence and to select parameters. The strict locality requirement is relaxed but parallelism of computation is maintained, allowing efficient use of concurrent computation. While requiring only limited changes to BP, this method yields a speed-up factor of 100 \u2013 500 for the medium-size networks considered."
            },
            "slug": "BFGS-Optimization-for-Faster-and-Automated-Learning-Battiti-Masulli",
            "title": {
                "fragments": [],
                "text": "BFGS Optimization for Faster and Automated Supervised Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The Broyden-Fletcher-Goldfarb-Shanno memoryless quasi-Newton method is employed to speed up convergence and to select parameters, allowing efficient use of concurrent computation."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145468098"
                        ],
                        "name": "M. M\u00f8ller",
                        "slug": "M.-M\u00f8ller",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "M\u00f8ller",
                            "middleNames": [
                                "Fodslette"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. M\u00f8ller"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 9963836,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e0631a99f68cb0c159b15f1cbbaa894bc9f5a738",
            "isKey": false,
            "numCitedBy": 47,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "A novel algorithm combining the good properties of offline and online algorithms is introduced. The efficiency of supervised learning algorithms on small-scale problems does not necessarily scale up to large-scale problems. The redundancy of large training sets is reflected as redundancy gradient vectors in the network. Accumulating these gradient vectors implies redundant computations. In order to avoid these redundant computations a learning algorithm has to be able to update weights independently of the size of the training set. The stochastic learning algorithm proposed, the stochastic scaled conjugate gradient (SSCG) algorithm, has this property. Experimentally, it is shown that SSCG converges faster than the online backpropagation algorithm on the nettalk problem.<<ETX>>"
            },
            "slug": "Supervised-learning-on-large-redundant-training-M\u00f8ller",
            "title": {
                "fragments": [],
                "text": "Supervised learning on large redundant training sets"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "A novel algorithm combining the good properties of offline and online algorithms is introduced, the stochastic scaled conjugate gradient (SSCG), and it is shown that SSCG converges faster than the online backpropagation algorithm on the nettalk problem."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks for Signal Processing II Proceedings of the 1992 IEEE Workshop"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107780685"
                        ],
                        "name": "T. Yoshida",
                        "slug": "T.-Yoshida",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Yoshida",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Yoshida"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 28
                            }
                        ],
                        "text": "Since learning in realistic neural network applications often involves adjustment of several thousand weights only optimization methods that are applicable to large-scale problems, are relevant as alternative learning algorithms."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 61775144,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "16fa9cdd4e6ad20baa3a9d89dfd424551c5b070d",
            "isKey": false,
            "numCitedBy": 5,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Summary form only given, as follows. A learning algorithm for multilayered neural networks which is implemented by a Newton method using automatic differentiation was compared to the back-propagation method. It has been thought that the computational cost for obtaining second-order derivatives of an error function is very high, and that a system of linear equations (the Newton equations) cannot be solved practically for large-scale neural networks. However, a forward method of automatic differentiation enables one to calculate the product of the Hessian of the error function and a search direction vector, without calculation of the Hessian itself, with a cost proportional to the cost for the error function. Therefore, even if the network is large, the Newton equations can be solved. Computer simulations show that this method converges to the solutions more rapidly than the back-propagation method.<<ETX>>"
            },
            "slug": "A-learning-algorithm-for-multilayered-neural-a-Yoshida",
            "title": {
                "fragments": [],
                "text": "A learning algorithm for multilayered neural networks: a Newton method using automatic differentiation"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "Computer simulations show that the learning algorithm for multilayered neural networks which is implemented by a Newton method using automatic differentiation converges to the solutions more rapidly than the back-propagation method."
            },
            "venue": {
                "fragments": [],
                "text": "IJCNN-91-Seattle International Joint Conference on Neural Networks"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40371087"
                        ],
                        "name": "E. Johansson",
                        "slug": "E.-Johansson",
                        "structuredName": {
                            "firstName": "Erik",
                            "lastName": "Johansson",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Johansson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50247236"
                        ],
                        "name": "F. Dowla",
                        "slug": "F.-Dowla",
                        "structuredName": {
                            "firstName": "Farid",
                            "lastName": "Dowla",
                            "middleNames": [
                                "U."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Dowla"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46821262"
                        ],
                        "name": "D. Goodman",
                        "slug": "D.-Goodman",
                        "structuredName": {
                            "firstName": "Dennis",
                            "lastName": "Goodman",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Goodman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 40190917,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "74a063a505ec68d7e6558abbf8ead5b783074c90",
            "isKey": false,
            "numCitedBy": 312,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "In many applications, the number of interconnects or weights in a neural network is so large that the learning time for the conventional backpropagation algorithm can become excessively long. Numerical optimization theory offers a rich and robust set of techniques which can be applied to neural networks to improve learning rates. In particular, the conjugate gradient method is easily adapted to the backpropagation learning problem. This paper describes the conjugate gradient method, its application to the backpropagation learning problem and presents results of numerical tests which compare conventional backpropagation, steepest descent and the conjugate gradient methods. For the parity problem, we find that the conjugate gradient method is an order of magnitude faster than conventional backpropagation with momentum."
            },
            "slug": "Backpropagation-Learning-for-Multilayer-Neural-the-Johansson-Dowla",
            "title": {
                "fragments": [],
                "text": "Backpropagation Learning for Multilayer Feed-Forward Neural Networks Using the Conjugate Gradient Method"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "For the parity problem, it is found that the conjugate gradient method is an order of magnitude faster than conventional backpropagation with momentum."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Neural Syst."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700994"
                        ],
                        "name": "R. Battiti",
                        "slug": "R.-Battiti",
                        "structuredName": {
                            "firstName": "Roberto",
                            "lastName": "Battiti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Battiti"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "While BP is inefficient on these ravine phenomena, it is shown that SCG handles them effectively."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17193734,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1861ba1d857984384e93dc7ab5658751099182ee",
            "isKey": false,
            "numCitedBy": 202,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Two methods for incr easing performance of th e backpropagat ion learning algorithm are present ed and their result s are compared with those obtained by optimi zing par ameters in the standard method . The first method requires adaptation of a scalar learning rat e in order to decrease th e energy value along the gradient direction in a close-to-optimal way. Th e second is derived from the conjugate gradient method with inexact linear searches . The strict locality requirement is relaxed but parallelism of computation is maintained, allowing efficient use of concurrent computation. For medium-size probl ems, typical speedups of one order of magnitude are obtained."
            },
            "slug": "Accelerated-Backpropagation-Learning:-Two-Methods-Battiti",
            "title": {
                "fragments": [],
                "text": "Accelerated Backpropagation Learning: Two Optimization Methods"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "Two methods for easing performance of the backpropagat ion learning algorithm are presented and their result s are compared with those obtained by optimi zing par ameters in the standard method."
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48306096"
                        ],
                        "name": "M. Hestenes",
                        "slug": "M.-Hestenes",
                        "structuredName": {
                            "firstName": "Magnus",
                            "lastName": "Hestenes",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hestenes"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 65
                            }
                        ],
                        "text": ",pk is a subset of a conjugate system, is called a k-plane or \u03c0k [4]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 4
                            }
                        ],
                        "text": "The Conjugate Gradient Methods are also based on the above general optimization strategy, but chooses the search direction and the step size more carefully by using information from the second order approximation E(w+y) \u2248 E(w)+E'(w)Ty + 1 2 yTE''(w)y. Quadratic functions have some nice properties that general functions not necessarily have."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 103
                            }
                        ],
                        "text": "The algorithm is based upon a class of optimization techniques well known in numerical analysis as the Conjugate Gradient Methods."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 77
                            }
                        ],
                        "text": "8 We are now able to formulate a conjugate gradient algorithm as proposed by [4]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 79
                            }
                        ],
                        "text": ", initializing p k+1 to the current steepest descent direction r k+1 [2], [3], [4], [12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 40
                            }
                        ],
                        "text": "5 A proof for theorem 1 can be found in [4]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 79
                            }
                        ],
                        "text": "If a conjugate system is available the solution can be simplified considerable [4], [6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 70
                            }
                        ],
                        "text": "These methods are often referred to as the Conjugate Gradient Methods [4], [2], [3], [12]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 28
                            }
                        ],
                        "text": "SCG belongs to the class of Conjugate Gradient Methods, which shows superlinear convergence on most problems."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 148
                            }
                        ],
                        "text": "It might also be useful to recall that the error function E(w) in a given point (w+y) in RN can be expressed by the well known Taylor expansion [3],[4] (5) E(w+y) = E(w)+ E'(w)Ty + 1 2 yTE''(w)y + ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 40
                            }
                        ],
                        "text": "6 A proof for theorem 2 can be found in [4] and [6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 97
                            }
                        ],
                        "text": "Only if the Hessian matrix E ''(w) is positive definite then E qw(y) has a unique global minimum [4]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 118
                            }
                        ],
                        "text": "The set is said to be a conjugate system with respect to a non-singular symmetric NxN matrix A if the following holds [4]"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 122943214,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ca7d18f675511a90c882a6f268c61d36c41658e7",
            "isKey": true,
            "numCitedBy": 439,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "I Newton's Method and the Gradient Method.- 1 Introduction.- 2 Fundamental Concepts.- 3 Iterative Methods for Solving g(x) = 0.- 4 Convergence Theorems.- 5 Minimization of Functions by Newton's Method.- 6 Gradient Methods-The Quadratic Case.- 7 General Descent Methods.- 8 Iterative Methods for Solving Linear Equations.- 9 Constrained Minima.- II Conjugate Direction Methods.- 1 Introduction.- 2 Quadratic Functions on En.- 3 Basic Properties of Quadratic Functions.- 4 Minimization of a Quadratic Function F on k-Planes.- 5 Method of Conjugate Directions (CD-Method).- 6 Method of Conjugate Gradients (CG-Algorithm).- 7 Gradient PARTAN.- 8 CG-Algorithms for Nonquadratic Functions.- 9 Numerical Examples.- 10 Least Square Solutions.- III Conjugate Gram-Schmidt Processes.- 1 Introduction.- 2 A Conjugate Gram-Schmidt Process.- 3 CGS-CG-Algorithms.- 4 A Connection of CGS-Algorithms with Gaussian Elimination.- 5 Method of Parallel Displacements.- 6 Methods of Parallel Planes (PARP).- 7 Modifications of Parallel Displacements Algorithms.- 8 CGS-Algorithms for Nonquadratic Functions.- 9 CGS-CG-Routines for Nonquadratic Functions.- 10 Gauss-Seidel CGS-Routines.- 11 The Case of Nonnegative Components.- 12 General Linear Inequality Constraints.- IV Conjugate Gradient Algorithms.- 1 Introduction.- 2 Conjugate Gradient Algorithms.- 3 The Normalized CG-Algorithm.- 4 Termination.- 5 Clustered Eigenvalues.- 6 Nonnegative Hessians.- 7 A Planar CG-Algorithm.- 8 Justification of the Planar CG-Algorithm.- 9 Modifications of the CG-Algorithm.- 10 Two Examples.- 11 Connections between Generalized CG-Algorithms and Stadard CG- and CD-Algorithm.- 12 Least Square Solutions.- 13 Variable Metric Algorithms.- 14 A Planar CG-Algorithm for Nonquadratic Functions.- References."
            },
            "slug": "Conjugate-Direction-Methods-in-Optimization-Hestenes",
            "title": {
                "fragments": [],
                "text": "Conjugate Direction Methods in Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This chapter discusses the development of Conjugate Gradient Algorithms, a type of Algorithm for solving the problem of Nonquadratic Functions without resorting to Eigenvalues."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 99
                            }
                        ],
                        "text": "Several adaptive learning algorithms for feedforward neural networks have recently been discovered (Hinton, 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7840452,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a57c6d627ffc667ae3547073876c35d6420accff",
            "isKey": false,
            "numCitedBy": 1575,
            "numCiting": 122,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Connectionist-Learning-Procedures-Hinton",
            "title": {
                "fragments": [],
                "text": "Connectionist Learning Procedures"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680145"
                        ],
                        "name": "H. M\u00fchlenbein",
                        "slug": "H.-M\u00fchlenbein",
                        "structuredName": {
                            "firstName": "Heinz",
                            "lastName": "M\u00fchlenbein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. M\u00fchlenbein"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Figure 4. Composed modular network for =1 problem (n = 8) [ 10 ]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "the limitations of multi-layer perceptrons [ 10 ], that the backpropagation algorithm is not"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5082416,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "983d24b804455c6763f15a7b52f127a46b854885",
            "isKey": false,
            "numCitedBy": 64,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Limitations-of-multi-layer-perceptron-towards-M\u00fchlenbein",
            "title": {
                "fragments": [],
                "text": "Limitations of multi-layer perceptron networks-steps towards genetic neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Parallel Comput."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699108"
                        ],
                        "name": "G. Tesauro",
                        "slug": "G.-Tesauro",
                        "structuredName": {
                            "firstName": "Gerald",
                            "lastName": "Tesauro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Tesauro"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 50
                            }
                        ],
                        "text": "This depends, however, of the nature of task [5], [15]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13265971,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "948e0898db672a86841b10567afe3a15355531b1",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "We st ud y th e amount of ti me needed to learn a fixed t rain\u00ad ing se t in the \"back-pro pagation\" proced ure for learning in multi-layer ne ural network models. The task chosen was 32-bit parity, a hi gh\u00ad order fu nct ion for wh ich memor iza ti on o f specific in p u t- out put pairs is necessary. For small t raining sets, the learning time is consistent with a ~-power law depen dence on the nu mber of patterns in the t ra ining set. For lar ger training set s, t he learn ing t ime dive rges at a critical t ra ining set size which appears to be related to the st orage capacity of t he network."
            },
            "slug": "Scaling-Relationships-in-Back-Propagation-Learning:-Tesauro",
            "title": {
                "fragments": [],
                "text": "Scaling Relationships in Back-Propagation Learning: Dependence on Training Set Size"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "The task chosen was 32-bit parity, a challenge for learning in multi-layer ne ural network models where the learning time is consistent with a ~-power law depen dence on the nu mber of patterns in the t ra ining set."
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst."
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144932833"
                        ],
                        "name": "J. S. Judd",
                        "slug": "J.-S.-Judd",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Judd",
                            "middleNames": [
                                "Stephen"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. S. Judd"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 56909682,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ed6f076369825e36254c2554824544f840663768",
            "isKey": false,
            "numCitedBy": 47,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "WE FORMALIZE A NOTION OF LEARNING IN CONNECTIONIST NETWORKS THAT CHARAC- TERIZES THE TRAINING OF FEED-FORWARD NETWORKS. CONSIDERING DIFFERENT FAM- ILIES OF NODE FUNCTIONS, WE PROVE THE LEARNING PROBLEM NP-COMPLETE AND THUS DEMONSTRATE THAT IS HAS NO EFFICIENT GENERAL SOLUTION. ONE FAMILY OF NODE FUNCTIONS STUDIED IS THE SET OF LOGISTIC-LINEAR FUNCTIONS, AS USED BY THE POPULAR BACK-PROPOGATION ALGORITHM. SEVERAL IMPLICATIONS OF THE THEOREM ARE DISCUSSED, INCLUDING WHY THIS RESULT IS ACTUALLY HELPFUL FOR CONNECTION IST LEARNING RESEARCH."
            },
            "slug": "Complexity-of-Connectionist-Learning-with-Various-Judd",
            "title": {
                "fragments": [],
                "text": "Complexity of Connectionist Learning with Various Node Functions"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is proved the learning problem NP-COMPLETE has no effective general solution, and the set of logistic-linear FUNCTIONS, as used by the popular back-PROPOGATION ALGORITHM, is considered."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744288"
                        ],
                        "name": "P. Gill",
                        "slug": "P.-Gill",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Gill",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Gill"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143873253"
                        ],
                        "name": "W. Murray",
                        "slug": "W.-Murray",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Murray",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Murray"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685738"
                        ],
                        "name": "M. H. Wright",
                        "slug": "M.-H.-Wright",
                        "structuredName": {
                            "firstName": "Margaret",
                            "lastName": "Wright",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. H. Wright"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 20611582,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b8d9abd1c078573188b13d36c1b1efb7cb2fa865",
            "isKey": false,
            "numCitedBy": 7627,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Practical Optimization MethodsFree eBook: Practical Aspects of Structural Optimization [1701.01450] Practical optimization for hybrid quantum Practical Optimization | 4e70c9cf5faf796993a61adc9f46f2c5Acces PDF Practical OptimizationPractical Bayesian Optimization of Machine Learning Particle Swarm Optimization (PSO) An Overview Practical Issues Optimization Algorithms in Physics Practical Mathematical Optimization Universit T BremenA Practical Price Optimization Approach for Omnichannel A Gentle Introduction to Stochastic Optimization AlgorithmsApplied Sciences | Free Full-Text | Evolutionary 0387986316 Practical Optimization Methods: with A Lecture on Model Predictive ControlPractical Optimization : Algorithms and Engineering Wiley Series in Discrete Mathematics and Optimization Ser PRACTICAL OPTIMIZATION uCozEvolutionary practical optimization | DeepDyveA Practical Guide To Hyperparameter Optimization.Blood platelet production: a novel approach for practical [PDF] Practical Bilevel Optimization Download and Read Stability and Sample-based Approximations of Composite Practical portfolio optimization in Python (2/3) machine (PDF) Practical Financial Optimization. Decision making A Multiobjective Optimization Model for Prevention and Particle swarm optimization WikipediaPractical Methods Of Optimization|RPractical Portfolio Optimization London Business SchoolBao: Making Learned Query Optimization PracticalApache Spark Core Practical Optimization DatabricksPractical Methods of Optimization by R. FletcherChapter 11 Nonlinear Optimization Examples4.7 Applied Optimization Problems \u2013 Calculus Volume 1Practical bayesian optimization using Goptuna | by Masashi Practical Optimization Methods For 4th Generation Cellular Facility location problems \u2014 Mathematical Optimization Practical optimization (2004 edition) | Open Library[J726.Ebook] PDF Download Practical Optimization of Multi-objective Exploration for Practical Optimization Practical Optimization: a Gentle Introduction has moved!?Practical Rod Pumping Optimization on Apple Books(PDF) Practical Optimization with MATLAB The Free StudyPractical portfolio optimization in Python (3/3) code (PDF) Practical, Fast and Robust Point Cloud Registration Numerical Optimization Stanford UniversityPractical Optimization Methods with Mathematica ApplicationsPractical Optimization | 4e70c9cf5faf796993a61adc9f46f2c5Search Engine Optimization: Practical Marketing TechniquesLagout.orgMeter Placement in Active Distribution System using Manual: Practical guide to optimization for mobiles Unity"
            },
            "slug": "Practical-optimization-Gill-Murray",
            "title": {
                "fragments": [],
                "text": "Practical optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "This ebook Practical Optimization by Philip E. Gill is presented in pdf format and the full version of this ebook in DjVu, ePub, doc, txt, PDF forms is presented."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144733293"
                        ],
                        "name": "R. Fletcher",
                        "slug": "R.-Fletcher",
                        "structuredName": {
                            "firstName": "Roger",
                            "lastName": "Fletcher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fletcher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "referred to as the Conjugate Gradient Methods [4], [ 2 ], [3], [12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "If the algorithm has not converged after N steps, the algorithm is restarted, i.e., initializing p k+1 to the current steepest descent direction r k+1 [ 2 ], [3], [4],"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Several other formulas for \u03b2k can be derived [4], [ 2 ], [3], [6], but when the conjugate"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "\u03bbk is raised and lowered following the formula [ 2 ]"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "considerable. Instead a Levenberg-Marq uardt approach was used in modifying the algorithm [ 2 ].7 The idea is to introduce a scalar \u03bbk, which is supposed to regulate the"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The idea of the strategy is illustrated in the pseudo algorithm presented below, which minimizes the error function E(w) [ 2 ]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The Levenberg-Marquardt algorithm [ 2 ] has to raise \u03bbk with a constant factor, whenever"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Levenberg-Marquardt approach [ 2 ] in order to scale the step size."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "8 \u03bbk is also known as a Lagrange Multiplier [ 2 ]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Praxis shows that this is often the case [ 2 ]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 123487779,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1b84b383ad59f79e607ad0f08a8a10876631a0cd",
            "isKey": true,
            "numCitedBy": 9912,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface Table of Notation Part 1: Unconstrained Optimization Introduction Structure of Methods Newton-like Methods Conjugate Direction Methods Restricted Step Methods Sums of Squares and Nonlinear Equations Part 2: Constrained Optimization Introduction Linear Programming The Theory of Constrained Optimization Quadratic Programming General Linearly Constrained Optimization Nonlinear Programming Other Optimization Problems Non-Smooth Optimization References Subject Index."
            },
            "slug": "Practical-Methods-of-Optimization-Fletcher",
            "title": {
                "fragments": [],
                "text": "Practical Methods of Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "The aim of this book is to provide a Discussion of Constrained Optimization and its Applications to Linear Programming and Other Optimization Problems."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 62245742,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "111fd833a4ae576cfdbb27d87d2f8fc0640af355",
            "isKey": false,
            "numCitedBy": 19355,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-internal-representations-by-error-Rumelhart-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning internal representations by error propagation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 292,
                                "start": 216
                            }
                        ],
                        "text": "The general opinion in the numerical analysis community is that especially one class of optimization methods, called the Conjugate Gradient Methods, are well suited to handle large-scale problems in an effective way (Fletcher, 1975; Gill, Murray, & Wright, 1980; Hestenes, 1980; Powell, 1977)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 68
                            }
                        ],
                        "text": ", initializing fik+J to the current steepest descent direction ~k+~ (Fletcher, 1975; Powell, 1977)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 43
                            }
                        ],
                        "text": "Practice shows that this is often the case (Fletcher, 1975)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 44
                            }
                        ],
                        "text": "7 Xk is also known as a Lagrange Multiplier ( Fletcher, 1975)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 46
                            }
                        ],
                        "text": "Several other formulas for \u00a2/k can be derived (Fletcher, 1975; Gill, Murray, & Wright, 1980; Hestenes, 1980), but when the conjugate gradient methods are applied to nonquadratic functions, the above formula, called the Hestenes-Stiefel formula, for /3k is considered superior."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 23
                            }
                        ],
                        "text": "8 \u03bbk is also known as a Lagrange Multiplier [2]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Practical methods ofoptimi=ation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1975
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 165
                            }
                        ],
                        "text": "Battiti and Masulli has used a variation of the standard conjugate gradient method, the one-step Broyden-Fletcher-Goldfarb-Shanno memoryless quasi-Newton algorithm (BFGS), as an alternative learning algorithm."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 88
                            }
                        ],
                        "text": "A line-search could also be used in the above algorithm, but this would like in CGB and BFGS raise the calculation complexity per iteration considerable."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 253,
                                "start": 249
                            }
                        ],
                        "text": "The performance of SCG is benchmarked against the performance of the standard backpropagation algorithm (BP) [13], the conjugate gradient backpropagation (CGB) [6] and the one-step Broyden-Fletcher-Goldfarb-Shanno memoryless quasi-Newton algorithm (BFGS) [1]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 100
                            }
                        ],
                        "text": "Battiti and Massuli have proposed another method from the optimization literature known as the BFGS (Battiti, 1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 29
                            }
                        ],
                        "text": "In order to compare SCG with BFGS, SCG was tested on a problem original introduced by Lapedes and Farber [8], but used by Battiti and Masulli [1] in testing BFGS."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 38
                            }
                        ],
                        "text": "The calculation complexity of CGB and BFGS is about O(2-15N2) since the line-search in average involves 2-15 calls of E(w) or E'(w) per iteration [3]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 117
                            }
                        ],
                        "text": "SCG is fully automated including no user dependent parameters and avoids a time consuming line-search, which CGB and BFGS uses in each iteration in order to determine an appropriate step size."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 21
                            }
                        ],
                        "text": "Both the CGB and the BFGS algorithm solves the problem by using a line-search per iteration in order to\n10\ndetermine a better step size."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 13
                            }
                        ],
                        "text": "Both CGB and BFGS raise the calculation complexity per learning iteration considerable, because they have to perform a line-search in order to determine an appropriate step size."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Optimization methods for back-propagation: Automatic parameter tuning and faster convergence"
            },
            "venue": {
                "fragments": [],
                "text": "IJNNC - 90 - I ~ ISH . DC ."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46648391"
                        ],
                        "name": "J. T. Schwartz",
                        "slug": "J.-T.-Schwartz",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Schwartz",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. T. Schwartz"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 223,
                                "start": 219
                            }
                        ],
                        "text": "If a neural network has to function in a dynamical environment the structure of the neural network must reflect the natural geometry or at any rate some informationally significant dimensions of the problem domain [9], [14]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 141036709,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "1dbd964cd4a26172276babfc85c9bb62e46dc8e5",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-new-connectionism:-developing-relationships-and-Schwartz",
            "title": {
                "fragments": [],
                "text": "The new connectionism: developing relationships between neuroscience and artificical intelligence"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "113094833"
                        ],
                        "name": "I. Pratt",
                        "slug": "I.-Pratt",
                        "structuredName": {
                            "firstName": "Ianthe",
                            "lastName": "Pratt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Pratt"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 146099776,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cc31e31005077eb24c5d51c31137ee7eabda4fad",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-artificial-intelligence-debate:-false-starts,-Pratt",
            "title": {
                "fragments": [],
                "text": "The artificial intelligence debate: false starts, real foundations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 105
                            }
                        ],
                        "text": "Including a\u05bcmomentum term in the BP algorithm is an attempt in an ad hoc fashion to force the algorithm to use second order information from the network."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 59861896,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "01b6affe3ea4eae1978aec54e87087feb76d9215",
            "isKey": false,
            "numCitedBy": 863,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Generalization-and-network-design-strategies-LeCun",
            "title": {
                "fragments": [],
                "text": "Generalization and network design strategies"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Practical methods ofoptimi=ation Practical optimization"
            },
            "venue": {
                "fragments": [],
                "text": "Practical methods ofoptimi=ation Practical optimization"
            },
            "year": 1975
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Complexit)' of connectionist learning with various nodefimctions"
            },
            "venue": {
                "fragments": [],
                "text": "COINS Technical Report 87-60,"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Sqfeguardedsteplength algorithms .for optimization using descent methods"
            },
            "venue": {
                "fragments": [],
                "text": "NPL Report NAC"
            },
            "year": 1974
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The New Connectionism : Developing Relationships Between Neuroscience and Artificial Intelligence , in : The Artificial Intelligence Debate"
            },
            "venue": {
                "fragments": [],
                "text": ". False Starts , Real Foundations"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Sqfeguardedsteplength algorithms . for optimization using descent methods"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1974
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Sqfeguardedsteplength algorithms .for optimization using descent methods. NPL Report NAC 37, National Physica Laboratory, Division of Numerical Analysis and Computing, Middlesex, England"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1974
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning by Conjugate Gradients, The 6th International Meeting of Young Computer Scientists, Czechoslovakia, in press"
            },
            "venue": {
                "fragments": [],
                "text": "Learning by Conjugate Gradients, The 6th International Meeting of Young Computer Scientists, Czechoslovakia, in press"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "School of Cognitive and Computing Sciences, University of Sussex, UK"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "School of Cognitive and Computing Sciences"
            },
            "venue": {
                "fragments": [],
                "text": "School of Cognitive and Computing Sciences"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 166
                            }
                        ],
                        "text": "a variation of BP, called the Bold Driver Method (BD), where the learning rate is raised or lowered depending on whether the global error is increasing or decreasing [8], [16], [1]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 105
                            }
                        ],
                        "text": "In order to compare SCG with BFGS, SCG was tested on a problem original introduced by Lapedes and Farber [8], but used by Battiti and Masulli [1] in testing BFGS."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A self-optimizing neural net for content addressable memory and pattern recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Physia"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 101
                            }
                        ],
                        "text": "If a neural network has to function in a dynamical environment the structure of the neural network must reflect the natural geometry or at any rate some informationally significant dimensions of the problem domain [9], [14]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Optimization methods for back-propagation: Automatic parameter tuning and faster convergence. IJNNC-90- I~ISH"
            },
            "venue": {
                "fragments": [],
                "text": "Optimization methods for back-propagation: Automatic parameter tuning and faster convergence. IJNNC-90- I~ISH"
            },
            "year": 1990
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 12,
            "methodology": 8
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 31,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/A-scaled-conjugate-gradient-algorithm-for-fast-M\u00f8ller/2f4a097b2131784d7ac3fc3c47d1e9283e9ac207?sort=total-citations"
}