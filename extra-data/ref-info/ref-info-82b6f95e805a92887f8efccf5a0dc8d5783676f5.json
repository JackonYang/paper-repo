{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1925316"
                        ],
                        "name": "M. Pilu",
                        "slug": "M.-Pilu",
                        "structuredName": {
                            "firstName": "Maurizio",
                            "lastName": "Pilu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pilu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145099732"
                        ],
                        "name": "S. Pollard",
                        "slug": "S.-Pollard",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Pollard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Pollard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 42
                            }
                        ],
                        "text": "Using a set of simple and fast algorithms [79], HP has demonstrated a PDA text recognizer and translator [118]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 66
                            }
                        ],
                        "text": "Bilinear interpolation has been found effective in many instances [42,53,75,79,90]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 34
                            }
                        ],
                        "text": "Although it has many restrictions [79], it showed great potential in its combination of mobile devices and cameras."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6409155,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c761cb062571848c1f12ff311aa1d8678be78b39",
            "isKey": false,
            "numCitedBy": 54,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "In the past few years we have witnessed the migration of cheap imaging devices into portable and mobile computing appliances such as PDAs and mobile phones. As these devices become ever more powerful in terms of processor speed and memory, new exciting applications and uses are being developed. A particularly useful one is the casual capture of text images for faxing, note taking, OCR, etcetera. This paper describes the image processing pipeline used to enhance images of text captured by a hand-held low-resolution camera, and a fast text extraction method. The main advantages of the approach are its inherent lightweight structure, speed and relative robustness under poor lighting and focus conditions. The computational efficiency (and a careful implementation) of the approach has allowed its deployment in an interactive-time text capture and foreign-text translation demo on a PDA with a VGA camera attachment."
            },
            "slug": "A-light-weight-text-image-processing-method-for-Pilu-Pollard",
            "title": {
                "fragments": [],
                "text": "A light-weight text image processing method for handheld embedded cameras"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The image processing pipeline used to enhance images of text captured by a hand-held low-resolution camera, and a fast text extraction method, and the main advantages of the approach are its inherent lightweight structure, speed and relative robustness under poor lighting and focus conditions."
            },
            "venue": {
                "fragments": [],
                "text": "BMVC"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3344005"
                        ],
                        "name": "C. Dance",
                        "slug": "C.-Dance",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Dance",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Dance"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 206409349,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6d1292478a176ac9cfa39390a7db5402d37f8f53",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "There has been increasing interest in document capture with digital cameras, since they are often more convenient to use than conventional devices such as flatbed scanners. Unlike flatbed scanners, cameras can acquire document images with arbitrary perspectives. Without correction, perspective distortions are unappealing to human readers. They also make subsequent image analysis slower, more complicated and less reliable. The novel contribution of this paper is to view perspective estimation as a generalization of the well-studied skew estimation problem. Rather than estimating one angle of rotation we must determine four angles describing the perspective. In our method, separate estimates are made for angles describing lines that are parallel and perpendicular to text lines. Each of these estimates is based on a twice-iterated projection profile computation. We give a probabilistic argument for the method and describe an efficient implementation. Our results illustrate its primary benefits: it is robust and accurate. The method is efficient compared with the time required to warp the image to correct for perspective."
            },
            "slug": "Perspective-estimation-for-document-images-Dance",
            "title": {
                "fragments": [],
                "text": "Perspective estimation for document images"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The novel contribution of this paper is to view perspective estimation as a generalization of the well-studied skew estimation problem, where rather than estimating one angle of rotation the authors must determine four angles describing the perspective."
            },
            "venue": {
                "fragments": [],
                "text": "IS&T/SPIE Electronic Imaging"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116741074"
                        ],
                        "name": "Michael J. Taylor",
                        "slug": "Michael-J.-Taylor",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Taylor",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3344005"
                        ],
                        "name": "C. Dance",
                        "slug": "C.-Dance",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Dance",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Dance"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9198704,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "53f279573f7fd37c932431cfeb941bbc4fc265b8",
            "isKey": false,
            "numCitedBy": 32,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "As digital cameras become cheaper and more powerful, driven by the consumer digital photography market, we anticipate significant value in extending their utility as a general office peripheral by adding a paper scanning capability. The main technical challenges in realizing this new scanning interface are insufficient resolution, blur and lighting variations. We have developed an efficient technique for the recovery of text from digital camera images, which simultaneously treats these three problems, unlike other local thresholding algorithms which do not cope with blur and resolution enhancement. The technique first performs deblurring by deconvolution, and then resolution enhancement by linear interpolation. We compare the performance of a threshold derived from the local mean and variance of all pixel values within a neighborhood with a threshold derived from the local mean of just those pixels with high gradient. We assess performance using OCR error scores."
            },
            "slug": "Enhancement-of-document-images-from-cameras-Taylor-Dance",
            "title": {
                "fragments": [],
                "text": "Enhancement of document images from cameras"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work has developed an efficient technique for the recovery of text from digital camera images, which simultaneously treats these three problems, unlike other local thresholding algorithms which do not cope with blur and resolution enhancement."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2317491"
                        ],
                        "name": "Eve Bertucci",
                        "slug": "Eve-Bertucci",
                        "structuredName": {
                            "firstName": "Eve",
                            "lastName": "Bertucci",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eve Bertucci"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1925316"
                        ],
                        "name": "M. Pilu",
                        "slug": "M.-Pilu",
                        "structuredName": {
                            "firstName": "Maurizio",
                            "lastName": "Pilu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pilu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728108"
                        ],
                        "name": "M. Mirmehdi",
                        "slug": "M.-Mirmehdi",
                        "structuredName": {
                            "firstName": "Majid",
                            "lastName": "Mirmehdi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mirmehdi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9044255,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a2e742b8ad8e082a534139c6927dc18b89b091a1",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a method to enable the selection of specific text regions with a hand-held camera by means of projecting a structured light pointer on the document. The user indicates the text required by dragging the laser pointer over it while a sequence of images is captured. By tracking the motion of the camera over the document and matching the trajectory to the actual text, the system is able to precisely determine the text portion the user intended to capture. By using this selection method along with a text-processing pipeline and OCR, a general purpose hand-held device (such as a PDA or mobile phone) with a camera could be used as effectively as single-purpose pen scanning devices. We present our results showing successful capture and extraction of text."
            },
            "slug": "Text-selection-by-structured-light-marking-for-Bertucci-Pilu",
            "title": {
                "fragments": [],
                "text": "Text selection by structured light marking for hand-held cameras"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Using this selection method along with a text-processing pipeline and OCR, a general purpose hand-held device (such as a PDA or mobile phone) with a camera could be used as effectively as single-purpose pen scanning devices."
            },
            "venue": {
                "fragments": [],
                "text": "Seventh International Conference on Document Analysis and Recognition, 2003. Proceedings."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728108"
                        ],
                        "name": "M. Mirmehdi",
                        "slug": "M.-Mirmehdi",
                        "structuredName": {
                            "firstName": "Majid",
                            "lastName": "Mirmehdi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mirmehdi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2069641275"
                        ],
                        "name": "Paul Clark",
                        "slug": "Paul-Clark",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Clark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paul Clark"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[68] describe a system that can automatically locate text documents in a zoom-out view and control the camera to pan, tilt, and zoom in to get a closer look at the document."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15812878,
            "fieldsOfStudy": [
                "Computer Science",
                "Art"
            ],
            "id": "55035de5f18e8bfea9bbd74c484a84cbbcc96c48",
            "isKey": false,
            "numCitedBy": 27,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "Reading text in any scene is useful in the context of wearable computing, robotic vision or as an aid for visually handicapped people. Here, we present a novel automatic text reading system using an active camera focused on text regions already located in the scene (using our recent work). A region of text found is analysed to determine the optimal zoom that would foveate onto it. Then a number of images are captured over the text region to reconstruct a high-resolution mosaic of the whole region. This magni ed image of the text is good enough for reading by humans or for recognition by OCR. Even with a low resolution camera we obtained very good results."
            },
            "slug": "Extracting-Low-Resolution-Text-with-an-Active-for-Mirmehdi-Clark",
            "title": {
                "fragments": [],
                "text": "Extracting Low Resolution Text with an Active Camera for OCR"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A novel automatic text reading system using an active camera focused on text regions already located in the scene (using recent work) that is good enough for reading by humans or for recognition by OCR."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1705470"
                        ],
                        "name": "W. Newman",
                        "slug": "W.-Newman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Newman",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Newman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3344005"
                        ],
                        "name": "C. Dance",
                        "slug": "C.-Dance",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Dance",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Dance"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1920225402"
                        ],
                        "name": "Alex S. Taylor",
                        "slug": "Alex-S.-Taylor",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Taylor",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex S. Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111123186"
                        ],
                        "name": "Stuart A. Taylor",
                        "slug": "Stuart-A.-Taylor",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Taylor",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stuart A. Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116741074"
                        ],
                        "name": "Michael J. Taylor",
                        "slug": "Michael-J.-Taylor",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Taylor",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2687725"
                        ],
                        "name": "T. Aldhous",
                        "slug": "T.-Aldhous",
                        "structuredName": {
                            "firstName": "Tony",
                            "lastName": "Aldhous",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Aldhous"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 24
                            }
                        ],
                        "text": "In the CamWorks project [75], mosaicing is used to put together the images of the upper and lower part of a document page."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 58
                            }
                        ],
                        "text": "Other lightweight desktop applications are also available [75,96]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 66
                            }
                        ],
                        "text": "Instead of adaptively changing the smoothness parameter, [90] and [75] achieved the similar effect by testing the local variance and replacing the pixel with the local average if the local variance is low (i."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[75] shows that desktop OCR using PC-cams is more productive than a scanner-based OCR for extracting text paragraphs from newspapers."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 46
                            }
                        ],
                        "text": "For example, XEROX\u2019s Desktop PC-cam OCR suite [75], based on their CamWorks project, aims to replace scanners with PC-cams in light-workload environments."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 17
                            }
                        ],
                        "text": "In both [90] and [75] Tikhonov\u2013 Miller regularization is used so that the solution is regularized by a smoothness constraint."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 66
                            }
                        ],
                        "text": "Bilinear interpolation has been found effective in many instances [42,53,75,79,90]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 35779,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ecff4edac0a85829fc4ff1a98567279be40accb0",
            "isKey": true,
            "numCitedBy": 33,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe the design and evaluation of CamWorks, a system that employs a video camera as a means of supporting capture from paper sources during reading and writing. The user can view a live video image of the source document alongside the electronic document in preparation. We describe a novel user interface developed to support selection of text in the video window, and several new techniques for segmentation, restoration and resolution enhancement of camera images. An evaluation shows substantially faster text capture than with flatbed scanning."
            },
            "slug": "CamWorks:-a-video-based-tool-for-efficient-capture-Newman-Dance",
            "title": {
                "fragments": [],
                "text": "CamWorks: a video-based tool for efficient capture from paper source documents"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "The design and evaluation of CamWorks, a system that employs a video camera as a means of supporting capture from paper sources during reading and writing, shows substantially faster text capture than with flatbed scanning."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings IEEE International Conference on Multimedia Computing and Systems"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48764203"
                        ],
                        "name": "Victor Wu",
                        "slug": "Victor-Wu",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Victor Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758550"
                        ],
                        "name": "R. Manmatha",
                        "slug": "R.-Manmatha",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Manmatha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Manmatha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31338632"
                        ],
                        "name": "E. Riseman",
                        "slug": "E.-Riseman",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Riseman",
                            "middleNames": [
                                "M."
                            ],
                            "suffix": ""
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Riseman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 208945,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3420ab835c1af02071364b1f4e0f69abf733d88c",
            "isKey": false,
            "numCitedBy": 263,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "There are many applications in which the automatic detection and recognition of text embedded in images is useful. These applications include digad libraries, multimedia systems, and Geographical Information Systems. When machine generated text is prdnted against clean backgrounds, it can be converted to a computer readable form (ASCII) using current Optical Character Recognition (OCR) technology. However, text is often printed against shaded or textured backgrounds or is embedded in images. Examples include maps, advertisements, photographs, videos and stock certificates. Current document segmentation and recognition technologies cannot handle these situafons well. In this paper, a four-step system which automaticnlly detects and extracts text in images i& proposed. First, a texture segmentation scheme is used to focus attention on regions where text may occur. Second, strokes are extracted from the segmented text regions. Using reasonable heuristics on text strings such as height similarity, spacing and alignment, the extracted strokes are then processed to form rectangular boxes surrounding the corresponding ttzt strings. To detect text over a wide range of font sizes, the above steps are first applied to a pyramid of images generated from the input image, and then the boxes formed at each resolution level of the pyramid are fused at the image in the original resolution level. Third, text is extracted by cleaning up the background and binarizing the detected ted strings. Finally, better text bounding boxes are generated by srsiny the binarized text as strokes. Text is then cleaned and binarized from these new boxes, and can then be passed through a commercial OCR engine for recognition if the text is of an OCR-recognizable font. The system is stable, robust, and works well on imayes (with or without structured layouts) from a wide van\u2019ety of sources, including digitized video frames, photographs, *This material is based on work supported in part by the National Science Foundation, Library of Congress and Department of Commerce under cooperative agreement number EEC9209623, in part by the United States Patent and mademark Office and Defense Advanced Research Projects Agency/IT0 under ARPA order number D468, issued by ESC/AXS contract number F19628-96-C-0235, in part by the National Science Foundation under grant number IF&9619117 and in part by NSF Multimedia CDA-9502639. Any opinions, findings and conclusions or recommendations expressed in this material are the author(s) and do not necessarily reflect those of the sponsors. Prrmission to make digital/hard copies ofall or part oflhis material for personal or clrrssroom use is granted without fee provided that the copies are not made or distributed for profit or commercial advantage, the copyright notice, the title ofthe publication and its date appear, and notice is given that copyright is by permission of the ACM, Inc. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires specific permission and/or fe DL 97 Philadelphia PA, USA Copyright 1997 AChi 0-89791~868-1197/7..$3.50 newspapers, advertisements, stock certifimtes, and personal checks. All parameters remain the same for-all the experiments."
            },
            "slug": "Finding-text-in-images-Wu-Manmatha",
            "title": {
                "fragments": [],
                "text": "Finding text in images"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A four-step system which automaticnlly detects and extracts text in images is proposed and works well on imayes (with or without structured layouts) from a wide range of sources, including digitized video frames, photographs, and personal checks."
            },
            "venue": {
                "fragments": [],
                "text": "DL '97"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39784761"
                        ],
                        "name": "Huaigu Cao",
                        "slug": "Huaigu-Cao",
                        "structuredName": {
                            "firstName": "Huaigu",
                            "lastName": "Cao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huaigu Cao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145507765"
                        ],
                        "name": "X. Ding",
                        "slug": "X.-Ding",
                        "structuredName": {
                            "firstName": "Xiaoqing",
                            "lastName": "Ding",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Ding"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107903806"
                        ],
                        "name": "Changsong Liu",
                        "slug": "Changsong-Liu",
                        "structuredName": {
                            "firstName": "Changsong",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Changsong Liu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 6553895,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b8e5aefba5596e04e6f36dcad92062db854f16a5",
            "isKey": false,
            "numCitedBy": 70,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "A model based approach for rectifying the camera image of the bound document has been developed, i.e., the surface of the document is represented by a general cylindrical surface. The principle of using the model to unwrap the image is discussed. Practically, the skeleton of each horizontal text line is extracted to help estimate the parameter of the model, and rectify the images. To use the model, only a few priori is required, and no more auxiliary device is necessary. Experiment results are given to demonstrate the feasibility and the stability of the method."
            },
            "slug": "Rectifying-the-bound-document-image-captured-by-the-Cao-Ding",
            "title": {
                "fragments": [],
                "text": "Rectifying the bound document image captured by the camera: a model based approach"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "A model based approach for rectifying the camera image of the bound document has been developed, i.e., the surface of the document is represented by a general cylindrical surface and the principle of using the model to unwrap the image is discussed."
            },
            "venue": {
                "fragments": [],
                "text": "Seventh International Conference on Document Analysis and Recognition, 2003. Proceedings."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2069641275"
                        ],
                        "name": "Paul Clark",
                        "slug": "Paul-Clark",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Clark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paul Clark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728108"
                        ],
                        "name": "M. Mirmehdi",
                        "slug": "M.-Mirmehdi",
                        "structuredName": {
                            "firstName": "Majid",
                            "lastName": "Mirmehdi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mirmehdi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17433875,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c3ce832224f54eec7e7e17474be0467cd56d5811",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "A method is presented for the fronto-parallel recovery of text documents in images of real scenes. Initially an extension of the standard 2D projection profile, commonly used in document recognition, is introduced to locate the horizontal vanishing point of the text plane. This allows us to segment the lines of text, which are then analysed to reveal the style of justification of the paragraphs. The change in line spacings exhibited due to perspective is then used to recover the vertical vanishing point of the document. We do not assume any knowledge of the focal length of the camera. Finally, a fronto-parallel view is recovered, suitable for OCR or other highlevel recognition. We provide results demonstrating the algorithm\u2019s performance on documents over a wide range of orientations."
            },
            "slug": "On-the-Recovery-of-Oriented-Documents-from-Single-Clark-Mirmehdi",
            "title": {
                "fragments": [],
                "text": "On the Recovery of Oriented Documents from Single Images"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "A method for the fronto-parallel recovery of text documents in images of real scenes, suitable for OCR or other highlevel recognition, and results demonstrating the algorithm\u2019s performance on documents over a wide range of orientations are provided."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2907696"
                        ],
                        "name": "J. Shim",
                        "slug": "J.-Shim",
                        "structuredName": {
                            "firstName": "Jae",
                            "lastName": "Shim",
                            "middleNames": [
                                "Chang"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145253787"
                        ],
                        "name": "C. Dorai",
                        "slug": "C.-Dorai",
                        "structuredName": {
                            "firstName": "Chitra",
                            "lastName": "Dorai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Dorai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70029967"
                        ],
                        "name": "R. Bolle",
                        "slug": "R.-Bolle",
                        "structuredName": {
                            "firstName": "Ruud",
                            "lastName": "Bolle",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Bolle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12062439,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1eb854ce0539b6fd18dbba942f52a80a735f5c8e",
            "isKey": false,
            "numCitedBy": 142,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Efficient content-based retrieval of image and video databases is an important application due to rapid proliferation of digital video data on the Internet and corporate intranets. Text either embedded or superimposed within video frames is very useful for describing the contents of the frames, as it enables both keyword and free-text based search, automatic video logging, and video cataloging. We have developed a scheme for automatically extracting text from digital images and videos for content annotation and retrieval. We present our approach to robust text extraction from video frames, which can handle complex image backgrounds, deal with different font sizes, font styles, and font appearances such as normal and inverse video. Our algorithm results in segmented characters that can be directly processed by an OCR system to produce ASCII text. Results from our experiments with over 5000 frames obtained from twelve MPEG video streams demonstrate the good performance of our system in terms of text identification accuracy and computational efficiency."
            },
            "slug": "Automatic-text-extraction-from-video-for-annotation-Shim-Dorai",
            "title": {
                "fragments": [],
                "text": "Automatic text extraction from video for content-based annotation and retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work has developed a scheme for automatically extracting text from digital images and videos for content annotation and retrieval that results in segmented characters that can be directly processed by an OCR system to produce ASCII text."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. Fourteenth International Conference on Pattern Recognition (Cat. No.98EX170)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2821130"
                        ],
                        "name": "David J. Crandall",
                        "slug": "David-J.-Crandall",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Crandall",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David J. Crandall"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721328"
                        ],
                        "name": "Sameer Kiran Antani",
                        "slug": "Sameer-Kiran-Antani",
                        "structuredName": {
                            "firstName": "Sameer",
                            "lastName": "Antani",
                            "middleNames": [
                                "Kiran"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sameer Kiran Antani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3110392"
                        ],
                        "name": "R. Kasturi",
                        "slug": "R.-Kasturi",
                        "structuredName": {
                            "firstName": "Rangachar",
                            "lastName": "Kasturi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kasturi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[17] introduce a tracking method that uses contour-based shape matching to connect the text object in adjacent frames."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18084231,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "37903a00047e1cf377408ca4119b48f2bfab89c4",
            "isKey": false,
            "numCitedBy": 94,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract. The popularity of digital video is increasing rapidly. To help users navigate libraries of video, algorithms that automatically index video based on content are needed. One approach is to extract text appearing in video, which often reflects a scene's semantic content. This is a difficult problem due to the unconstrained nature of general-purpose video. Text can have arbitrary color, size, and orientation. Backgrounds may be complex and changing. Most work so far has made restrictive assumptions about the nature of text occurring in video. Such work is therefore not directly applicable to unconstrained, general-purpose video. In addition, most work so far has focused only on detecting the spatial extent of text in individual video frames. However, text occurring in video usually persists for several seconds. This constitutes a text event that should be entered only once in the video index. Therefore it is also necessary to determine the temporal extent of text events. This is a non-trivial problem because text may move, rotate, grow, shrink, or otherwise change over time. Such text effects are common in television programs and commercials but so far have received little attention in the literature. This paper discusses detecting, binarizing, and tracking caption text in general-purpose MPEG-1 video. Solutions are proposed for each of these problems and compared with existing work found in the literature."
            },
            "slug": "Extraction-of-special-effects-caption-text-events-Crandall-Antani",
            "title": {
                "fragments": [],
                "text": "Extraction of special effects caption text events from digital video"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper discusses detecting, binarizing, and tracking caption text in general-purpose MPEG-1 video, and solutions are proposed for each of these problems and compared with existing work found in the literature."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal on Document Analysis and Recognition"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3342671"
                        ],
                        "name": "A. Miene",
                        "slug": "A.-Miene",
                        "structuredName": {
                            "firstName": "Andrea",
                            "lastName": "Miene",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Miene"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2193995"
                        ],
                        "name": "T. Hermes",
                        "slug": "T.-Hermes",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "Hermes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Hermes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144581170"
                        ],
                        "name": "G. Ioannidis",
                        "slug": "G.-Ioannidis",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Ioannidis",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Ioannidis"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 46
                            }
                        ],
                        "text": "use a fast one-pass color segmentation method [66] where pixels are scanned first in the x direction, then in the y direction."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2813133,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "609274b4d2f79b8a85ff1653a15d9b490747c10a",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Textual inserts and closed captures superimposed on digital videos often contain important and exclusive information about the video contents which cannot be found in other information channels. Therefore, it is very helpful to extract this information automatically and add it to a video index as generated by video archiving and retrieval systems like e.g. ADViSOR, AVAnTA, DiVA or Informedia. Owing to the fact that common OCR systems are restricted to binary images, the video frames have to be preprocessed in order to extract the textual inserts from the image in the background. In this paper we present our approach to the segmentation of textual inserts from digital videos or images, which consists of a region-growing method for color segmentation and a method of separating text regions from background based on character size and alignment constraints. A new method on segmentation refinement taking into account the results of the classification step leads to a significant enhancement of quality of the resulting binary images. The main difficulties in extracting textual inserts from video are caused by the low resolution and quality of digital video material, the high amount of image data, the very complex structured and textured background, and the unknown color size, and position of the text to be extracted from the image."
            },
            "slug": "Extracting-textual-inserts-from-digital-videos-Miene-Hermes",
            "title": {
                "fragments": [],
                "text": "Extracting textual inserts from digital videos"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The approach to the segmentation of textual inserts from digital videos or images is presented, which consists of a region-growing method for color segmentation and a method of separating text regions from background based on character size and alignment constraints."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of Sixth International Conference on Document Analysis and Recognition"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144739319"
                        ],
                        "name": "R. Lienhart",
                        "slug": "R.-Lienhart",
                        "structuredName": {
                            "firstName": "Rainer",
                            "lastName": "Lienhart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lienhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32544716"
                        ],
                        "name": "Axel Wernicke",
                        "slug": "Axel-Wernicke",
                        "structuredName": {
                            "firstName": "Axel",
                            "lastName": "Wernicke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Axel Wernicke"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[48,57, 58 ]. It has the shortcoming of wasting computing"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Lienhart and Wernicle [ 58 ] use a different method to track text objects."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "an image and apply a neural network classifier on a sliding small window to identify possible text pixels [ 58 ]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 143774,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8387d4998f810cd2b60bd81545cb993087bc8788",
            "isKey": true,
            "numCitedBy": 467,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Many images, especially those used for page design on Web pages, as well as videos contain visible text. If these text occurrences could be detected, segmented, and recognized automatically, they would be a valuable source of high-level semantics for indexing and retrieval. We propose a novel method for localizing and segmenting text in complex images and videos. Text lines are identified by using a complex-valued multilayer feed-forward network trained to detect text at a fixed scale and position. The network's output at all scales and positions is integrated into a single text-saliency map, serving as a starting point for candidate text lines. In the case of video, these candidate text lines are refined by exploiting the temporal redundancy of text in video. Localized text lines are then scaled to a fixed height of 100 pixels and segmented into a binary image with black characters on white background. For videos, temporal redundancy is exploited to improve segmentation performance. Input images and videos can be of any size due to a true multiresolution approach. Moreover, the system is not only able to locate and segment text occurrences into large binary images, but is also able to track each text line with sub-pixel accuracy over the entire occurrence in a video, so that one text bitmap is created for all instances of that text line. Therefore, our text segmentation results can also be used for object-based video encoding such as that enabled by MPEG-4."
            },
            "slug": "Localizing-and-segmenting-text-in-images-and-videos-Lienhart-Wernicke",
            "title": {
                "fragments": [],
                "text": "Localizing and segmenting text in images and videos"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work proposes a novel method for localizing and segmenting text in complex images and videos that is not only able to locate and segment text occurrences into large binary images, but is also able to track each text line with sub-pixel accuracy over the entire occurrence in a video."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Circuits Syst. Video Technol."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710238"
                        ],
                        "name": "R. Haralick",
                        "slug": "R.-Haralick",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Haralick",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Haralick"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 13615381,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d91e0d202fa23b7a2e81c5b3b04eb4cc5327b0f9",
            "isKey": false,
            "numCitedBy": 144,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": "Document image understanding encompasses the technology required to make paper documents equivalent to other computer exchange media like floppies, tapes, and CDROMs. The physical reader of the paper document is the scanner just like the physical reader of the floppy is the floppy drive and the physical reader of the tape cartridge is the tape cartridge drive, and the physical reader of the CDROM is the CDROM drive. In the survey presented, we restrict ourselves to documents such as business letters, forms, and scientific and technical articles such as those found in archival journals and technical conferences. Understanding such documents involves estimating the rotation skew of each document page, determining the geometric page layout, labeling blocks as text or non-text, determining the read order for text blocks, recognizing the text of text blocks through an OCR system, determining the logical page layout, and formatting the data and information of the document in a suitable way for use by a word processing system or by an information retrieval system.<<ETX>>"
            },
            "slug": "Document-image-understanding:-geometric-and-logical-Haralick",
            "title": {
                "fragments": [],
                "text": "Document image understanding: geometric and logical layout"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "Document image understanding encompasses the technology required to make paper documents equivalent to other computer exchange media like floppies, tapes, and CDROMs and restricts ourselves to documents such as business letters, forms, and scientific and technical articles such as those found in archival journals and technical conferences."
            },
            "venue": {
                "fragments": [],
                "text": "1994 Proceedings of IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2148905379"
                        ],
                        "name": "Zheng Zhang",
                        "slug": "Zheng-Zhang",
                        "structuredName": {
                            "firstName": "Zheng",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zheng Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679749"
                        ],
                        "name": "C. Tan",
                        "slug": "C.-Tan",
                        "structuredName": {
                            "firstName": "Chew",
                            "lastName": "Tan",
                            "middleNames": [
                                "Lim"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Tan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 14
                            }
                        ],
                        "text": "Zhang and Tan [111] study the case of scanning thick books."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5844049,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "db0dddecbdd5a6d7ee69471abc782d2e013217ca",
            "isKey": false,
            "numCitedBy": 49,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Perspective distortion always occurs while scanning thick, bound documents. This distortion mainly causes two sources of degradation for the scanned grayscale image: (i) shadow along the 'spine' of the book; and (ii) warp of the words in the shadow. We propose a restoration system to solve these two problems. Our system first produces a vertical projection profile to detect which side of the image the shadow lies on, and a runlength method is used to find the boundary between the shadow and the clean area. We then apply a modified Niblack method to remove the shadow. We use a connected component analysis to adjust the location and orientation of the warped word in the shadow area, ie, the words within the boundary detected earlier. The implementation results are presented. Our system will be used in a text retrieval project for the National Archives of Singapore."
            },
            "slug": "Restoration-of-images-scanned-from-thick-bound-Zhang-Tan",
            "title": {
                "fragments": [],
                "text": "Restoration of images scanned from thick bound documents"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A connected component analysis is used to adjust the location and orientation of the warped word in the shadow area, ie, the words within the boundary detected earlier, which will be used in a text retrieval project for the National Archives of Singapore."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 2001 International Conference on Image Processing (Cat. No.01CH37205)"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49974309"
                        ],
                        "name": "H. Suen",
                        "slug": "H.-Suen",
                        "structuredName": {
                            "firstName": "Hong-Ming",
                            "lastName": "Suen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Suen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "71563124"
                        ],
                        "name": "Jhing-Fa Wang",
                        "slug": "Jhing-Fa-Wang",
                        "structuredName": {
                            "firstName": "Jhing-Fa",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jhing-Fa Wang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62225714,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f7419fc6b44c327c21ded2347a087e003de0284a",
            "isKey": false,
            "numCitedBy": 30,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Given the mass of printed documents today, an automated entry system is highly desirable. Many techniques focusing on processing monochrome documents have been proposed in the past years but few techniques have been proposed for dealing with colour-printed documents. The authors discuss the processing of colour-printed documents in 24-bit true colour images and propose an approach for extracting text strings from them. Due to the very large amount of data in a 24-bit true colour image, processing is usually very time consuming. To reduce the computational complexity and thus speed up processing, the original colour image is first transformed into a binary image of edge representation for page segmentation. Then a new method is used to identify the text blocks. Finally, all the identified text blocks are transformed into white-background/black-text binary images for an OCR system. The proposed approach was implemented and tested on a Pentium/90 PC and experimental results have demonstrated its feasibility."
            },
            "slug": "Text-string-extraction-from-images-of-documents-Suen-Wang",
            "title": {
                "fragments": [],
                "text": "Text string extraction from images of colour-printed documents"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The authors discuss the processing of colour-printed documents in 24-bit true colour images and propose an approach for extracting text strings from them and results have demonstrated its feasibility."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144899680"
                        ],
                        "name": "Christian Wolf",
                        "slug": "Christian-Wolf",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Wolf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian Wolf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680935"
                        ],
                        "name": "J. Jolion",
                        "slug": "J.-Jolion",
                        "structuredName": {
                            "firstName": "Jean-Michel",
                            "lastName": "Jolion",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Jolion"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "23341564"
                        ],
                        "name": "F. Chassaing",
                        "slug": "F.-Chassaing",
                        "structuredName": {
                            "firstName": "Fran\u00e7oise",
                            "lastName": "Chassaing",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Chassaing"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15872163,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6b4f762d9a5acd964411d8c737073c24ce16a3c8",
            "isKey": false,
            "numCitedBy": 271,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "The systems currently available for content based image and video retrieval work without semantic knowledge, i.e. they use image processing methods to extract low level features of the data. The similarity obtained by these approaches does not always correspond to the similarity a human user would expect. A way to include more semantic knowledge into the indexing process is to use the text included in the images and video sequences. It is rich in information but easy to use, e.g. by key word based queries. In this paper we present an algorithm to localize artificial text in images and videos using a measure of accumulated gradients and morphological post processing to detect the text. The quality of the localized text is improved by robust multiple frame integration. Anew technique for the binarization of the text boxes is proposed. Finally, detection and OCR results for a commercial OCR are presented."
            },
            "slug": "Text-localization,-enhancement-and-binarization-in-Wolf-Jolion",
            "title": {
                "fragments": [],
                "text": "Text localization, enhancement and binarization in multimedia documents"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "An algorithm to localize artificial text in images and videos using a measure of accumulated gradients and morphological post processing to detect the text is presented and the quality of the localized text is improved by robust multiple frame integration."
            },
            "venue": {
                "fragments": [],
                "text": "Object recognition supported by user interaction for service robots"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2215674"
                        ],
                        "name": "K. Moravec",
                        "slug": "K.-Moravec",
                        "structuredName": {
                            "firstName": "Kimberly",
                            "lastName": "Moravec",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Moravec"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 71
                            }
                        ],
                        "text": "A recent trend is to develop barcode readers based on PDAs and cameras [69]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 73
                            }
                        ],
                        "text": "When the recognizer is perspective tolerant, rectification is not needed [69]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5411708,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bac48d1a923a2c4d83104dfd8725f632ba419280",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Xerox DataGlyph technology is a 2-D matrix barcode encoding which com- bines relatively high data density with an error correction scheme and an unobtrusive format. Some of the more exciting applications proposed for this technology involve the decoding of unconstrained camera images of DataGlyphs. This paper details a set of grayscale methods including steered gradient filters and the Radon transform which can decode many uncon- strained camera images. This method has been successfully tested on nearly camera images with very good results. Flowport\u2122 and is an active area of development 1. This paper describes robust methods of decoding DataGlyphs from images taken by a hand-heldcamera or webcam, extending the range of applications for which DataGlyphs can be used. Thisintroductionexplainsindetail the DataGlyphformatand theuniquechallengesof decoding camera images of DataGlyphs. The paper then describes the proposed decoder and the methods used in it. Results of the new decoder are compared with a binarization and correlation-based method, followed by the paper's conclusions"
            },
            "slug": "A-Grayscale-Reader-for-Camera-Images-of-Xerox-Moravec",
            "title": {
                "fragments": [],
                "text": "A Grayscale Reader for Camera Images of Xerox DataGlyphs"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper details a set of grayscale methods including steered gradient filters and the Radon transform which can decode many uncon- strained camera images of DataGlyphs from images taken by a hand-held camera or webcam, extending the range of applications for which DataGelphs can be used."
            },
            "venue": {
                "fragments": [],
                "text": "BMVC"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109552037"
                        ],
                        "name": "Dongqing Zhang",
                        "slug": "Dongqing-Zhang",
                        "structuredName": {
                            "firstName": "Dongqing",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dongqing Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3077945"
                        ],
                        "name": "R. Rajendran",
                        "slug": "R.-Rajendran",
                        "structuredName": {
                            "firstName": "Raj",
                            "lastName": "Rajendran",
                            "middleNames": [
                                "Kumar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rajendran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9546964"
                        ],
                        "name": "Shih-Fu Chang",
                        "slug": "Shih-Fu-Chang",
                        "structuredName": {
                            "firstName": "Shih-Fu",
                            "lastName": "Chang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shih-Fu Chang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 3
                            }
                        ],
                        "text": "In [109], the goal is to detect score information appearing in sports videos, while ignoring other text such as text in commercials."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10439305,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "059b137e671bbc4193870dc0064f9415de7ae1b6",
            "isKey": false,
            "numCitedBy": 54,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "We have developed generic and domain-specific video algorithms for caption text extraction and recognition in digital video. Our system includes several unique features: for caption box location, we combine the compressed-domain features derived from DCT coefficients and motion vectors. Long-term temporal consistency is employed to enhance localization performance. For character segmentation, we use a single-pass threshold free approach combining classification and projection to address noisy segmentation, text intensity variation, and algorithm complexity. In recognition, we use Zernike moments to achieve more accurate recognition performance. Finally, domain knowledge is explored and a statistical transition graph model is used to enhance recognition of domain-specific characters, such as ball counts and game score of baseball videos. The algorithms achieved real-time speed and significantly improved recognition accuracy. Furthermore, although the experiments were conducted in baseball videos only, these algorithms (except the transition model) are general and can be used in other applications, such as news and films."
            },
            "slug": "General-and-domain-specific-techniques-for-and-text-Zhang-Rajendran",
            "title": {
                "fragments": [],
                "text": "General and domain-specific techniques for detecting and recognizing superimposed text in video"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A statistical transition graph model is used to enhance recognition of domain-specific characters, such as ball counts and game score of baseball videos, and real-time speed and significantly improved recognition accuracy are achieved."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. International Conference on Image Processing"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2069641275"
                        ],
                        "name": "Paul Clark",
                        "slug": "Paul-Clark",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Clark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paul Clark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728108"
                        ],
                        "name": "M. Mirmehdi",
                        "slug": "M.-Mirmehdi",
                        "structuredName": {
                            "firstName": "Majid",
                            "lastName": "Mirmehdi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mirmehdi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 99
                            }
                        ],
                        "text": "Clark and Mirmehdi introduce an approximate rectification technique without using vanishing points [11], providing an effective and efficient approximation for OCR purposes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1028027,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "54c72b7999fe51f7054c73683034941c78183093",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method for extracting text from images where the text plane is not necessarily fronto-parallel to the camera. Initially, we locate local image features such as borders and page edges. We then use perceptual grouping on these features to find rectangular regions in the scene. These regions are hypothesized to be pages or planes that may contain text. Edge distributions are then used for the assessment of these potential regions, providing a measure of confidence. It will be shown that the text may then be transformed to a fronto- parallel view suitable, for example, for an OCR system or other higher level recognition. The proposed method is scale independent (of the size of the text). We illustrate the algorithm using various examples."
            },
            "slug": "Location-and-recovery-of-text-on-oriented-surfaces-Clark-Mirmehdi",
            "title": {
                "fragments": [],
                "text": "Location and recovery of text on oriented surfaces"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "A method for extracting text from images where the text plane is not necessarily fronto-parallel to the camera, and it will be shown that the text may then be transformed to a backo- parallel view suitable for an OCR system or other higher level recognition."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "12075023"
                        ],
                        "name": "M. Wienecke",
                        "slug": "M.-Wienecke",
                        "structuredName": {
                            "firstName": "Markus",
                            "lastName": "Wienecke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Wienecke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749475"
                        ],
                        "name": "G. Fink",
                        "slug": "G.-Fink",
                        "structuredName": {
                            "firstName": "Gernot",
                            "lastName": "Fink",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Fink"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689610"
                        ],
                        "name": "G. Sagerer",
                        "slug": "G.-Sagerer",
                        "structuredName": {
                            "firstName": "Gerhard",
                            "lastName": "Sagerer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Sagerer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 150
                            }
                        ],
                        "text": "For example, a camera mounted over a writing surface can be used for handwriting recognition [26,70,96], or cameras can be used in whiteboard reading [89,97]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 31378205,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d882917c237fc7ce0ee7dbae6c770c86a5b76d46",
            "isKey": false,
            "numCitedBy": 35,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "As whiteboards have become a popular tool in meeting rooms, there has been a growing interest in making use of the whiteboard as a user interface for human computer interaction. Therefore, systems based on electronic whiteboards have been developed in order to serve as meeting assistants for e.g. collaborative working. However, as special pens and erasers are required, the natural interaction is restricted. In order to render this communication method more natural it was proposed to retain ordinary whiteboard and pens and to visually observe the writing process using a video camera by Stafford-Fraser and Robinson (1996). In this paper a prototype system for automatic video-based whiteboard reading is presented. The system is designed for recognizing unconstrained handwritten text and is further characterized by an incremental processing strategy in order to facilitate recognizing portions of text as soon as they have been written on the board. We present the methods employed for extracting text regions, pre-processing, feature extraction, and statistical modeling and recognition. Evaluation results on a writer independent unconstrained handwriting recognition task demonstrate the feasibility of the proposed approach."
            },
            "slug": "Towards-automatic-video-based-whiteboard-reading-Wienecke-Fink",
            "title": {
                "fragments": [],
                "text": "Towards automatic video-based whiteboard reading"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A prototype system for automatic video-based whiteboard reading is presented and is designed for recognizing unconstrained handwritten text and is further characterized by an incremental processing strategy in order to facilitate recognizing portions of text as soon as they have been written on the board."
            },
            "venue": {
                "fragments": [],
                "text": "Seventh International Conference on Document Analysis and Recognition, 2003. Proceedings."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "121267347"
                        ],
                        "name": "K. Jung",
                        "slug": "K.-Jung",
                        "structuredName": {
                            "firstName": "Keechul",
                            "lastName": "Jung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Jung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144602022"
                        ],
                        "name": "K. Kim",
                        "slug": "K.-Kim",
                        "structuredName": {
                            "firstName": "Kwang",
                            "lastName": "Kim",
                            "middleNames": [
                                "In"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2146355817"
                        ],
                        "name": "Junghyun Han",
                        "slug": "Junghyun-Han",
                        "structuredName": {
                            "firstName": "Junghyun",
                            "lastName": "Han",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Junghyun Han"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 127
                            }
                        ],
                        "text": "trained a multiple layer percepton (MLP) that works directly on small image blocks to assign each pixel a text area likelihood [40]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[40] use mosaicing to put together long text strings that appear in multiple video frames into a panorama image."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[40] use a straightforward approach to establish four pairs of correspondences and proceed with rectification."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 62748767,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8e1f46e27a61ee566ae803c072e1886a52fb2d8f",
            "isKey": true,
            "numCitedBy": 14,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a hybrid approach of a texture-based method and a connected component-based one for extracting texts in real scene images. For detecting texts having a lot of variations in size, shape, etc, we use a multiple-continuously adaptive mean shift algorithm on the text probability image produced by a multi-layer perceptron. It is assumed that the scene text lies on planar rectangular surfaces with homogeneous background colors. We correct perspective distortion using warping parameters calculated after segmentation of an input image. We can detect and reconstruct text images accurately and efficiently."
            },
            "slug": "Text-extraction-in-real-scene-images-on-planar-Jung-Kim",
            "title": {
                "fragments": [],
                "text": "Text extraction in real scene images on planar planes"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "A hybrid approach of a texture-based method and a connected component-based one for extracting texts in real scene images using a multiple-continuously adaptive mean shift algorithm on the text probability image produced by a multi-layer perceptron."
            },
            "venue": {
                "fragments": [],
                "text": "Object recognition supported by user interaction for service robots"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116415943"
                        ],
                        "name": "B. Yu",
                        "slug": "B.-Yu",
                        "structuredName": {
                            "firstName": "Bin",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Yu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 5196787,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f565f502ad1acb81c5659b051c04683a34ed138f",
            "isKey": false,
            "numCitedBy": 576,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Automatic text location (without character recognition capabilities) deals with extracting image regions that contain text only. The images of these regions can then be fed to an optical character recognition module or highlighted for users. This is very useful in a number of applications such as database indexing and converting paper documents to their electronic versions. The performance of our automatic text location algorithm is shown in several applications. Compared with some traditional text location methods, our method has the following advantages: 1) low computational cost; 2) robust to font size; and 3) high accuracy."
            },
            "slug": "Automatic-text-location-in-images-and-video-frames-Jain-Yu",
            "title": {
                "fragments": [],
                "text": "Automatic text location in images and video frames"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Compared with some traditional text location methods, this method has the following advantages: 1) low computational cost; 2) robust to font size; and 3) high accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. Fourteenth International Conference on Pattern Recognition (Cat. No.98EX170)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143626870"
                        ],
                        "name": "T. Kanungo",
                        "slug": "T.-Kanungo",
                        "structuredName": {
                            "firstName": "Tapas",
                            "lastName": "Kanungo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanungo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710238"
                        ],
                        "name": "R. Haralick",
                        "slug": "R.-Haralick",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Haralick",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Haralick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744200"
                        ],
                        "name": "I. T. Phillips",
                        "slug": "I.-T.-Phillips",
                        "structuredName": {
                            "firstName": "Ihsin",
                            "lastName": "Phillips",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. T. Phillips"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[43] discuss the degradation around the book spine using a cylinder model."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16489337,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "7f62ed0e11910cf534f705a0f81527e9a3007f29",
            "isKey": false,
            "numCitedBy": 163,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Two sources of document degradation are modeled: i) perspective distortion that occurs while photocopying or scanning thick, bound documents, and ii) degradation due to perturbations in the optical scanning and digitization process: speckle, blurr, jitter, thresholding. Perspective distortion is modeled by studying the underlying perspective geometry of the optical system of photocopiers and scanners. An illumination model is described to account for the nonlinear intensity change occuring across a page in a perspective-distorted document. The optical distortion process is modeled morphologically. First, a distance transform on the foreground is performed, followed by a random inversion of binary pixels where the probability of flip is a function of the distance of the pixel to the boundary of the foreground. Correlating the flipped pixels is modeled by a morphological closing operation.<<ETX>>"
            },
            "slug": "Global-and-local-document-degradation-models-Kanungo-Haralick",
            "title": {
                "fragments": [],
                "text": "Global and local document degradation models"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "An illumination model is described to account for the nonlinear intensity change occuring across a page in a perspective-distorted document."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 2nd International Conference on Document Analysis and Recognition (ICDAR '93)"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39319377"
                        ],
                        "name": "Yu Zhong",
                        "slug": "Yu-Zhong",
                        "structuredName": {
                            "firstName": "Yu",
                            "lastName": "Zhong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yu Zhong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145211604"
                        ],
                        "name": "K. Karu",
                        "slug": "K.-Karu",
                        "structuredName": {
                            "firstName": "Kalle",
                            "lastName": "Karu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Karu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 168
                            }
                        ],
                        "text": "assume that text areas contain certain high horizontal and vertical frequencies and detect these frequencies directly from the DCT coefficients of the compressed image [115,116]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 29853292,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6a4af75831ed098d9fea02507f36cdbc38852fe6",
            "isKey": false,
            "numCitedBy": 181,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "There is a substantial interest in retrieving images from a large database using the textual information contained in the images. An algorithm which will automatically locate the textual regions in the input image will facilitate this task; the optical character recognizer can then be applied to only those regions of the image which contain text. We present a method for automatically locating text in complex color images. The algorithm first finds the approximate locations of text lines using horizontal spatial variance, and then extracts text components in these boxes using color segmentation. The proposed method has been used to locate text in compact disc (CD) and book cover images, as well as in the images of traffic scenes captured by a video camera. Initial results are encouraging and suggest that these algorithms can be used in image retrieval applications."
            },
            "slug": "Locating-text-in-complex-color-images-Zhong-Karu",
            "title": {
                "fragments": [],
                "text": "Locating text in complex color images"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The proposed algorithm has been used to locate text in compact disc and book cover images, as well as in the images of traffic scenes captured by a video camera, and initial results suggest that these algorithms can be used in image retrieval applications."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 3rd International Conference on Document Analysis and Recognition"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749475"
                        ],
                        "name": "G. Fink",
                        "slug": "G.-Fink",
                        "structuredName": {
                            "firstName": "Gernot",
                            "lastName": "Fink",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Fink"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "12075023"
                        ],
                        "name": "M. Wienecke",
                        "slug": "M.-Wienecke",
                        "structuredName": {
                            "firstName": "Markus",
                            "lastName": "Wienecke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Wienecke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689610"
                        ],
                        "name": "G. Sagerer",
                        "slug": "G.-Sagerer",
                        "structuredName": {
                            "firstName": "Gerhard",
                            "lastName": "Sagerer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Sagerer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 93
                            }
                        ],
                        "text": "For example, a camera mounted over a writing surface can be used for handwriting recognition [26,70,96], or cameras can be used in whiteboard reading [89,97]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 39
                            }
                        ],
                        "text": "c Camera-based handwriting recognition [26,70,96]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14295759,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c2c16eb79910fa95d78d4354bdb49458a871451a",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "The use of handwriting provides a natural way of interacting with small portable computers. However, in order to capture handwritten text. online, special input devices are necessary. Therefore, M.E. Munich & P. Perona (1996) proposed to use visual input for pen-based computers. Writing can then be performed on ordinary paper, and pen trajectories are automatically extracted from image sequences recorded during the writing process. On the basis of this work, we developed a complete video-based online handwriting recognition system. We will present the techniques applied for pen tracking, pre-processing, feature extraction, and statistical modeling and recognition. Evaluation results on a writer-independent unconstrained handwriting recognition task demonstrate that the inherent limitations of the video-based approach can be compensated using robust modeling combined with adaptation techniques."
            },
            "slug": "Video-based-on-line-handwriting-recognition-Fink-Wienecke",
            "title": {
                "fragments": [],
                "text": "Video-based on-line handwriting recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Evaluation results on a writer-independent unconstrained handwriting recognition task demonstrate that the inherent limitations of the video-based approach can be compensated using robust modeling combined with adaptation techniques."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of Sixth International Conference on Document Analysis and Recognition"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33852772"
                        ],
                        "name": "H. Kuwano",
                        "slug": "H.-Kuwano",
                        "structuredName": {
                            "firstName": "Hidetaka",
                            "lastName": "Kuwano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Kuwano"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113938"
                        ],
                        "name": "Y. Taniguchi",
                        "slug": "Y.-Taniguchi",
                        "structuredName": {
                            "firstName": "Yukinobu",
                            "lastName": "Taniguchi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Taniguchi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153027997"
                        ],
                        "name": "Hiroyuki Arai",
                        "slug": "Hiroyuki-Arai",
                        "structuredName": {
                            "firstName": "Hiroyuki",
                            "lastName": "Arai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hiroyuki Arai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113759578"
                        ],
                        "name": "M. Mori",
                        "slug": "M.-Mori",
                        "structuredName": {
                            "firstName": "Minoru",
                            "lastName": "Mori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mori"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3267020"
                        ],
                        "name": "S. Kurakake",
                        "slug": "S.-Kurakake",
                        "structuredName": {
                            "firstName": "Shoji",
                            "lastName": "Kurakake",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kurakake"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054536982"
                        ],
                        "name": "H. Kojima",
                        "slug": "H.-Kojima",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Kojima",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Kojima"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 108
                            }
                        ],
                        "text": "A simple solution is to treat every frame as a potential text frame and apply text region detection to them [48,57,58]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[48] make use of both vertical and horizontal scan line segments."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 26591876,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ddcc1bae71d70c52be48301c2fe45bc570d2e69b",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper presents a telop-on-demand system that anatomically recognizes texts in video frames to create the indices needed for content based video browsing and retrieval. Superimposed texts are important as they provide semantic information about scene contents. Their attributes such as fonts, size, and position in a frame are important as they are carefully designed by the video editor and so reflect the intent of captioning. In news programs, for instance, the headline text is displayed in larger fonts than the subtitles. Our system takes into account not only the texts themselves but also their attributes for structuring videos. We describe: (i) novel methods for detecting and extracting texts that are robust against the presence of complex backgrounds and intensity degradation of the character patterns, and (ii) a method for structuring a video based on the text attributes."
            },
            "slug": "Telop-on-demand:-video-structuring-and-retrieval-on-Kuwano-Taniguchi",
            "title": {
                "fragments": [],
                "text": "Telop-on-demand: video structuring and retrieval based on text recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "A telop-on-demand system that anatomically recognizes texts in video frames to create the indices needed for content based video browsing and retrieval and a method for structuring a video based on the text attributes."
            },
            "venue": {
                "fragments": [],
                "text": "2000 IEEE International Conference on Multimedia and Expo. ICME2000. Proceedings. Latest Advances in the Fast Changing World of Multimedia (Cat. No.00TH8532)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1772880"
                        ],
                        "name": "David P. Capel",
                        "slug": "David-P.-Capel",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Capel",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David P. Capel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14170862,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "fde2a3281701e5a85e8f8c1256e48950a4840578",
            "isKey": false,
            "numCitedBy": 161,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "The objective of this work is the super-resolution enhancement of image sequences. We consider in particular images of scenes for which the point-to-point image transformation is a plane projective transformation. We first describe the imaging model, and a maximum likelihood (ML) estimator of the super-resolution image. We demonstrate the extreme noise sensitivity of the unconstrained ML estimator. We show that the Irani and Peleg (1991, 1993) super-resolution algorithm does not suffer from this sensitivity, and explain that this stability is due to the error back-projection method which effectively constrains the solution. We then propose two estimators suitable for the enhancement of text images: a maximum a posteriori (MAP) estimator based on a Huber prior and an estimator regularized using the total variation norm. We demonstrate the improved noise robustness of these approaches over the Irani and Peleg estimator. We also show the effects of a poorly estimated point spread function (PSF) on the super-resolution result and explain conditions necessary for this parameter to be included in the optimization. Results are evaluated on both real and synthetic sequences of text images. In the case of the real images, the projective transformations relating the images are estimated automatically from the image data, so that the entire algorithm is automatic."
            },
            "slug": "Super-resolution-enhancement-of-text-image-Capel-Zisserman",
            "title": {
                "fragments": [],
                "text": "Super-resolution enhancement of text image sequences"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Two estimators suitable for the enhancement of text images are proposed: a maximum a posteriori (MAP) estimator based on a Huber prior and an estimator regularized using the total variation norm, which demonstrates the improved noise robustness of these approaches over the Irani and Peleg estimator."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 15th International Conference on Pattern Recognition. ICPR-2000"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115196978"
                        ],
                        "name": "Huiping Li",
                        "slug": "Huiping-Li",
                        "structuredName": {
                            "firstName": "Huiping",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huiping Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48471936"
                        ],
                        "name": "D. Doermann",
                        "slug": "D.-Doermann",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Doermann",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Doermann"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 12
                            }
                        ],
                        "text": "However, in [53] only linear-space-invariant (LSI) blurring is considered."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 9
                            }
                        ],
                        "text": "Wolf and Doermann [98] obtain the a priori distribution of 4\u00d74 binary cliques in text images from training samples and use a MAP estimator to binarize any 4\u00d74 cliques in input grayscale images [98]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 11
                            }
                        ],
                        "text": "Similar to [53] and [54], a text detector is invoked every five frames to calibrate the tracking."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 16
                            }
                        ],
                        "text": "Li and Doermann [53] also present a scheme for enhancing moving text."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 16
                            }
                        ],
                        "text": "Li and Doermann [53] study the tracking of moving text in videos."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 217,
                                "start": 213
                            }
                        ],
                        "text": "Under the assumption that the Nyquist criterion is met when a continuous object image is sampled by a CCD array, it is theoretically possible to reconstruct the original light field by sinc function interpolation [53]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 27
                            }
                        ],
                        "text": "For example, both [42] and [53] use locally adaptive thresholding to extract text pixels from video frames."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 16
                            }
                        ],
                        "text": "Li and Doermann [53] utilize a projection-onto-convex-sets (POCS)based method to deblur scene text to improve readability."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 66
                            }
                        ],
                        "text": "Bilinear interpolation has been found effective in many instances [42,53,75,79,90]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18856719,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1484e9fc337e99fbd70ece46005c7cf6a8541bb7",
            "isKey": false,
            "numCitedBy": 108,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper a multiple frame based technique to enhance text in digital video is presented. After extracting a reference text block, we use an image matching technique to find the corresponding text blocks in consecutive frames. We register these text blocks to subpixel levels by using image interpolation techniques to improve both correspondence and text resolution. The registered text blocks are averaged to obtain a new text block with a clean background and a higher resolution. Experiments conducted on several video sequences show that our enhancement scheme can improve the accuracy of commercial off-the-shelf OCR considerably."
            },
            "slug": "Text-enhancement-in-digital-video-using-multiple-Li-Doermann",
            "title": {
                "fragments": [],
                "text": "Text enhancement in digital video using multiple frame integration"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Experiments conducted on several video sequences show that the multiple frame based enhancement scheme can improve the accuracy of commercial off-the-shelf OCR considerably."
            },
            "venue": {
                "fragments": [],
                "text": "MULTIMEDIA '99"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3314902"
                        ],
                        "name": "Edward K. Wong",
                        "slug": "Edward-K.-Wong",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Wong",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Edward K. Wong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50133585"
                        ],
                        "name": "Minya Chen",
                        "slug": "Minya-Chen",
                        "structuredName": {
                            "firstName": "Minya",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Minya Chen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Wong and Chen [ 102 ,117] start by computing the horizontal gradient of"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 44601365,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b84b397a325afb12f768080705f5bb7b719397f0",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We report on the development and implementation of a new algorithm for extracting text in digitized color video. The algorithm operates by locating potential text line segments from horizontal scan lines. Detected text line segments are expanded or combined with text line segments from adjacent scan lines to form larger text blocks, which are then subject to filtering and refinement. Text pixels within text blocks are then found using bi-color clustering and connected-component analysis. Morphological contour smoothing and morphological resolution enhancement algorithms are then applied to the detected binary texts to enhance their visual quality. The implemented algorithm has fast execution time and is effective in detecting text in difficult cases, such as scenes with highly textured background, and scenes with small text. A variety of color images digitized from broadcast television are used to test the algorithm and excellent performance result was obtained."
            },
            "slug": "A-robust-algorithm-for-text-extraction-in-color-Wong-Chen",
            "title": {
                "fragments": [],
                "text": "A robust algorithm for text extraction in color video"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The implemented algorithm has fast execution time and is effective in detecting text in difficult cases, such as scenes with highly textured background, and scenes with small text."
            },
            "venue": {
                "fragments": [],
                "text": "2000 IEEE International Conference on Multimedia and Expo. ICME2000. Proceedings. Latest Advances in the Fast Changing World of Multimedia (Cat. No.00TH8532)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1785083"
                        ],
                        "name": "Michael R. Lyu",
                        "slug": "Michael-R.-Lyu",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Lyu",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael R. Lyu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3332642"
                        ],
                        "name": "Jiqiang Song",
                        "slug": "Jiqiang-Song",
                        "structuredName": {
                            "firstName": "Jiqiang",
                            "lastName": "Song",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiqiang Song"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052052370"
                        ],
                        "name": "Min Cai",
                        "slug": "Min-Cai",
                        "structuredName": {
                            "firstName": "Min",
                            "lastName": "Cai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Min Cai"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 25
                            }
                        ],
                        "text": "use Sobel edge detectors [6] in all Y, U, and V channels of a color image to find text that is similar to background in luminance but not in color."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12862847,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0f64a1d2e366eb476be69cc431f053dcaa22935a",
            "isKey": false,
            "numCitedBy": 151,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Text detection is fundamental to video information retrieval and indexing. Existing methods cannot handle well those texts with different contrast or embedded in a complex background. To handle these difficulties, this paper proposes an efficient text detection approach, which is based on invariant features, such as edge strength, edge density, and horizontal distribution. First, it applies edge detection and uses a low threshold to filter out definitely non-text edges. Then, a local threshold is selected to both keep low-contrast text and simplify complex background of high-contrast text. Next, two text-area enhancement operators are proposed to highlight those areas with either high edge strength or high edge density. Finally, coarse-to-fine detection locates text regions efficiently. Experimental results show that this approach is robust for contrast, font-size, font-color, language, and background complexity."
            },
            "slug": "A-new-approach-for-video-text-detection-Lyu-Song",
            "title": {
                "fragments": [],
                "text": "A new approach for video text detection"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper proposes an efficient text detection approach, which is based on invariant features, such as edge strength, edge density, and horizontal distribution, and it applies edge detection and uses a low threshold to filter out definitely non-text edges."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. International Conference on Image Processing"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144717607"
                        ],
                        "name": "Jun-Wei Hsieh",
                        "slug": "Jun-Wei-Hsieh",
                        "structuredName": {
                            "firstName": "Jun-Wei",
                            "lastName": "Hsieh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jun-Wei Hsieh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1755952"
                        ],
                        "name": "Shih-Hao Yu",
                        "slug": "Shih-Hao-Yu",
                        "structuredName": {
                            "firstName": "Shih-Hao",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shih-Hao Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9407097"
                        ],
                        "name": "Yung-Sheng Chen",
                        "slug": "Yung-Sheng-Chen",
                        "structuredName": {
                            "firstName": "Yung-Sheng",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yung-Sheng Chen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18190834,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "537e657b4d56d01f800c50489948a8e1e6d179cd",
            "isKey": false,
            "numCitedBy": 191,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a morphology-based method for detecting license plates from cluttered images. The proposed system consists of three major components. At the first, a morphology-based method is proposed to extract important contrast features as guides to search the desired license plates. The contrast feature is robust to lighting changes and invariant to several transformations like scaling, translation, and skewing. Then, a recovery algorithm is applied for reconstructing a license plate if the plate is fragmented into several parts. The last step is to do license plate verification. The morphology based method can significantly reduce the number of candidates extracted from the cluttered images and thus speeds up the subsequent plate recognition. Under the experimental database, 128 examples got from 130 images were successfully detected. The average accuracy of license plate detection is 98%. Experimental results show that the proposed method improves the state-of-the-art work in terms of effectiveness and robustness of license plate detection."
            },
            "slug": "Morphology-based-license-plate-detection-from-Hsieh-Yu",
            "title": {
                "fragments": [],
                "text": "Morphology-based license plate detection from complex scenes"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "Experimental results show that the proposed morphology-based method for detecting license plates from cluttered images improves the state-of-the-art work in terms of effectiveness and robustness of license plate detection."
            },
            "venue": {
                "fragments": [],
                "text": "Object recognition supported by user interaction for service robots"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33587489"
                        ],
                        "name": "Yingzi Du",
                        "slug": "Yingzi-Du",
                        "structuredName": {
                            "firstName": "Yingzi",
                            "lastName": "Du",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yingzi Du"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1785977"
                        ],
                        "name": "Chein-I. Chang",
                        "slug": "Chein-I.-Chang",
                        "structuredName": {
                            "firstName": "Chein-I.",
                            "lastName": "Chang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chein-I. Chang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 18
                            }
                        ],
                        "text": "Others, including [22] and [39], use adaptive thresholding, too."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15290451,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e62e3b2d4d86da27be0486683aaf5a6e34f45eb2",
            "isKey": false,
            "numCitedBy": 6,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Thresholding video images is very challenging due to the fact that image background generally has low resolution and is also more complicated and highly distorted than document images. As a result, thresholding methods that work well for document images may not work effectively for video images in some applications. This paper investigates the issue of thresholding video images for text detection and further develops a relative entropy-based thresholding approach that can effectively extract text from complicated video images. In order to demonstrate its performance a comparative study is conducted among the proposed thresholding method and several thresholding techniques which are widely used for document and gray scale images. The experimental results show that thresholding video images is far more difficult than thresholding document images and simple histogram-based methods generally do not perform well."
            },
            "slug": "Thresholding-video-images-for-text-detection-Du-Chang",
            "title": {
                "fragments": [],
                "text": "Thresholding video images for text detection"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper investigates the issue of thresholding video images for text detection and further develops a relative entropy-based thresholding approach that can effectively extract text from complicated video images."
            },
            "venue": {
                "fragments": [],
                "text": "Object recognition supported by user interaction for service robots"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48471936"
                        ],
                        "name": "D. Doermann",
                        "slug": "D.-Doermann",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Doermann",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Doermann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "domains, including office automation, forensics, and digital libraries. Some surveys include [ 19 ,72,81,93]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11498408,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b76bbddf92d247705c839436b5836081ab0add8a",
            "isKey": false,
            "numCitedBy": 310,
            "numCiting": 95,
            "paperAbstract": {
                "fragments": [],
                "text": "The economic feasibility of maintaining large data bases of document images has created a tremendous demand for robust ways to access and manipulate the information these images contain. In an attempt to move toward a paperless office, large quantities of printed documents are often scanned and archived as images, without adequate index information. One way to provide traditional data-base indexing and retrieval capabilities is to fully convert the document to an electronic representation which can be indexed automatically. Unfortunately, there are many factors which prohibit complete conversion including high cost, low document quality, and the fact that many nontext components cannot be adequately represented in a converted form. In such cases, it can be advantageous to maintain a copy of and use the document in image form. In this paper, we provide a survey of methods developed by researchers to access and manipulate document images without the need for complete and accurate conversion. We briefly discuss traditional text indexing techniques on imperfect data and the retrieval of partially converted documents. This is followed by a more comprehensive review of techniques for the direct characterization, manipulation, and retrieval, of images of documents containing text, graphics, and scene images."
            },
            "slug": "The-Indexing-and-Retrieval-of-Document-Images:-A-Doermann",
            "title": {
                "fragments": [],
                "text": "The Indexing and Retrieval of Document Images: A Survey"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A survey of methods developed by researchers to access and manipulate document images without the need for complete and accurate conversion is provided."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Vis. Image Underst."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108487560"
                        ],
                        "name": "J. C. Lee",
                        "slug": "J.-C.-Lee",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lee",
                            "middleNames": [
                                "Chung-Mong"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. C. Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2257741"
                        ],
                        "name": "A. Kankanhalli",
                        "slug": "A.-Kankanhalli",
                        "structuredName": {
                            "firstName": "Atreyi",
                            "lastName": "Kankanhalli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kankanhalli"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "code reader. \u2013 Lee and Kankanhalli [ 49 ] present a sys-"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "[ 49 ] take a similar approach where the character regions are grown out of vertical scan line segments."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "a Video caption text recognition. b Cargo container code recognition [ 49 ]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11365167,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "07a0d7a8938fa04cad3a1afc28c3dcaf1724bde8",
            "isKey": true,
            "numCitedBy": 74,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "We have developed a generalized alphanumeric character extraction algorithm that can efficiently and accurately locate and extract characters from complex scene images. A scene image may be complex due to the following reasons: (1) the characters are embedded in an image with other objects, such as structural bars, company logos and smears; (2) the characters may be painted or printed in any color including white, and the background color may differ only slightly from that of the characters; (3) the font, size and format of the characters may be different; and (4) the lighting may be uneven. The main contribution of this research is that it permits the quick and accurate extraction of characters in a complex scene. A coarse search technique is used to locate potential characters, and then a fine grouping technique is used to extract characters accurately. Several additional techniques in the postprocessing phase eliminate spurious as well as overlapping characters. Experimental results of segmenting characters written on cargo container surfaces show that the system is feasible under real-life constraints. The program has been installed as part of a vision system which verifies container codes on vehicles passing through the Port of Singapore."
            },
            "slug": "Automatic-Extraction-of-Characters-in-Complex-Scene-Lee-Kankanhalli",
            "title": {
                "fragments": [],
                "text": "Automatic Extraction of Characters in Complex Scene Images"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "A generalized alphanumeric character extraction algorithm that can efficiently and accurately locate and extract characters from complex scene images is developed that permits the quick and accurate extraction of characters in a complex scene."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Pattern Recognit. Artif. Intell."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2071412"
                        ],
                        "name": "S. Kuo",
                        "slug": "S.-Kuo",
                        "structuredName": {
                            "firstName": "Shyh-shiaw",
                            "lastName": "Kuo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kuo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2090530068"
                        ],
                        "name": "M. V. Ranganath",
                        "slug": "M.-V.-Ranganath",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Ranganath",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. V. Ranganath"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "and Ranganath [ 46 ] introduce a method for enhancing"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 32495530,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fda86d3b8596285dc45ee8c4f252f962c153a158",
            "isKey": false,
            "numCitedBy": 13,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "Two efficient algorithms are presented for enhancing color images and gray-level scanned document images. For color images, a generic approach for enhancing both contrast and color saturation of color images is first developed. For real time application in NTSC devices, an efficient algorithm is then proposed to optimally enhance a color image in its native YCbCr color space without any color space conversion. As to enhance a gray-level text image, an algorithm is also proposed to successfully remove the blurriness, and maximize the contrast in real time. The resulting image possesses a much higher readability than its original form. Both algorithms achieve highly satisfactory results without introducing excess complexity. The simplicity of the algorithm and its ability to produce better results than the conventional approaches make it an excellent candidate for practical implementation in real-time applications. Experimental results are given to demonstrate the superiority of the proposed algorithms."
            },
            "slug": "Real-time-image-enhancement-for-both-text-and-color-Kuo-Ranganath",
            "title": {
                "fragments": [],
                "text": "Real time image enhancement for both text and color photo images"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "Two efficient algorithms are presented for enhancing color images and gray-level scanned document images that achieve highly satisfactory results without introducing excess complexity and are an excellent candidate for practical implementation in real-time applications."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings., International Conference on Image Processing"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110062608"
                        ],
                        "name": "Toshio Sato",
                        "slug": "Toshio-Sato",
                        "structuredName": {
                            "firstName": "Toshio",
                            "lastName": "Sato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Toshio Sato"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733113"
                        ],
                        "name": "T. Kanade",
                        "slug": "T.-Kanade",
                        "structuredName": {
                            "firstName": "Takeo",
                            "lastName": "Kanade",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanade"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2816639"
                        ],
                        "name": "Ellen K. Hughes",
                        "slug": "Ellen-K.-Hughes",
                        "structuredName": {
                            "firstName": "Ellen",
                            "lastName": "Hughes",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ellen K. Hughes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116645471"
                        ],
                        "name": "Michael A. Smith",
                        "slug": "Michael-A.-Smith",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Smith",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael A. Smith"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[85] assume that graphic text in a TV news program is static, so no spatial transformation (warping, or translation) is necessary."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[85] couple character segmentation and recognition."
                    },
                    "intents": []
                }
            ],
            "corpusId": 43395565,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "67c4ed0ef1c978defe1c44868029790aaad21752",
            "isKey": false,
            "numCitedBy": 275,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Video OCR is a technique that can greatly help to locate topics of interest in a large digital news video archive via the automatic extraction and reading of captions and annotations. News captions generally provide vital search information about the video being presented, the names of people and places or descriptions of objects. In this paper, two difficult problems of character recognition for videos are addressed: low resolution characters and extremely complex backgrounds. We apply an interpolation filter, multi-frame integration and a combination of four filters to solve these problems. Segmenting characters is done by a recognition-based segmentation method and intermediate character recognition results are used to improve the segmentation. The overall recognition results are good enough for use in news indexing. Performing video OCR on news video and combining its results with other video understanding techniques will improve the overall understanding of the news video content."
            },
            "slug": "Video-OCR-for-digital-news-archive-Sato-Kanade",
            "title": {
                "fragments": [],
                "text": "Video OCR for digital news archive"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper applies an interpolation filter, multi-frame integration and a combination of four filters to solve the problems of character recognition for videos: low resolution characters and extremely complex backgrounds."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 1998 IEEE International Workshop on Content-Based Access of Image and Video Database"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3014799"
                        ],
                        "name": "Ali Zandifar",
                        "slug": "Ali-Zandifar",
                        "structuredName": {
                            "firstName": "Ali",
                            "lastName": "Zandifar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ali Zandifar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719541"
                        ],
                        "name": "R. Duraiswami",
                        "slug": "R.-Duraiswami",
                        "structuredName": {
                            "firstName": "Ramani",
                            "lastName": "Duraiswami",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Duraiswami"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2022518"
                        ],
                        "name": "Antoine Chahine",
                        "slug": "Antoine-Chahine",
                        "structuredName": {
                            "firstName": "Antoine",
                            "lastName": "Chahine",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Antoine Chahine"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693428"
                        ],
                        "name": "L. Davis",
                        "slug": "L.-Davis",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Davis",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Davis"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "[107] propose to use camera-based OCR techniques in a head-mounted smart video camera system to help the visually impaired."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16684049,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f587fac653867513c6e8e9b64cf371e008691dcf",
            "isKey": false,
            "numCitedBy": 44,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe the development of an interface to textual information for the visually impaired that uses video, image processing, optical-character-recognition (OCR) and text-to-speech (TTS). The video provides a sequence of low resolution images in which text must be detected, rectified and converted into high resolution rectangular blocks that are capable of being analyzed via off-the-shelf OCR. To achieve this, various problems related to feature detection, mosaicing, auto-focus, zoom, and systems integration were solved in the development of the system."
            },
            "slug": "A-video-based-interface-to-textual-information-for-Zandifar-Duraiswami",
            "title": {
                "fragments": [],
                "text": "A video based interface to textual information for the visually impaired"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "An interface to textual information for the visually impaired that uses video, image processing, optical-character-recognition (OCR) and text-to-speech (TTS) is described."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. Fourth IEEE International Conference on Multimodal Interfaces"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2856656"
                        ],
                        "name": "Hae-Kwang Kim",
                        "slug": "Hae-Kwang-Kim",
                        "structuredName": {
                            "firstName": "Hae-Kwang",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hae-Kwang Kim"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 8081258,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f455a0f0e6d402648da33930fb39ef5587aab0e3",
            "isKey": false,
            "numCitedBy": 116,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract An efficient automatic text detection and location method for video documents is proposed and its application for the content-based retrieval of video is presented and discussed. Target frames are selected at fixed time intervals from shots detected by a scene-change detection method. For each selected frame, segmentation by color clustering is performed around color peaks using a color histogram. For each color plane, text-lines are detected using heuristics, and the temporal and spatial position and the text-image of each text-line are stored in a database. Experimental results for text detection in video images and the performance of the method are reported for various video documents. A user interface for text-image based browsing is designed for direct content-based access to video documents, and other applications are discussed."
            },
            "slug": "Efficient-Automatic-Text-Location-Method-and-and-of-Kim",
            "title": {
                "fragments": [],
                "text": "Efficient Automatic Text Location Method and Content-Based Indexing and Structuring of Video Database"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "An efficient automatic text detection and location method for video documents is proposed and its application for the content-based retrieval of video is presented and discussed."
            },
            "venue": {
                "fragments": [],
                "text": "J. Vis. Commun. Image Represent."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115196978"
                        ],
                        "name": "Huiping Li",
                        "slug": "Huiping-Li",
                        "structuredName": {
                            "firstName": "Huiping",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huiping Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48471936"
                        ],
                        "name": "D. Doermann",
                        "slug": "D.-Doermann",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Doermann",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Doermann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3272081"
                        ],
                        "name": "O. Kia",
                        "slug": "O.-Kia",
                        "structuredName": {
                            "firstName": "Omid",
                            "lastName": "Kia",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Kia"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 92
                            }
                        ],
                        "text": "perform wavelet decomposition on multiscale layers generated from the input grayscale image [54,55]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15485643,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f8f5c282dc11937d29183b955dc3e4fbb677571b",
            "isKey": false,
            "numCitedBy": 652,
            "numCiting": 74,
            "paperAbstract": {
                "fragments": [],
                "text": "Text that appears in a scene or is graphically added to video can provide an important supplemental source of index information as well as clues for decoding the video's structure and for classification. In this work, we present algorithms for detecting and tracking text in digital video. Our system implements a scale-space feature extractor that feeds an artificial neural processor to detect text blocks. Our text tracking scheme consists of two modules: a sum of squared difference (SSD)-based module to find the initial position and a contour-based module to refine the position. Experiments conducted with a variety of video sources show that our scheme can detect and track text robustly."
            },
            "slug": "Automatic-text-detection-and-tracking-in-digital-Li-Doermann",
            "title": {
                "fragments": [],
                "text": "Automatic text detection and tracking in digital video"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work presents algorithms for detecting and tracking text in digital video that implements a scale-space feature extractor that feeds an artificial neural processor to detect text blocks."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Image Process."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2069641275"
                        ],
                        "name": "Paul Clark",
                        "slug": "Paul-Clark",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Clark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paul Clark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728108"
                        ],
                        "name": "M. Mirmehdi",
                        "slug": "M.-Mirmehdi",
                        "structuredName": {
                            "firstName": "Majid",
                            "lastName": "Mirmehdi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mirmehdi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 21029507,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ed92505aa6d7fb60c0ef764e60e0ee043e28eb7a",
            "isKey": false,
            "numCitedBy": 110,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract. We present two different approaches to the location and recovery of text in images of real scenes. The techniques we describe are invariant to the scale and 3D orientation of the text, and allow recovery of text in cluttered scenes. The first approach uses page edges and other rectangular boundaries around text to locate a surface containing text, and to recover a fronto-parallel view. This is performed using line detection, perceptual grouping, and comparison of potential text regions using a confidence measure. The second approach uses low-level texture measures with a neural network classifier to locate regions of text in an image. Then we recover a fronto-parallel view of each located paragraph of text by separating the individual lines of text and determining the vanishing points of the text plane. We illustrate our results using a number of images."
            },
            "slug": "Recognising-text-in-real-scenes-Clark-Mirmehdi",
            "title": {
                "fragments": [],
                "text": "Recognising text in real scenes"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "Two different approaches to the location and recovery of text in images of real scenes are presented, one using page edges and other rectangular boundaries around text, and the other using low-level texture measures with a neural network classifier."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal on Document Analysis and Recognition"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2751729"
                        ],
                        "name": "P. Wellner",
                        "slug": "P.-Wellner",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Wellner",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Wellner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 93
                            }
                        ],
                        "text": "For example, a camera mounted over a writing surface can be used for handwriting recognition [26,70,96], or cameras can be used in whiteboard reading [89,97]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 24
                            }
                        ],
                        "text": "The DigitalDesk project [96] turns the desktop into a digital working area through the use of cameras and projectors."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 39
                            }
                        ],
                        "text": "c Camera-based handwriting recognition [26,70,96]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 58
                            }
                        ],
                        "text": "Other lightweight desktop applications are also available [75,96]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 207174911,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c04eaf31d294cbcbb6ae30d691e7129446bdb5e3",
            "isKey": true,
            "numCitedBy": 1195,
            "numCiting": 143,
            "paperAbstract": {
                "fragments": [],
                "text": "In the 1970's Xerox PARC developed the \" desktop metaphor, \" which made computers easy to use by making them look and act like ordinary desks and paper. This led visionaries to predict the \" paperless office \" would dominate within a few years, but the trouble with this prediction is that people like paper too much. It is portable, tactile, universally accepted, and easier to read than a screen. Today, we continue to use paper, and computers produce more of it than they replace. Instead of trying to use computers to replace paper, the DigitalDesk takes the opposite approach. It keeps the paper, but uses computers to make it more powerful. It provides a Computer Augmented Environment for paper. The DigitalDesk is built around an ordinary physical desk and can be used as such, but it has extra capabilities. A video camera is mounted above the desk, pointing down at the work surface. This camera's output is fed through a system that can detect where the user is pointing, and it can read documents that are placed on the desk. A computer-driven electronic projector is also mounted above the desk, allowing the system to project electronic objects onto the work surface and onto real paper documents \u2014 something that can't be done with flat display panels or rear-projection. The system is called DigitalDesk because it allows pointing with the fingers. Several applications have been prototyped on the DigitalDesk. The first was a calculator where a sheet of paper such as an annual report can be placed on the desk allowing the user to point at numbers with a finger or pen. The camera reads the numbers off the paper, recognizes them, and enters them into the display for further calculations. Another is a translation system which allows users to point at unfamiliar French words to get their English definitions projected down next to the paper. A third is a paper-based paint program (PaperPaint) that allows users to sketch on paper using traditional tools, but also be able to select and paste these sketches with the camera and projector to create merged paper and electronic documents. A fourth application is the DoubleDigitalDesk, which allows remote colleagues to \" share \" their desks, look at each other's paper documents and sketch on them remotely. This dissertation introduces the concept of Computer Augmented Environments, describes the DigitalDesk and applications for it, and \u2026"
            },
            "slug": "Interacting-with-paper-on-the-DigitalDesk-Wellner",
            "title": {
                "fragments": [],
                "text": "Interacting with paper on the DigitalDesk"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The DigitalDesk is built around an ordinary physical desk and can be used as such, but it has extra capabilities, including a video camera mounted above the desk that can detect where the user is pointing, and it can read documents that are placed on the desk."
            },
            "venue": {
                "fragments": [],
                "text": "CACM"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2155703534"
                        ],
                        "name": "Jing Zhang",
                        "slug": "Jing-Zhang",
                        "structuredName": {
                            "firstName": "Jing",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jing Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46772547"
                        ],
                        "name": "Xilin Chen",
                        "slug": "Xilin-Chen",
                        "structuredName": {
                            "firstName": "Xilin",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xilin Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2714754"
                        ],
                        "name": "Andreas Hanneman",
                        "slug": "Andreas-Hanneman",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Hanneman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andreas Hanneman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118579343"
                        ],
                        "name": "Jie Yang",
                        "slug": "Jie-Yang",
                        "structuredName": {
                            "firstName": "Jie",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jie Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724972"
                        ],
                        "name": "A. Waibel",
                        "slug": "A.-Waibel",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Waibel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Waibel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 108
                            }
                        ],
                        "text": "present their OCR engine design, which works directly on graylevel images and reported satisfactory results [110]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7832131,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9b2b1e9755b1ff1fa2df74bd53212e8f0c0113fb",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose a robust approach for recognition of text embedded in natural scenes. Instead of using binary information as most other OCR systems do, we extract features from intensity of an image directly. We utilize a local intensity normalization method to effectively handle lighting variations. We then employ Gabor transform to obtain local features, and use the linear discriminant analysis (LDA) for selection and classification of features. The proposed method has been applied to a Chinese sign recognition task. The system can recognize a vocabulary of 3755 level I Chinese characters in the Chinese national standard character set GB2312-80 with various print fonts. We tested the system on 1630 test characters in sign images captured from the natural scenes, and the recognition accuracy was 92.46%. We have integrated the system into our automatic Chinese sign translation system."
            },
            "slug": "A-robust-approach-for-recognition-of-text-embedded-Zhang-Chen",
            "title": {
                "fragments": [],
                "text": "A robust approach for recognition of text embedded in natural scenes"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "A robust approach for recognition of text embedded in natural scenes by utilizing a local intensity normalization method to effectively handle lighting variations and using the linear discriminant analysis (LDA) for selection and classification of features."
            },
            "venue": {
                "fragments": [],
                "text": "Object recognition supported by user interaction for service robots"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472153"
                        ],
                        "name": "S. Messelodi",
                        "slug": "S.-Messelodi",
                        "structuredName": {
                            "firstName": "Stefano",
                            "lastName": "Messelodi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Messelodi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2389886"
                        ],
                        "name": "C. M. Modena",
                        "slug": "C.-M.-Modena",
                        "structuredName": {
                            "firstName": "Carla",
                            "lastName": "Modena",
                            "middleNames": [
                                "Maria"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. M. Modena"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13597946,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "55f6886a7a70c800aa3c899d16e784a611dc9ba2",
            "isKey": false,
            "numCitedBy": 105,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Automatic-identification-and-skew-estimation-of-in-Messelodi-Modena",
            "title": {
                "fragments": [],
                "text": "Automatic identification and skew estimation of text lines in real scene images"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1828940"
                        ],
                        "name": "D. Lopresti",
                        "slug": "D.-Lopresti",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Lopresti",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lopresti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2461436"
                        ],
                        "name": "Jiangying Zhou",
                        "slug": "Jiangying-Zhou",
                        "structuredName": {
                            "firstName": "Jiangying",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiangying Zhou"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 164
                            }
                        ],
                        "text": "Similarly, reading image text has been widely addressed in the WWW community because of the desire to index text that appears in graphical images of many Web pages [59]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14588638,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4776d319dcadf2619f90f5373925d961f70f3fcb",
            "isKey": false,
            "numCitedBy": 72,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "The explosive growth of the World Wide Web has resulted in a distributed database consisting of hundreds of millions of documents. While existing search engines index a page based on the text that is readily extracted from its HTML encoding, an increasing amount of the information on the Web is embedded in images. This situation presents a new and exciting challenge for the fields of document analysis and information retrieval, as WWW image text is typically rendered in color and at very low spatial resolutions. In this paper, we survey the results of several years of our work in the area. For the problem of locating text in Web images, we describe a procedure based on clustering in color space followed by a connected-components analysis that seems promising. For character recognition, we discuss techniques using polynomial surface fitting and \u201cfuzzy\u201d n-tuple classifiers. Also presented are the results of several experiments that demonstrate where our methods perform well and where more work needs to be done. We conclude with a discussion of topics for further research."
            },
            "slug": "Locating-and-Recognizing-Text-in-WWW-Images-Lopresti-Zhou",
            "title": {
                "fragments": [],
                "text": "Locating and Recognizing Text in WWW Images"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A procedure based on clustering in color space followed by a connected-components analysis that seems promising for locating text in Web images and techniques using polynomial surface fitting and \u201cfuzzy\u201d n-tuple classifiers are described."
            },
            "venue": {
                "fragments": [],
                "text": "Information Retrieval"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2069641275"
                        ],
                        "name": "Paul Clark",
                        "slug": "Paul-Clark",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Clark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paul Clark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728108"
                        ],
                        "name": "M. Mirmehdi",
                        "slug": "M.-Mirmehdi",
                        "structuredName": {
                            "firstName": "Majid",
                            "lastName": "Mirmehdi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mirmehdi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2378082,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fdc6ff7b7fc4f1769e8e76897065282e436c634b",
            "isKey": false,
            "numCitedBy": 53,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "A method for the fronto-parallel recovery of paragraphs of text under full perspective transformation is presented. The horizontal vanishing point of the text plane is found using an extension of 2D projection profiles. This allows the accurate segmentation of the lines of text. Analysis of the lines will then reveal the style of justification of the paragraph, and provide an estimate of the vertical vanishing point of the plane. The text is finally recovered to a fronto-parallel view suitable for OCR or other higher-level recognition."
            },
            "slug": "Estimating-the-Orientation-and-Recovery-of-Text-in-Clark-Mirmehdi",
            "title": {
                "fragments": [],
                "text": "Estimating the Orientation and Recovery of Text Planes in a Single Image"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "A method for the fronto-parallel recovery of paragraphs of text under full perspective transformation is presented, and the horizontal vanishing point of the text plane is found using an extension of 2D projection profiles for accurate segmentation of the lines of text."
            },
            "venue": {
                "fragments": [],
                "text": "BMVC"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49325667"
                        ],
                        "name": "H. Kamada",
                        "slug": "H.-Kamada",
                        "structuredName": {
                            "firstName": "Hiroshi",
                            "lastName": "Kamada",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Kamada"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2646558"
                        ],
                        "name": "K. Fujimoto",
                        "slug": "K.-Fujimoto",
                        "structuredName": {
                            "firstName": "Katsuhito",
                            "lastName": "Fujimoto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Fujimoto"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "For example, both [ 42 ] and [53] use locally adaptive thresholding to extract text pixels from video frames."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 39579517,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fe51d711ede8600caeb14262fb49a5df5ed4c67f",
            "isKey": false,
            "numCitedBy": 32,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a new high-speed, high-accuracy binarization method for recognizing text in document images. First character neighborhoods are extracted from input images using a global thresholding value that is shifted to the background pixel value from the thresholding value of conventional global binarization. Second, characters are extracted using an original local binarization process integrated with image interpolation. Our method takes only 1/100 the processing time over the method that performs image interpolation first. Therefore our method binarizes an A4 size text image (150dpi) in an average of only 3.3 seconds using a 166 MHz Pentium processor. Furthermore, our method reduced unrecognized characters by 46.5%, compared with conventional global binarization."
            },
            "slug": "High-speed,-high-accuracy-binarization-method-for-Kamada-Fujimoto",
            "title": {
                "fragments": [],
                "text": "High-speed, high-accuracy binarization method for recognizing text in images of low spatial resolutions"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "A new high-speed, high-accuracy binarization method for recognizing text in document images that takes only 1/100 the processing time over the method that performs image interpolation first and reduced unrecognized characters by 46.5%, compared with conventional global binarizing."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Fifth International Conference on Document Analysis and Recognition. ICDAR '99 (Cat. No.PR00318)"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38846884"
                        ],
                        "name": "Mauritius Seeger",
                        "slug": "Mauritius-Seeger",
                        "structuredName": {
                            "firstName": "Mauritius",
                            "lastName": "Seeger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mauritius Seeger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3344005"
                        ],
                        "name": "C. Dance",
                        "slug": "C.-Dance",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Dance",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Dance"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "In [86], Seeger and Dance present an alternate approach to computing locally adaptive thresholds that is in effect a Niblack method with adaptive window size."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13632834,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "66a1177c31caf2bcc00af7dc451fd9de1310235a",
            "isKey": false,
            "numCitedBy": 74,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a binarisation method designed specifically for OCR of low quality camera images: background surface thresholding or BST. This method is robust to lighting variations and produces images with very little noise and consistent stroke width. BST computes a \"surface\" of background intensities at every point in the image and performs adaptive thresholding based on this result. The surface is estimated by identifying regions of low-resolution text and interpolating neighbouring background intensities into these regions. The final threshold is a combination of this surface and a global offset. According to our evaluation BST produces considerably fewer OCR errors than Niblack's local average method while also being more runtime efficient."
            },
            "slug": "Binarising-camera-images-for-OCR-Seeger-Dance",
            "title": {
                "fragments": [],
                "text": "Binarising camera images for OCR"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "A binarisation method designed specifically for OCR of low quality camera images: background surface thresholding or BST, which produces considerably fewer OCR errors than Niblack's local average method while also being more runtime efficient."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of Sixth International Conference on Document Analysis and Recognition"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2148905379"
                        ],
                        "name": "Zheng Zhang",
                        "slug": "Zheng-Zhang",
                        "structuredName": {
                            "firstName": "Zheng",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zheng Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679749"
                        ],
                        "name": "C. Tan",
                        "slug": "C.-Tan",
                        "structuredName": {
                            "firstName": "Chew",
                            "lastName": "Tan",
                            "middleNames": [
                                "Lim"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Tan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 3
                            }
                        ],
                        "text": "In [112] they extend this work by fitting a quadratic polynomial curve to the distorted text line, which will increase the accuracy of text line straightening."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14312797,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6e307d16665a83b70738b204165934e422a74dcb",
            "isKey": false,
            "numCitedBy": 73,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Image warping is a common problem when one scans or photocopies a document page from a thick bound volume, resulting in shading and curved text lines in the spine area of the bound volume. This will not only impair readability, but will also reduce the OCR accuracy. Further to our earlier attempt to correct such images, this paper proposes a simpler connected component analysis and regression technique. Compared to our earlier method, the present system is computationally less expensive and is resolution independent too. The implementation of the new system and improvement of OCR accuracy are presented in this paper."
            },
            "slug": "Correcting-document-image-warping-based-on-of-text-Zhang-Tan",
            "title": {
                "fragments": [],
                "text": "Correcting document image warping based on regression of curved text lines"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A simpler connected component analysis and regression technique is proposed for OCR accuracy improvement that is computationally less expensive and is resolution independent too."
            },
            "venue": {
                "fragments": [],
                "text": "Seventh International Conference on Document Analysis and Recognition, 2003. Proceedings."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48016595"
                        ],
                        "name": "Hao Wang",
                        "slug": "Hao-Wang",
                        "structuredName": {
                            "firstName": "Hao",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hao Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145800809"
                        ],
                        "name": "J. Kangas",
                        "slug": "J.-Kangas",
                        "structuredName": {
                            "firstName": "Jari",
                            "lastName": "Kangas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kangas"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 50
                            }
                        ],
                        "text": "Wang and Kangas also use multigroup decomposition [94] where they combine quantization results from the hue component layer, the low saturation layer, the luminance layer, and the edge map."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8140967,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "12b945deacb72e2357e71c4f582545ca693d34b8",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a method of identifying character-like regions in order to extract and recognize characters in natural color scene images automatically. After connected component extraction based on a multi-group decomposition scheme, alignment analysis is used to check the block candidates, namely, the character-like regions in each binary image layer and the final composed image. Priority adaptive segmentation (PAS) is implemented to obtain accurate foreground pixels of the character in each block. Then some heuristic meanings such as statistical features, recognition confidence, and alignment properties, are employed to justify the segmented characters. The algorithms are robust for a wide range of character fonts, shooting conditions, and color backgrounds. Results of our experiments are promising for real applications."
            },
            "slug": "Character-like-region-verification-for-extracting-Wang-Kangas",
            "title": {
                "fragments": [],
                "text": "Character-like region verification for extracting text in scene images"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "A method of identifying character-like regions in order to extract and recognize characters in natural color scene images automatically and some heuristic meanings such as statistical features, recognition confidence, and alignment properties, are employed to justify the segmented characters."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of Sixth International Conference on Document Analysis and Recognition"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144899680"
                        ],
                        "name": "Christian Wolf",
                        "slug": "Christian-Wolf",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Wolf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian Wolf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48471936"
                        ],
                        "name": "D. Doermann",
                        "slug": "D.-Doermann",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Doermann",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Doermann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8165672,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "86452e55663b5b31f509d047dd8dae4ca124d912",
            "isKey": false,
            "numCitedBy": 80,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Binarization techniques have been developed in the document analysis community for over 30 years and many algorithms have been used successfully. On the other hand, document analysis tasks are more and more frequently being applied to multimedia documents such as video sequences. Due to low resolution and lossy compression, the binarization of text included in the frames is a non-trivial task. Existing techniques work without a model of the spatial relationships in the image, which makes them less powerful. We introduce a new technique based on a Markov random field model of the document. The model parameters (clique potentials) are learned from training data and the binary image is estimated in a Bayesian framework. The performance is evaluated using commercial OCR software."
            },
            "slug": "Binarization-of-low-quality-text-using-a-Markov-Wolf-Doermann",
            "title": {
                "fragments": [],
                "text": "Binarization of low quality text using a Markov random field model"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A new technique based on a Markov random field model of the document and the model parameters (clique potentials) are learned from training data and the binary image is estimated in a Bayesian framework is introduced."
            },
            "venue": {
                "fragments": [],
                "text": "Object recognition supported by user interaction for service robots"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2069641275"
                        ],
                        "name": "Paul Clark",
                        "slug": "Paul-Clark",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Clark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paul Clark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728108"
                        ],
                        "name": "M. Mirmehdi",
                        "slug": "M.-Mirmehdi",
                        "structuredName": {
                            "firstName": "Majid",
                            "lastName": "Mirmehdi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mirmehdi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 178
                            }
                        ],
                        "text": "Clark and Mirmehdi design five statistical texture measurements for text areas and build an ANN classifier to classify each pixel as text or nontext based on the five statistics [12]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6790561,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "67aba5cf20efe4bf14d85d13363fae16ebb167c5",
            "isKey": false,
            "numCitedBy": 58,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method based on statistical properties of local image neighbourhoods for the location of text in real-scene images. This has applications in robot vision, and desktop and wearable computing. The statistical measures we describe extract properties of the image which characterise text, invariant to a large degree to the orientation, scale or colour of the text in the scene. The measures are employed by a neural network to classify regions of an image as text or non-text. We thus avoid the use of different thresholds for the various situations we expect, including when text is too small to read, or when the text plane is not fronto-parallel to the camera. We briefly discuss applications and the possibility of recovery of the text for optical character recognition."
            },
            "slug": "Finding-Text-Regions-Using-Localised-Measures-Clark-Mirmehdi",
            "title": {
                "fragments": [],
                "text": "Finding Text Regions Using Localised Measures"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "A method based on statistical properties of local image neighbourhoods for the location of text in real-scene images, which has applications in robot vision, and desktop and wearable computing, and the possibility of recovery of the text for optical character recognition."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115196978"
                        ],
                        "name": "Huiping Li",
                        "slug": "Huiping-Li",
                        "structuredName": {
                            "firstName": "Huiping",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huiping Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3272081"
                        ],
                        "name": "O. Kia",
                        "slug": "O.-Kia",
                        "structuredName": {
                            "firstName": "Omid",
                            "lastName": "Kia",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Kia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48471936"
                        ],
                        "name": "D. Doermann",
                        "slug": "D.-Doermann",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Doermann",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Doermann"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 206406471,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9612a282060e74a6fe64e34cfc0643f13f6672e1",
            "isKey": false,
            "numCitedBy": 42,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "One difficulty with using text from digital video for indexing and retrieval is that video images are often in low resolution and poor quality, and as a result, the text can not be recognized adequately by most commercial OCR software. Text image enhancement is necessary to achieve reasonable OCR accuracy. Our enhancement consists of two main procedures, resolution enhancement based on Shannon interpolation and text separation from complex image background. Experiments show our enhancement approach improves OCR accuracy considerably."
            },
            "slug": "Text-enhancement-in-digital-video-Li-Kia",
            "title": {
                "fragments": [],
                "text": "Text enhancement in digital video"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Experiments show the enhancement approach improves OCR accuracy considerably, and consists of two main procedures, resolution enhancement based on Shannon interpolation and text separation from complex image background."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784196"
                        ],
                        "name": "Datong Chen",
                        "slug": "Datong-Chen",
                        "structuredName": {
                            "firstName": "Datong",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Datong Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144040003"
                        ],
                        "name": "K. Shearer",
                        "slug": "K.-Shearer",
                        "structuredName": {
                            "firstName": "Kim",
                            "lastName": "Shearer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Shearer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733733"
                        ],
                        "name": "H. Bourlard",
                        "slug": "H.-Bourlard",
                        "structuredName": {
                            "firstName": "Herv\u00e9",
                            "lastName": "Bourlard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bourlard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[10] present a set of Gabor-based filters to measure the text stroke properties and then selectively enhance only those edges most likely to represent text at a specific scale."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17316443,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e581b09dd70f70c95571bab6ec02fb58b4edf3da",
            "isKey": false,
            "numCitedBy": 73,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Stripes are common sub-structures of text characters, and the scale of these stripes varies little within a word. This scale consistency thus provides us with a useful feature for text detection and segmentation. A new form of filter is derived from the Gabor filter, and it is shown that this filter can efficiently estimate the scales of these stripes. The contrast of text in video can then be increased by enhancing the edges of only those stripes found to correspond to a suitable scale. More specifically the algorithm presented here enhances the stripes in three pre-selected scale ranges. The resulting enhancement yields much better performance from the binarization process, which is the step required before character recognition."
            },
            "slug": "Text-enhancement-with-asymmetric-filter-for-video-Chen-Shearer",
            "title": {
                "fragments": [],
                "text": "Text enhancement with asymmetric filter for video OCR"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A new form of filter is derived from the Gabor filter, and it is shown that this filter can efficiently estimate the scales of these stripes and enhance the edges of only those stripes found to correspond to a suitable scale."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 11th International Conference on Image Analysis and Processing"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143863244"
                        ],
                        "name": "Xiansheng Hua",
                        "slug": "Xiansheng-Hua",
                        "structuredName": {
                            "firstName": "Xiansheng",
                            "lastName": "Hua",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiansheng Hua"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46772671"
                        ],
                        "name": "Xiangrong Chen",
                        "slug": "Xiangrong-Chen",
                        "structuredName": {
                            "firstName": "Xiangrong",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiangrong Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120640002"
                        ],
                        "name": "Wenyin Liu",
                        "slug": "Wenyin-Liu",
                        "structuredName": {
                            "firstName": "Wenyin",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wenyin Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108698841"
                        ],
                        "name": "HongJiang Zhang",
                        "slug": "HongJiang-Zhang",
                        "structuredName": {
                            "firstName": "HongJiang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "HongJiang Zhang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 13999848,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "24591ec88e706697bffa18f36728f192e0d797b6",
            "isKey": false,
            "numCitedBy": 111,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "A new automatic text location approach for videos is proposed. First of all, the corner points of the selected video frames are detected. After deleting some isolate corners, we merge the remaining corners to form candidate text regions. The regions are then decomposed vertically and horizontally using edge maps of the video frames to get candidate text lines. Finally, a text box verification step based on the feature derived from edge maps is taken to significantly reduce false alarms. Experimental results show that the new text location scheme proposed in this paper is accurate."
            },
            "slug": "Automatic-location-of-text-in-video-frames-Hua-Chen",
            "title": {
                "fragments": [],
                "text": "Automatic location of text in video frames"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Experimental results show that the new text location scheme proposed in this paper is accurate and can be used to significantly reduce false alarms."
            },
            "venue": {
                "fragments": [],
                "text": "MULTIMEDIA '01"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2148905379"
                        ],
                        "name": "Zheng Zhang",
                        "slug": "Zheng-Zhang",
                        "structuredName": {
                            "firstName": "Zheng",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zheng Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679749"
                        ],
                        "name": "C. Tan",
                        "slug": "C.-Tan",
                        "structuredName": {
                            "firstName": "Chew",
                            "lastName": "Tan",
                            "middleNames": [
                                "Lim"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Tan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2417250"
                        ],
                        "name": "Liying Fan",
                        "slug": "Liying-Fan",
                        "structuredName": {
                            "firstName": "Liying",
                            "lastName": "Fan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liying Fan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 1
                            }
                        ],
                        "text": "([113,114]) propose a method to estimate the cylinder page shape near the spine of an opened book during scanning."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9237243,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5a530b555e416119a1d180a71d8a9250b3f76931",
            "isKey": false,
            "numCitedBy": 41,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we address the problem of discovering the 3D shape of a book surface from the shading information in a scanned document image. This shape-from-shading problem is characterized in real world environments by a proximal and a moving light source, Lambertian reflection and a non-uniform albedo distribution. By considering all these factors, we first build the practical model (consists of geometric model and optical model) to reconstruct the 3D shape of book surface. We next restore the scanned image using this shape based on two models, namely de-shading and dewarping models. Finally, we compare the OCR results on the original and restored document image. The experiments show that the geometric and photometric distortions are mostly removed and the OCR results are improved markedly."
            },
            "slug": "Restoration-of-curved-document-images-through-3D-Zhang-Tan",
            "title": {
                "fragments": [],
                "text": "Restoration of curved document images through 3D shape modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "This paper builds the practical model (consists of geometric model and optical model) to reconstruct the 3D shape of book surface and restores the scanned image using this shape based on two models, namely de-shading and dewarping models."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49860655"
                        ],
                        "name": "L. Brown",
                        "slug": "L.-Brown",
                        "structuredName": {
                            "firstName": "Lisa",
                            "lastName": "Brown",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Brown"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 61
                            }
                        ],
                        "text": "Brown discusses the general problem of image registration in [4]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14576088,
            "fieldsOfStudy": [
                "Environmental Science"
            ],
            "id": "a1068be6f66ae6a56b7dffbdfabc73b253de0ab3",
            "isKey": false,
            "numCitedBy": 4576,
            "numCiting": 137,
            "paperAbstract": {
                "fragments": [],
                "text": "Registration is a fundamental task in image processing used to match two or more pictures taken, for example, at different times, from different sensors, or from different viewpoints. Virtually all large systems which evaluate images require the registration of images, or a closely related operation, as an intermediate step. Specific examples of systems where image registration is a significant component include matching a target with a real-time image of a scene for target recognition, monitoring global land usage using satellite images, matching stereo images to recover shape for autonomous navigation, and aligning images from different medical modalities for diagnosis.\nOver the years, a broad range of techniques has been developed for various types of data and problems. These techniques have been independently studied for several different applications, resulting in a large body of research. This paper organizes this material by establishing the relationship between the variations in the images and the type of registration techniques which can most appropriately be applied. Three major types of variations are distinguished. The first type are the variations due to the differences in acquisition which cause the images to be misaligned. To register images, a spatial transformation is found which will remove these variations. The class of transformations which must be searched to find the optimal transformation is determined by knowledge about the variations of this type. The transformation class in turn influences the general technique that should be taken. The second type of variations are those which are also due to differences in acquisition, but cannot be modeled easily such as lighting and atmospheric conditions. This type usually effects intensity values, but they may also be spatial, such as perspective distortions. The third type of variations are differences in the images that are of interest such as object movements, growths, or other scene changes. Variations of the second and third type are not directly removed by registration, but they make registration more difficult since an exact match is no longer possible. In particular, it is critical that variations of the third type are not removed. Knowledge about the characteristics of each type of variation effect the choice of feature space, similarity measure, search space, and search strategy which will make up the final technique. All registration techniques can be viewed as different combinations of these choices. This framework is useful for understanding the merits and relationships between the wide variety of existing techniques and for assisting in the selection of the most suitable technique for a specific problem."
            },
            "slug": "A-survey-of-image-registration-techniques-Brown",
            "title": {
                "fragments": [],
                "text": "A survey of image registration techniques"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper organizes this material by establishing the relationship between the variations in the images and the type of registration techniques which can most appropriately be applied, and establishing a framework for understanding the merits and relationships between the wide variety of existing techniques."
            },
            "venue": {
                "fragments": [],
                "text": "CSUR"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "121267347"
                        ],
                        "name": "K. Jung",
                        "slug": "K.-Jung",
                        "structuredName": {
                            "firstName": "Keechul",
                            "lastName": "Jung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Jung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144602022"
                        ],
                        "name": "K. Kim",
                        "slug": "K.-Kim",
                        "structuredName": {
                            "firstName": "Kwang",
                            "lastName": "Kim",
                            "middleNames": [
                                "In"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2146355817"
                        ],
                        "name": "Junghyun Han",
                        "slug": "Junghyun-Han",
                        "structuredName": {
                            "firstName": "Junghyun",
                            "lastName": "Han",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Junghyun Han"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15194582,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "03b6bfddb3109cdf7d383f316fc639e1435af6c5",
            "isKey": false,
            "numCitedBy": 35,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a text scanner which detects wide text strings in a sequence of scene images. For scene text detection, we use a multiple-CAMShift algorithm on a text probability image produced by a multi-layer perceptron. To provide enhanced resolution of the extracted text images, we perform the text detection process after generating a mosaic image in a fast and robust image registration method."
            },
            "slug": "Text-scanner-with-text-detection-technology-on-Jung-Kim",
            "title": {
                "fragments": [],
                "text": "Text scanner with text detection technology on image sequences"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "A text scanner which detects wide text strings in a sequence of scene images by using a multiple-CAMShift algorithm on a text probability image produced by a multi-layer perceptron."
            },
            "venue": {
                "fragments": [],
                "text": "Object recognition supported by user interaction for service robots"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144739319"
                        ],
                        "name": "R. Lienhart",
                        "slug": "R.-Lienhart",
                        "structuredName": {
                            "firstName": "Rainer",
                            "lastName": "Lienhart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lienhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1750165"
                        ],
                        "name": "W. Effelsberg",
                        "slug": "W.-Effelsberg",
                        "structuredName": {
                            "firstName": "Wolfgang",
                            "lastName": "Effelsberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Effelsberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 108
                            }
                        ],
                        "text": "A simple solution is to treat every frame as a potential text frame and apply text region detection to them [48,57,58]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 16
                            }
                        ],
                        "text": "In another work [57], Lienhart and Effelsberg notice that the split and merge method is sensitive to noise in extremely noisy images such as video frames since those segmentation algorithms do not explore unique text properties and therefore cannot distinguish structured text objects from random objects."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1306072,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "53ecdc232d2e640f890c831944761fe5604af033",
            "isKey": false,
            "numCitedBy": 202,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract. Efficient indexing and retrieval of digital video is an important function of video databases. One powerful index for retrieval is the text appearing in them. It enables content-based browsing. We present our new methods for automatic segmentation of text in digital videos. The algorithms we propose make use of typical characteristics of text in videos in order to enable and enhance segmentation performance. The unique features of our approach are the tracking of characters and words over their complete duration of occurrence in a video and the integration of the multiple bitmaps of a character over time into a single bitmap. The output of the text segmentation step is then directly passed to a standard OCR software package in order to translate the segmented text into ASCII. Also, a straightforward indexing and retrieval scheme is introduced. It is used in the experiments to demonstrate that the proposed text segmentation algorithms together with existing text recognition algorithms are suitable for indexing and retrieval of relevant video sequences in and from a video database. Our experimental results are very encouraging and suggest that these algorithms can be used in video retrieval applications as well as to recognize higher level semantics in videos."
            },
            "slug": "Automatic-text-segmentation-and-text-recognition-Lienhart-Effelsberg",
            "title": {
                "fragments": [],
                "text": "Automatic text segmentation and text recognition for video indexing"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The proposed text segmentation algorithms together with existing text recognition algorithms are suitable for indexing and retrieval of relevant video sequences in and from a video database."
            },
            "venue": {
                "fragments": [],
                "text": "Multimedia Systems"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48189908"
                        ],
                        "name": "Yasuhiko Watanabe",
                        "slug": "Yasuhiko-Watanabe",
                        "structuredName": {
                            "firstName": "Yasuhiko",
                            "lastName": "Watanabe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yasuhiko Watanabe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2090132835"
                        ],
                        "name": "Yoshihiro Okada",
                        "slug": "Yoshihiro-Okada",
                        "structuredName": {
                            "firstName": "Yoshihiro",
                            "lastName": "Okada",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshihiro Okada"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2146300157"
                        ],
                        "name": "Yeun-Bae Kim",
                        "slug": "Yeun-Bae-Kim",
                        "structuredName": {
                            "firstName": "Yeun-Bae",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yeun-Bae Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105963409"
                        ],
                        "name": "Tetsuya Takeda",
                        "slug": "Tetsuya-Takeda",
                        "structuredName": {
                            "firstName": "Tetsuya",
                            "lastName": "Takeda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tetsuya Takeda"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10167499,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "61ae735749334af00d99a78a71df2595913ea5b4",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose a camera system which translates Japanese texts in a scene. The system is portable and consists of four components: digital camera, character image extraction process, character recognition process, and translation process. The system extracts character strings from a region which a user specifies, and translates them into English."
            },
            "slug": "Translation-camera-Watanabe-Okada",
            "title": {
                "fragments": [],
                "text": "Translation camera"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "A camera system which translates Japanese texts in a scene using a digital camera, which extracts character strings from a region which a user specifies, and translates them into English."
            },
            "venue": {
                "fragments": [],
                "text": "MTSUMMIT"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144586498"
                        ],
                        "name": "R. Plamondon",
                        "slug": "R.-Plamondon",
                        "structuredName": {
                            "firstName": "R\u00e9jean",
                            "lastName": "Plamondon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Plamondon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696384"
                        ],
                        "name": "S. Srihari",
                        "slug": "S.-Srihari",
                        "structuredName": {
                            "firstName": "Sargur",
                            "lastName": "Srihari",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Srihari"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "domains, including office automation, forensics, and digital libraries. Some surveys include [19,72, 81 ,93]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15782139,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d12864a8acbab1830be755bfb9cb177e31ca5e20",
            "isKey": false,
            "numCitedBy": 2743,
            "numCiting": 719,
            "paperAbstract": {
                "fragments": [],
                "text": "Handwriting has continued to persist as a means of communication and recording information in day-to-day life even with the introduction of new technologies. Given its ubiquity in human transactions, machine recognition of handwriting has practical significance, as in reading handwritten notes in a PDA, in postal addresses on envelopes, in amounts in bank checks, in handwritten fields in forms, etc. This overview describes the nature of handwritten language, how it is transduced into electronic data, and the basic concepts behind written language recognition algorithms. Both the online case (which pertains to the availability of trajectory data during writing) and the off-line case (which pertains to scanned images) are considered. Algorithms for preprocessing, character and word recognition, and performance with practical systems are indicated. Other fields of application, like signature verification, writer authentification, handwriting learning tools are also considered."
            },
            "slug": "On-Line-and-Off-Line-Handwriting-Recognition:-A-Plamondon-Srihari",
            "title": {
                "fragments": [],
                "text": "On-Line and Off-Line Handwriting Recognition: A Comprehensive Survey"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The nature of handwritten language, how it is transduced into electronic data, and the basic concepts behind written language recognition algorithms are described."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143955418"
                        ],
                        "name": "M. S. Brown",
                        "slug": "M.-S.-Brown",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Brown",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. S. Brown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9324035"
                        ],
                        "name": "W. B. Seales",
                        "slug": "W.-B.-Seales",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Seales",
                            "middleNames": [
                                "Brent"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. B. Seales"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 8
                            }
                        ],
                        "text": "In both [5] and [77], the 3D shape of the page is obtained using a structured light method."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 17
                            }
                        ],
                        "text": "Brown and Seales [5] propose to model a page by an elastic mesh and flatten the page by pushing the mesh down to a plane."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18903638,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c6f989451bac14f973b131059fc51ed8d0aa6fa3",
            "isKey": false,
            "numCitedBy": 115,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a framework for restoring arbitrarily warped and deformed documents to their original planar shape. The impetus for this work is the need for tools and techniques to help digitally preserve and restore fragile manuscripts. Current digitization is performed under the assumption that the documents are flat, with subsequent image-processing and restoration algorithms either relying on this assumption or attempting to overcome it without shape information. Although most manuscripts were originally flat, many become deformed from damage and deterioration. Physical flattening is not possible without risking further, possibly irreversible, damage. Our framework addresses this restoration problem with two primary contributions. First, we present a working 3D digitization setup that acquires a 3D model with accurate shape-to-texture registration under multiple lighting conditions. Second, we show how the 3D model and a mass-spring particle system can be used together as a framework for digital flattening. We show that this restoration process can correct document deformations and can significantly improve subsequent document analysis."
            },
            "slug": "Document-restoration-using-3D-shape:-a-general-for-Brown-Seales",
            "title": {
                "fragments": [],
                "text": "Document restoration using 3D shape: a general deskewing algorithm for arbitrarily warped documents"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "A framework for restoring arbitrarily warped and deformed documents to their original planar shape is presented and it is shown how the 3D model and a mass-spring particle system can be used together as a framework for digital flattening."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2148905379"
                        ],
                        "name": "Zheng Zhang",
                        "slug": "Zheng-Zhang",
                        "structuredName": {
                            "firstName": "Zheng",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zheng Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679749"
                        ],
                        "name": "C. Tan",
                        "slug": "C.-Tan",
                        "structuredName": {
                            "firstName": "Chew",
                            "lastName": "Tan",
                            "middleNames": [
                                "Lim"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Tan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2417250"
                        ],
                        "name": "Liying Fan",
                        "slug": "Liying-Fan",
                        "structuredName": {
                            "firstName": "Liying",
                            "lastName": "Fan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liying Fan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 1
                            }
                        ],
                        "text": "([113,114]) propose a method to estimate the cylinder page shape near the spine of an opened book during scanning."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 331123,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ce2b8bf14ccc9a242f5196e3a1f6b7b5a6db29e7",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "While scanning pages from a thick, bound book, there are two sources of distortion in the document images: 1) shade along the book 'spine', and 2) warping of the book surface in the shade area. We propose a fast method to estimate the 3D shape of a book surface. Based on this shape, we remove the shade and correct the warping to restore the document images. The experiments show that the photometric and geometric distortions are mostly removed. The OCR tests on the original and restored document images are also presented."
            },
            "slug": "Estimation-of-3D-shape-of-warped-document-surface-Zhang-Tan",
            "title": {
                "fragments": [],
                "text": "Estimation of 3D shape of warped document surface for image restoration"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A fast method to estimate the 3D shape of a book surface is proposed and is shown to remove the shade and correct the warping to restore the document images."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118021384"
                        ],
                        "name": "Sunghoon Kim",
                        "slug": "Sunghoon-Kim",
                        "structuredName": {
                            "firstName": "Sunghoon",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sunghoon Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111889373"
                        ],
                        "name": "D. Kim",
                        "slug": "D.-Kim",
                        "structuredName": {
                            "firstName": "Dae",
                            "lastName": "Kim",
                            "middleNames": [
                                "Sung"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3354836"
                        ],
                        "name": "Younbok Ryu",
                        "slug": "Younbok-Ryu",
                        "structuredName": {
                            "firstName": "Younbok",
                            "lastName": "Ryu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Younbok Ryu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2157691706"
                        ],
                        "name": "Gyeonghwan Kim",
                        "slug": "Gyeonghwan-Kim",
                        "structuredName": {
                            "firstName": "Gyeonghwan",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gyeonghwan Kim"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Kim et al. present another method of finding edge pixels for potential text regions [ 45 ]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14088001,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0a049d297a842c150503b4f432dd9db111c519a6",
            "isKey": false,
            "numCitedBy": 162,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "A robust approach for extracting car license plate from images with complex background and relatively poor quality is presented. The approach focuses on dealing with images taken under weak lighting condition. The proposed method is divided into two steps: 1) searching candidate areas from the input image using gradient information, and 2) determining the plate area among the candidates and adjusting the boundary of the area by introducing a plate template. A set of experiments has been performed to prove the robustness and accuracy of the approach. For many images collected from a large underground parking place the result shows that 90% of them are correctly segmented."
            },
            "slug": "A-robust-license-plate-extraction-method-under-Kim-Kim",
            "title": {
                "fragments": [],
                "text": "A robust license-plate extraction method under complex image conditions"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "A robust approach for extracting car license plate from images with complex background and relatively poor quality is presented and shows that 90% of images collected from a large underground parking place are correctly segmented."
            },
            "venue": {
                "fragments": [],
                "text": "Object recognition supported by user interaction for service robots"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47260288"
                        ],
                        "name": "L. Fletcher",
                        "slug": "L.-Fletcher",
                        "structuredName": {
                            "firstName": "Lloyd",
                            "lastName": "Fletcher",
                            "middleNames": [
                                "Alan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Fletcher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3110392"
                        ],
                        "name": "R. Kasturi",
                        "slug": "R.-Kasturi",
                        "structuredName": {
                            "firstName": "Rangachar",
                            "lastName": "Kasturi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kasturi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 2685456,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b08e547ba4edb60902d1708a5593d71f075aa7f1",
            "isKey": false,
            "numCitedBy": 657,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "The development and implementation of an algorithm for automated text string separation that is relatively independent of changes in text font style and size and of string orientation are described. It is intended for use in an automated system for document analysis. The principal parts of the algorithm are the generation of connected components and the application of the Hough transform in order to group components into logical character strings that can then be separated from the graphics. The algorithm outputs two images, one containing text strings and the other graphics. These images can then be processed by suitable character recognition and graphics recognition systems. The performance of the algorithm, both in terms of its effectiveness and computational efficiency, was evaluated using several test images and showed superior performance compared to other techniques. >"
            },
            "slug": "A-Robust-Algorithm-for-Text-String-Separation-from-Fletcher-Kasturi",
            "title": {
                "fragments": [],
                "text": "A Robust Algorithm for Text String Separation from Mixed Text/Graphics Images"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "The development and implementation of an algorithm for automated text string separation that is relatively independent of changes in text font style and size and of string orientation are described and showed superior performance compared to other techniques."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144739319"
                        ],
                        "name": "R. Lienhart",
                        "slug": "R.-Lienhart",
                        "structuredName": {
                            "firstName": "Rainer",
                            "lastName": "Lienhart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lienhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2083891040"
                        ],
                        "name": "F. Stuber",
                        "slug": "F.-Stuber",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Stuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Stuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "In [56] Lienhart and Stuber report how a split and merge method can be used to perform the segmentation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 20
                            }
                        ],
                        "text": "Lienhart and Stuber [56] describe their scheme to handle the moving text, which is based on recognition results: for each character that appears in multiple frames, they gather all corresponding recognition results and select the most frequent one."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14147742,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "778a307aa0cf8b2ed273b9089cb9aa8210f49f24",
            "isKey": false,
            "numCitedBy": 205,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We have developed algorithms for automatic character segmentation in motion pictures which extract automatically and reliably the text in pre-title sequences, credit titles, and closing sequences with title and credits. The algorithms we propose make use of typical characteristics of text in videos in order to enhance segmentation and, consequently, recognition performance. As a result, we get segmented characters from video pictures. These can be parsed by any OCR software. The recognition results of multiple instances of the same character throughout subsequent frames are combined to enhance recognition result and to compute the final output. We have tested our segmentation algorithms in a series of experiments with video clips recorded from television and achieved good segmentation results."
            },
            "slug": "Automatic-text-recognition-in-digital-videos-Lienhart-Stuber",
            "title": {
                "fragments": [],
                "text": "Automatic text recognition in digital videos"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "Algorithms for automatic character segmentation in motion pictures which extract automatically and reliably the text in pre-title sequences, credit titles, and closing sequences with title and credits are developed."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145140641"
                        ],
                        "name": "R. Zunino",
                        "slug": "R.-Zunino",
                        "structuredName": {
                            "firstName": "Rodolfo",
                            "lastName": "Zunino",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Zunino"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710901"
                        ],
                        "name": "S. Rovetta",
                        "slug": "S.-Rovetta",
                        "structuredName": {
                            "firstName": "Stefano",
                            "lastName": "Rovetta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Rovetta"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1692308,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2c73c6ff229d0afc63fe5a7a18059b6ccdc6f7f5",
            "isKey": false,
            "numCitedBy": 123,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "License-plate location in sensor images plays an important role in vehicle identification for automated transport systems (ATS). This paper presents a novel method based on vector quantization (VQ) to process vehicle images. The proposed method makes it possible to perform superior picture compression for archival purposes and to support effective location at the same time. As compared with classical approaches, VQ encoding can give some hints about the contents of image regions; such additional information can be exploited to boost location performance. The VQ system can be trained by way of examples; this gives the advantages of adaptiveness and on-field tuning. The approach has been tested in a real industrial application and included satisfactorily in a complete ATS for vehicle identification."
            },
            "slug": "Vector-quantization-for-license-plate-location-and-Zunino-Rovetta",
            "title": {
                "fragments": [],
                "text": "Vector quantization for license-plate location and image coding"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A novel method based on vector quantization (VQ) to process vehicle images that makes it possible to perform superior picture compression for archival purposes and to support effective location at the same time."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Ind. Electron."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "72754267"
                        ],
                        "name": "P. Comelli",
                        "slug": "P.-Comelli",
                        "structuredName": {
                            "firstName": "Paolo",
                            "lastName": "Comelli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Comelli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681278"
                        ],
                        "name": "P. Ferragina",
                        "slug": "P.-Ferragina",
                        "structuredName": {
                            "firstName": "Paolo",
                            "lastName": "Ferragina",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Ferragina"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39838785"
                        ],
                        "name": "M. N. Granieri",
                        "slug": "M.-N.-Granieri",
                        "structuredName": {
                            "firstName": "Mario",
                            "lastName": "Granieri",
                            "middleNames": [
                                "Notturno"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. N. Granieri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70312893"
                        ],
                        "name": "F. Stabile",
                        "slug": "F.-Stabile",
                        "structuredName": {
                            "firstName": "F.",
                            "lastName": "Stabile",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Stabile"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 13
                            }
                        ],
                        "text": "Many systems [8,16,117] have been developed for ALPR and many commercial products put to practical use [119\u2013121] in parking lot billing, toll collecting monitoring, road law enforcement, and security management."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 6
                            }
                        ],
                        "text": "Dance [16] has similar work in estimating vanishing points."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 61675047,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bf12f0fa24e10581e4f311e59cd030c5daa795ef",
            "isKey": false,
            "numCitedBy": 268,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "A system for the recognition of car license plates is presented. The aim of the system is to read automatically the Italian license number of a car passing through a tollgate. A CCTV camera and a frame grabber card are used to acquire a rear-view image of the vehicle. The recognition process consists of three main phases. First, a segmentation phase locates the license plate within the image. Then, a procedure based upon feature projection estimates some image parameters needed to normalize the license plate characters. Finally, the character recognizer extracts some feature points and uses template matching operators to get a robust solution under multiple acquisition conditions. A test has been done on more than three thousand real images acquired under different weather and illumination conditions, thus obtaining a recognition rate close to 91%. >"
            },
            "slug": "Optical-recognition-of-motor-vehicle-license-plates-Comelli-Ferragina",
            "title": {
                "fragments": [],
                "text": "Optical recognition of motor vehicle license plates"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "A system to read automatically the Italian license number of a car passing through a tollgate using a CCTV camera and a frame grabber card to acquire a rear-view image of the vehicle is presented."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39319377"
                        ],
                        "name": "Yu Zhong",
                        "slug": "Yu-Zhong",
                        "structuredName": {
                            "firstName": "Yu",
                            "lastName": "Zhong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yu Zhong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108698841"
                        ],
                        "name": "HongJiang Zhang",
                        "slug": "HongJiang-Zhang",
                        "structuredName": {
                            "firstName": "HongJiang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "HongJiang Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 168
                            }
                        ],
                        "text": "assume that text areas contain certain high horizontal and vertical frequencies and detect these frequencies directly from the DCT coefficients of the compressed image [115,116]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6781817,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f7721138e41d82fedabca59c9a66e67d9b7053f3",
            "isKey": false,
            "numCitedBy": 330,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method to automatically locate captions in MPEG video. Caption text regions are segmented from the background using their distinguishing texture characteristics. This method first locates candidate text regions directly in the DCT compressed domain, and then reconstructs the candidate regions for further refinement in the spatial domain. Therefore, only a small amount of decoding is required. The proposed algorithm achieves about 4.0% false reject rate and less than 5.7% false positive rate on a variety of MPEG compressed video containing more than 42,000 frames."
            },
            "slug": "Automatic-caption-localization-in-compressed-video-Zhong-Zhang",
            "title": {
                "fragments": [],
                "text": "Automatic caption localization in compressed video"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This method first locates candidate text regions directly in the DCT compressed domain, and then reconstructs the candidate regions for further refinement in the spatial domain, so that only a small amount of decoding is required."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348)"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680223"
                        ],
                        "name": "A. Smeaton",
                        "slug": "A.-Smeaton",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Smeaton",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Smeaton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143645506"
                        ],
                        "name": "P. Over",
                        "slug": "P.-Over",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Over",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Over"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "71367253"
                        ],
                        "name": "R. Taban",
                        "slug": "R.-Taban",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Taban",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Taban"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "f Sign translation [95, 105, 106]. g Whiteboard reading [ 88 , 97]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9476340,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8d62986fab37c99fa3b2a5f556a78df62cca259c",
            "isKey": false,
            "numCitedBy": 110,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "TREC-2002 saw the second running of the Video Track, the goal of which was to promote progress in content-based retrieval from digital video via open, metrics-based evaluation. The track used 73.3 hours of publicly available digital video (in MPEG-1/VCD format) downloaded by the participants directly from the Internet Archive (Prelinger Archives) (internetarchive, 2002) and some from the Open \nVideo Project (Marchionini, 2001). The material comprised advertising, educational, industrial, and amateur films produced between the 1930's and the 1970's by corporations, nonprofit organizations, trade associations, community and interest groups, educational institutions, and individuals. 17 teams representing 5 companies and 12 universities - 4 from Asia, 9 from Europe, and 4 from the US - participated in one or more of three tasks in the 2001 video track: shot boundary determination, feature extraction, and search (manual or interactive). Results were scored by NIST using manually created truth data for shot boundary determination and manual assessment of feature extraction and search results. This paper is an introduction to, and an overview \nof, the track framework - the tasks, data, and measures - the approaches taken by the participating groups, the results, and issues regrading the evaluation. For detailed information about the approaches and results, the reader should see the various site reports in the final workshop proceedings."
            },
            "slug": "The-TREC-2002-Video-Track-Report-Smeaton-Over",
            "title": {
                "fragments": [],
                "text": "The TREC-2002 Video Track Report"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper is an introduction to, and an overview of the track framework - the tasks, data, and measures - the approaches taken by the participating groups, the results, and issues regrading the evaluation."
            },
            "venue": {
                "fragments": [],
                "text": "TREC"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3267020"
                        ],
                        "name": "S. Kurakake",
                        "slug": "S.-Kurakake",
                        "structuredName": {
                            "firstName": "Shoji",
                            "lastName": "Kurakake",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kurakake"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33852772"
                        ],
                        "name": "H. Kuwano",
                        "slug": "H.-Kuwano",
                        "structuredName": {
                            "firstName": "Hidetaka",
                            "lastName": "Kuwano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Kuwano"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3080357"
                        ],
                        "name": "K. Odaka",
                        "slug": "K.-Odaka",
                        "structuredName": {
                            "firstName": "Kazumi",
                            "lastName": "Odaka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Odaka"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[47] propose a detection method based on the difference of intensity histograms of successive frames (maybe several frames ahead or behind)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[47] present an approach to couple character segmentation and OCR to improve both performances."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "In [47], after text lines are segmented, one character at a time is segmented from left to right, then verified by recognition."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 46142323,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bdf40964371c7543aaf5bdfac5326f69d5a76451",
            "isKey": true,
            "numCitedBy": 26,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "An indexing method for content-based image retrieval by using textual information in video is proposed. Indices extracted from textual information make it possible to retrieve video data by a conceptual query, such as a topic or a person's name, and organize flat video data into structured video data based on its conceptual content. To this end, we developed a text extraction and recognition algorithm and a visual feature matching algorithm for indexing and organizing video data at a conceptual level. The text extraction and recognition algorithm identifies frames in the video which contain text, extracts the text regions from the frame, finds text lines, and recognizes characters in the text line. The visual feature matching algorithm measures the similarity of frames containing text of find frames with similar appearances text, which can be considered topic change frames. Experiments using real video data showed that our algorithm can index textual information reliably and that it has good potential as a tool for making content-based conceptual-level queries to video databases."
            },
            "slug": "Recognition-and-visual-feature-matching-of-text-in-Kurakake-Kuwano",
            "title": {
                "fragments": [],
                "text": "Recognition and visual feature matching of text region in video for conceptual indexing"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "An indexing method for content-based image retrieval by using textual information in video by developing a text extraction and recognition algorithm and a visual feature matching algorithm for indexing and organizing video data at a conceptual level."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403827167"
                        ],
                        "name": "Q. Stafford-Fraser",
                        "slug": "Q.-Stafford-Fraser",
                        "structuredName": {
                            "firstName": "Quentin",
                            "lastName": "Stafford-Fraser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Q. Stafford-Fraser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2149814967"
                        ],
                        "name": "P. Robinson",
                        "slug": "P.-Robinson",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Robinson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Robinson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 150
                            }
                        ],
                        "text": "For example, a camera mounted over a writing surface can be used for handwriting recognition [26,70,96], or cameras can be used in whiteboard reading [89,97]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 5093146,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "32a7da948e53502bf6d63d7da8f1707d4990e985",
            "isKey": false,
            "numCitedBy": 91,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "The goal of \u2018Computer Augmented Environments\u2019 is to bring computational power to everyday objects with which users are already familiar, so that the user interface to this computational power becomes almost invisible. Video is a very important tool in creating Augmented Environments and recent camera-manufacturing techniques make it an economically viable proposition in the general marketplace. BrightBoard is an example system which uses a video camera and audio feedback to enhance the facilities of an ordinary whiteboard, allowing a user to control a computer through simple marks made on the board. We describe its operation in some detail, and discuss how it tackles some of the problems common to these \u2018Video-Augmented Environments\u2019."
            },
            "slug": "BrightBoard:-a-video-augmented-environment-Stafford-Fraser-Robinson",
            "title": {
                "fragments": [],
                "text": "BrightBoard: a video-augmented environment"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "BrightBoard is an example system which uses a video camera and audio feedback to enhance the facilities of an ordinary whiteboard, allowing a user to control a computer through simple marks made on the board."
            },
            "venue": {
                "fragments": [],
                "text": "CHI"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143863244"
                        ],
                        "name": "Xiansheng Hua",
                        "slug": "Xiansheng-Hua",
                        "structuredName": {
                            "firstName": "Xiansheng",
                            "lastName": "Hua",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiansheng Hua"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120640002"
                        ],
                        "name": "Wenyin Liu",
                        "slug": "Wenyin-Liu",
                        "structuredName": {
                            "firstName": "Wenyin",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wenyin Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108698841"
                        ],
                        "name": "HongJiang Zhang",
                        "slug": "HongJiang-Zhang",
                        "structuredName": {
                            "firstName": "HongJiang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "HongJiang Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Hua et al. [ 36 ] propose to weigh detected bounding boxes against ground truth boxes in terms of predefined detection difficulty, detection importance, and recognition importance."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1989026,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "844ceedec94d46b15198de140646e0f2ae953eb0",
            "isKey": false,
            "numCitedBy": 47,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose an objective, comprehensive and difficulty-independent performance evaluation protocol for video text detection algorithms. The protocol includes a positive set and a negative set of indices at textbox level, which evaluate the detection quality in terms of both location accuracy and fragmentation of the detected textboxes. In the protocol, we assign a detection difficulty (DD) level to each ground truth textbox. The performance indices can then be normalized with respect to the textbox DD level and are therefore independent of the ground truth difficulty. We also assign a detection importance (DI) level to each ground truth textbox. The overall detection rate is the DI-weighted average of the detection qualities of all ground truth textboxes, which makes the detection rate more accurate to reveal the real performance. The automatic performance evaluation scheme has been applied on a text detection approach to determine the best parameters that can yield the best detection results."
            },
            "slug": "Automatic-performance-evaluation-for-video-text-Hua-Liu",
            "title": {
                "fragments": [],
                "text": "Automatic performance evaluation for video text detection"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "An objective, comprehensive and difficulty-independent performance evaluation protocol for video text detection algorithms that includes a positive set and a negative set of indices at textbox level, which evaluate the detection quality in terms of both location accuracy and fragmentation of the detected textboxes."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of Sixth International Conference on Document Analysis and Recognition"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753908"
                        ],
                        "name": "Michael Elad",
                        "slug": "Michael-Elad",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Elad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Elad"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2153902"
                        ],
                        "name": "A. Feuer",
                        "slug": "A.-Feuer",
                        "structuredName": {
                            "firstName": "Arie",
                            "lastName": "Feuer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Feuer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 15
                            }
                        ],
                        "text": "Elad and Feuer [23] summarize several super-resolution approaches including IBP, ML, MAP, and POCS."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1724361,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "e12040588665f76bcca11690afa5537ecb4cc446",
            "isKey": false,
            "numCitedBy": 1089,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "The three main tools in the single image restoration theory are the maximum likelihood (ML) estimator, the maximum a posteriori probability (MAP) estimator, and the set theoretic approach using projection onto convex sets (POCS). This paper utilizes the above known tools to propose a unified methodology toward the more complicated problem of superresolution restoration. In the superresolution restoration problem, an improved resolution image is restored from several geometrically warped, blurred, noisy and downsampled measured images. The superresolution restoration problem is modeled and analyzed from the ML, the MAP, and POCS points of view, yielding a generalization of the known superresolution restoration methods. The proposed restoration approach is general but assumes explicit knowledge of the linear space- and time-variant blur, the (additive Gaussian) noise, the different measured resolutions, and the (smooth) motion characteristics. A hybrid method combining the simplicity of the ML and the incorporation of nonellipsoid constraints is presented, giving improved restoration performance, compared with the ML and the POCS approaches. The hybrid method is shown to converge to the unique optimal solution of a new definition of the optimization problem. Superresolution restoration from motionless measurements is also discussed. Simulations demonstrate the power of the proposed methodology."
            },
            "slug": "Restoration-of-a-single-superresolution-image-from-Elad-Feuer",
            "title": {
                "fragments": [],
                "text": "Restoration of a single superresolution image from several blurred, noisy, and undersampled measured images"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A hybrid method combining the simplicity of theML and the incorporation of nonellipsoid constraints is presented, giving improved restoration performance, compared with the ML and the POCS approaches."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Image Process."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145007805"
                        ],
                        "name": "Y. Hasan",
                        "slug": "Y.-Hasan",
                        "structuredName": {
                            "firstName": "Yassin",
                            "lastName": "Hasan",
                            "middleNames": [
                                "M.",
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Hasan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47209857"
                        ],
                        "name": "Lina Karam",
                        "slug": "Lina-Karam",
                        "structuredName": {
                            "firstName": "Lina",
                            "lastName": "Karam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lina Karam"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 5
                            }
                        ],
                        "text": "Both [33] and [34] find the strength of edges using morphological operators."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5778124,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "ab09f0ac0f2700c47b3a0a303f13edc76177e66b",
            "isKey": false,
            "numCitedBy": 174,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a morphological technique for text extraction from images. The proposed morphological technique is insensitive to noise, skew and text orientation. It is also free from artifacts that are usually introduced by both fixed/optimal global thresholding and fixed-size block-based local thresholding. Examples are presented to illustrate the performance of the proposed method."
            },
            "slug": "Morphological-text-extraction-from-images-Hasan-Karam",
            "title": {
                "fragments": [],
                "text": "Morphological text extraction from images"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "The proposed morphological technique is insensitive to noise, skew and text orientation, and is also free from artifacts that are usually introduced by both fixed/optimal global thresholding and fixed-size block-based local thresholding."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Image Process."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115196978"
                        ],
                        "name": "Huiping Li",
                        "slug": "Huiping-Li",
                        "structuredName": {
                            "firstName": "Huiping",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huiping Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48471936"
                        ],
                        "name": "D. Doermann",
                        "slug": "D.-Doermann",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Doermann",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Doermann"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 92
                            }
                        ],
                        "text": "perform wavelet decomposition on multiscale layers generated from the input grayscale image [54,55]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 20
                            }
                        ],
                        "text": "Similar to [53] and [54], a text detector is invoked every five frames to calibrate the tracking."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 38586525,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3224ce96a5d8174a546dcfba0f2a202bed3ded16",
            "isKey": false,
            "numCitedBy": 49,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present a video text detection system based on automated neural network training. Compared with previous work which detects only graphical text with fixed parameters, our system (1) provides a training mechanism so the parameters of the system can be adapted to changing environments, (2) can detect both graphical text and scene text located in complex backgrounds, (3) can detect text in any orientation and (4) can perform multilingual text detection. Experiments show the effectiveness of our system in various text detection tasks."
            },
            "slug": "A-video-text-detection-system-based-on-automated-Li-Doermann",
            "title": {
                "fragments": [],
                "text": "A video text detection system based on automated training"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "A video text detection system based on automated neural network training that can detect both graphical text and scene text located in complex backgrounds, can detect text in any orientation, and can perform multilingual text detection."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 15th International Conference on Pattern Recognition. ICPR-2000"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2695496"
                        ],
                        "name": "S. Chang",
                        "slug": "S.-Chang",
                        "structuredName": {
                            "firstName": "Shyang-Lih",
                            "lastName": "Chang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Chang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109049667"
                        ],
                        "name": "Li-Shien Chen",
                        "slug": "Li-Shien-Chen",
                        "structuredName": {
                            "firstName": "Li-Shien",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li-Shien Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2476349"
                        ],
                        "name": "Y. Chung",
                        "slug": "Y.-Chung",
                        "structuredName": {
                            "firstName": "Yun-Chung",
                            "lastName": "Chung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Chung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1800132"
                        ],
                        "name": "Sei-Wang Chen",
                        "slug": "Sei-Wang-Chen",
                        "structuredName": {
                            "firstName": "Sei-Wang",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sei-Wang Chen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 30
                            }
                        ],
                        "text": "Later on, Capel and Zisserman [8] show that Irani\u2019s method is superior to the baseline unconstrained maximum likelihood (ML) estimator in noisy cases."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 13
                            }
                        ],
                        "text": "Many systems [8,16,117] have been developed for ALPR and many commercial products put to practical use [119\u2013121] in parking lot billing, toll collecting monitoring, road law enforcement, and security management."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7355075,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "600f3f9e22420c69cbb2c95a6de62bfadba02d84",
            "isKey": false,
            "numCitedBy": 829,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": "Automatic license plate recognition (LPR) plays an important role in numerous applications and a number of techniques have been proposed. However, most of them worked under restricted conditions, such as fixed illumination, limited vehicle speed, designated routes, and stationary backgrounds. In this study, as few constraints as possible on the working environment are considered. The proposed LPR technique consists of two main modules: a license plate locating module and a license number identification module. The former characterized by fuzzy disciplines attempts to extract license plates from an input image, while the latter conceptualized in terms of neural subjects aims to identify the number present in a license plate. Experiments have been conducted for the respective modules. In the experiment on locating license plates, 1088 images taken from various scenes and under different conditions were employed. Of which, 23 images have been failed to locate the license plates present in the images; the license plate location rate of success is 97.9%. In the experiment on identifying license number, 1065 images, from which license plates have been successfully located, were used. Of which, 47 images have been failed to identify the numbers of the license plates located in the images; the identification rate of success is 95.6%. Combining the above two rates, the overall rate of success for our LPR algorithm is 93.7%."
            },
            "slug": "Automatic-license-plate-recognition-Chang-Chen",
            "title": {
                "fragments": [],
                "text": "Automatic license plate recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The proposed LPR technique consists of two main modules: a license plate locating module and a license number identification module, the former characterized by fuzzy disciplines attempts to extract license plates from an input image, while the latter conceptualized in terms of neural subjects aims to identify the number present in alicense plate."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Intelligent Transportation Systems"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118579343"
                        ],
                        "name": "Jie Yang",
                        "slug": "Jie-Yang",
                        "structuredName": {
                            "firstName": "Jie",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jie Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144117646"
                        ],
                        "name": "Jiang Gao",
                        "slug": "Jiang-Gao",
                        "structuredName": {
                            "firstName": "Jiang",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiang Gao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2153391519"
                        ],
                        "name": "Ying Zhang",
                        "slug": "Ying-Zhang",
                        "structuredName": {
                            "firstName": "Ying",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ying Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46772547"
                        ],
                        "name": "Xilin Chen",
                        "slug": "Xilin-Chen",
                        "structuredName": {
                            "firstName": "Xilin",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xilin Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724972"
                        ],
                        "name": "A. Waibel",
                        "slug": "A.-Waibel",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Waibel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Waibel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 0
                            }
                        ],
                        "text": "[105,106] implement an experimental Chinese-to-English sign translation system and give a good analysis on the three components of a general sign translation system: sign detection, character recognition, and sign translation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 144
                            }
                        ],
                        "text": "Other applications are being enabled as well, such as intelligent digital cameras to recognize and translate signs written in foreign languages [95,105,106]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9190854,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4e68aee92394ea69e0a2e98f2123dc4892876101",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "A sign is something that suggests the presence of a fact, condition, or quality. Signs are everywhere in our lives. They make our lives easier when we are familiar with them. But sometimes they pose problems. For example, a tourist might not be able to understand signs in a foreign country. This paper discusses problems of automatic sign recognition and translation. We present a system capable of capturing images, detecting and recognizing signs, and translating them into a target language. We describe methods for automatic sign extraction and translation. We use a user-centered approach in system development. The approach takes advantage of human intelligence if needed and leverage human capabilities. We are currently working on Chinese sign translation. We have developed a prototype system that can recognize Chinese sign input from a video camera that is a common gadget for a tourist, and translate the signs into English or voice stream. The sign translation, in conjunction with spoken language translation, can help international tourists to overcome language barriers. The technology can also help a visually handicapped person to increase environmental awareness."
            },
            "slug": "An-automatic-sign-recognition-and-translation-Yang-Gao",
            "title": {
                "fragments": [],
                "text": "An automatic sign recognition and translation system"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A system capable of capturing images, detecting and recognizing signs, and translating them into a target language, using a user-centered approach in system development and describing methods for automatic sign extraction and translation is presented."
            },
            "venue": {
                "fragments": [],
                "text": "PUI '01"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48764203"
                        ],
                        "name": "Victor Wu",
                        "slug": "Victor-Wu",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Victor Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758550"
                        ],
                        "name": "R. Manmatha",
                        "slug": "R.-Manmatha",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Manmatha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Manmatha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31338632"
                        ],
                        "name": "E. Riseman",
                        "slug": "E.-Riseman",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Riseman",
                            "middleNames": [
                                "M."
                            ],
                            "suffix": ""
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Riseman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 144
                            }
                        ],
                        "text": "use Gaussian derivative filters to extract local texture features and apply K-means clustering to group pixels that have similar filter outputs [103,104]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1830124,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e5c342ba0edbebadc7c95c7e59d1bef87d7e4add",
            "isKey": false,
            "numCitedBy": 451,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "A robust system is proposed to automatically detect and extract text in images from different sources, including video, newspapers, advertisements, stock certificates, photographs, and checks. Text is first detected using multiscale texture segmentation and spatial cohesion constraints, then cleaned up and extracted using a histogram-based binarization algorithm. An automatic performance evaluation scheme is also proposed."
            },
            "slug": "TextFinder:-An-Automatic-System-to-Detect-and-Text-Wu-Manmatha",
            "title": {
                "fragments": [],
                "text": "TextFinder: An Automatic System to Detect and Recognize Text In Images"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A robust system is proposed to automatically detect and extract text in images from different sources, including video, newspapers, advertisements, stock certificates, photographs, and checks."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1925316"
                        ],
                        "name": "M. Pilu",
                        "slug": "M.-Pilu",
                        "structuredName": {
                            "firstName": "Maurizio",
                            "lastName": "Pilu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pilu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 7247823,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5d764bee30dabb873f64f4870ce2a1c2e54980e9",
            "isKey": false,
            "numCitedBy": 64,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "This work is concerned with the reconstruction of the flat state of a curled document or book page when captured face up by a camera. It is reconstructed with the support of sparse depth measurements of its surface. A novel method based on physical modeling of paper deformation with an applicable surface is proposed and a relaxation algorithm is described that allows us to fit this model and unroll it to a plane so as to produce the undistorted document. Some very promising results are shown that confirm that the use of applicable surfaces is the right way to address this problem."
            },
            "slug": "Undoing-page-curl-distortion-using-applicable-Pilu",
            "title": {
                "fragments": [],
                "text": "Undoing page curl distortion using applicable surfaces"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A novel method based on physical modeling of paper deformation with an applicable surface is proposed and a relaxation algorithm is described that allows us to fit this model and unroll it to a plane so as to produce the undistorted document."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 2001 International Conference on Image Processing (Cat. No.01CH37205)"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730720"
                        ],
                        "name": "F. Fekri",
                        "slug": "F.-Fekri",
                        "structuredName": {
                            "firstName": "Faramarz",
                            "lastName": "Fekri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Fekri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698598"
                        ],
                        "name": "R. Mersereau",
                        "slug": "R.-Mersereau",
                        "structuredName": {
                            "firstName": "Russell",
                            "lastName": "Mersereau",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mersereau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693333"
                        ],
                        "name": "R. Schafer",
                        "slug": "R.-Schafer",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Schafer",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schafer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 103
                            }
                        ],
                        "text": "In essence, their idea is similar to the joint quantization, interpolation, and binarization method in [25], which does a good job in optimizing the codebook."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13121386,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "38af6de1399c16c0a8fb14960b72c2dd3a99a356",
            "isKey": false,
            "numCitedBy": 3,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents an approach for the effective combination of interpolation with binarization of gray level text images to reconstruct a high resolution binary image from a lower resolution gray level one. We study two nonlinear interpolative techniques for text image interpolation. These nonlinear interpolation methods map quantized low dimensional 2 x 2 image blocks to higher dimensional 4 x 4 (possibly binary) blocks using a table lookup operation. The first method performs interpolation of text images using context-based, nonlinear, interpolative, vector quantization (NLIVQ). This system has a simple training procedure and has performance (for gray-level high resolution images) that is comparable to our more sophisticated generalized interpolative VQ (GIVQ) approach, which is the second method. In it, we jointly optimize the quantizer and interpolator to find matched codebooks for the low and high resolution images. Then, to obtain the binary codebook that incorporates binarization with interpolation, we introduce a binary constrained optimization method using GIVQ. In order to incorporate the nearest neighbor constraint on the quantizer while minimizing the distortion in the interpolated image, a deterministic-annealing-based optimization technique is applied. With a few interpolation examples, we demonstrate the superior performance of this method over the NLIVQ method (especially for binary outputs) and other standard techniques e.g., bilinear interpolation and pixel replication."
            },
            "slug": "A-generalized-interpolative-vector-quantization-for-Fekri-Mersereau",
            "title": {
                "fragments": [],
                "text": "A generalized interpolative vector quantization method for jointly optimal quantization, interpolation, and binarization of text images"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The superior performance of this method over the NLIVQ method (especially for binary outputs) and other standard techniques e.g., bilinear interpolation and pixel replication is demonstrated."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Image Process."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118671548"
                        ],
                        "name": "Chuang Li",
                        "slug": "Chuang-Li",
                        "structuredName": {
                            "firstName": "Chuang",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chuang Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145507765"
                        ],
                        "name": "X. Ding",
                        "slug": "X.-Ding",
                        "structuredName": {
                            "firstName": "Xiaoqing",
                            "lastName": "Ding",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Ding"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115859002"
                        ],
                        "name": "Youshou Wu",
                        "slug": "Youshou-Wu",
                        "structuredName": {
                            "firstName": "Youshou",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Youshou Wu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "space when doing quantization. In [ 50 ] Li et al. propose"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 41953629,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "162a9556143bdbde27ca76440bbe0d61cf6ee13f",
            "isKey": false,
            "numCitedBy": 47,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "A new connected component based segmentation algorithm which automatically extracts text regions from natural scene images is proposed in this paper. This approach utilizes a multichannel decomposition method to locate text blocks in complex backgrounds. Block alignment analysis and recognition confidence values are used in the combination and identification of the connected components. The algorithm is applied to a test image database and shows promising results."
            },
            "slug": "Automatic-text-location-in-natural-scene-images-Li-Ding",
            "title": {
                "fragments": [],
                "text": "Automatic text location in natural scene images"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A new connected component based segmentation algorithm which automatically extracts text regions from natural scene images is proposed in this paper, utilizing a multichannel decomposition method to locate text blocks in complex backgrounds."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of Sixth International Conference on Document Analysis and Recognition"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1925316"
                        ],
                        "name": "M. Pilu",
                        "slug": "M.-Pilu",
                        "structuredName": {
                            "firstName": "Maurizio",
                            "lastName": "Pilu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pilu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 16
                            }
                        ],
                        "text": "In both [5] and [77], the 3D shape of the page is obtained using a structured light method."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 5
                            }
                        ],
                        "text": "Pilu [77] initializes the mesh with the available 3D shape data, then imposes the applicable constraints onto the mesh."
                    },
                    "intents": []
                }
            ],
            "corpusId": 20516827,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a4a98ee2875def90fdba4e566bc96212e579de12",
            "isKey": false,
            "numCitedBy": 69,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "The work presented is concerned with the reconstruction of the original undistorted image of a curled document or book when captured face-up by a camera with the support of sparse depth measurements. A novel method based on physically modeling paper deformation with an applicable surface is proposed and a relaxation algorithm is described that allows us to fit it to noisy data and then flatten it in order to produce the final undistorted image. The promising results obtained confirm that the use of applicable surfaces is the right way to address this problem."
            },
            "slug": "Undoing-paper-curl-distortion-using-applicable-Pilu",
            "title": {
                "fragments": [],
                "text": "Undoing paper curl distortion using applicable surfaces"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "The work presented is concerned with the reconstruction of the original undistorted image of a curled document or book when captured face-up by a camera with the support of sparse depth measurements and a novel method based on physically modeling paper deformation with an applicable surface is proposed and a relaxation algorithm is described."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144893145"
                        ],
                        "name": "Wei Jiang",
                        "slug": "Wei-Jiang",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Jiang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Jiang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 27
                            }
                        ],
                        "text": "Others, including [22] and [39], use adaptive thresholding, too."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7984810,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "483349b38459faf5ea30ab13ea588fa6b6ef7002",
            "isKey": false,
            "numCitedBy": 4,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "A scheme which converts graytone text images of low spatial resolution to bi-level images of higher spatial resolution for character recognition are presented. Higher recognition rates are achieved when text images are processed using the proposed scheme. A good application of the proposed scheme is the recognition of characters in scene images."
            },
            "slug": "Thresholding-and-enhancement-of-text-images-for-Jiang",
            "title": {
                "fragments": [],
                "text": "Thresholding and enhancement of text images for character recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "A scheme which converts graytone text images of low spatial resolution to bi-level images of higher spatial resolution for character recognition are presented and higher recognition rates are achieved when text images are processed using the proposed scheme."
            },
            "venue": {
                "fragments": [],
                "text": "1995 International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145815031"
                        ],
                        "name": "S. Lucas",
                        "slug": "S.-Lucas",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Lucas",
                            "middleNames": [
                                "M.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lucas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "87531536"
                        ],
                        "name": "A. Panaretos",
                        "slug": "A.-Panaretos",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Panaretos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Panaretos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2073684197"
                        ],
                        "name": "Luis Sosa",
                        "slug": "Luis-Sosa",
                        "structuredName": {
                            "firstName": "Luis",
                            "lastName": "Sosa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luis Sosa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052189571"
                        ],
                        "name": "Anthony Tang",
                        "slug": "Anthony-Tang",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Tang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anthony Tang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108862960"
                        ],
                        "name": "Shirley Wong",
                        "slug": "Shirley-Wong",
                        "structuredName": {
                            "firstName": "Shirley",
                            "lastName": "Wong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shirley Wong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2114080648"
                        ],
                        "name": "Robert Young",
                        "slug": "Robert-Young",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Young",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robert Young"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6379469,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ce39eb5cc1049a1060a499d6b6e94c8b2ec11da1",
            "isKey": false,
            "numCitedBy": 591,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes the robust reading competitions forICDAR 2003. With the rapid growth in research over thelast few years on recognizing text in natural scenes, thereis an urgent need to establish some common benchmarkdatasets, and gain a clear understanding of the current stateof the art. We use the term robust reading to refer to text imagesthat are beyond the capabilities of current commercialOCR packages. We chose to break down the robust readingproblem into three sub-problems, and run competitionsfor each stage, and also a competition for the best overallsystem. The sub-problems we chose were text locating,character recognition and word recognition.By breaking down the problem in this way, we hope togain a better understanding of the state of the art in eachof the sub-problems. Furthermore, our methodology involvesstoring detailed results of applying each algorithm toeach image in the data sets, allowing researchers to study indepth the strengths and weaknesses of each algorithm. Thetext locating contest was the only one to have any entries.We report the results of this contest, and show cases wherethe leading algorithms succeed and fail."
            },
            "slug": "ICDAR-2003-robust-reading-competitions-Lucas-Panaretos",
            "title": {
                "fragments": [],
                "text": "ICDAR 2003 robust reading competitions"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The robust reading problem was broken down into three sub-problems, and competitions for each stage, and also a competition for the best overall system, which was the only one to have any entries."
            },
            "venue": {
                "fragments": [],
                "text": "Seventh International Conference on Document Analysis and Recognition, 2003. Proceedings."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708785"
                        ],
                        "name": "J. Ohya",
                        "slug": "J.-Ohya",
                        "structuredName": {
                            "firstName": "Jun",
                            "lastName": "Ohya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ohya"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2019875"
                        ],
                        "name": "A. Shio",
                        "slug": "A.-Shio",
                        "structuredName": {
                            "firstName": "Akio",
                            "lastName": "Shio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Shio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49052113"
                        ],
                        "name": "S. Akamatsu",
                        "slug": "S.-Akamatsu",
                        "structuredName": {
                            "firstName": "Shigeru",
                            "lastName": "Akamatsu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Akamatsu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 75
                            }
                        ],
                        "text": "present a unique solution of combining character detection and recognition [76]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1565945,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e94d1ff801fce49eea8d8aa51a477b130ca755de",
            "isKey": false,
            "numCitedBy": 263,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "An effective algorithm for character recognition in scene images is studied. Scene images are segmented into regions by an image segmentation method based on adaptive thresholding. Character candidate regions are detected by observing gray-level differences between adjacent regions. To ensure extraction of multisegment characters as well as single-segment characters, character pattern candidates are obtained by associating the detected regions according to their positions and gray levels. A character recognition process selects patterns with high similarities by calculating the similarities between character pattern candidates and the standard patterns in a dictionary and then comparing the similarities to the thresholds. A relaxational approach to determine character patterns updates the similarities by evaluating the interactions between categories of patterns, and finally character patterns and their recognition results are obtained. Highly promising experimental results have been obtained using the method on 100 images involving characters of different sizes and formats under uncontrolled lighting. >"
            },
            "slug": "Recognizing-Characters-in-Scene-Images-Ohya-Shio",
            "title": {
                "fragments": [],
                "text": "Recognizing Characters in Scene Images"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "An effective algorithm for character recognition in scene images is studied and highly promising experimental results have been obtained using the method on 100 images involving characters of different sizes and formats under uncontrolled lighting."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1798672"
                        ],
                        "name": "A. Doncescu",
                        "slug": "A.-Doncescu",
                        "structuredName": {
                            "firstName": "Andrei",
                            "lastName": "Doncescu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Doncescu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783064"
                        ],
                        "name": "Alain Bouju",
                        "slug": "Alain-Bouju",
                        "structuredName": {
                            "firstName": "Alain",
                            "lastName": "Bouju",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alain Bouju"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "92159929"
                        ],
                        "name": "V. Quillet",
                        "slug": "V.-Quillet",
                        "structuredName": {
                            "firstName": "V\u00e9ronique",
                            "lastName": "Quillet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Quillet"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[21] present similar work using additional lighting equipment."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 123678342,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "f8f2722359888b874e63579cd5d37c45a641a730",
            "isKey": false,
            "numCitedBy": 32,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "In the present paper we will discuss the way to correct the text image warping due to old bookbindings which can not open to 180/spl deg/. Two methods have been experimented to measure the surface distortion of the paper The first method uses a diffracted laser beam and is operational for 1D distortion measurements. As former bookbindings may also present 2D distortion, we have developed a second method which uses a light projected network and which gives encouraging results."
            },
            "slug": "Former-books-digital-processing:-image-warping-Doncescu-Bouju",
            "title": {
                "fragments": [],
                "text": "Former books digital processing: image warping"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Workshop on Document Image Analysis (DIA'97)"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2124754"
                        ],
                        "name": "U. Gargi",
                        "slug": "U.-Gargi",
                        "structuredName": {
                            "firstName": "Ullas",
                            "lastName": "Gargi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Gargi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2821130"
                        ],
                        "name": "David J. Crandall",
                        "slug": "David-J.-Crandall",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Crandall",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David J. Crandall"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721328"
                        ],
                        "name": "Sameer Kiran Antani",
                        "slug": "Sameer-Kiran-Antani",
                        "structuredName": {
                            "firstName": "Sameer",
                            "lastName": "Antani",
                            "middleNames": [
                                "Kiran"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sameer Kiran Antani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2372810"
                        ],
                        "name": "T. Gandhi",
                        "slug": "T.-Gandhi",
                        "structuredName": {
                            "firstName": "Tarak",
                            "lastName": "Gandhi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Gandhi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "115338247"
                        ],
                        "name": "Ryan Keener",
                        "slug": "Ryan-Keener",
                        "structuredName": {
                            "firstName": "Ryan",
                            "lastName": "Keener",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ryan Keener"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3110392"
                        ],
                        "name": "R. Kasturi",
                        "slug": "R.-Kasturi",
                        "structuredName": {
                            "firstName": "Rangachar",
                            "lastName": "Kasturi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kasturi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[29] propose a text frame detection scheme based on the increase in the number of intracoded blocks in MPEG P- and B-frames."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[29] present their video text detection system, which aims at more unconstrained text motion."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 57425516,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1d784187382befa5cb50b62f10ddb87feb487102",
            "isKey": false,
            "numCitedBy": 42,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Video indexing is an important problem that has occupied recent research efforts. The text appearing in video can provide semantic information about the scene content. Detecting and recognizing text events can provide indices into the video for content based querying. We describe a system for detecting, tracking, and extracting artificial and scene text in MPEG-1 video. Preliminary results are presented."
            },
            "slug": "A-system-for-automatic-text-detection-in-video-Gargi-Crandall",
            "title": {
                "fragments": [],
                "text": "A system for automatic text detection in video"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A system for detecting, tracking, and extracting artificial and scene text in MPEG-1 video to provide indices into the video for content based querying is described."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Fifth International Conference on Document Analysis and Recognition. ICDAR '99 (Cat. No.PR00318)"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118579343"
                        ],
                        "name": "Jie Yang",
                        "slug": "Jie-Yang",
                        "structuredName": {
                            "firstName": "Jie",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jie Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144117646"
                        ],
                        "name": "Jiang Gao",
                        "slug": "Jiang-Gao",
                        "structuredName": {
                            "firstName": "Jiang",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiang Gao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48379623"
                        ],
                        "name": "Y. Zhang",
                        "slug": "Y.-Zhang",
                        "structuredName": {
                            "firstName": "Y.",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724972"
                        ],
                        "name": "A. Waibel",
                        "slug": "A.-Waibel",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Waibel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Waibel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 0
                            }
                        ],
                        "text": "[105,106] implement an experimental Chinese-to-English sign translation system and give a good analysis on the three components of a general sign translation system: sign detection, character recognition, and sign translation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 144
                            }
                        ],
                        "text": "Other applications are being enabled as well, such as intelligent digital cameras to recognize and translate signs written in foreign languages [95,105,106]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9974904,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "27876ea01c6300eea6ae7e0fab93b3007b865bfe",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Signs are everywhere in our lives. They make our lives easier when we are familiar with them. But sometimes they also pose problems. For example, a tourist might not be able to understand signs in a foreign country. In this paper, we present our efforts towards automatic sign translation. We discuss methods for automatic sign detection. We describe sign translation using example based machine translation technology. We use a user-centered approach in developing an automatic sign translation system. The approach takes advantage of human intelligence in selecting an area of interest and domain for translation if needed. A user can determine which sign is to be translated if multiple signs have been detected within the image. The selected part of the image is then processed, recognized, and translated. We have developed a prototype system that can recognize Chinese signs input from a video camera which is a common gadget for a tourist, and translate them into English text or voice stream."
            },
            "slug": "Towards-Automatic-Sign-Translation-Yang-Gao",
            "title": {
                "fragments": [],
                "text": "Towards Automatic Sign Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A prototype system that can recognize Chinese signs input from a video camera which is a common gadget for a tourist, and translate them into English text or voice stream is developed."
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756036"
                        ],
                        "name": "C. Rother",
                        "slug": "C.-Rother",
                        "structuredName": {
                            "firstName": "Carsten",
                            "lastName": "Rother",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Rother"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 16
                            }
                        ],
                        "text": "As described in [84], in a man-made environment where many 3D orthogonal lines exist, the estimation of vanishing points provides a way to recover the perspective."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 50
                            }
                        ],
                        "text": "In their study of removing perspective distortion [71, 84], Myers et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2048989,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "06d4a6e2c59b3c4bf5d7216d32e6e811eb843f43",
            "isKey": false,
            "numCitedBy": 265,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "A man-made environment is characterized by a lot of parallel lines and a lot of orthogonal edges. In this article, a new method for detecting the three mutual orthogonal directions of such an environment is presented. Since realtime performance is not necessary for architectural application, like building reconstruction, a computationally more intensive approach was chosen. On the other hand, our approach is more rigorous than existing techniques, since the information given by the condition of three mutual orthogonal directions in the scene is identified and incorporated. Since knowledge about the camera geometry can be deduced from the vanishing points of three mutual orthogonal directions, we use this knowledge to reject falsely detected vanishing points. Results are presented from interpreting outdoor scenes of buildings."
            },
            "slug": "A-New-Approach-for-Vanishing-Point-Detection-in-Rother",
            "title": {
                "fragments": [],
                "text": "A New Approach for Vanishing Point Detection in Architectural Environments"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This approach is more rigorous than existing techniques, since the information given by the condition of three mutual orthogonal directions in the scene is identified and incorporated and used to reject falsely detected vanishing points."
            },
            "venue": {
                "fragments": [],
                "text": "BMVC"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1925316"
                        ],
                        "name": "M. Pilu",
                        "slug": "M.-Pilu",
                        "structuredName": {
                            "firstName": "Maurizio",
                            "lastName": "Pilu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pilu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34675913"
                        ],
                        "name": "F. Isgr\u00f2",
                        "slug": "F.-Isgr\u00f2",
                        "structuredName": {
                            "firstName": "Francesco",
                            "lastName": "Isgr\u00f2",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Isgr\u00f2"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 15
                            }
                        ],
                        "text": "Pilu and Isgro [80] propos similar work in document mosaicing."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3041028,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cc6b73fd9a73333684cfdb6740134fc9670a8cf2",
            "isKey": false,
            "numCitedBy": 10,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a fast and extremely robust feature-based method for planar registration of partly overlapping images that uses a twostage robust tting approach comprising a fast estimation of a transformation hypothesis (that we show is highly likely to be correct) followed by a con rmation and re nement stage. The method is particularly suited for automatic stitching of oversize documents scanned in two or more parts. We show simulations, also supported by practical experiments, that prove both the robustness and computational eAEciency of the approach."
            },
            "slug": "A-fast-and-reliable-planar-registration-method-with-Pilu-Isgr\u00f2",
            "title": {
                "fragments": [],
                "text": "A fast and reliable planar registration method with applications to document stitching"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "This paper presents a fast and extremely robust feature-based method for planar registration of partly overlapping images that uses a twostage robust approach comprising a fast estimation of a transformation hypothesis (that it is shown is highly likely to be correct) followed by a con rmation and renement stage."
            },
            "venue": {
                "fragments": [],
                "text": "BMVC"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40116905"
                        ],
                        "name": "Jia Li",
                        "slug": "Jia-Li",
                        "structuredName": {
                            "firstName": "Jia",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jia Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144790332"
                        ],
                        "name": "R. Gray",
                        "slug": "R.-Gray",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Gray",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Gray"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13983488,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9109aea1c445de7a607c6c17736209bc7d897cef",
            "isKey": false,
            "numCitedBy": 53,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents an algorithm to segment text and picture in an image using two features based on the statistical distribution of the wavelet coefficients in high frequency bands. The algorithm breaks the image into blocks and classifies every block as background, text or picture according to the two features. The block size is variable so that the segmentation can be accurate at the boundary of two types and avoids misclassifying due to over-localized region analysis."
            },
            "slug": "Text-and-picture-segmentation-by-the-distribution-Li-Gray",
            "title": {
                "fragments": [],
                "text": "Text and picture segmentation by the distribution analysis of wavelet coefficients"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "This paper presents an algorithm to segment text and picture in an image using two features based on the statistical distribution of the wavelet coefficients in high frequency bands that can be accurate at the boundary of two types and avoids misclassifying due to over-localized region analysis."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 1998 International Conference on Image Processing. ICIP98 (Cat. No.98CB36269)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52322585"
                        ],
                        "name": "T. Gotoh",
                        "slug": "T.-Gotoh",
                        "structuredName": {
                            "firstName": "Toshiyuki",
                            "lastName": "Gotoh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Gotoh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783505"
                        ],
                        "name": "T. Toriu",
                        "slug": "T.-Toriu",
                        "structuredName": {
                            "firstName": "Takashi",
                            "lastName": "Toriu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Toriu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144909403"
                        ],
                        "name": "S. Sasaki",
                        "slug": "S.-Sasaki",
                        "structuredName": {
                            "firstName": "Shigeru",
                            "lastName": "Sasaki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sasaki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107828741"
                        ],
                        "name": "M. Yoshida",
                        "slug": "M.-Yoshida",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Yoshida",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Yoshida"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 141
                            }
                        ],
                        "text": "Over the past 20 years, there have been numerous applications on camera-based text recognition, such as reading license plates, book sorting [31], visual classification of magazines and books, reading freight train IDs, road sign recognition, detection of danger labels, and reading signs in warehouses."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1943061,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3d417988c7b5d8cd089ada273842f0bd25e45ffa",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "A flexible vision-based algorithm for a book sorting system is presented. The algorithm is based on a discrimination model that is adaptively generated for the current object classes by learning. The algorithm consists of an image normalization process, a feature element extraction process, a learning process, and a recognition process. The image normalization process extracts the contour of the object in an image, and geometrically normalizes the image. The feature extraction process converts the normalized image to the pyramidal representation, and the feature element is extracted from each resolution level. The learning process generates a discrimination model, which represents the differences between classes, based on hierarchical clustering. In the recognition process, the input images are hierarchically discriminated under the control of the decision tree. To evaluate the algorithm, a simulation system was implemented on a general-purpose computer and an image processor was developed. >"
            },
            "slug": "A-Flexible-Vision-Based-Algorithm-for-a-Book-System-Gotoh-Toriu",
            "title": {
                "fragments": [],
                "text": "A Flexible Vision-Based Algorithm for a Book Sorting System"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "A flexible vision-based algorithm for a book sorting system that is adaptively generated for the current object classes by learning is presented, based on a discrimination model based on hierarchical clustering."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144611617"
                        ],
                        "name": "M. Irani",
                        "slug": "M.-Irani",
                        "structuredName": {
                            "firstName": "Michal",
                            "lastName": "Irani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Irani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144406261"
                        ],
                        "name": "Shmuel Peleg",
                        "slug": "Shmuel-Peleg",
                        "structuredName": {
                            "firstName": "Shmuel",
                            "lastName": "Peleg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shmuel Peleg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "tation, Irani and Peleg [ 37 ] use an iterative approach to gradually match two images."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In [ 37 ], Irani and Peleg demonstrate the effect of super resolution in improving image quality."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4834546,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "42d60f7faaa2f6fdd2b928c352d65eb57b4791aa",
            "isKey": false,
            "numCitedBy": 1933,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Improving-resolution-by-image-registration-Irani-Peleg",
            "title": {
                "fragments": [],
                "text": "Improving resolution by image registration"
            },
            "venue": {
                "fragments": [],
                "text": "CVGIP Graph. Model. Image Process."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2258400"
                        ],
                        "name": "\u00d8. Trier",
                        "slug": "\u00d8.-Trier",
                        "structuredName": {
                            "firstName": "\u00d8ivind",
                            "lastName": "Trier",
                            "middleNames": [
                                "Due"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u00d8. Trier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48717516"
                        ],
                        "name": "T. Taxt",
                        "slug": "T.-Taxt",
                        "structuredName": {
                            "firstName": "Torfinn",
                            "lastName": "Taxt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Taxt"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 23
                            }
                        ],
                        "text": "In a well-known survey [92], Trier and Taxt compare 11 locally adaptive thresholding techniques and conclude that Niblack\u2019s method is the most effective for general images [92]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17374833,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "66126ec1fe61b833ae695db9c5bac54641fab482",
            "isKey": false,
            "numCitedBy": 451,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents an evaluation of eleven locally adaptive binarization methods for gray scale images with low contrast, variable background intensity and noise. Niblack's method (1986) with the addition of the postprocessing step of Yanowitz and Bruckstein's method (1989) added performed the best and was also one of the fastest binarization methods. >"
            },
            "slug": "Evaluation-of-Binarization-Methods-for-Document-Trier-Taxt",
            "title": {
                "fragments": [],
                "text": "Evaluation of Binarization Methods for Document Images"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "This paper presents an evaluation of eleven locally adaptive binarization methods for gray scale images with low contrast, variable background intensity and noise and Niblack's method with the addition of the postprocessing step of Yanowitz and Bruckstein's method (1989) performed the best and was also one of the fastest binarized methods."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145695526"
                        ],
                        "name": "S. Mao",
                        "slug": "S.-Mao",
                        "structuredName": {
                            "firstName": "Song",
                            "lastName": "Mao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143626870"
                        ],
                        "name": "T. Kanungo",
                        "slug": "T.-Kanungo",
                        "structuredName": {
                            "firstName": "Tapas",
                            "lastName": "Kanungo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanungo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5453548,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "82328a31d7366c4ecfd339deeae41c3152b43c98",
            "isKey": false,
            "numCitedBy": 98,
            "numCiting": 64,
            "paperAbstract": {
                "fragments": [],
                "text": "While numerous page segmentation algorithms have been proposed in the literature, there is lack of comparative evaluation of these algorithms. In the existing performance evaluation methods, two crucial components are usually missing: 1) automatic training of algorithms with free parameters and 2) statistical and error analysis of experimental results. We use the following five-step methodology to quantitatively compare the performance of page segmentation algorithms: 1) first, we create mutually exclusive training and test data sets with groundtruth, 2) we then select a meaningful and computable performance metric, 3) an optimization procedure is then used to search automatically for the optimal parameter values of the segmentation algorithms on the training data set, 4) the segmentation algorithms are then evaluated on the test data set, and, finally, 5) a statistical and error analysis is performed to give the statistical significance of the experimental results. In particular, instead of the ad hoc and manual approach typically used in the literature for training algorithms, we pose the automatic training of algorithms as an optimization problem and use the simplex algorithm to search for the optimal parameter value. A paired-model statistical analysis and an error analysis are then conducted to provide confidence intervals for the experimental results of the algorithms. This methodology is applied to the evaluation of live page segmentation algorithms of which, three are representative research algorithms and the other two are well-known commercial products, on 978 images from the University of Washington III data set. It is found that the performance indices of the Voronoi, Docstrum, and Caere segmentation algorithms are not significantly different from each other, but they are significantly better than that of ScanSoft's segmentation algorithm, which, in turn, is significantly better than that of X-Y cut."
            },
            "slug": "Empirical-Performance-Evaluation-Methodology-and-to-Mao-Kanungo",
            "title": {
                "fragments": [],
                "text": "Empirical Performance Evaluation Methodology and Its Application to Page Segmentation Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The performance indices of the Voronoi, Docstrum, and Caere segmentation algorithms are not significantly different from each other, but they are significantly better than that of ScanSoft's segmentation algorithm, which, in turn, is significantly betterthan that of X-Y cut."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1925316"
                        ],
                        "name": "M. Pilu",
                        "slug": "M.-Pilu",
                        "structuredName": {
                            "firstName": "Maurizio",
                            "lastName": "Pilu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pilu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 58
                            }
                        ],
                        "text": "Pilu discusses his method of vanishing point detection in [78]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9009321,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "18221f843f531627f1a808f8ce0592894717e85e",
            "isKey": false,
            "numCitedBy": 64,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "The article deals with the recovery of illusory linear clues from perspectively skewed documents, with the purpose of using them for rectification. The computational approach proposed implements the perceptual organization principles implicitly used in textual layouts. The numerous examples provided show that the method is robust and viewpoint and scale invariant."
            },
            "slug": "Extraction-of-illusory-linear-clues-in-skewed-Pilu",
            "title": {
                "fragments": [],
                "text": "Extraction of illusory linear clues in perspectively skewed documents"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "The article deals with the recovery of illusory linear clues from perspectively skewed documents, with the purpose of using them for rectification in perceptual organization principles implicitly used in textual layouts."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48471936"
                        ],
                        "name": "D. Doermann",
                        "slug": "D.-Doermann",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Doermann",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Doermann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2082032608"
                        ],
                        "name": "David Mihalcik",
                        "slug": "David-Mihalcik",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mihalcik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Mihalcik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "In [20], related work on a video performance evaluation resource toolkit (ViPER) is reported."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 25521649,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "442644532f74506d4b34052871366ba5afc21f5d",
            "isKey": false,
            "numCitedBy": 229,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "We outline a reconfigurable video performance evaluation resource (ViPER), which provides an interface for ground truth generation, metrics for evaluation and tools for visualization of video analysis results. A key component is that the approach provides the basic infrastructure, and allows users to configure data generation and evaluation. Although ViPER can be used for any type of data, we focus on applications which require video content."
            },
            "slug": "Tools-and-techniques-for-video-performance-Doermann-Mihalcik",
            "title": {
                "fragments": [],
                "text": "Tools and techniques for video performance evaluation"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "A reconfigurable video performance evaluation resource (ViPER), which provides an interface for ground truth generation, metrics for evaluation and tools for visualization of video analysis results, is outlined."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 15th International Conference on Pattern Recognition. ICPR-2000"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728108"
                        ],
                        "name": "M. Mirmehdi",
                        "slug": "M.-Mirmehdi",
                        "structuredName": {
                            "firstName": "Majid",
                            "lastName": "Mirmehdi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mirmehdi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "26549275"
                        ],
                        "name": "P. Palmer",
                        "slug": "P.-Palmer",
                        "structuredName": {
                            "firstName": "Phil",
                            "lastName": "Palmer",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Palmer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145801638"
                        ],
                        "name": "J. Kittler",
                        "slug": "J.-Kittler",
                        "structuredName": {
                            "firstName": "Josef",
                            "lastName": "Kittler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kittler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In [ 67 ], Mirmehdi et al. propose a simple approach to autozooming for general recognition problems."
                    },
                    "intents": []
                }
            ],
            "corpusId": 11723888,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "236b0f1553f13e1f15df79164f81e360b5a4dbff",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "A critical issue in Automatic Target Recognition is the detection and identification of low contrast targets in Forward Looking Infra-Red (FLIR) images at sufficiently long ranges. In this paper, we describe a technique basedon determining the best magnification to zoom in onto a target object while it is still at a long range. We model the zoom process based on the expected signal from the object and the background, and show that the variance in a region of interest can be used in a feedback control loop to determine the optimal zoom. The method is highly suitable for real-time implementation and is demonstrated through a series of experiments."
            },
            "slug": "Towards-Optimal-Zoom-for-Automatic-target-Mirmehdi-Palmer",
            "title": {
                "fragments": [],
                "text": "Towards Optimal Zoom for Automatic target Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A technique based on determining the best magnification to zoom in onto a target object while it is still at a long range, and it is shown that the variance in a region of interest can be used in a feedback control loop to determine the optimal zoom."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2673919"
                        ],
                        "name": "V. Mariano",
                        "slug": "V.-Mariano",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Mariano",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Mariano"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3054467"
                        ],
                        "name": "J. Min",
                        "slug": "J.-Min",
                        "structuredName": {
                            "firstName": "Junghye",
                            "lastName": "Min",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Min"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109375307"
                        ],
                        "name": "J. Park",
                        "slug": "J.-Park",
                        "structuredName": {
                            "firstName": "Jin",
                            "lastName": "Park",
                            "middleNames": [
                                "Hyeong"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Park"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3110392"
                        ],
                        "name": "R. Kasturi",
                        "slug": "R.-Kasturi",
                        "structuredName": {
                            "firstName": "Rangachar",
                            "lastName": "Kasturi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kasturi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2082032608"
                        ],
                        "name": "David Mihalcik",
                        "slug": "David-Mihalcik",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mihalcik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Mihalcik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115196978"
                        ],
                        "name": "Huiping Li",
                        "slug": "Huiping-Li",
                        "structuredName": {
                            "firstName": "Huiping",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huiping Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48471936"
                        ],
                        "name": "D. Doermann",
                        "slug": "D.-Doermann",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Doermann",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Doermann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8050430"
                        ],
                        "name": "Thomas Drayer",
                        "slug": "Thomas-Drayer",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Drayer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Drayer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 19509530,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2fc7be0f43a4fef1ae24420d1a5216f8dfd41d4b",
            "isKey": false,
            "numCitedBy": 155,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "The continuous development of object detection algorithms is ushering in the need for evaluation tools to quantify algorithm performance. In this paper a set of seven metrics are proposed for quantifying different aspects of a detection algorithm's performance. The strengths and weaknesses of these metrics are described. They are implemented in the Video Performance Evaluation Resource (ViPER) system and will be used to evaluate algorithms for detecting text, faces, moving people and vehicles. Results for running two previous text-detection algorithms on a common data set are presented."
            },
            "slug": "Performance-evaluation-of-object-detection-Mariano-Min",
            "title": {
                "fragments": [],
                "text": "Performance evaluation of object detection algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A set of seven metrics are proposed for quantifying different aspects of a detection algorithm's performance and will be used to evaluate algorithms for detecting text, faces, moving people and vehicles."
            },
            "venue": {
                "fragments": [],
                "text": "Object recognition supported by user interaction for service robots"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688357"
                        ],
                        "name": "S. V. Rice",
                        "slug": "S.-V.-Rice",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Rice",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. V. Rice"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "15307178"
                        ],
                        "name": "F. Jenkins",
                        "slug": "F.-Jenkins",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Jenkins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jenkins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688975"
                        ],
                        "name": "T. Nartker",
                        "slug": "T.-Nartker",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Nartker",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Nartker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 78
                            }
                        ],
                        "text": "Several well-known competitions include the DOE/UNLV annual OCR accuracy test [73,74,82, 83], the Census/NIST OCR Systems Conference [30,98], text detection evaluations of TREC Video Track [125] (TRECVID) and by NIST, and the Robust Reading Competition of ICDAR 2003 [60]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 17017303,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fae039cc89b2cd453acb85d208e021907528b062",
            "isKey": false,
            "numCitedBy": 214,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "For four years, ISRI has conducted an annual test of optical character recognition (OCR) systems known as \u201cpage readers.\u201d These systems accept as input a bitmapped image of any document page, and attempt to identify the machine-printed characters on the page. In the annual test, we measure the accuracy of this process by comparing the text that is produced as output with the correct text. The goals of the test include:"
            },
            "slug": "The-Fourth-Annual-Test-of-OCR-Accuracy-Rice-Jenkins",
            "title": {
                "fragments": [],
                "text": "The Fourth Annual Test of OCR Accuracy"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "The annual test of optical character recognition systems known as \u201cpage readers\u201d accepts as input a bitmapped image of any document page, and attempts to identify the machine-printed characters on the page."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145347688"
                        ],
                        "name": "S. Baker",
                        "slug": "S.-Baker",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Baker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Baker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733113"
                        ],
                        "name": "T. Kanade",
                        "slug": "T.-Kanade",
                        "structuredName": {
                            "firstName": "Takeo",
                            "lastName": "Kanade",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanade"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 18
                            }
                        ],
                        "text": "In a recent study [1], Baker and Kanade analyze the difficulties of the above super-resolution methods and point out that the increase of low-resolution images does not necessarily compensate the decrease of resolution if only smoothness constraints are incorporated into the restoration process."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 30452203,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "66d890912381e2536d2dbc117a0ce59158c3be90",
            "isKey": false,
            "numCitedBy": 1232,
            "numCiting": 90,
            "paperAbstract": {
                "fragments": [],
                "text": "We analyze the super-resolution reconstruction constraints. In particular we derive a sequence of results which all show that the constraints provide far less useful information as the magnification factor increases. It is well established that the use of a smoothness prior may help somewhat, however for large enough magnification factors any smoothness prior leads to overly smooth results. We therefore propose an algorithm that learns recognition-based priors for specific classes of scenes, the use of which gives far better super-resolution results for both faces and text."
            },
            "slug": "Limits-on-super-resolution-and-how-to-break-them-Baker-Kanade",
            "title": {
                "fragments": [],
                "text": "Limits on super-resolution and how to break them"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "An algorithm is proposed that learns recognition-based priors for specific classes of scenes, the use of which gives far better super-resolution results for both faces and text."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings IEEE Conference on Computer Vision and Pattern Recognition. CVPR 2000 (Cat. No.PR00662)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145916951"
                        ],
                        "name": "G. Nagy",
                        "slug": "G.-Nagy",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Nagy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Nagy"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 620082,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ce3b569e18670f6c10e61aa9a8bda7c30fd37411",
            "isKey": false,
            "numCitedBy": 554,
            "numCiting": 95,
            "paperAbstract": {
                "fragments": [],
                "text": "The contributions to document image analysis of 99 papers published in the IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI) are clustered, summarized, interpolated, interpreted, and evaluated."
            },
            "slug": "Twenty-Years-of-Document-Image-Analysis-in-PAMI-Nagy",
            "title": {
                "fragments": [],
                "text": "Twenty Years of Document Image Analysis in PAMI"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The contributions to document image analysis of 99 papers published in the IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI) are clustered, summarized, interpolated, interpreted, and evaluated."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2074498114"
                        ],
                        "name": "V. Maergner",
                        "slug": "V.-Maergner",
                        "structuredName": {
                            "firstName": "V.",
                            "lastName": "Maergner",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Maergner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144813098"
                        ],
                        "name": "P. Karcher",
                        "slug": "P.-Karcher",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Karcher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Karcher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057730658"
                        ],
                        "name": "A.-K. Pawlowski",
                        "slug": "A.-K.-Pawlowski",
                        "structuredName": {
                            "firstName": "A.-K.",
                            "lastName": "Pawlowski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A.-K. Pawlowski"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[62] believe that no single metric can reveal all aspects of system performance and designed a set of seven metrics to summarize performance."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 29619008,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2bed2562a8582b181ad58fffcd1b640eaaeff0a8",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Thes paper is a contribution to the development of evaluation methods in general and in particular of the evaluation of document analysis systems (DAS). We give a general concept of benchmarking DAS and present a method to benchmark single modules of a DAS where we focus on the definition and generation of ground truth (GT), especially of the image processing modules of a DAS. The proposed method describes how to build a database with specific ground truth for the output of each module of interest. Another method to build a database for benchmarking more generally is to use synthetic data."
            },
            "slug": "On-benchmarking-of-document-analysis-systems-Maergner-Karcher",
            "title": {
                "fragments": [],
                "text": "On benchmarking of document analysis systems"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The proposed method describes how to build a database with specific ground truth for the output of each module of interest, especially of the image processing modules of a DAS."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Fourth International Conference on Document Analysis and Recognition"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1971263"
                        ],
                        "name": "K. Etemad",
                        "slug": "K.-Etemad",
                        "structuredName": {
                            "firstName": "Kamran",
                            "lastName": "Etemad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Etemad"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48471936"
                        ],
                        "name": "D. Doermann",
                        "slug": "D.-Doermann",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Doermann",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Doermann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9215658"
                        ],
                        "name": "R. Chellappa",
                        "slug": "R.-Chellappa",
                        "structuredName": {
                            "firstName": "Rama",
                            "lastName": "Chellappa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Chellappa"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 27678281,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "95dffcc92bda88d9f4f5b112d100f43951745b8c",
            "isKey": false,
            "numCitedBy": 112,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an algorithm for layout-independent document page segmentation based on document texture using multiscale feature vectors and fuzzy local decision information. Multiscale feature vectors are classified locally using a neural network to allow soft/fuzzy multi-class membership assignments. Segmentation is performed by integrating soft local decision vectors to reduce their \"ambiguities\"."
            },
            "slug": "Multiscale-Segmentation-of-Unstructured-Document-Etemad-Doermann",
            "title": {
                "fragments": [],
                "text": "Multiscale Segmentation of Unstructured Document Pages Using Soft Decision Integration"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "An algorithm for layout-independent document page segmentation based on document texture using multiscale feature vectors and fuzzy local decision information is presented."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31693932"
                        ],
                        "name": "G. Myers",
                        "slug": "G.-Myers",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Myers",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Myers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764443"
                        ],
                        "name": "R. Bolles",
                        "slug": "R.-Bolles",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Bolles",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Bolles"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48804780"
                        ],
                        "name": "James A. Herson",
                        "slug": "James-A.-Herson",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Herson",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James A. Herson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 50
                            }
                        ],
                        "text": "In their study of removing perspective distortion [71, 84], Myers et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[71] point out, OCR engines are capable of handling different x-to-y scales and are not affected by x- or y-translations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 61108969,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "7cd8434022c068c5b72c71e5de0df49d3bddfd55",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Continuous system for separating a liquid from a liquid-solid admixture comprises a feed conveyor means for transporting the admixture, an electrically-driven centrifugal separator and control means therefor, a discharge conveyor means for receiving and transporting away the separated solids, and a feed conveyor control means responsive to the rotational speed of the separator so that the feed conveyor is operable only when the separator is operating at a predetermined speed."
            },
            "slug": "RECOGNITION-OF-TEXT-IN-3-D-SCENES-Myers-Bolles",
            "title": {
                "fragments": [],
                "text": "RECOGNITION OF TEXT IN 3-D SCENES"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "Continuous system for separating a liquid from a liquid-solid admixture comprises a feed conveyor means for transporting the admixture, an electrically-driven centrifugal separator and control means therefor, and a discharge conveyor for receiving and transporting away the separated solids."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719436"
                        ],
                        "name": "A. Vinciarelli",
                        "slug": "A.-Vinciarelli",
                        "structuredName": {
                            "firstName": "Alessandro",
                            "lastName": "Vinciarelli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Vinciarelli"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16082889,
            "fieldsOfStudy": [
                "Sociology"
            ],
            "id": "7f124251d5476213d0bbd95bcc4698458169cd51",
            "isKey": false,
            "numCitedBy": 257,
            "numCiting": 125,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-survey-on-off-line-Cursive-Word-Recognition-Vinciarelli",
            "title": {
                "fragments": [],
                "text": "A survey on off-line Cursive Word Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688975"
                        ],
                        "name": "T. Nartker",
                        "slug": "T.-Nartker",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Nartker",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Nartker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688357"
                        ],
                        "name": "S. V. Rice",
                        "slug": "S.-V.-Rice",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Rice",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. V. Rice"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 114311981,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d6c589decc94bc47bd433093349afc27900b2091",
            "isKey": false,
            "numCitedBy": 11,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Each year the Information Science Research Institute (ISRI) at the University of Nevada, Las Vegas (UNVL) conducts a test of the performance of OCR (Optical Character Recognition) systems: overview of the state-of-art of OCR from machine-printed documents"
            },
            "slug": "OCR-Accuracy:-UNLV's-third-annual-test-Nartker-Rice",
            "title": {
                "fragments": [],
                "text": "OCR Accuracy: UNLV's third annual test"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This paper presents an overview of the state-of-art of OCR from machine-printed documents and describes the current state of the art in this area."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 18
                            }
                        ],
                        "text": "Wolf and Doermann [98] obtain the a priori distribution of 4\u00d74 binary cliques in text images from training samples and use a MAP estimator to binarize any 4\u00d74 cliques in input grayscale images [98]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 133
                            }
                        ],
                        "text": "Several well-known competitions include the DOE/UNLV annual OCR accuracy test [73,74,82, 83], the Census/NIST OCR Systems Conference [30,98], text detection evaluations of TREC Video Track [125] (TRECVID) and by NIST, and the Robust Reading Competition of ICDAR 2003 [60]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 47
                            }
                        ],
                        "text": "The VQ-based method faces the same problems as [98], i."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 7
                            }
                        ],
                        "text": "Li and Doermann [53] also present a scheme for enhancing moving text."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 7
                            }
                        ],
                        "text": "Li and Doermann [53] study the tracking of moving text in videos."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 7
                            }
                        ],
                        "text": "Li and Doermann [53] utilize a projection-onto-convex-sets (POCS)based method to deblur scene text to improve readability."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 18
                            }
                        ],
                        "text": "To overcome this, [98] uses a simulated annealing technique."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The first optical character recognition systems conference"
            },
            "venue": {
                "fragments": [],
                "text": "Technical Report NISTIR"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 10
                            }
                        ],
                        "text": "As Fisher [27] found, if on-camera flash is used, the center of the view is the brightest, and then lighting decays outward."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 3
                            }
                        ],
                        "text": "As Fisher [27] found, if on-camera flash\nis used, the center of the view is the brightest, and then lighting decays outward."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 7
                            }
                        ],
                        "text": "Fisher [27] investigates the possibility of replacing sheet-fed scanners used by soldiers in the battlefield, with digital cameras."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Digital camera for document acquisition"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. symposium on document image understanding technology,"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2070811111"
                        ],
                        "name": "Jon C. Geist",
                        "slug": "Jon-C.-Geist",
                        "structuredName": {
                            "firstName": "Jon",
                            "lastName": "Geist",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jon C. Geist"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145662615"
                        ],
                        "name": "R. A. Wilkinson",
                        "slug": "R.-A.-Wilkinson",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Wilkinson",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. A. Wilkinson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2123720"
                        ],
                        "name": "S. Janet",
                        "slug": "S.-Janet",
                        "structuredName": {
                            "firstName": "Stanley",
                            "lastName": "Janet",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Janet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2136478"
                        ],
                        "name": "P. Grother",
                        "slug": "P.-Grother",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Grother",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Grother"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057890920"
                        ],
                        "name": "Bob Hammond",
                        "slug": "Bob-Hammond",
                        "structuredName": {
                            "firstName": "Bob",
                            "lastName": "Hammond",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bob Hammond"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69051435"
                        ],
                        "name": "N. W. Larsen",
                        "slug": "N.-W.-Larsen",
                        "structuredName": {
                            "firstName": "Norman",
                            "lastName": "Larsen",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. W. Larsen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "75115665"
                        ],
                        "name": "R. Klear",
                        "slug": "R.-Klear",
                        "structuredName": {
                            "firstName": "Randy",
                            "lastName": "Klear",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Klear"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "75089502"
                        ],
                        "name": "M. J. Matsko",
                        "slug": "M.-J.-Matsko",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Matsko",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. J. Matsko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2676309"
                        ],
                        "name": "C. Burges",
                        "slug": "C.-Burges",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Burges",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burges"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40199624"
                        ],
                        "name": "R. Creecy",
                        "slug": "R.-Creecy",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Creecy",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Creecy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694191"
                        ],
                        "name": "J. Hull",
                        "slug": "J.-Hull",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Hull",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hull"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47327848"
                        ],
                        "name": "T. P. Vogl",
                        "slug": "T.-P.-Vogl",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Vogl",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. P. Vogl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144286385"
                        ],
                        "name": "Charles L. Wilson",
                        "slug": "Charles-L.-Wilson",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Wilson",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charles L. Wilson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 133
                            }
                        ],
                        "text": "Several well-known competitions include the DOE/UNLV annual OCR accuracy test [73,74,82, 83], the Census/NIST OCR Systems Conference [30,98], text detection evaluations of TREC Video Track [125] (TRECVID) and by NIST, and the Robust Reading Competition of ICDAR 2003 [60]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 69821341,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c06f6ee9cc628c8ceefc10e5df0b12db443bef8c",
            "isKey": false,
            "numCitedBy": 62,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Second-Census-Optical-Character-Recognition-Geist-Wilkinson",
            "title": {
                "fragments": [],
                "text": "The Second Census Optical Character Recognition Systems Conference"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145662615"
                        ],
                        "name": "R. A. Wilkinson",
                        "slug": "R.-A.-Wilkinson",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Wilkinson",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. A. Wilkinson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2070811111"
                        ],
                        "name": "Jon C. Geist",
                        "slug": "Jon-C.-Geist",
                        "structuredName": {
                            "firstName": "Jon",
                            "lastName": "Geist",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jon C. Geist"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2123720"
                        ],
                        "name": "S. Janet",
                        "slug": "S.-Janet",
                        "structuredName": {
                            "firstName": "Stanley",
                            "lastName": "Janet",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Janet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2136478"
                        ],
                        "name": "P. Grother",
                        "slug": "P.-Grother",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Grother",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Grother"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2676309"
                        ],
                        "name": "C. Burges",
                        "slug": "C.-Burges",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Burges",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burges"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40199624"
                        ],
                        "name": "R. Creecy",
                        "slug": "R.-Creecy",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Creecy",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Creecy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057890920"
                        ],
                        "name": "Bob Hammond",
                        "slug": "Bob-Hammond",
                        "structuredName": {
                            "firstName": "Bob",
                            "lastName": "Hammond",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bob Hammond"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694191"
                        ],
                        "name": "J. Hull",
                        "slug": "J.-Hull",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Hull",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hull"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69051435"
                        ],
                        "name": "N. W. Larsen",
                        "slug": "N.-W.-Larsen",
                        "structuredName": {
                            "firstName": "Norman",
                            "lastName": "Larsen",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. W. Larsen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47327848"
                        ],
                        "name": "T. P. Vogl",
                        "slug": "T.-P.-Vogl",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Vogl",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. P. Vogl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144286385"
                        ],
                        "name": "Charles L. Wilson",
                        "slug": "Charles-L.-Wilson",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Wilson",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charles L. Wilson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 59705557,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c22b24ec3969a5759dc634c188044197d09663a0",
            "isKey": false,
            "numCitedBy": 161,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-First-Census-Optical-Character-Recognition-|-Wilkinson-Geist",
            "title": {
                "fragments": [],
                "text": "The First Census Optical Character Recognition Systems Conference | NIST"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2069641275"
                        ],
                        "name": "Paul Clark",
                        "slug": "Paul-Clark",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Clark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paul Clark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728108"
                        ],
                        "name": "M. Mirmehdi",
                        "slug": "M.-Mirmehdi",
                        "structuredName": {
                            "firstName": "Majid",
                            "lastName": "Mirmehdi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mirmehdi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 32050415,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6f81b1e315b1733cddb1f503bb6497c6f45eaccf",
            "isKey": false,
            "numCitedBy": 21,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Finding-Text-Regions-using-Localised-Statistical-Clark-Mirmehdi",
            "title": {
                "fragments": [],
                "text": "Finding Text Regions using Localised Statistical Measures"
            },
            "venue": {
                "fragments": [],
                "text": "BMVC"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116415943"
                        ],
                        "name": "B. Yu",
                        "slug": "B.-Yu",
                        "structuredName": {
                            "firstName": "Bin",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Yu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 34993677,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "73bf7b2fdb26498c05896d99fee4b8f3608c3bd6",
            "isKey": false,
            "numCitedBy": 101,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Automatic-text-location-in-images-and-video-frames-Jain-Yu",
            "title": {
                "fragments": [],
                "text": "Automatic text location in images and video frames"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "OCR accuracy : UNLV \u2019 s second annual test"
            },
            "venue": {
                "fragments": [],
                "text": "INFORM"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Survey on off-line word recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recogn"
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 5
                            }
                        ],
                        "text": "Wolf [101] points out that Lucas\u2019s scheme used in the ICDAR03 competition did not take into account possible one-to-many correspondences between ground truth objects and experimental results."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 190,
                                "start": 185
                            }
                        ],
                        "text": "Although more computationally expensive, micro-feature-based binarization shows more potential than traditional adaptive thresholding for domainspecific tasks such as document analysis [101]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Text detection in images taken from video sequences for semantic indexing"
            },
            "venue": {
                "fragments": [],
                "text": "PhD thesis, Institut National de Sciences Applique\u0301es de Lyon, France"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Document mosaic"
            },
            "venue": {
                "fragments": [],
                "text": "Image Vis Comput"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 78
                            }
                        ],
                        "text": "Several well-known competitions include the DOE/UNLV annual OCR accuracy test [73,74,82, 83], the Census/NIST OCR Systems Conference [30,98], text detection evaluations of TREC Video Track [125] (TRECVID) and by NIST, and the Robust Reading Competition of ICDAR 2003 [60]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "OCR accuracy: UNLV\u2019s second annual test. INFORM"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Camera-based analysis of text and documents"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Performance evaluation of object detection"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 67
                            }
                        ],
                        "text": "Myers considers the problem of temporal fragmentation and accuracy [64]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Metrics for evaluating the performance of video text recognition systems"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. symposium on document image understanding technology,"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Twenty years of document image analysis research in PAMI"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans Pattern Anal Mach Intell"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "ICDAR 2003 robust reading competition . In: Proc. ICDAR"
            },
            "venue": {
                "fragments": [],
                "text": "ICDAR 2003 robust reading competition . In: Proc. ICDAR"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 95
                            }
                        ],
                        "text": "In practice, when perspective is weak, the rectification can be done with simple approximation [34]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 14
                            }
                        ],
                        "text": "Both [33] and [34] find the strength of edges using morphological operators."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Morphologybased license plate detection from complex"
            },
            "venue": {
                "fragments": [],
                "text": "scenes. In: Proc. ICPR,"
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 108
                            }
                        ],
                        "text": "A simple solution is to treat every frame as a potential text frame and apply text region detection to them [48,57,58]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 156
                            }
                        ],
                        "text": "Lienhart and Wernicle compute the gradient map of an image and apply a neural network classifier on a sliding small window to identify possible text pixels [58]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 22
                            }
                        ],
                        "text": "Lienhart and Wernicle [58] use a different method to track text objects."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Wernicle A (2002) Localizing and segmenting text in images and videos"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans Circuits Syst Video Technol"
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 99
                            }
                        ],
                        "text": "Under the assumption that two images are related only by translation and rotation, Irani and Peleg [37] use an iterative approach to gradually match two images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "In [37], Irani and Peleg demonstrate the effect of super resolution in improving image quality."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Improving resolution by image registration. CVGIP Graphical Models and Image Processing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A fast and reliable planar registration method with applications to document stitch"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 93
                            }
                        ],
                        "text": "For example, a camera mounted over a writing surface can be used for handwriting recognition [26,70,96], or cameras can be used in whiteboard reading [89,97]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 39
                            }
                        ],
                        "text": "c Camera-based handwriting recognition [26,70,96]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Visual input for penbased computers"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans Pattern Anal Mach Intell"
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "tomatic license plate recognition"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans Intell Transport Syst"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Automatic identification and skew estimation of text lines in real scene images. Pattern Recog"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Image Vis Comput"
            },
            "venue": {
                "fragments": [],
                "text": "Image Vis Comput"
            },
            "year": 1999
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 56,
            "methodology": 37
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 132,
        "totalPages": 14
    },
    "page_url": "https://www.semanticscholar.org/paper/Camera-based-analysis-of-text-and-documents:-a-Liang-Doermann/82b6f95e805a92887f8efccf5a0dc8d5783676f5?sort=total-citations"
}