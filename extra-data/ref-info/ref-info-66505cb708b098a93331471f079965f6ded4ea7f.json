{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740300"
                        ],
                        "name": "D. Beymer",
                        "slug": "D.-Beymer",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Beymer",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Beymer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 115
                            }
                        ],
                        "text": "In this section we report the recognition rates obtained when virtual views were used in our view-based recognizer [10]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 34
                            }
                        ],
                        "text": "In our view-based face recognizer [10], the 15 example views of Fig."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 13
                            }
                        ],
                        "text": "[34], Beymer [10]) and deformable template approaches (Manjunath, et."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 60
                            }
                        ],
                        "text": "3 and originally used in the viewbased recognizer of Beymer [10]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 50
                            }
                        ],
                        "text": "As with the experiments with real views in Beymer [10], the recognition rates were recorded for a forced choice scenario { the recognizer always reports the best match."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 56
                            }
                        ],
                        "text": "Starting on the right, we repeat the result from Beymer [10] where we use 15 real views per person."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2546027,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9e5949d95c53dd041c721bf40e67b3966805e385",
            "isKey": false,
            "numCitedBy": 488,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "Researchers in computer vision and pattern recognition have worked on automatic techniques for recognizing human faces for the last 20 years. While some systems, especially template-based ones, have been quite successful on expressionless, frontal views of faces with controlled lighting, not much work has taken face recognizers beyond these narrow imaging conditions. Our goal is to build a face recognizer that works under varying pose, the difficult part of which is to handle face relations in depth. Building on successful template-based systems, our basic approach is to represent faces with templates from multiple model views that cover different poses from the viewing sphere. To recognize a novel view, the recognizer locates the eyes and nose features, uses these locations to geometrically register the input with model views, and then uses correlation on model templates to find the best match in the data base of people. Our system has achieved a recognition rate of 98% on a data base of 62 people containing 10 testing and 15 modeling views per person.<<ETX>>"
            },
            "slug": "Face-recognition-under-varying-pose-Beymer",
            "title": {
                "fragments": [],
                "text": "Face recognition under varying pose"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The goal is to build a face recognizer that works under varying pose, the difficult part of which is to handle face relations in depth."
            },
            "venue": {
                "fragments": [],
                "text": "1994 Proceedings of IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2331213"
                        ],
                        "name": "S. Edelman",
                        "slug": "S.-Edelman",
                        "structuredName": {
                            "firstName": "Shimon",
                            "lastName": "Edelman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Edelman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 10
                            }
                        ],
                        "text": "Lando and Edelman [26] have recently performed computational experiments to replicate earlier psychophysical results in [32]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 19
                            }
                        ],
                        "text": "Moses, Ullman, and Edelman [32] have performed this experiment using testing views at a variety of poses and lighting conditions."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 37
                            }
                        ],
                        "text": "Independent from our work, Lando and Edelman [26] have recently investigated the same overall question { generalization from a single view in face recognition { using a similar example-based technique for representing prior knowledge of faces."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 43
                            }
                        ],
                        "text": "The closest systems are those of Lando and Edelman [26] and Maurer and von der Malsburg [31]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 30
                            }
                        ],
                        "text": "The prior knowledge Lando and Edelman used to transform faces is similar to ours, views of prototype faces at standard and virtual views."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 97
                            }
                        ],
                        "text": "The view representation, in contrast to our template-based approach, is feature-based: Lando and Edelman use di erence of Gaussian features, and Maurer and von der Malsburg use a set of Gabor lters at a variety of scales and rotations (called \\jets\")."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 121
                            }
                        ],
                        "text": "This vectorized representation for 2D shape has been widely used, including network-based object recognition (Poggio and Edelman [37]), the linear combinations approach to recognition (Ullman and Basri [49], Poggio [35]), active shape models (Cootes and Taylor [15], Cootes, et al. [16]) and face recognition (Craw and Cameron [17][18])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 129
                            }
                        ],
                        "text": "This vectorized representation for 2D shape has been widely used, including network-based object recognition (Poggio and Edelman [37]), the linear combinations approach to recognition (Ullman and Basri [49], Poggio [35]), active shape models (Cootes and Taylor [15], Cootes, et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 4361875,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "50899b2355d6908a304bacb5e406f800f3dde558",
            "isKey": false,
            "numCitedBy": 1019,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "THE visual recognition of three-dimensional (3-D) objects on the basis of their shape poses at least two difficult problems. First, there is the problem of variable illumination, which can be addressed by working with relatively stable features such as intensity edges rather than the raw intensity images1,2. Second, there is the problem of the initially unknown pose of the object relative to the viewer. In one approach to this problem, a hypothesis is first made about the viewpoint, then the appearance of a model object from such a viewpoint is computed and compared with the actual image3\u20137. Such recognition schemes generally employ 3-D models of objects, but the automatic learning of 3-D models is itself a difficult problem8,9. To address this problem in computational vision, we have developed a scheme, based on the theory of approximation of multivariate functions, that learns from a small set of perspective views a function mapping any viewpoint to a standard view. A network equivalent to this scheme will thus 'recognize' the object on which it was trained from any viewpoint."
            },
            "slug": "A-network-that-learns-to-recognize-objects-Poggio-Edelman",
            "title": {
                "fragments": [],
                "text": "A network that learns to recognize three-dimensional objects"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A scheme is developed, based on the theory of approximation of multivariate functions, that learns from a small set of perspective views a function mapping any viewpoint to a standard view, and a network equivalent to this scheme will 'recognize' the object on which it was trained from any viewpoint."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5746940"
                        ],
                        "name": "M. Lando",
                        "slug": "M.-Lando",
                        "structuredName": {
                            "firstName": "Maria",
                            "lastName": "Lando",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lando"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2331213"
                        ],
                        "name": "S. Edelman",
                        "slug": "S.-Edelman",
                        "structuredName": {
                            "firstName": "Shimon",
                            "lastName": "Edelman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Edelman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 18
                            }
                        ],
                        "text": "Lando and Edelman [26] have recently performed computational experiments to replicate earlier psychophysical results in [32]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 45
                            }
                        ],
                        "text": "Independent from our work, Lando and Edelman [26] have recently investigated the same overall question { generalization from a single view in face recognition { using a similar example-based technique for representing prior knowledge of faces."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 51
                            }
                        ],
                        "text": "The closest systems are those of Lando and Edelman [26] and Maurer and von der Malsburg [31]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 20
                            }
                        ],
                        "text": "The prior knowledge Lando and Edelman used to transform faces is similar to ours, views of prototype faces at standard and virtual views."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 87
                            }
                        ],
                        "text": "The view representation, in contrast to our template-based approach, is feature-based: Lando and Edelman use di erence of Gaussian features, and Maurer and von der Malsburg use a set of Gabor lters at a variety of scales and rotations (called \\jets\")."
                    },
                    "intents": []
                }
            ],
            "corpusId": 12558836,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aa1eea18e569a13bb262e3e6b266509b36bf6bb4",
            "isKey": true,
            "numCitedBy": 79,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Abtraet. We describe a computational model of face recognition, which generalizes from single views of faces by taking advantage of prior experience with other faces. seen under a wider range of viewing conditions. The model represents face images by veclo~s of activities of graded overlapping receptive fields (m). It relies on high-spatial-frequency information to estimate the~viewing conditions, which are then used to normalize (via a h\u2019ansfonnation specific for faces), and identify, the low-spatial-frequency representation of the input. The class-specific msformatian approach allows the model to replicate a series of psychophysical findings on face recognition and constitutes an advance over cmnt face-recognition methods, which are incapable of generalization from a single example."
            },
            "slug": "Receptive-field-spaces-and-class-based-from-a-view-Lando-Edelman",
            "title": {
                "fragments": [],
                "text": "Receptive field spaces and class-based generalization from a single view in face recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A computational model of face recognition, which generalizes from single views of faces by taking advantage of prior experience with other faces, constitutes an advance over cmnt face-recognition methods, which are incapable of generalization from a single example."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3157627"
                        ],
                        "name": "Andrew C. Aitchison",
                        "slug": "Andrew-C.-Aitchison",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Aitchison",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew C. Aitchison"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144986880"
                        ],
                        "name": "I. Craw",
                        "slug": "I.-Craw",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Craw",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Craw"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5918615,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d174f808496df233b28a1c3ca0357040f779ffbd",
            "isKey": false,
            "numCitedBy": 7,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method of generating realistic views of the head of any individual from a single photograph of the individual and a generic model of a human head. The generic model is distorted so that it projects correctly onto the photograph. This is done by finding the least energy position of a springframe model, some of whose vertices are constrained to lie on particular features in the photograph. This gives us a model of the individual, which is then texture mapped using the original photograph to generate the final view. This is perceived to be a realistic view of the individual, even if the model is quite crude."
            },
            "slug": "Synthetic-Images-of-Faces-An-Approach-to-Face-Aitchison-Craw",
            "title": {
                "fragments": [],
                "text": "Synthetic Images of Faces - An Approach to Model-Based Face Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "A method of generating realistic views of the head of any individual from a single photograph of the individual and a generic model of a human head, which is then texture mapped using the original photograph to generate the final view."
            },
            "venue": {
                "fragments": [],
                "text": "BMVC"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144517651"
                        ],
                        "name": "T. Vetter",
                        "slug": "T.-Vetter",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Vetter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Vetter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 60
                            }
                        ],
                        "text": "Mathematically, we model this using a matrix L\ny = LY; (2)\n1Vetter and Poggio [51] have explored the implications of 3D linear combinations of shape on image grey levels, or texture."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 18
                            }
                        ],
                        "text": "Vetter and Poggio [51] have already done some work in applying the linear class idea to both shape and texture."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 110
                            }
                        ],
                        "text": "The ideas presented in this section were developed by the authors and also independently by Vetter and Poggio [51]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 19
                            }
                        ],
                        "text": "1Vetter and Poggio [51] have explored the implications of 3D linear combinations of shape on image grey levels, or texture."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10047234,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "617b34332fcd1cb196f93656ee1d49561b81ebf8",
            "isKey": false,
            "numCitedBy": 471,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "The need to generate new views of a 3D object from a single real image arises in several fields, including graphics and object recognition. While the traditional approach relies on the use of 3D models, simpler techniques are applicable under restricted conditions. The approach exploits image transformations that are specific to the relevant object class, and learnable from example views of other \"prototypical\" objects of the same class. In this paper, we introduce such a technique by extending the notion of linear class proposed by the authors (1992). For linear object classes, it is shown that linear transformations can be learned exactly from a basis set of 2D prototypical views. We demonstrate the approach on artificial objects and then show preliminary evidence that the technique can effectively \"rotate\" high-resolution face images from a single 2D view."
            },
            "slug": "Linear-Object-Classes-and-Image-Synthesis-From-a-Vetter-Poggio",
            "title": {
                "fragments": [],
                "text": "Linear Object Classes and Image Synthesis From a Single Example Image"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "For linear object classes, it is shown that linear transformations can be learned exactly from a basis set of 2D prototypical views and preliminary evidence that the technique can effectively \"rotate\" high-resolution face images from a single 2D view is shown."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1830709"
                        ],
                        "name": "A. Lanitis",
                        "slug": "A.-Lanitis",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Lanitis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Lanitis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144482985"
                        ],
                        "name": "C. Taylor",
                        "slug": "C.-Taylor",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Taylor",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7205190"
                        ],
                        "name": "Tim Cootes",
                        "slug": "Tim-Cootes",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Cootes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tim Cootes"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 136
                            }
                        ],
                        "text": "The iterative processing of shape and texture is similar to the active shape models of Cootes and Taylor [15], Cootes, et al. [16], and Lanitis, Taylor, and Cootes [27]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 249,
                                "start": 243
                            }
                        ],
                        "text": "This vectorized representation for 2D shape has been widely used, including network-based object recognition (Poggio and Edelman [37]), the linear combinations approach to recognition (Ullman and Basri [49], Poggio [35]), active shape models (Cootes and Taylor [15], Cootes, et al. [16]) and face recognition (Craw and Cameron [17][18])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 38
                            }
                        ],
                        "text": "[16], and Lanitis, Taylor, and Cootes [27]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 18648884,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bf474fd5b89b6b38bec83cd1e8d3b11166ba2a1a",
            "isKey": false,
            "numCitedBy": 220,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Face images are difficult to interpret because they are highly variable. Sources of variability include individual appearance, 3D pose, facial expression and lighting. We describe a compact parametrised model of facial appearance which takes into account all these sources of variability. The model represents both shape and grey-level appearance and is created by performing a statistical analysis over a training set of face images. A robust multi-resolution search algorithm is used to fit the model to faces in new images. This allows the main facial features to be located and a set of shape and grey-level appearance parameters to be recovered. A good approximation to a given face can be reconstructed using less than 100 of these parameters. This representation can be used for tasks such as image coding, person identification, pose recovery, gender recognition and expression recognition. The system performs well on all the tasks listed above.<<ETX>>"
            },
            "slug": "A-unified-approach-to-coding-and-interpreting-face-Lanitis-Taylor",
            "title": {
                "fragments": [],
                "text": "A unified approach to coding and interpreting face images"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A compact parametrised model of facial appearance which takes into account all sources of variability and can be used for tasks such as image coding, person identification, pose recovery, gender recognition and expression recognition is described."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE International Conference on Computer Vision"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2957934"
                        ],
                        "name": "Y. Moses",
                        "slug": "Y.-Moses",
                        "structuredName": {
                            "firstName": "Yael",
                            "lastName": "Moses",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Moses"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743045"
                        ],
                        "name": "S. Ullman",
                        "slug": "S.-Ullman",
                        "structuredName": {
                            "firstName": "Shimon",
                            "lastName": "Ullman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ullman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2331213"
                        ],
                        "name": "S. Edelman",
                        "slug": "S.-Edelman",
                        "structuredName": {
                            "firstName": "Shimon",
                            "lastName": "Edelman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Edelman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 120
                            }
                        ],
                        "text": "Lando and Edelman [26] have recently performed computational experiments to replicate earlier psychophysical results in [32]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 27
                            }
                        ],
                        "text": "Moses, Ullman, and Edelman [32] have performed this experiment using testing views at a variety of poses and lighting conditions."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 37
                            }
                        ],
                        "text": "Independent from our work, Lando and Edelman [26] have recently investigated the same overall question { generalization from a single view in face recognition { using a similar example-based technique for representing prior knowledge of faces."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 43
                            }
                        ],
                        "text": "The closest systems are those of Lando and Edelman [26] and Maurer and von der Malsburg [31]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 30
                            }
                        ],
                        "text": "The prior knowledge Lando and Edelman used to transform faces is similar to ours, views of prototype faces at standard and virtual views."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 97
                            }
                        ],
                        "text": "The view representation, in contrast to our template-based approach, is feature-based: Lando and Edelman use di erence of Gaussian features, and Maurer and von der Malsburg use a set of Gabor lters at a variety of scales and rotations (called \\jets\")."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 121
                            }
                        ],
                        "text": "This vectorized representation for 2D shape has been widely used, including network-based object recognition (Poggio and Edelman [37]), the linear combinations approach to recognition (Ullman and Basri [49], Poggio [35]), active shape models (Cootes and Taylor [15], Cootes, et al. [16]) and face recognition (Craw and Cameron [17][18])."
                    },
                    "intents": []
                }
            ],
            "corpusId": 8411547,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "13f752e5ec3b6187014e97ae5d719500f648f41d",
            "isKey": true,
            "numCitedBy": 129,
            "numCiting": 78,
            "paperAbstract": {
                "fragments": [],
                "text": "An image of a face depends not only on its shape, but also on the viewpoint, illumination conditions, and facial expression. A face recognition system must overcome the changes in face appearance induced by these factors. Two related questions were investigated: the capacity of the human visual system to generalize the recognition of faces to novel images, and the level at which this generalization occurs. This problem was approached by comparing the identification and generalization capacity for upright and inverted faces. For upright faces, remarkably good generalization to novel conditions was found. For inverted faces, the generalization to novel views was significantly worse for both new illumination and viewpoint, although the performance on the training images was similar to that on the upright condition. The results indicate that at least some of the processes that support generalization across viewpoint and illumination are neither universal (because subjects did not generalize as easily for inverted faces as for upright ones) nor strictly object specific (because in upright faces nearly perfect generalization was possible from a single view, by itself insufficient for building a complete object-specific model). It is proposed that generalization in face recognition occurs at an intermediate level that is applicable to a class of objects, and that at this level upright and inverted faces initially constitute distinct object classes."
            },
            "slug": "Generalization-to-Novel-Images-in-Upright-and-Faces-Moses-Ullman",
            "title": {
                "fragments": [],
                "text": "Generalization to Novel Images in Upright and Inverted Faces"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is proposed that generalization in face recognition occurs at an intermediate level that is applicable to a class of objects, and that at this level upright and inverted faces initially constitute distinct object classes."
            },
            "venue": {
                "fragments": [],
                "text": "Perception"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144517651"
                        ],
                        "name": "T. Vetter",
                        "slug": "T.-Vetter",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Vetter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Vetter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 18
                            }
                        ],
                        "text": "Poggio and Vetter [39] have discussed this linear combinations approach in the case where only one example view is available for an object, laying the groundwork for virtual views."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 28
                            }
                        ],
                        "text": "Following Poggio and Vetter [39], we call these synthesized views virtual views."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 172
                            }
                        ],
                        "text": "For the single real view, an o -center view was favored over, say, a frontal view because of the recognition results for bilaterally symmetric objects of Poggio and Vetter [39]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 71
                            }
                        ],
                        "text": "Using this approach, as discussed in Poggio [36] and Poggio and Vetter [39], it is possible to \\learn\" a direct mapping from standard pose to a particular virtual pose."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 162
                            }
                        ],
                        "text": "Another way to formulate the solution as a direct mapping is to train a network to learn the association between standard and virtual pose (see Poggio and Vetter [39])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 49
                            }
                        ],
                        "text": "This idea was rst developed by Poggio and Vetter [39]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3893740,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "82962da5c273a9e6627a040d56c8a7973fe22440",
            "isKey": false,
            "numCitedBy": 192,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "In this note we discuss how recognition can be achieved from a single 2D model view exploiting prior knowledge of an object''s structure (e.g. symmetry). We prove that for any bilaterally symmetric 3D object one non- accidental 2D model view is sufficient for recognition. Symmetries of higher order allow the recovery of structure from one 2D view. Linear transformations can be learned exactly from a small set of examples in the case of \"linear object classes\" and used to produce new views of an object from a single view."
            },
            "slug": "Recognition-and-Structure-from-one-2D-Model-View:-Poggio-Vetter",
            "title": {
                "fragments": [],
                "text": "Recognition and Structure from one 2D Model View: Observations on Prototypes, Object Classes and Symmetries"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is proved that for any bilaterally symmetric 3D object one non- accidental 2D model view is sufficient for recognition and linear transformations can be learned exactly from a small set of examples in the case of \"linear object classes\"."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144097660"
                        ],
                        "name": "M. Turk",
                        "slug": "M.-Turk",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Turk",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Turk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144994682"
                        ],
                        "name": "A. Pentland",
                        "slug": "A.-Pentland",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Pentland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pentland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 26127529,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a6f1dfcc44277d4cfd8507284d994c9283dc3a2f",
            "isKey": false,
            "numCitedBy": 14955,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We have developed a near-real-time computer system that can locate and track a subject's head, and then recognize the person by comparing characteristics of the face to those of known individuals. The computational approach taken in this system is motivated by both physiology and information theory, as well as by the practical requirements of near-real-time performance and accuracy. Our approach treats the face recognition problem as an intrinsically two-dimensional (2-D) recognition problem rather than requiring recovery of three-dimensional geometry, taking advantage of the fact that faces are normally upright and thus may be described by a small set of 2-D characteristic views. The system functions by projecting face images onto a feature space that spans the significant variations among known face images. The significant features are known as \"eigenfaces,\" because they are the eigenvectors (principal components) of the set of faces; they do not necessarily correspond to features such as eyes, ears, and noses. The projection operation characterizes an individual face by a weighted sum of the eigenface features, and so to recognize a particular face it is necessary only to compare these weights to those of known individuals. Some particular advantages of our approach are that it provides for the ability to learn and later recognize new faces in an unsupervised manner, and that it is easy to implement using a neural network architecture."
            },
            "slug": "Eigenfaces-for-Recognition-Turk-Pentland",
            "title": {
                "fragments": [],
                "text": "Eigenfaces for Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "A near-real-time computer system that can locate and track a subject's head, and then recognize the person by comparing characteristics of the face to those of known individuals, and that is easy to implement using a neural network architecture."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Cognitive Neuroscience"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50591689"
                        ],
                        "name": "B. S. Manjunath",
                        "slug": "B.-S.-Manjunath",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Manjunath",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. S. Manjunath"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9215658"
                        ],
                        "name": "R. Chellappa",
                        "slug": "R.-Chellappa",
                        "structuredName": {
                            "firstName": "Rama",
                            "lastName": "Chellappa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Chellappa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704573"
                        ],
                        "name": "C. Malsburg",
                        "slug": "C.-Malsburg",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Malsburg",
                            "middleNames": [
                                "von",
                                "der"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Malsburg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 68
                            }
                        ],
                        "text": "In the exible matching approach (von der Malsburg and collaborators [30][25]), the input image is deformed in 2D to match the example view."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "In [30], the deformation is driven by a matching of local \\end-stop\" features so that the resulting transformation between model and input is like a 2D warp rather than a global, rigid transform."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[30], who obtain 86% on a database of 86 people, and Pentland, et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 46325945,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a735c80fd1a467d82efb3960faf88a522f690be2",
            "isKey": false,
            "numCitedBy": 393,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "A feature-based approach to face recognition in which the features are derived from the intensity data without assuming any knowledge of the face structure is presented. The feature extraction model is biologically motivated, and the locations of the features often correspond to salient facial features such as the eyes, nose, etc. Topological graphs are used to represent relations between features, and a simple deterministic graph-matching scheme that exploits the basic structure is used to recognize familiar faces from a database. Each of the stages in the system can be fully implemented in parallel to achieve real-time recognition. Experimental results for a 128*128 image with very little noise are evaluated.<<ETX>>"
            },
            "slug": "A-feature-based-approach-to-face-recognition-Manjunath-Chellappa",
            "title": {
                "fragments": [],
                "text": "A feature based approach to face recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A feature-based approach to face recognition in which the features are derived from the intensity data without assuming any knowledge of the face structure is presented."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144994682"
                        ],
                        "name": "A. Pentland",
                        "slug": "A.-Pentland",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Pentland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pentland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780935"
                        ],
                        "name": "B. Moghaddam",
                        "slug": "B.-Moghaddam",
                        "structuredName": {
                            "firstName": "Baback",
                            "lastName": "Moghaddam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Moghaddam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738894"
                        ],
                        "name": "T. Starner",
                        "slug": "T.-Starner",
                        "structuredName": {
                            "firstName": "Thad",
                            "lastName": "Starner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Starner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 136280,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b0bf5d558220d39698ce96d59ee5772e8e1a0663",
            "isKey": false,
            "numCitedBy": 2234,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe experiments with eigenfaces for recognition and interactive search in a large-scale face database. Accurate visual recognition is demonstrated using a database of O(10/sup 3/) faces. The problem of recognition under general viewing orientation is also examined. A view-based multiple-observer eigenspace technique is proposed for use in face recognition under variable pose. In addition, a modular eigenspace description technique is used which incorporates salient features such as the eyes, nose and mouth, in an eigenfeature layer. This modular representation yields higher recognition rates as well as a more robust framework for face recognition. An automatic feature extraction technique using feature eigentemplates is also demonstrated.<<ETX>>"
            },
            "slug": "View-based-and-modular-eigenspaces-for-face-Pentland-Moghaddam",
            "title": {
                "fragments": [],
                "text": "View-based and modular eigenspaces for face recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A modular eigenspace description technique is used which incorporates salient features such as the eyes, nose and mouth, in an eigenfeature layer, which yields higher recognition rates as well as a more robust framework for face recognition."
            },
            "venue": {
                "fragments": [],
                "text": "1994 Proceedings of IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055501916"
                        ],
                        "name": "Thomas Maurer",
                        "slug": "Thomas-Maurer",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Maurer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Maurer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704573"
                        ],
                        "name": "C. Malsburg",
                        "slug": "C.-Malsburg",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Malsburg",
                            "middleNames": [
                                "von",
                                "der"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Malsburg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 41
                            }
                        ],
                        "text": "In the exible matching approach (von der Malsburg and collaborators [30][25]), the input image is deformed in 2D to match the example view."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 41
                            }
                        ],
                        "text": "In addition, Maurer and von der Malsburg [31] have investigated a technique for transforming their \\jet\" features across rotations in depth."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 88
                            }
                        ],
                        "text": "The closest systems are those of Lando and Edelman [26] and Maurer and von der Malsburg [31]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 164
                            }
                        ],
                        "text": "The view representation, in contrast to our template-based approach, is feature-based: Lando and Edelman use di erence of Gaussian features, and Maurer and von der Malsburg use a set of Gabor lters at a variety of scales and rotations (called \\jets\")."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 19
                            }
                        ],
                        "text": "Maurer and von der Malsburg transform their Gabor jet features by approximating the facial surface at each feature point as a plane and then estimating how the Gabor jet changes as the plane rotates in 3D."
                    },
                    "intents": []
                }
            ],
            "corpusId": 11534904,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "04c306621210fd9dc96b6106e1f5a6bd745ff5dd",
            "isKey": true,
            "numCitedBy": 70,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method for recognizing objects (faces) on the basis of just one stored view, in spite of rotation in depth. The method is not based on the construction of a three-dimensional model for the object. Our recognition results represent a signi cant improvement over a previous system developed in our laboratory. We achieve this with the help of a simple assumption about the transformation of local feature vectors with rotation in depth."
            },
            "slug": "Single-View-Based-Recognition-of-Faces-Rotated-in-Maurer-Malsburg",
            "title": {
                "fragments": [],
                "text": "Single-View Based Recognition of Faces Rotated in Depth"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "This work presents a method for recognizing objects (faces) on the basis of just one stored view, in spite of rotation in depth, with the help of a simple assumption about the transformation of local feature vectors with rotation in Depth."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36427051"
                        ],
                        "name": "Peter W. Hallinan",
                        "slug": "Peter-W.-Hallinan",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Hallinan",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter W. Hallinan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 46324024,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "236c132eda073ad7e80fcc45a248ac2baea9a786",
            "isKey": false,
            "numCitedBy": 342,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "When recognizing a fixed object from a fixed viewpoint, the dominant source of variation in image intensity is lighting changes. We propose a low-dimensional model for human faces that can both synthesize a face image when given lighting conditions and can estimate lighting conditions when given a face image. The model can handle non-Lambertian and self-shadowing surfaces such as faces because it does not make any assumptions about either the surface geometry or bidirectional reflectance function. The model can be adapted to handle any arbitrary lighting condition, and is easily extendable to any other viewpoint or to any other object.<<ETX>>"
            },
            "slug": "A-low-dimensional-representation-of-human-faces-for-Hallinan",
            "title": {
                "fragments": [],
                "text": "A low-dimensional representation of human faces for arbitrary lighting conditions"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A low-dimensional model for human faces is proposed that can both synthesize a face image when given lighting conditions and can estimate lighting conditions when given a face images."
            },
            "venue": {
                "fragments": [],
                "text": "1994 Proceedings of IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740300"
                        ],
                        "name": "D. Beymer",
                        "slug": "D.-Beymer",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Beymer",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Beymer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 93
                            }
                        ],
                        "text": "The automatic segments shown on the right were located using our face vectorizer from Beymer [9]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 7
                            }
                        ],
                        "text": "Beymer [9] provides the details; here we only set the problem up and sketch the solution."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 32
                            }
                        ],
                        "text": "Our face vectorizer (see Beymer [9]), which uses optical ow as a subroutine, is also used to automatically compute the vectorized representation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 29
                            }
                        ],
                        "text": "Our image vectorizer (Beymer [9]) is used to solve for the correspondences ystd n std between in and standard shape ystd."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 13
                            }
                        ],
                        "text": "1 and Beymer [9]) to automatically locate features."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13952915,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "34a823d412c2134f87ab4f5e8a87c8a203a08b5c",
            "isKey": false,
            "numCitedBy": 72,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "The correspondence problem in computer vision is basically a matching task between two or more sets of features. In this paper, we introduce a vectorized image representation, which is a feature-based representation where correspondence has been established with respect to a reference image. This representation has two components: (1) shape, or (x, y) feature locations, and (2) texture, defined as the image grey levels mapped onto the standard reference image. This paper explores an automatic technique for ``vectorizing'''' face images. Our face vectorizer alternates back and forth between computation steps for shape and texture, and a key idea is to structure the two computations so that each one uses the output of the other. A hierarchical coarse-to-fine implementation is discussed, and applications are presented to the problems of facial feature detection and registration of two arbitrary faces."
            },
            "slug": "Vectorizing-Face-Images-by-Interleaving-Shape-and-Beymer",
            "title": {
                "fragments": [],
                "text": "Vectorizing Face Images by Interleaving Shape and Texture Computations"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "This paper introduces a vectorized image representation, which is a feature-based representation where correspondence has been established with respect to a reference image, and explores an automatic technique for ``vectorizing'' face images."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743045"
                        ],
                        "name": "S. Ullman",
                        "slug": "S.-Ullman",
                        "structuredName": {
                            "firstName": "Shimon",
                            "lastName": "Ullman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ullman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760994"
                        ],
                        "name": "R. Basri",
                        "slug": "R.-Basri",
                        "structuredName": {
                            "firstName": "Ronen",
                            "lastName": "Basri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Basri"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8989489,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d781d5e651e12bf666cf993ae307db785113b9ae",
            "isKey": false,
            "numCitedBy": 951,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "An approach to visual object recognition in which a 3D object is represented by the linear combination of 2D images of the object is proposed. It is shown that for objects with sharp edges as well as with smooth bounding contours, the set of possible images of a given object is embedded in a linear space spanned by a small number of views. For objects with sharp edges, the linear combination representation is exact. For objects with smooth boundaries, it is an approximation that often holds over a wide range of viewing angles. Rigid transformations (with or without scaling) can be distinguished from more general linear transformations of the object by testing certain constraints placed on the coefficients of the linear combinations. Three alternative methods of determining the transformation that matches a model to a given image are proposed. >"
            },
            "slug": "Recognition-by-Linear-Combinations-of-Models-Ullman-Basri",
            "title": {
                "fragments": [],
                "text": "Recognition by Linear Combinations of Models"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "An approach to visual object recognition in which a 3D object is represented by the linear combination of 2D images of the object is proposed and it is shown that for objects with sharp edges as well as with smooth bounding contours, the set of possible images of a given object is embedded in a linear space spanned by a small number of views."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "121478544"
                        ],
                        "name": "M. Shackleton",
                        "slug": "M.-Shackleton",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Shackleton",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Shackleton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50748720"
                        ],
                        "name": "W. Welsh",
                        "slug": "W.-Welsh",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Welsh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Welsh"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 148
                            }
                        ],
                        "text": "This strategy for representing texture has been used, for example, in the face recognition works of Craw and Cameron [17], and Shackleton and Welsh [42]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11500685,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a78236ff0815f2ccbc465f7fb23a7d827a670906",
            "isKey": false,
            "numCitedBy": 69,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "A facial feature classification technique that independently captures both the geometric configuration and the image detail of a particular feature is described. The geometric configuration is first extracted by fitting a deformable template to the shape of the feature (for example, an eye) in the image. This information is then used to geometrically normalize the image in such a way that the feature in the image attains a standard shape. The normalized image of the facial feature is then classified in terms of a set of principal components previously obtained from a representative set of training images of similar features. This classification stage yields a representation vector which can be used for recognition matching of the feature in terms of image detail alone without the complication of changes in facial expression. Implementation of the system is described and results are given for its application to a set of test faces. These results show that features can be reliably recognized using the representation vectors obtained.<<ETX>>"
            },
            "slug": "Classification-of-facial-features-for-recognition-Shackleton-Welsh",
            "title": {
                "fragments": [],
                "text": "Classification of facial features for recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "A facial feature classification technique that independently captures both the geometric configuration and the image detail of a particular feature is described and results show that features can be reliably recognized using the representation vectors obtained."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144517651"
                        ],
                        "name": "T. Vetter",
                        "slug": "T.-Vetter",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Vetter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Vetter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1684073"
                        ],
                        "name": "A. Hurlbert",
                        "slug": "A.-Hurlbert",
                        "structuredName": {
                            "firstName": "Anya",
                            "lastName": "Hurlbert",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hurlbert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 23650102,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7e7bc6278d4bc7a5e1cdca46ef4230246e2d3bde",
            "isKey": false,
            "numCitedBy": 77,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This report describes the main features of a view-based model of object recognition. The model does not attempt to account for specific cortical structures; it tries to capture general properties to be expected in a biological architecture for object recognition. The basic module is a regularization network (RBF-like; see Poggio and Girosi, 1989; Poggio, 1990) in which each of the hidden units is broadly tuned to a specific view of the object to be recognized. The network output, which may be largely view independent, is first described in terms of some simple simulations. The following refinements and details of the basic module are then discussed: (1) some of the units may represent only components of views of the object--the optimal stimulus for the unit, its \"center,\" is effectively a complex feature; (2) the units' properties are consistent with the usual description of cortical neurons as tuned to multidimensional optimal stimuli and may be realized in terms of plausible biophysical mechanisms; (3) in learning to recognize new objects, preexisting centers may be used and modified, but also new centers may be created incrementally so as to provide maximal view invariance; (4) modules are part of a hierarchical structure--the output of a network may be used as one of the inputs to another, in this way synthesizing increasingly complex features and templates; (5) in several recognition tasks, in particular at the basic level, a single center using view-invariant features may be sufficient."
            },
            "slug": "View-based-models-of-3D-object-recognition:-to-Vetter-Hurlbert",
            "title": {
                "fragments": [],
                "text": "View-based models of 3D object recognition: invariance to imaging transformations."
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "The main features of a view-based model of object recognition, a regularization network in which each of the hidden units is broadly tuned to a specific view of the object to be recognized, are described."
            },
            "venue": {
                "fragments": [],
                "text": "Cerebral cortex"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144975752"
                        ],
                        "name": "V. Bruce",
                        "slug": "V.-Bruce",
                        "structuredName": {
                            "firstName": "Vicki",
                            "lastName": "Bruce",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Bruce"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 6
                            }
                        ],
                        "text": "Bruce [12] performs a similar experiment where the subject is asked whether a face had appeared during training, and detection rates go down to either 76% or 60%, depending on the amount of pose/expression di erence between the testing and training views."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 41644163,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "842c9afcbe64da82ee55ef8945fe4157cc5c1db7",
            "isKey": false,
            "numCitedBy": 450,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Two experiments examined the effect of recognition accuracy and latency of changing the view of faces between presentation and test. In Expt 1, all the faces were unfamiliar to the subjects. Faces at test were either unchanged, or changed in angle (e.g. full face to 3/4), expression (e.g. smiling to unsmiling) or both. Unchanged faces were recognized more quickly and accurately than faces with a change in angle or expression which were in turn better than faces with both changed. In Expt 2, half the faces were highly familiar to the subjects, and at test unfamiliar and familiar faces were either unchanged or changed in both angle and expression. Unfamiliar faces were recognized more slowly and less accurately if changed at test, while familiar faces were recognized more slowly though no less accurately if change (though performance was effectively at ceiling). Familiar faces were recognized more quickly and accurately than unfamiliar, though false positive rates and rejection latencies were similar for familiars and unfamiliars. The results are discussed in terms of the combination of information from \"pictorial', \"structural', and \"semantic' and \"name' codes."
            },
            "slug": "Changing-faces:-visual-and-non-visual-coding-in-Bruce",
            "title": {
                "fragments": [],
                "text": "Changing faces: visual and non-visual coding processes in face recognition."
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "The effect of recognition accuracy and latency of changing the view of faces between presentation and test and Familiar faces were recognized more quickly and accurately than unfamiliar, though false positive rates and rejection latencies were similar for familiars and unfamiliars."
            },
            "venue": {
                "fragments": [],
                "text": "British journal of psychology"
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3140335"
                        ],
                        "name": "A. Shashua",
                        "slug": "A.-Shashua",
                        "structuredName": {
                            "firstName": "Amnon",
                            "lastName": "Shashua",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Shashua"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 156
                            }
                        ],
                        "text": "In this paper we use a dense representation of one feature per pixel, a representation originally suggested to us by the object recognition work of Shashua [43]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 60
                            }
                        ],
                        "text": "This relative shape representation has been used by Beymer, Shashua, and Poggio [8] in an example-based approach to image analysis and synthesis."
                    },
                    "intents": []
                }
            ],
            "corpusId": 15785342,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6726e2264c3a35c6a4b4da88520706665005835a",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper presents a simple model for recovering affine shape and correspondence from two orthographic views of a 3D object. It is shown that four corresponding points along two orthographic views, taken under similar illumination conditions, determine affine shape and correspondence for all other points. The scheme is useful for purposes of visual recognition by generating novel views of an object given two model views. It is also shown that the scheme can handle objects with smooth boundaries, to a good approximation, without introducing any modifications or additional model views."
            },
            "slug": "Correspondence-and-Affine-Shape-from-Two-Views:-and-Shashua",
            "title": {
                "fragments": [],
                "text": "Correspondence and Affine Shape from Two Orthographic Views: Motion and Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "It is shown that four corresponding points along two orthographic views, taken under similar illumination conditions, determine affine shape and correspondence for all other points and the scheme is useful for purposes of visual recognition by generating novel views of an object given two model views."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144986880"
                        ],
                        "name": "I. Craw",
                        "slug": "I.-Craw",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Craw",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Craw"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066105510"
                        ],
                        "name": "Peter Cameron",
                        "slug": "Peter-Cameron",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Cameron",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter Cameron"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 49
                            }
                        ],
                        "text": "[16]) and face recognition (Craw and Cameron [17][18])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8657476,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c66dcbbfbe7414ae9fcd769fc72fd2c2123f471d",
            "isKey": false,
            "numCitedBy": 68,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a coding scheme to index face images for subsequent retrieval, which seems effective, under some conditions, at coding the faces themselves, rather than particular face images, and uses typically 100 bytes. We report tests searching a pool of 100 faces, using as cue a different image of a face in the pool, taken 10 years later. In two of three tests with different faces, the target face best matches the corresponding cue."
            },
            "slug": "Face-Recognition-by-Computer-Craw-Cameron",
            "title": {
                "fragments": [],
                "text": "Face Recognition by Computer"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "A coding scheme to index face images for subsequent retrieval seems effective, under some conditions, at coding the faces themselves, rather than particular face images, and uses typically 100 bytes."
            },
            "venue": {
                "fragments": [],
                "text": "BMVC"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2675319"
                        ],
                        "name": "Chii-Yuan Kang",
                        "slug": "Chii-Yuan-Kang",
                        "structuredName": {
                            "firstName": "Chii-Yuan",
                            "lastName": "Kang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chii-Yuan Kang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9407097"
                        ],
                        "name": "Yung-Sheng Chen",
                        "slug": "Yung-Sheng-Chen",
                        "structuredName": {
                            "firstName": "Yung-Sheng",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yung-Sheng Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144887272"
                        ],
                        "name": "W. Hsu",
                        "slug": "W.-Hsu",
                        "structuredName": {
                            "firstName": "Wen-Hsing",
                            "lastName": "Hsu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Hsu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 10957251,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1bff82f1d7426c48bc1484970393d18e31ac2e98",
            "isKey": false,
            "numCitedBy": 2,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "An automatic approach used for making a human face model is proposed. It is a system with input forms of a 2.5D range data set and a 2D color image. The information obtained from the range data set includes the anatomical sites of features and the geometrical data of face. The information extracted from the 2D color image includes the boundaries of facial features and the attributes of facial textures. These two sources are integrated to form a volumetric facial model.<<ETX>>"
            },
            "slug": "Mapping-a-lifelike-2.5-D-human-face-via-an-approach-Kang-Chen",
            "title": {
                "fragments": [],
                "text": "Mapping a lifelike 2.5 D human face via an automatic approach"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "An automatic approach used for making a human face model is proposed, with input forms of a 2.5D range data set and a 2D color image integrated to form a volumetric facial model."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145319478"
                        ],
                        "name": "Michael J. Jones",
                        "slug": "Michael-J.-Jones",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jones",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Jones"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 69
                            }
                        ],
                        "text": "This is similar to performance-driven animation (Williams [52]), and Poggio and Brunelli [38], who call it parallel deformation, have suggested it as a computer graphics tool for animating objects when provided with just one view."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 170
                            }
                        ],
                        "text": "Using an example-based approach to bypass 3D models for 3D object recognition was rst explored in the linear combinations approach to recognition (Ullman and Basri [49], Poggio [35])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 220,
                                "start": 214
                            }
                        ],
                        "text": "Existing work in face recognition has demonstrated good recognition performance on frontal, expressionless views of faces with controlled lighting (see Baron [4], Turk and Pentland [48], Bichsel [11], Brunelli and Poggio [14], and Gilbert and Yang [20])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 73
                            }
                        ],
                        "text": "This relative shape representation has been used by Beymer, Shashua, and Poggio [8] in an example-based approach to image analysis and synthesis."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 0
                            }
                        ],
                        "text": "Poggio and Vetter [39] have discussed this linear combinations approach in the case where only one example view is available for an object, laying the groundwork for virtual views."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 59
                            }
                        ],
                        "text": "The technique has been explored previously by Brunelli and Poggio [38] within the context of an \\example-based\" approach to computer graphics and by researchers in performance-driven animation (Williams [52][53], Patterson, Litwinowicz,\nand Greene [33])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 10
                            }
                        ],
                        "text": "Following Poggio and Vetter [39], we call these synthesized views virtual views."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 154
                            }
                        ],
                        "text": "For the single real view, an o -center view was favored over, say, a frontal view because of the recognition results for bilaterally symmetric objects of Poggio and Vetter [39]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 71
                            }
                        ],
                        "text": "Mathematically, we model this using a matrix L\ny = LY; (2)\n1Vetter and Poggio [51] have explored the implications of 3D linear combinations of shape on image grey levels, or texture."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 103
                            }
                        ],
                        "text": "The ideas presented in this section were developed by the authors and also independently by Vetter and Poggio [51]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 37
                            }
                        ],
                        "text": "Using this approach, as discussed in Poggio [36] and Poggio and Vetter [39], it is possible to \\learn\" a direct mapping from standard pose to a particular virtual pose."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 110
                            }
                        ],
                        "text": "This vectorized representation for 2D shape has been widely used, including network-based object recognition (Poggio and Edelman [37]), the linear combinations approach to recognition (Ullman and Basri [49], Poggio [35]), active shape models (Cootes and Taylor [15], Cootes, et al. [16]) and face recognition (Craw and Cameron [17][18])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 144
                            }
                        ],
                        "text": "Another way to formulate the solution as a direct mapping is to train a network to learn the association between standard and virtual pose (see Poggio and Vetter [39])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 17
                            }
                        ],
                        "text": "Jones and Poggio [23] also describe a related system that uses linear combinations of prototype shapes to analyze line drawings."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 15
                            }
                        ],
                        "text": "This motivated Poggio and Vetter to introduce the idea of using prior knowledge of object class to generate virtual views."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 31
                            }
                        ],
                        "text": "This idea was rst developed by Poggio and Vetter [39]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 4754199,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a393ad864f6fe95002e8d4412f3ebe5c42f699d6",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a technique for finding pixelwise correspondences between two images by using models of objects of the same class to guide the search. The object models are \"learned\" from example images (also called prototypes) of an object class. The models consist of a linear combination of prototypes. The flow fields giving pixelwise correspondences between a base prototype and each of the other prototypes must be given. A novel image of an object of the same class is matched to a model by minimizing an error between the novel image and the current guess for the closest model image. Currently, the algorithm applies to line drawings of objects. An extension to real grey level images is discussed.<<ETX>>"
            },
            "slug": "Model-based-matching-of-line-drawings-by-linear-of-Jones-Poggio",
            "title": {
                "fragments": [],
                "text": "Model-based matching of line drawings by linear combinations of prototypes"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "A technique for finding pixelwise correspondences between two images by using models of objects of the same class to guide the search, which applies to line drawings of objects."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE International Conference on Computer Vision"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2086700338"
                        ],
                        "name": "J. M. Gilbert",
                        "slug": "J.-M.-Gilbert",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Gilbert",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. M. Gilbert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2157814340"
                        ],
                        "name": "W. Yang",
                        "slug": "W.-Yang",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Yang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 252,
                                "start": 248
                            }
                        ],
                        "text": "Existing work in face recognition has demonstrated good recognition performance on frontal, expressionless views of faces with controlled lighting (see Baron [4], Turk and Pentland [48], Bichsel [11], Brunelli and Poggio [14], and Gilbert and Yang [20])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6551109,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "62f46db7bf0ad47c8c06f5196596ac7518e13e61",
            "isKey": false,
            "numCitedBy": 48,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "A real-time face recognition system has been implemented on an IBM compatible personal computer with a video camera, image digitizer, and custom VLSI image correlator chip. With a single frontal facial image under semicontrolled lighting conditions, the system performs (i) image preprocessing and template extraction, (ii) template correlation with a database of 173 images, and (iii) postprocessing of correlation results to identify the user. System performance issues including image preprocessing, face recognition algorithm, software development, and VLSI hardware implementation are addressed. In particular, the parallel, fully pipelined VLSI image correlator is able to perform 340 Mop/second and achieve a speed up of 20 over optimized assembly code on a 80486/66DX2. The complete system is able to identify a user from a database of 173 images of 34 persons in approximately two to three seconds. While the recognition performance of the system is difficult to quantify simply, the system achieves a very conservative 88% recognition rate using cross-validation on the moderately varied database."
            },
            "slug": "A-real-time-face-recognition-system-using-custom-Gilbert-Yang",
            "title": {
                "fragments": [],
                "text": "A real-time face recognition system using custom VLSI hardware"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "A real-time face recognition system has been implemented on an IBM compatible personal computer with a video camera, image digitizer, and custom VLSI image correlator chip that achieves a very conservative 88% recognition rate using cross-validation on the moderately varied database."
            },
            "venue": {
                "fragments": [],
                "text": "1993 Computer Architectures for Machine Perception"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47246616"
                        ],
                        "name": "R. Brunelli",
                        "slug": "R.-Brunelli",
                        "structuredName": {
                            "firstName": "Roberto",
                            "lastName": "Brunelli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Brunelli"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8112841,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "b46511d437887055b45f885d18fd7431564940ec",
            "isKey": false,
            "numCitedBy": 70,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Estimation-of-pose-and-illuminant-direction-for-Brunelli",
            "title": {
                "fragments": [],
                "text": "Estimation of pose and illuminant direction for face processing"
            },
            "venue": {
                "fragments": [],
                "text": "Image Vis. Comput."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "21472040"
                        ],
                        "name": "Irfan Essa",
                        "slug": "Irfan-Essa",
                        "structuredName": {
                            "firstName": "Irfan",
                            "lastName": "Essa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Irfan Essa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144994682"
                        ],
                        "name": "A. Pentland",
                        "slug": "A.-Pentland",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Pentland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pentland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13200879,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4eed9d20232953a1265ef35edeeb4843a519b319",
            "isKey": false,
            "numCitedBy": 197,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a computer vision system for observing the \"action units\" of a face using video sequences as input. The visual observation (sensing) is achieved by using an optimal estimation optical flow method coupled with a geometric and a physical (muscle) model describing the facial structure. This modeling results in a time-varying spatial patterning of facial shape and a parametric representation of the independent muscle action groups, responsible for the observed facial motions. These muscle action patterns may then be used for analysis, interpretation, and synthesis. Thus, by interpreting facial motions within a physics-based optimal estimation framework, a new control model of facial movement is developed. The newly extracted action units (which we name \"FACS+\") are both physics and geometry-based, and extend the well-known FACS parameters for facial expressions by adding temporal information and non-local spatial patterning of facial motion.<<ETX>>"
            },
            "slug": "A-vision-system-for-observing-and-extracting-facial-Essa-Pentland",
            "title": {
                "fragments": [],
                "text": "A vision system for observing and extracting facial action parameters"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "By interpreting facial motions within a physics-based optimal estimation framework, a new control model of facial movement is developed and the newly extracted action units are both physics and geometry-based, and extend the well-known FACS parameters for facial expressions by adding temporal information and non-local spatial patterning of facial motion."
            },
            "venue": {
                "fragments": [],
                "text": "1994 Proceedings of IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47246616"
                        ],
                        "name": "R. Brunelli",
                        "slug": "R.-Brunelli",
                        "structuredName": {
                            "firstName": "Roberto",
                            "lastName": "Brunelli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Brunelli"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 89
                            }
                        ],
                        "text": "This is similar to performance-driven animation (Williams [52]), and Poggio and Brunelli [38], who call it parallel deformation, have suggested it as a computer graphics tool for animating objects when provided with just one view."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 170
                            }
                        ],
                        "text": "Using an example-based approach to bypass 3D models for 3D object recognition was rst explored in the linear combinations approach to recognition (Ullman and Basri [49], Poggio [35])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 220,
                                "start": 201
                            }
                        ],
                        "text": "Existing work in face recognition has demonstrated good recognition performance on frontal, expressionless views of faces with controlled lighting (see Baron [4], Turk and Pentland [48], Bichsel [11], Brunelli and Poggio [14], and Gilbert and Yang [20])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 73
                            }
                        ],
                        "text": "This relative shape representation has been used by Beymer, Shashua, and Poggio [8] in an example-based approach to image analysis and synthesis."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 0
                            }
                        ],
                        "text": "Poggio and Vetter [39] have discussed this linear combinations approach in the case where only one example view is available for an object, laying the groundwork for virtual views."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 46
                            }
                        ],
                        "text": "The technique has been explored previously by Brunelli and Poggio [38] within the context of an \\example-based\" approach to computer graphics and by researchers in performance-driven animation (Williams [52][53], Patterson, Litwinowicz,\nand Greene [33])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 10
                            }
                        ],
                        "text": "Following Poggio and Vetter [39], we call these synthesized views virtual views."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 154
                            }
                        ],
                        "text": "For the single real view, an o -center view was favored over, say, a frontal view because of the recognition results for bilaterally symmetric objects of Poggio and Vetter [39]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 71
                            }
                        ],
                        "text": "Mathematically, we model this using a matrix L\ny = LY; (2)\n1Vetter and Poggio [51] have explored the implications of 3D linear combinations of shape on image grey levels, or texture."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 103
                            }
                        ],
                        "text": "The ideas presented in this section were developed by the authors and also independently by Vetter and Poggio [51]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 66
                            }
                        ],
                        "text": "The technique has been explored previously by Brunelli and Poggio [38] within the context of an \\example-based\" approach to computer graphics and by researchers in performance-driven animation (Williams [52][53], Patterson, Litwinowicz, and Greene [33])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 37
                            }
                        ],
                        "text": "Using this approach, as discussed in Poggio [36] and Poggio and Vetter [39], it is possible to \\learn\" a direct mapping from standard pose to a particular virtual pose."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 110
                            }
                        ],
                        "text": "This vectorized representation for 2D shape has been widely used, including network-based object recognition (Poggio and Edelman [37]), the linear combinations approach to recognition (Ullman and Basri [49], Poggio [35]), active shape models (Cootes and Taylor [15], Cootes, et al. [16]) and face recognition (Craw and Cameron [17][18])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 144
                            }
                        ],
                        "text": "Another way to formulate the solution as a direct mapping is to train a network to learn the association between standard and virtual pose (see Poggio and Vetter [39])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 10
                            }
                        ],
                        "text": "Jones and Poggio [23] also describe a related system that uses linear combinations of prototype shapes to analyze line drawings."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 15
                            }
                        ],
                        "text": "This motivated Poggio and Vetter to introduce the idea of using prior knowledge of object class to generate virtual views."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 31
                            }
                        ],
                        "text": "This idea was rst developed by Poggio and Vetter [39]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 61001553,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d34bac36fcaa3fa5bdc9d843a7fd0972649116b0",
            "isKey": false,
            "numCitedBy": 43,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "We show that we can optimally represent the set of 2D images produced by the point features of a rigid 3D model as two lines in two high-dimensional spaces. We then decribe a working recognition system in which we represent these spaces discretely in a hash table. We can access this table at run time to find all the groups of model features that could match a group of image features, accounting for the effects of sensing error. We also use this representation of a model''s images to demonstrate significant new limitations of two other approaches to recognition: invariants, and non- accidental properties."
            },
            "slug": "A-Novel-Approach-to-Graphics-Poggio-Brunelli",
            "title": {
                "fragments": [],
                "text": "A Novel Approach to Graphics"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "This work shows that it can optimally represent the set of 2D images produced by the point features of a rigid 3D model as two lines in two high-dimensional spaces, and decribes a working recognition system in which these spaces are represented discretely in a hash table."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712839"
                        ],
                        "name": "K. Aizawa",
                        "slug": "K.-Aizawa",
                        "structuredName": {
                            "firstName": "Kiyoharu",
                            "lastName": "Aizawa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Aizawa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698996"
                        ],
                        "name": "H. Harashima",
                        "slug": "H.-Harashima",
                        "structuredName": {
                            "firstName": "Hiroshi",
                            "lastName": "Harashima",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Harashima"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144428073"
                        ],
                        "name": "T. Saito",
                        "slug": "T.-Saito",
                        "structuredName": {
                            "firstName": "Takahiro",
                            "lastName": "Saito",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Saito"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 321,
                                "start": 318
                            }
                        ],
                        "text": "For synthesizing images of faces, 3D facial models have been explored in the computer graphics, computer vision, and model-based image coding communities (Aitchison and Craw[1], Kang, Chen, and Hsu[24], Essa and Pentland [19], Akimoto, Suennaga, and Wallace[3], Waters and Terzopoulos[47], Aizawa, Harashima, and Saito[2])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 21258033,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c192cb49ef5235b0a3e9f7c40f53e5c16bf684d5",
            "isKey": false,
            "numCitedBy": 298,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Model-based-analysis-synthesis-image-coding-system-Aizawa-Harashima",
            "title": {
                "fragments": [],
                "text": "Model-based analysis synthesis image coding (MBASIC) system for a person's face"
            },
            "venue": {
                "fragments": [],
                "text": "Signal Process. Image Commun."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35146156"
                        ],
                        "name": "M. Lades",
                        "slug": "M.-Lades",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Lades",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lades"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3189439"
                        ],
                        "name": "J. Vorbr\u00fcggen",
                        "slug": "J.-Vorbr\u00fcggen",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Vorbr\u00fcggen",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Vorbr\u00fcggen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682548"
                        ],
                        "name": "J. Buhmann",
                        "slug": "J.-Buhmann",
                        "structuredName": {
                            "firstName": "Joachim",
                            "lastName": "Buhmann",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Buhmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2075388397"
                        ],
                        "name": "J. Lange",
                        "slug": "J.-Lange",
                        "structuredName": {
                            "firstName": "J\u00f6rg",
                            "lastName": "Lange",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lange"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704573"
                        ],
                        "name": "C. Malsburg",
                        "slug": "C.-Malsburg",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Malsburg",
                            "middleNames": [
                                "von",
                                "der"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Malsburg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3038814"
                        ],
                        "name": "R. W\u00fcrtz",
                        "slug": "R.-W\u00fcrtz",
                        "structuredName": {
                            "firstName": "Rolf",
                            "lastName": "W\u00fcrtz",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. W\u00fcrtz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34993891"
                        ],
                        "name": "W. Konen",
                        "slug": "W.-Konen",
                        "structuredName": {
                            "firstName": "Wolfgang",
                            "lastName": "Konen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Konen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1266405,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4fb52984078d75ec5655962dc94dc7848182286b",
            "isKey": false,
            "numCitedBy": 2072,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": "An object recognition system based on the dynamic link architecture, an extension to classical artificial neural networks (ANNs), is presented. The dynamic link architecture exploits correlations in the fine-scale temporal structure of cellular signals to group neurons dynamically into higher-order entities. These entities represent a rich structure and can code for high-level objects. To demonstrate the capabilities of the dynamic link architecture, a program was implemented that can recognize human faces and other objects from video images. Memorized objects are represented by sparse graphs, whose vertices are labeled by a multiresolution description in terms of a local power spectrum, and whose edges are labeled by geometrical distance vectors. Object recognition can be formulated as elastic graph matching, which is performed here by stochastic optimization of a matching cost function. The implementation on a transputer network achieved recognition of human faces and office objects from gray-level camera images. The performance of the program is evaluated by a statistical analysis of recognition results from a portrait gallery comprising images of 87 persons. >"
            },
            "slug": "Distortion-Invariant-Object-Recognition-in-the-Link-Lades-Vorbr\u00fcggen",
            "title": {
                "fragments": [],
                "text": "Distortion Invariant Object Recognition in the Dynamic Link Architecture"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "An object recognition system based on the dynamic link architecture, an extension to classical artificial neural networks (ANNs), is presented and the implementation on a transputer network achieved recognition of human faces and office objects from gray-level camera images."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Computers"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2973523"
                        ],
                        "name": "T. Akimoto",
                        "slug": "T.-Akimoto",
                        "structuredName": {
                            "firstName": "Takaaki",
                            "lastName": "Akimoto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Akimoto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34857543"
                        ],
                        "name": "Y. Suenaga",
                        "slug": "Y.-Suenaga",
                        "structuredName": {
                            "firstName": "Yasuhito",
                            "lastName": "Suenaga",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Suenaga"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145356886"
                        ],
                        "name": "R. Wallace",
                        "slug": "R.-Wallace",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Wallace",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Wallace"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 260,
                                "start": 257
                            }
                        ],
                        "text": "For synthesizing images of faces, 3D facial models have been explored in the computer graphics, computer vision, and model-based image coding communities (Aitchison and Craw[1], Kang, Chen, and Hsu[24], Essa and Pentland [19], Akimoto, Suennaga, and Wallace[3], Waters and Terzopoulos[47], Aizawa, Harashima, and Saito[2])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15210583,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c8463d15dd6820e58b3b0db8e9e40878e0e3108d",
            "isKey": false,
            "numCitedBy": 218,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Model-based encoding of human facial features for narrowband visual communication is described. Based on an already prepared 3D human model, this coding method detects and understands a person's body motion and facial expressions. It expresses the essential information as compact codes and transmits it. At the receiving end, this code becomes the basis for modifying the 3D model of the person and thereby generating lifelike human images. The feature extraction used by the system to acquire data for regions or edges that express the eyes, nose, mouth, and outlines of the face and hair is discussed. The way in which the system creates a 3D model of the person by using the features extracted in the first part to modify a generic head model is also discussed.<<ETX>>"
            },
            "slug": "Automatic-creation-of-3D-facial-models-Akimoto-Suenaga",
            "title": {
                "fragments": [],
                "text": "Automatic creation of 3D facial models"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "Model-based encoding of human facial features for narrowband visual communication based on an already prepared 3D human model detects and understands a person's body motion and facial expressions and becomes the basis for modifying the 3D model of the person and thereby generating lifelike human images."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Computer Graphics and Applications"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145706573"
                        ],
                        "name": "L. Williams",
                        "slug": "L.-Williams",
                        "structuredName": {
                            "firstName": "Lance",
                            "lastName": "Williams",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Williams"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 58
                            }
                        ],
                        "text": "This is similar to performance-driven animation (Williams [52]), and Poggio and Brunelli [38], who call it parallel deformation, have suggested it as a computer graphics tool for animating objects when provided with just one view."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 207,
                                "start": 203
                            }
                        ],
                        "text": "The technique has been explored previously by Brunelli and Poggio [38] within the context of an \\example-based\" approach to computer graphics and by researchers in performance-driven animation (Williams [52][53], Patterson, Litwinowicz, and Greene [33])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 53222976,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "a2fc19a23bdc4b99dcac925d83efd5bf2a7a1393",
            "isKey": false,
            "numCitedBy": 344,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "As computer graphics technique rises to the challenge of rendering lifelike performers, more lifelike performance is required. The techniques used to animate robots, arthropods, and suits of armor, have been extended to flexible surfaces of fur and flesh. Physical models of muscle and skin have been devised. But more complex databases and sophisticated physical modeling do not directly address the performance problem. The gestures and expressions of a human actor are not the solution to a dynamic system. This paper describes a means of acquiring the expressions of real faces, and applying them to computer-generated faces. Such an \"electronic mask\" offers a means for the traditional talents of actors to be flexibly incorporated in digital animations. Efforts in a similar spirit have resulted in servo-controlled \"animatrons,\" high-technology puppets, and CG puppetry [1]. The manner in which the skills of actors and puppetteers as well as animators are accommodated in such systems may point the way for a more general incorporation of human nuance into our emerging computer media.The ensuing description is divided into two major subjects: the construction of a highly-resoved human head model with photographic texture mapping, and the concept demonstration of a system to animate this model by tracking and applying the expressions of a human performer."
            },
            "slug": "Performance-driven-facial-animation-Williams",
            "title": {
                "fragments": [],
                "text": "Performance-driven facial animation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A means of acquiring the expressions of real faces, and applying them to computer-generated faces is described as an \"electronic mask\" that offers a means for the traditional talents of actors to be flexibly incorporated in digital animations."
            },
            "venue": {
                "fragments": [],
                "text": "SIGGRAPH"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47246616"
                        ],
                        "name": "R. Brunelli",
                        "slug": "R.-Brunelli",
                        "structuredName": {
                            "firstName": "Roberto",
                            "lastName": "Brunelli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Brunelli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 69
                            }
                        ],
                        "text": "This is similar to performance-driven animation (Williams [52]), and Poggio and Brunelli [38], who call it parallel deformation, have suggested it as a computer graphics tool for animating objects when provided with just one view."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 170
                            }
                        ],
                        "text": "Using an example-based approach to bypass 3D models for 3D object recognition was rst explored in the linear combinations approach to recognition (Ullman and Basri [49], Poggio [35])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 220,
                                "start": 201
                            }
                        ],
                        "text": "Existing work in face recognition has demonstrated good recognition performance on frontal, expressionless views of faces with controlled lighting (see Baron [4], Turk and Pentland [48], Bichsel [11], Brunelli and Poggio [14], and Gilbert and Yang [20])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 73
                            }
                        ],
                        "text": "This relative shape representation has been used by Beymer, Shashua, and Poggio [8] in an example-based approach to image analysis and synthesis."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 0
                            }
                        ],
                        "text": "Poggio and Vetter [39] have discussed this linear combinations approach in the case where only one example view is available for an object, laying the groundwork for virtual views."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 46
                            }
                        ],
                        "text": "The technique has been explored previously by Brunelli and Poggio [38] within the context of an \\example-based\" approach to computer graphics and by researchers in performance-driven animation (Williams [52][53], Patterson, Litwinowicz,\nand Greene [33])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 10
                            }
                        ],
                        "text": "Following Poggio and Vetter [39], we call these synthesized views virtual views."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 154
                            }
                        ],
                        "text": "For the single real view, an o -center view was favored over, say, a frontal view because of the recognition results for bilaterally symmetric objects of Poggio and Vetter [39]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 71
                            }
                        ],
                        "text": "Mathematically, we model this using a matrix L\ny = LY; (2)\n1Vetter and Poggio [51] have explored the implications of 3D linear combinations of shape on image grey levels, or texture."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 103
                            }
                        ],
                        "text": "The ideas presented in this section were developed by the authors and also independently by Vetter and Poggio [51]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 37
                            }
                        ],
                        "text": "Using this approach, as discussed in Poggio [36] and Poggio and Vetter [39], it is possible to \\learn\" a direct mapping from standard pose to a particular virtual pose."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 110
                            }
                        ],
                        "text": "This vectorized representation for 2D shape has been widely used, including network-based object recognition (Poggio and Edelman [37]), the linear combinations approach to recognition (Ullman and Basri [49], Poggio [35]), active shape models (Cootes and Taylor [15], Cootes, et al. [16]) and face recognition (Craw and Cameron [17][18])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 144
                            }
                        ],
                        "text": "Another way to formulate the solution as a direct mapping is to train a network to learn the association between standard and virtual pose (see Poggio and Vetter [39])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 240,
                                "start": 236
                            }
                        ],
                        "text": "1 Introduction Existing work in face recognition has demonstrated good recognition performance on frontal, expressionless views of faces with controlled lighting (see Baron [4], Turk and Pentland [48], Bichsel [11], Brunelli and Poggio [14], and Gilbert and Yang [20])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 10
                            }
                        ],
                        "text": "Jones and Poggio [23] also describe a related system that uses linear combinations of prototype shapes to analyze line drawings."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 15
                            }
                        ],
                        "text": "This motivated Poggio and Vetter to introduce the idea of using prior knowledge of object class to generate virtual views."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 31
                            }
                        ],
                        "text": "This idea was rst developed by Poggio and Vetter [39]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 16859093,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "239beb3861ceceb4c7c7f229234d97198d5c7697",
            "isKey": false,
            "numCitedBy": 2828,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Two new algorithms for computer recognition of human faces, one based on the computation of a set of geometrical features, such as nose width and length, mouth position, and chin shape, and the second based on almost-gray-level template matching, are presented. The results obtained for the testing sets show about 90% correct recognition using geometrical features and perfect recognition using template matching. >"
            },
            "slug": "Face-Recognition:-Features-Versus-Templates-Brunelli-Poggio",
            "title": {
                "fragments": [],
                "text": "Face Recognition: Features Versus Templates"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Two new algorithms for computer recognition of human faces, one based on the computation of a set of geometrical features, such as nose width and length, mouth position, and chin shape, and the second based on almost-gray-level template matching are presented."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2037776"
                        ],
                        "name": "D. Reisfeld",
                        "slug": "D.-Reisfeld",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Reisfeld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Reisfeld"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2386138"
                        ],
                        "name": "Nur Arad",
                        "slug": "Nur-Arad",
                        "structuredName": {
                            "firstName": "Nur",
                            "lastName": "Arad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nur Arad"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47620675"
                        ],
                        "name": "Y. Yeshurun",
                        "slug": "Y.-Yeshurun",
                        "structuredName": {
                            "firstName": "Yehezkel",
                            "lastName": "Yeshurun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Yeshurun"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 167
                            }
                        ],
                        "text": "Example sparse data interpolation techniques include using splines (Litwinowicz and Williams [28], Wolberg [54]), radial basis functions (Reisfeld, Arad, and Yeshurun [40]), and inverse weighted distance metrics (Beier and Neely [5])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 46948380,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "c4caad1556ea738628fc17882e5b5583a5c31e5a",
            "isKey": false,
            "numCitedBy": 11,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Presents a procedure for facial normalization which is determined by the translocation of few anchors in two sets. The first set determines the affine part of the transformation and is used for overcoming change in viewpoint and scale. The second set is used for normalization of expression. A generalization of radial basis functions theory is used for generating the appropriate mapping. Only a few (1-3) anchors were used to demonstrate elaborate facial expressions without any face model."
            },
            "slug": "Normalization-of-face-images-using-few-anchors-Reisfeld-Arad",
            "title": {
                "fragments": [],
                "text": "Normalization of face images using few anchors"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A procedure for facial normalization which is determined by the translocation of few anchors in two sets is presented, which determines the affine part of the transformation and is used for overcoming change in viewpoint and scale."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 12th International Conference on Pattern Recognition"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7205190"
                        ],
                        "name": "Tim Cootes",
                        "slug": "Tim-Cootes",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Cootes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tim Cootes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144482985"
                        ],
                        "name": "C. Taylor",
                        "slug": "C.-Taylor",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Taylor",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Taylor"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 87
                            }
                        ],
                        "text": "The iterative processing of shape and texture is similar to the active shape models of Cootes and Taylor [15], Cootes, et al. [16], and Lanitis, Taylor, and Cootes [27]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 260,
                                "start": 243
                            }
                        ],
                        "text": "This vectorized representation for 2D shape has been widely used, including network-based object recognition (Poggio and Edelman [37]), the linear combinations approach to recognition (Ullman and Basri [49], Poggio [35]), active shape models (Cootes and Taylor [15], Cootes, et al. [16]) and face recognition (Craw and Cameron [17][18])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 105
                            }
                        ],
                        "text": "The iterative processing of shape and texture is similar to the active shape models of Cootes and Taylor [15], Cootes, et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 265,
                                "start": 261
                            }
                        ],
                        "text": "This vectorized representation for 2D shape has been widely used, including network-based object recognition (Poggio and Edelman [37]), the linear combinations approach to recognition (Ullman and Basri [49], Poggio [35]), active shape models (Cootes and Taylor [15], Cootes, et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11058546,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "4d79f7ebf833c549dd1429fd9440628d6c0824c3",
            "isKey": true,
            "numCitedBy": 346,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe 'Active Shape Models' which iteratively adapt to refine estimates of the pose, scale and shape of models of image objects. The method uses flexible models derived from sets of training examples. These models, known as Point Distribution Models, represent objects as sets of labelled points. An initial estimate of the location of the model points in an image is improved by attempting to move each point to a better position nearby. Adjustments to the pose variables and shape parameters are calculated. Limits are placed on the shape parameters ensuring that the example can only deform into shapes conforming to global constraints imposed by the training set. An iterative procedure deforms the model example to find the best fit to the image object. Results of applying the method are described. The technique is shown to be a powerful method for refining estimates of object shape and location."
            },
            "slug": "Active-Shape-Models-'smart-snakes'-Cootes-Taylor",
            "title": {
                "fragments": [],
                "text": "Active Shape Models - 'smart snakes'"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "'Active Shape Models' are described which iteratively adapt to refine estimates of the pose, scale and shape of models of image objects to be a powerful method for refining estimates of object shape and location."
            },
            "venue": {
                "fragments": [],
                "text": "BMVC"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3140335"
                        ],
                        "name": "A. Shashua",
                        "slug": "A.-Shashua",
                        "structuredName": {
                            "firstName": "Amnon",
                            "lastName": "Shashua",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Shashua"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 120989821,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "39c1352a5b70ad3c2dbe29f5f0ceebf163b414bd",
            "isKey": false,
            "numCitedBy": 211,
            "numCiting": 193,
            "paperAbstract": {
                "fragments": [],
                "text": "The report addresses the problem of visual recognition under two sources of variability: geometric and photometric. The geometric deals with the relation between 3D objects and their views under orthographic and perspective projection. The photometric deals with the relation between 3D matte objects and their images under changing illumination conditions. Taken together, an alignment-based method is presented for recognizing objects viewed from arbitrary viewing positions and illuminated by arbitrary settings of light sources."
            },
            "slug": "Geometry-and-Photometry-in-3D-Visual-Recognition-Shashua",
            "title": {
                "fragments": [],
                "text": "Geometry and Photometry in 3D Visual Recognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1964574"
                        ],
                        "name": "Y. Yacoob",
                        "slug": "Y.-Yacoob",
                        "structuredName": {
                            "firstName": "Yaser",
                            "lastName": "Yacoob",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Yacoob"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693428"
                        ],
                        "name": "L. Davis",
                        "slug": "L.-Davis",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Davis",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Davis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9794074,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4302d843e008bdc4444e7fb161044a9c60b7c01d",
            "isKey": false,
            "numCitedBy": 239,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "An approach for analysis and representation of facial dynamics for recognition of facial expressions from image sequences is proposed. The algorithms we develop utilize optical flow computation to identify the direction of rigid and non-rigid motions that are caused by human, facial expressions. A mid-level symbolic representation that is motivated by linguistic and psychological considerations is developed. Recognition of six facial expressions, as well as eye blinking, on a large set of image sequences is reported.<<ETX>>"
            },
            "slug": "Computing-spatio-temporal-representations-of-human-Yacoob-Davis",
            "title": {
                "fragments": [],
                "text": "Computing spatio-temporal representations of human faces"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "An approach for analysis and representation of facial dynamics for recognition of facial expressions from image sequences is proposed and a mid-level symbolic representation that is motivated by linguistic and psychological considerations is developed."
            },
            "venue": {
                "fragments": [],
                "text": "1994 Proceedings of IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3157627"
                        ],
                        "name": "Andrew C. Aitchison",
                        "slug": "Andrew-C.-Aitchison",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Aitchison",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew C. Aitchison"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144986880"
                        ],
                        "name": "I. Craw",
                        "slug": "I.-Craw",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Craw",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Craw"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61681386,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "57e006653b77c1182e9501b6401888442f18bfc0",
            "isKey": false,
            "numCitedBy": 6,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method of generating realistic views of the head of any individual from a single photograph of the individual and a generic model of a human head."
            },
            "slug": "Synthetic-Images-of-Faces-\u2014-An-Approach-to-Face-Aitchison-Craw",
            "title": {
                "fragments": [],
                "text": "Synthetic Images of Faces \u2014 An Approach to Model-Based Face Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "A method of generating realistic views of the head of any individual from a single photograph of the individual and a generic model of a human head is presented."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1750924"
                        ],
                        "name": "Demetri Terzopoulos",
                        "slug": "Demetri-Terzopoulos",
                        "structuredName": {
                            "firstName": "Demetri",
                            "lastName": "Terzopoulos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Demetri Terzopoulos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46398261"
                        ],
                        "name": "K. Waters",
                        "slug": "K.-Waters",
                        "structuredName": {
                            "firstName": "Keith",
                            "lastName": "Waters",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Waters"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 46283468,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1d0a6188890f378e4d18f4580cdb7a1801b17200",
            "isKey": false,
            "numCitedBy": 135,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "A novel approach is presented to the analysis of dynamic facial images. The approach exploits a realistic model of the human face. The face model, which may be simulated and rendered at interactive rates on a graphics workstation, incorporates a physically-based approximation to facial tissue and a set of anatomically-motivated facial muscle actuators. The authors consider the estimation of dynamic facial muscle contractions from video sequences of expressive human faces. They develop an estimation technique that uses deformable contour models to track the nonrigid motions of facial features in images. The technique computes robust muscle actuator controls, enabling the face model to resynthesize transient expressions accurately.<<ETX>>"
            },
            "slug": "Analysis-of-facial-images-using-physical-and-models-Terzopoulos-Waters",
            "title": {
                "fragments": [],
                "text": "Analysis of facial images using physical and anatomical models"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The authors consider the estimation of dynamic facial muscle contractions from video sequences of expressive human faces and develop an estimation technique that uses deformable contour models to track the nonrigid motions of facial features in images."
            },
            "venue": {
                "fragments": [],
                "text": "[1990] Proceedings Third International Conference on Computer Vision"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40588702"
                        ],
                        "name": "B. D. Lucas",
                        "slug": "B.-D.-Lucas",
                        "structuredName": {
                            "firstName": "Bruce",
                            "lastName": "Lucas",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. D. Lucas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733113"
                        ],
                        "name": "T. Kanade",
                        "slug": "T.-Kanade",
                        "structuredName": {
                            "firstName": "Takeo",
                            "lastName": "Kanade",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanade"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2121536,
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "id": "a06547951c97b2a32f23a6c2b5f79c8c75c9b9bd",
            "isKey": false,
            "numCitedBy": 13329,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Image registration finds a variety of applications in computer vision. Unfortunately, traditional image registration techniques tend to be costly. We present a new image registration technique that makes use of the spatial intensity gradient of the images to find a good match using a type of Newton-Raphson iteration. Our technique is taster because it examines far fewer potential matches between the images than existing techniques Furthermore, this registration technique can be generalized to handle rotation, scaling and shearing. We show how our technique can be adapted tor use in a stereo vision system."
            },
            "slug": "An-Iterative-Image-Registration-Technique-with-an-Lucas-Kanade",
            "title": {
                "fragments": [],
                "text": "An Iterative Image Registration Technique with an Application to Stereo Vision"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work presents a new image registration technique that makes use of the spatial intensity gradient of the images to find a good match using a type of Newton-Raphson iteration, and can be generalized to handle rotation, scaling and shearing."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2615673"
                        ],
                        "name": "Peter Litwinowicz",
                        "slug": "Peter-Litwinowicz",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Litwinowicz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter Litwinowicz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145706573"
                        ],
                        "name": "L. Williams",
                        "slug": "L.-Williams",
                        "structuredName": {
                            "firstName": "Lance",
                            "lastName": "Williams",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 49
                            }
                        ],
                        "text": "This is similar to performance-driven animation (Williams [52]), and Poggio and Brunelli [38], who call it parallel deformation, have suggested it as a computer graphics tool for animating objects when provided with just one view."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 93
                            }
                        ],
                        "text": "Example sparse data interpolation techniques include using splines (Litwinowicz and Williams [28], Wolberg [54]), radial basis functions (Reisfeld, Arad, and Yeshurun [40]), and inverse weighted distance metrics (Beier and Neely [5])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 202,
                                "start": 194
                            }
                        ],
                        "text": "The technique has been explored previously by Brunelli and Poggio [38] within the context of an \\example-based\" approach to computer graphics and by researchers in performance-driven animation (Williams [52][53], Patterson, Litwinowicz,\nand Greene [33])."
                    },
                    "intents": []
                }
            ],
            "corpusId": 5992667,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "486fd6cb2c7415125fce9fdb55799ff38fbe0246",
            "isKey": false,
            "numCitedBy": 177,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "The work described here extends the power of 2D animation with a form of texture mapping conveniently controlled by line drawings. By tracing points, line segments, spline curves, or filled regions on an image, the animator defines features which can be used to animate the image. Animations of the control features deform the image smoothly. This development is in the tradition of \u201cskeleton\u201d-based animation, and \u201cfeature\u201d-based image metamorphosis. By employing numerics developed in the computer vision community for rapid visual surface estimation, several important advantages are realized. Skeletons are generalized to include curved \u201cbones,\u201d the interpolating surface is better behaved, the expense of computing the animation is decoupled from the number of features in the drawing, and arbitrary holes or cuts in the interpolated surface can be accommodated. The same general scattered data interpolation technique is applied to the problem of mapping animation from one image and set of features to another, generalizing the prescriptive power of animated sequences and encouraging reuse of animated motion."
            },
            "slug": "Animating-images-with-drawings-Litwinowicz-Williams",
            "title": {
                "fragments": [],
                "text": "Animating images with drawings"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "The work described here extends the power of 2D animation with a form of texture mapping conveniently controlled by line drawings, generalizing the prescriptive power of animated sequences and encouraging reuse of animated motion."
            },
            "venue": {
                "fragments": [],
                "text": "SIGGRAPH"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740300"
                        ],
                        "name": "D. Beymer",
                        "slug": "D.-Beymer",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Beymer",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Beymer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3140335"
                        ],
                        "name": "A. Shashua",
                        "slug": "A.-Shashua",
                        "structuredName": {
                            "firstName": "Amnon",
                            "lastName": "Shashua",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Shashua"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 13151746,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8abe2824f9851d3c465b1aa11849661430d60ca0",
            "isKey": false,
            "numCitedBy": 217,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "Image analysis and graphics synthesis can be achieved with learning techniques using directly image examples without physically-based, 3D models. In our technique: -- the mapping from novel images to a vector of ``pose'''' and ``expression'''' parameters can be learned from a small set of example images using a function approximation technique that we call an {\\it analysis network}; -- the inverse mapping from input ``pose'''' and ``expression'''' parameters to output images can be synthesized from a small set of example images and used to produce new images using a similar {\\it synthesis network}. The techniques described here have several applications in computer graphics, special effects, interactive multimedia and very low bandwidth teleconferencing."
            },
            "slug": "Example-Based-Image-Analysis-and-Synthesis-Beymer-Shashua",
            "title": {
                "fragments": [],
                "text": "Example Based Image Analysis and Synthesis"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The mapping from novel images to a vector of ``pose'''' and ``expression'''' parameters can be learned from a small set of example images using a function approximation technique that it analysis network."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2160942"
                        ],
                        "name": "R. Baron",
                        "slug": "R.-Baron",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Baron",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Baron"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 29658209,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "0fb4d98ac0a2f3c6f058ec4b2c25835ee8a24fcc",
            "isKey": false,
            "numCitedBy": 226,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Mechanisms-of-Human-Facial-Recognition-Baron",
            "title": {
                "fragments": [],
                "text": "Mechanisms of Human Facial Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Man Mach. Stud."
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2075395716"
                        ],
                        "name": "Elizabeth Patterson",
                        "slug": "Elizabeth-Patterson",
                        "structuredName": {
                            "firstName": "Elizabeth",
                            "lastName": "Patterson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Elizabeth Patterson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2615673"
                        ],
                        "name": "Peter Litwinowicz",
                        "slug": "Peter-Litwinowicz",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Litwinowicz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter Litwinowicz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3213361"
                        ],
                        "name": "Ned Greene",
                        "slug": "Ned-Greene",
                        "structuredName": {
                            "firstName": "Ned",
                            "lastName": "Greene",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ned Greene"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 252,
                                "start": 248
                            }
                        ],
                        "text": "The technique has been explored previously by Brunelli and Poggio [38] within the context of an \\example-based\" approach to computer graphics and by researchers in performance-driven animation (Williams [52][53], Patterson, Litwinowicz, and Greene [33])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 247,
                                "start": 213
                            }
                        ],
                        "text": "The technique has been explored previously by Brunelli and Poggio [38] within the context of an \\example-based\" approach to computer graphics and by researchers in performance-driven animation (Williams [52][53], Patterson, Litwinowicz,\nand Greene [33])."
                    },
                    "intents": []
                }
            ],
            "corpusId": 54131424,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "0c2f3253e3b58e4ded6c4767fd37d28940956114",
            "isKey": false,
            "numCitedBy": 60,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a means for generating facial animation. Motion is captured on video from the actions of a live performer. Control points are obtained from the video, and this acquired motion is spatially mapped to conform to a synthetic actor\u2019s face. The mapping takes into account differences in proportions between the two faces. Animation is then generated by deforming the texture and geometry of the synthetic face around the control points. This technique was used in \u201cThe Audition\u201d to animate a talking dog."
            },
            "slug": "Facial-Animation-by-Spatial-Mapping-Patterson-Litwinowicz",
            "title": {
                "fragments": [],
                "text": "Facial Animation by Spatial Mapping"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "A means for generating facial animation using motion captured on video from the actions of a live performer to conform to a synthetic actor\u2019s face is described."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7205190"
                        ],
                        "name": "Tim Cootes",
                        "slug": "Tim-Cootes",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Cootes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tim Cootes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144482985"
                        ],
                        "name": "C. Taylor",
                        "slug": "C.-Taylor",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Taylor",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1830709"
                        ],
                        "name": "A. Lanitis",
                        "slug": "A.-Lanitis",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Lanitis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Lanitis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32250556"
                        ],
                        "name": "D. H. Cooper",
                        "slug": "D.-H.-Cooper",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Cooper",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. H. Cooper"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47581828"
                        ],
                        "name": "J. Graham",
                        "slug": "J.-Graham",
                        "structuredName": {
                            "firstName": "Jim",
                            "lastName": "Graham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Graham"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[16], and Lanitis, Taylor, and Cootes [27]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[16]) and face recognition (Craw and Cameron [17][18])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 36445761,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "a041199f33d69a949d9bd889068187ffe4140cc7",
            "isKey": false,
            "numCitedBy": 113,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors describe a technique for building compact models of the shape and appearance of flexible objects seen in 2-D images. The models are derived from the statistics of sets of labeled images of example objects. Each model consists of a flexible shape template, describing how important points of the object can vary, and a statistical model of the expected grey levels in regions around each model point. Such models have proved useful in a wide variety of applications. A description is given on how the models can be used in local image search, and examples of their application are included.<<ETX>>"
            },
            "slug": "Building-and-using-flexible-models-incorporating-Cootes-Taylor",
            "title": {
                "fragments": [],
                "text": "Building and using flexible models incorporating grey-level information"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "A technique for building compact models of the shape and appearance of flexible objects seen in 2-D images derived from the statistics of sets of labeled images of example objects, which have proved useful in a wide variety of applications."
            },
            "venue": {
                "fragments": [],
                "text": "1993 (4th) International Conference on Computer Vision"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "98326462"
                        ],
                        "name": "David A. Deamer",
                        "slug": "David-A.-Deamer",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Deamer",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David A. Deamer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4057530,
            "fieldsOfStudy": [
                "History"
            ],
            "id": "0084478ca98436228dd1b1ab827c1b1083f84483",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "THIS is a very interesting review of the gradual evolution of the various instruments which have been invented for the portrayal of objects in motion, from the earliest times to the present day. The work may be divided into two parts, of which the first, including Chapters i.\u2013iii. (pp. 1\u2013109) deals with the more distinctly historical aspect of the subject, while the remaining chapters (iv.\u2013vii.) are devoted to a very minute description of all the important machines in present use.Living Pictures.By H. V. Hopwood. Pp. xii + 275. (London: The Optician and Photographic Trades Review, 1899.)"
            },
            "slug": "Living-Pictures-Deamer",
            "title": {
                "fragments": [],
                "text": "Living Pictures"
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1899
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "116003860"
                        ],
                        "name": "J. Bergen",
                        "slug": "J.-Bergen",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Bergen",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bergen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145981906"
                        ],
                        "name": "P. Anandan",
                        "slug": "P.-Anandan",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Anandan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Anandan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3315356"
                        ],
                        "name": "K. Hanna",
                        "slug": "K.-Hanna",
                        "structuredName": {
                            "firstName": "Keith",
                            "lastName": "Hanna",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Hanna"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055557"
                        ],
                        "name": "R. Hingorani",
                        "slug": "R.-Hingorani",
                        "structuredName": {
                            "firstName": "Rajesh",
                            "lastName": "Hingorani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Hingorani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6267598,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "404f3544a7db67c38ba3b8f78f02759d2326684e",
            "isKey": false,
            "numCitedBy": 1510,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a hierarchical estimation framework for the computation of diverse representations of motion information. The key features of the resulting framework (or family of algorithms) are a global model that constrains the overall structure of the motion estimated, a local model that is used in the estimation process, and a coarse-fine refinement strategy. Four specific motion models: affine flow, planar surface flow, rigid body motion, and general optical flow, are described along with their application to specific examples."
            },
            "slug": "Hierarchical-Model-Based-Motion-Estimation-Bergen-Anandan",
            "title": {
                "fragments": [],
                "text": "Hierarchical Model-Based Motion Estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "This paper describes a hierarchical estimation framework for the computation of diverse representations of motion information that constrains the overall structure of the motion estimated, a local model that is used in the estimation process, and a coarse-fine refinement strategy."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145319478"
                        ],
                        "name": "Michael J. Jones",
                        "slug": "Michael-J.-Jones",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jones",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Jones"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 49743910,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "eae2430d9a984120bf511655a03c15089b007499",
            "isKey": false,
            "numCitedBy": 1366,
            "numCiting": 198,
            "paperAbstract": {
                "fragments": [],
                "text": "We had previously shown that regularization principles lead to approximation schemes that are equivalent to networks with one layer of hidden units, called regularization networks. In particular, standard smoothness functionals lead to a subclass of regularization networks, the well known radial basis functions approximation schemes. This paper shows that regularization networks encompass a much broader range of approximation schemes, including many of the popular general additive models and some of the neural networks. In particular, we introduce new classes of smoothness functionals that lead to different classes of basis functions. Additive splines as well as some tensor product splines can be obtained from appropriate classes of smoothness functionals. Furthermore, the same generalization that extends radial basis functions (RBF) to hyper basis functions (HBF) also leads from additive models to ridge approximation models, containing as special cases Breiman's hinge functions, some forms of projection pursuit regression, and several types of neural networks. We propose to use the term generalized regularization networks for this broad class of approximation schemes that follow from an extension of regularization. In the probabilistic interpretation of regularization, the different classes of basis functions correspond to different classes of prior probabilities on the approximating function spaces, and therefore to different types of smoothness assumptions. In summary, different multilayer networks with one hidden layer, which we collectively call generalized regularization networks, correspond to different classes of priors and associated smoothness functionals in a classical regularization principle. Three broad classes are (1) radial basis functions that can be generalized to hyper basis functions, (2) some tensor product splines, and (3) additive splines that can be generalized to schemes of the type of ridge approximation, hinge functions, and several perceptron-like neural networks with one hidden layer."
            },
            "slug": "Regularization-Theory-and-Neural-Networks-Girosi-Jones",
            "title": {
                "fragments": [],
                "text": "Regularization Theory and Neural Networks Architectures"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "This paper shows that regularization networks encompass a much broader range of approximation schemes, including many of the popular general additive models and some of the neural networks, and introduces new classes of smoothness functionals that lead to different classes of basis functions."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40019208"
                        ],
                        "name": "Thaddeus Beier",
                        "slug": "Thaddeus-Beier",
                        "structuredName": {
                            "firstName": "Thaddeus",
                            "lastName": "Beier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thaddeus Beier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2094398048"
                        ],
                        "name": "Shawn Neely",
                        "slug": "Shawn-Neely",
                        "structuredName": {
                            "firstName": "Shawn",
                            "lastName": "Neely",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shawn Neely"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 96
                            }
                        ],
                        "text": "The manual technique is borrowed from Beier and Neely's morphing technique in computer graphics [5]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 67
                            }
                        ],
                        "text": "Given these sets of correspondences, the interpolation method from Beier and Neely [5] (see section 5.1.1) is used to interpolate the correspondences to de ne a dense, pixelwise mapping from the prototype to novel face."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 83
                            }
                        ],
                        "text": "Given these sets of correspondences, the interpolation method from Beier and Neely [5] (see section 5."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 130
                            }
                        ],
                        "text": "Since the tpj 's can be put into correspondence manually in an o -line step (using the interpolation technique of Beier and Neely [5]), the primary di culty of this step is in converting in into its shape free representation tn."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 232,
                                "start": 229
                            }
                        ],
                        "text": "Example sparse data interpolation techniques include using splines (Litwinowicz and Williams [28], Wolberg [54]), radial basis functions (Reisfeld, Arad, and Yeshurun [40]), and inverse weighted distance metrics (Beier and Neely [5])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9124441,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "be73726c6a538bc3ed05e62ba5faec183f777ff6",
            "isKey": true,
            "numCitedBy": 833,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "2.1 Conventional Metamorphosis Techniques Mc[:ml(wpht)iii twlween lWo or mor\u2019c imafys (wer lime i) u uwi\u2019ul \\ i~u;ii tcchniquc. (Jflen uwd f\u2019orCducaliomd (n\u2019tMCid;liMll Cnt purpt>wi. \u20181\u2019l-:idi(ional Iilmmahing techniques for (his cflcc[ include ~\u2019lckcr c\u2019ut~(iuc\u2019h LISu chwwwr cxhibi(ing ch:mgm while running thr(mgll ;! toreil and prosing behind several trws ) tind op[ic:d cro\\\\diswdv<\u2019. in which onc image is f:ide(i out while wwther is sinwlt:lnLNNI\\l)f\u2019:idcdin (Mith makeup ch:mge. tippliwcm, or nhjecl subs[i [u[I(m ). Sc\\\u2019~\u2019riilclawic horror lilm~ illu$tfiite [he process: who ctwld hnycl ~hc b:lir-tai~ing (fiiniform;ilml of the Woitman. or the drw m:itic lllct;itll(~rpll(~sii from Dr. Jchyll [o Mr. Hyde\u2019? This pupcr prcwmls ii c(mtcnlp{mmy w~lu(i(mto the vi~u:d translonmrtion pnh lL\u2019nl."
            },
            "slug": "Feature-based-image-metamorphosis-Beier-Neely",
            "title": {
                "fragments": [],
                "text": "Feature-based image metamorphosis"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "2.1 Conventional Metamorphosis Techniques Mc[:ml(wpht)iii twlween lWo or mor\u2019c imafys (wer lime i) u uwi\u2019ul \\ i~u;ii tcchniquc."
            },
            "venue": {
                "fragments": [],
                "text": "SIGGRAPH"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1644052334"
                        ],
                        "name": "WilliamsLance",
                        "slug": "WilliamsLance",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "WilliamsLance",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "WilliamsLance"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 58
                            }
                        ],
                        "text": "This is similar to performance-driven animation (Williams [52]), and Poggio and Brunelli [38], who call it parallel deformation, have suggested it as a computer graphics tool for animating objects when provided with just one view."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 207,
                                "start": 203
                            }
                        ],
                        "text": "The technique has been explored previously by Brunelli and Poggio [38] within the context of an \\example-based\" approach to computer graphics and by researchers in performance-driven animation (Williams [52][53], Patterson, Litwinowicz, and Greene [33])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 215959405,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "a865fef8447f9e01dc51e53e990f1745ed782ac8",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "As computer graphics technique rises to the challenge of rendering lifelike performers, more lifelike performance is required. The techniques used to animate robots, arthropods, and suits of armor,..."
            },
            "slug": "Performance-driven-facial-animation-WilliamsLance",
            "title": {
                "fragments": [],
                "text": "Performance-driven facial animation"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The techniques used to animate robots, arthropods, and suits of armor, as well as other animals, have changed over time to provide more lifelike performance."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768661"
                        ],
                        "name": "G. Wolberg",
                        "slug": "G.-Wolberg",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Wolberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wolberg"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 5151458,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "466a33d805c99de465b457adc43acca840fc7cbc",
            "isKey": false,
            "numCitedBy": 1656,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nThis book is intended to be a practical guide for eclectic scientists and engineers who find themselves in need of implementing warping algorithms and comprehending the underlying concepts."
            },
            "slug": "Digital-image-warping-Wolberg",
            "title": {
                "fragments": [],
                "text": "Digital image warping"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "This book is intended to be a practical guide for eclectic scientists and engineers who find themselves in need of implementing warping algorithms and comprehending the underlying concepts."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 89
                            }
                        ],
                        "text": "This is similar to performance-driven animation (Williams [52]), and Poggio and Brunelli [38], who call it parallel deformation, have suggested it as a computer graphics tool for animating objects when provided with just one view."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 170
                            }
                        ],
                        "text": "Using an example-based approach to bypass 3D models for 3D object recognition was rst explored in the linear combinations approach to recognition (Ullman and Basri [49], Poggio [35])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 220,
                                "start": 201
                            }
                        ],
                        "text": "Existing work in face recognition has demonstrated good recognition performance on frontal, expressionless views of faces with controlled lighting (see Baron [4], Turk and Pentland [48], Bichsel [11], Brunelli and Poggio [14], and Gilbert and Yang [20])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 73
                            }
                        ],
                        "text": "This relative shape representation has been used by Beymer, Shashua, and Poggio [8] in an example-based approach to image analysis and synthesis."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 0
                            }
                        ],
                        "text": "Poggio and Vetter [39] have discussed this linear combinations approach in the case where only one example view is available for an object, laying the groundwork for virtual views."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 46
                            }
                        ],
                        "text": "The technique has been explored previously by Brunelli and Poggio [38] within the context of an \\example-based\" approach to computer graphics and by researchers in performance-driven animation (Williams [52][53], Patterson, Litwinowicz,\nand Greene [33])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 10
                            }
                        ],
                        "text": "Following Poggio and Vetter [39], we call these synthesized views virtual views."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 154
                            }
                        ],
                        "text": "For the single real view, an o -center view was favored over, say, a frontal view because of the recognition results for bilaterally symmetric objects of Poggio and Vetter [39]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 71
                            }
                        ],
                        "text": "Mathematically, we model this using a matrix L\ny = LY; (2)\n1Vetter and Poggio [51] have explored the implications of 3D linear combinations of shape on image grey levels, or texture."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 103
                            }
                        ],
                        "text": "The ideas presented in this section were developed by the authors and also independently by Vetter and Poggio [51]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 66
                            }
                        ],
                        "text": "The technique has been explored previously by Brunelli and Poggio [38] within the context of an \\example-based\" approach to computer graphics and by researchers in performance-driven animation (Williams [52][53], Patterson, Litwinowicz, and Greene [33])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 37
                            }
                        ],
                        "text": "Using this approach, as discussed in Poggio [36] and Poggio and Vetter [39], it is possible to \\learn\" a direct mapping from standard pose to a particular virtual pose."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 110
                            }
                        ],
                        "text": "This vectorized representation for 2D shape has been widely used, including network-based object recognition (Poggio and Edelman [37]), the linear combinations approach to recognition (Ullman and Basri [49], Poggio [35]), active shape models (Cootes and Taylor [15], Cootes, et al. [16]) and face recognition (Craw and Cameron [17][18])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 144
                            }
                        ],
                        "text": "Another way to formulate the solution as a direct mapping is to train a network to learn the association between standard and virtual pose (see Poggio and Vetter [39])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 10
                            }
                        ],
                        "text": "Jones and Poggio [23] also describe a related system that uses linear combinations of prototype shapes to analyze line drawings."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 15
                            }
                        ],
                        "text": "This motivated Poggio and Vetter to introduce the idea of using prior knowledge of object class to generate virtual views."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 31
                            }
                        ],
                        "text": "This idea was rst developed by Poggio and Vetter [39]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A novel ap-  proach to graphics"
            },
            "venue": {
                "fragments": [],
                "text": "A.I. Memo No. 1354, Arti cial  Intelligence Laboratory, Massachusetts Institute of  Technology,"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 69
                            }
                        ],
                        "text": "This is similar to performance-driven animation (Williams [52]), and Poggio and Brunelli [38], who call it parallel deformation, have suggested it as a computer graphics tool for animating objects when provided with just one view."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 170
                            }
                        ],
                        "text": "Using an example-based approach to bypass 3D models for 3D object recognition was rst explored in the linear combinations approach to recognition (Ullman and Basri [49], Poggio [35])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 220,
                                "start": 214
                            }
                        ],
                        "text": "Existing work in face recognition has demonstrated good recognition performance on frontal, expressionless views of faces with controlled lighting (see Baron [4], Turk and Pentland [48], Bichsel [11], Brunelli and Poggio [14], and Gilbert and Yang [20])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 80
                            }
                        ],
                        "text": "This relative shape representation has been used by Beymer, Shashua, and Poggio [8] in an example-based approach to image analysis and synthesis."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 0
                            }
                        ],
                        "text": "Poggio and Vetter [39] have discussed this linear combinations approach in the case where only one example view is available for an object, laying the groundwork for virtual views."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 59
                            }
                        ],
                        "text": "The technique has been explored previously by Brunelli and Poggio [38] within the context of an \\example-based\" approach to computer graphics and by researchers in performance-driven animation (Williams [52][53], Patterson, Litwinowicz,\nand Greene [33])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 10
                            }
                        ],
                        "text": "Following Poggio and Vetter [39], we call these synthesized views virtual views."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 154
                            }
                        ],
                        "text": "For the single real view, an o -center view was favored over, say, a frontal view because of the recognition results for bilaterally symmetric objects of Poggio and Vetter [39]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 71
                            }
                        ],
                        "text": "Mathematically, we model this using a matrix L\ny = LY; (2)\n1Vetter and Poggio [51] have explored the implications of 3D linear combinations of shape on image grey levels, or texture."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 103
                            }
                        ],
                        "text": "The ideas presented in this section were developed by the authors and also independently by Vetter and Poggio [51]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 37
                            }
                        ],
                        "text": "Using this approach, as discussed in Poggio [36] and Poggio and Vetter [39], it is possible to \\learn\" a direct mapping from standard pose to a particular virtual pose."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 110
                            }
                        ],
                        "text": "This vectorized representation for 2D shape has been widely used, including network-based object recognition (Poggio and Edelman [37]), the linear combinations approach to recognition (Ullman and Basri [49], Poggio [35]), active shape models (Cootes and Taylor [15], Cootes, et al. [16]) and face recognition (Craw and Cameron [17][18])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 144
                            }
                        ],
                        "text": "Another way to formulate the solution as a direct mapping is to train a network to learn the association between standard and virtual pose (see Poggio and Vetter [39])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 10
                            }
                        ],
                        "text": "Jones and Poggio [23] also describe a related system that uses linear combinations of prototype shapes to analyze line drawings."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 15
                            }
                        ],
                        "text": "This motivated Poggio and Vetter to introduce the idea of using prior knowledge of object class to generate virtual views."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 31
                            }
                        ],
                        "text": "This idea was rst developed by Poggio and Vetter [39]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Exam-  ple based image analysis and synthesis"
            },
            "venue": {
                "fragments": [],
                "text": "A.I. Memo  No. 1431, Arti cial Intelligence Laboratory, Mas-  sachusetts Institute of Technology,"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 69
                            }
                        ],
                        "text": "This is similar to performance-driven animation (Williams [52]), and Poggio and Brunelli [38], who call it parallel deformation, have suggested it as a computer graphics tool for animating objects when provided with just one view."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 170
                            }
                        ],
                        "text": "Using an example-based approach to bypass 3D models for 3D object recognition was rst explored in the linear combinations approach to recognition (Ullman and Basri [49], Poggio [35])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 220,
                                "start": 214
                            }
                        ],
                        "text": "Existing work in face recognition has demonstrated good recognition performance on frontal, expressionless views of faces with controlled lighting (see Baron [4], Turk and Pentland [48], Bichsel [11], Brunelli and Poggio [14], and Gilbert and Yang [20])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 73
                            }
                        ],
                        "text": "This relative shape representation has been used by Beymer, Shashua, and Poggio [8] in an example-based approach to image analysis and synthesis."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 0
                            }
                        ],
                        "text": "Poggio and Vetter [39] have discussed this linear combinations approach in the case where only one example view is available for an object, laying the groundwork for virtual views."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 59
                            }
                        ],
                        "text": "The technique has been explored previously by Brunelli and Poggio [38] within the context of an \\example-based\" approach to computer graphics and by researchers in performance-driven animation (Williams [52][53], Patterson, Litwinowicz,\nand Greene [33])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 10
                            }
                        ],
                        "text": "Following Poggio and Vetter [39], we call these synthesized views virtual views."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 154
                            }
                        ],
                        "text": "For the single real view, an o -center view was favored over, say, a frontal view because of the recognition results for bilaterally symmetric objects of Poggio and Vetter [39]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 71
                            }
                        ],
                        "text": "Mathematically, we model this using a matrix L\ny = LY; (2)\n1Vetter and Poggio [51] have explored the implications of 3D linear combinations of shape on image grey levels, or texture."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 103
                            }
                        ],
                        "text": "The ideas presented in this section were developed by the authors and also independently by Vetter and Poggio [51]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 44
                            }
                        ],
                        "text": "Using this approach, as discussed in Poggio [36] and Poggio and Vetter [39], it is possible to \\learn\" a direct mapping from standard pose to a particular virtual pose."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 110
                            }
                        ],
                        "text": "This vectorized representation for 2D shape has been widely used, including network-based object recognition (Poggio and Edelman [37]), the linear combinations approach to recognition (Ullman and Basri [49], Poggio [35]), active shape models (Cootes and Taylor [15], Cootes, et al. [16]) and face recognition (Craw and Cameron [17][18])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 144
                            }
                        ],
                        "text": "Another way to formulate the solution as a direct mapping is to train a network to learn the association between standard and virtual pose (see Poggio and Vetter [39])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 10
                            }
                        ],
                        "text": "Jones and Poggio [23] also describe a related system that uses linear combinations of prototype shapes to analyze line drawings."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 15
                            }
                        ],
                        "text": "This motivated Poggio and Vetter to introduce the idea of using prior knowledge of object class to generate virtual views."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 31
                            }
                        ],
                        "text": "This idea was rst developed by Poggio and Vetter [39]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "3D object recognition and prototypes:  one 2D view may be su cient"
            },
            "venue": {
                "fragments": [],
                "text": "Technical Report  9107{02,"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1893129"
                        ],
                        "name": "M. Bichsel",
                        "slug": "M.-Bichsel",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Bichsel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Bichsel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 210
                            }
                        ],
                        "text": "1 Introduction Existing work in face recognition has demonstrated good recognition performance on frontal, expressionless views of faces with controlled lighting (see Baron [4], Turk and Pentland [48], Bichsel [11], Brunelli and Poggio [14], and Gilbert and Yang [20])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 187
                            }
                        ],
                        "text": "Existing work in face recognition has demonstrated good recognition performance on frontal, expressionless views of faces with controlled lighting (see Baron [4], Turk and Pentland [48], Bichsel [11], Brunelli and Poggio [14], and Gilbert and Yang [20])."
                    },
                    "intents": []
                }
            ],
            "corpusId": 142804056,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9373c1a890f9d4d1847d602d6e14df00e1bd0115",
            "isKey": false,
            "numCitedBy": 80,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Strategies-of-robust-object-recognition-for-the-of-Bichsel",
            "title": {
                "fragments": [],
                "text": "Strategies of robust object recognition for the automatic identification of human faces"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 204
                            }
                        ],
                        "text": "While not yet applied to face recognition, this approach has been used for face detection under varying illumination (Sinha [45]) and for indexing of packaged grocery items using color (Swain and Ballard [46])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Color in-  dexing"
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision,"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 124
                            }
                        ],
                        "text": "While not yet applied to face recognition, this approach has been used for face detection under varying illumination (Sinha [45]) and for indexing of packaged grocery items using color (Swain and Ballard [46])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Object recognition via image invari-  ances"
            },
            "venue": {
                "fragments": [],
                "text": "Investigative Ophthalmology and Visual Sci-  ence,"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 177
                            }
                        ],
                        "text": "Using an example-based approach to bypass 3D models for 3D object recognition was rst explored in the linear combinations approach to recognition (Ullman and Basri [49], Poggio [35])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 219,
                                "start": 215
                            }
                        ],
                        "text": "This vectorized representation for 2D shape has been widely used, including network-based object recognition (Poggio and Edelman [37]), the linear combinations approach to recognition (Ullman and Basri [49], Poggio [35]), active shape models (Cootes and Taylor [15], Cootes, et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "3D object recognition: on a result by Basri and Ullman"
            },
            "venue": {
                "fragments": [],
                "text": "3D object recognition: on a result by Basri and Ullman"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 60
                            }
                        ],
                        "text": "However, we have found that a standard optical ow algorithm [7], preceded by normalization based on the eye locations, can do a good job at automatically computing dense pixelwise correspondences."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 73
                            }
                        ],
                        "text": "The optical ow technique uses the gradient-based, hierarchical method of Bergen and Hingorani [7] (also see Lucas and Kanade [29], Bergen, et al. [6])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 94
                            }
                        ],
                        "text": "The optical ow technique uses the gradient-based, hierarchical method of Bergen and Hingorani [7] (also see Lucas and Kanade [29], Bergen, et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Hierarchical motionbased frame rate conversion"
            },
            "venue": {
                "fragments": [],
                "text": "Hierarchical motionbased frame rate conversion"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "3D object recognition and prototypes: one 2D view may be suucient"
            },
            "venue": {
                "fragments": [],
                "text": "I.R.S.T"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 314,
                                "start": 310
                            }
                        ],
                        "text": "This vectorized representation for 2D shape has been widely used, including network-based object recognition (Poggio and Edelman [37]), the linear combinations approach to recognition (Ullman and Basri [49], Poggio [35]), active shape models (Cootes and Taylor [15], Cootes, et al. [16]) and face recognition (Craw and Cameron [17][18])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 173
                            }
                        ],
                        "text": "For synthesizing images of faces, 3D facial models have been explored in the computer graphics, computer vision, and model-based image coding communities (Aitchison and Craw[1], Kang, Chen, and Hsu[24], Essa and Pentland [19], Akimoto, Suennaga, and Wallace[3], Waters and Terzopoulos[47], Aizawa, Harashima, and Saito[2])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 100
                            }
                        ],
                        "text": "This strategy for representing texture has been used, for example, in the face recognition works of Craw and Cameron [17], and Shackleton and Welsh [42]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Synthetic im-  ages of faces { an approach to model-based face  recognition"
            },
            "venue": {
                "fragments": [],
                "text": "In Proc. British Machine Vision Con-  ference,"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Vectorizing face images by i n terleaving shape and texture computations. A.I. Memo No. 1537, Artiicial Intelligence Laboratory, M a s sachusetts Institute of Technology"
            },
            "venue": {
                "fragments": [],
                "text": "Vectorizing face images by i n terleaving shape and texture computations. A.I. Memo No. 1537, Artiicial Intelligence Laboratory, M a s sachusetts Institute of Technology"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Conditions for viewpoint i n variant face recognition. A.I. Memo No"
            },
            "venue": {
                "fragments": [],
                "text": "Conditions for viewpoint i n variant face recognition. A.I. Memo No"
            },
            "year": 1432
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 211,
                                "start": 207
                            }
                        ],
                        "text": "The technique has been explored previously by Brunelli and Poggio [38] within the context of an \\example-based\" approach to computer graphics and by researchers in performance-driven animation (Williams [52][53], Patterson, Litwinowicz, and Greene [33])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Living pictures. In Models and  Techniques in Computer Animation, pages 2{12"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "F eature-based image metamorphosis Hierarchical model-based motion estimation"
            },
            "venue": {
                "fragments": [],
                "text": "SIGGRAPH '92 Proceedings Proceedings of the European Conference on Computer Vision"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Proceedings of the International Conference on Computer Vision"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the International Conference on Computer Vision"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 124
                            }
                        ],
                        "text": "While not yet applied to face recognition, this approach has been used for face detection under varying illumination (Sinha [45]) and for indexing of packaged grocery items using color (Swain and Ballard [46])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Object recognition via image invariances"
            },
            "venue": {
                "fragments": [],
                "text": "Investigative Ophthalmology and Visual Science"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 60
                            }
                        ],
                        "text": "However, we have found that a standard optical ow algorithm [7], preceded by normalization based on the eye locations, can do a good job at automatically computing dense pixelwise correspondences."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 73
                            }
                        ],
                        "text": "The optical ow technique uses the gradient-based, hierarchical method of Bergen and Hingorani [7] (also see Lucas and Kanade [29], Bergen, et al. [6])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 94
                            }
                        ],
                        "text": "The optical ow technique uses the gradient-based, hierarchical method of Bergen and Hingorani [7] (also see Lucas and Kanade [29], Bergen, et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Hierarchical motion-  based frame rate conversion"
            },
            "venue": {
                "fragments": [],
                "text": "Technical report,"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Face recognition: Features versus templates Active shape models -`Smart snakes Building and using exible models incorporating grey-level information"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. British Machine Vision Conference Proceedings of the International Conference on Computer Vision"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Correspondence and aane shape from two orthographic views: Motion and Recognition. A.I. Memo No"
            },
            "venue": {
                "fragments": [],
                "text": "Correspondence and aane shape from two orthographic views: Motion and Recognition. A.I. Memo No"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Conditions for viewpoint invariant face recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Conditions for viewpoint invariant face recognition"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 107
                            }
                        ],
                        "text": "The choice of an o -center view is also supported by the psychophysical experiments of Schyns and B\u007f ultho [41]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 20
                            }
                        ],
                        "text": "Schyns and B\u007f ultho [41] obtain a low recognition rate, but their results are di cult to compare since their stimuli are Gouraud shaded 3D faces that exclude texture information."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "ditions for viewpoint invariant face recognition"
            },
            "venue": {
                "fragments": [],
                "text": "A.I.  Memo No. 1432,"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 72
                            }
                        ],
                        "text": "In the exible matching approach (von der Malsburg and collaborators [30][25]), the input image is deformed in 2D to match the example view."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Distortion invari-  ant object recognition in the dynamic link archi-  tecture"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Computers,"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A novel approach to graphics. A.I. Memo No"
            },
            "venue": {
                "fragments": [],
                "text": "A novel approach to graphics. A.I. Memo No"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 68
                            }
                        ],
                        "text": "In the exible matching approach (von der Malsburg and collaborators [30][25]), the input image is deformed in 2D to match the example view."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "In [30], the deformation is driven by a matching of local \\end-stop\" features so that the resulting transformation between model and input is like a 2D warp rather than a global, rigid transform."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[30], who obtain 86% on a database of 86 people, and Pentland, et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A feature based approach to face recog-  nition"
            },
            "venue": {
                "fragments": [],
                "text": "In Proceedings IEEE Conf. on Computer Vi-  sion and Pattern Recognition,"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Model-based matching of line drawings by linear combinations 15"
            },
            "venue": {
                "fragments": [],
                "text": "Model-based matching of line drawings by linear combinations 15"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A novel approach to graphics. A.I. Memo No. 1354, Artiicial Intelligence Laboratory"
            },
            "venue": {
                "fragments": [],
                "text": "Massachusetts Institute of Technology"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 267,
                                "start": 263
                            }
                        ],
                        "text": "1 Introduction Existing work in face recognition has demonstrated good recognition performance on frontal, expressionless views of faces with controlled lighting (see Baron [4], Turk and Pentland [48], Bichsel [11], Brunelli and Poggio [14], and Gilbert and Yang [20])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 247,
                                "start": 231
                            }
                        ],
                        "text": "Existing work in face recognition has demonstrated good recognition performance on frontal, expressionless views of faces with controlled lighting (see Baron [4], Turk and Pentland [48], Bichsel [11], Brunelli and Poggio [14], and Gilbert and Yang [20])."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A real-time  face recognition system using custom VLSI hard-  ware"
            },
            "venue": {
                "fragments": [],
                "text": "In IEEE Workshop on Computer Architec-  tures for Machine Perception,"
            },
            "year": 1993
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 18,
            "methodology": 33
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 76,
        "totalPages": 8
    },
    "page_url": "https://www.semanticscholar.org/paper/Face-recognition-from-one-example-view-Beymer-Poggio/66505cb708b098a93331471f079965f6ded4ea7f?sort=total-citations"
}