{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2219581"
                        ],
                        "name": "B. Boser",
                        "slug": "B.-Boser",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Boser",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Boser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743797"
                        ],
                        "name": "I. Guyon",
                        "slug": "I.-Guyon",
                        "structuredName": {
                            "firstName": "Isabelle",
                            "lastName": "Guyon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Guyon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 207165665,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2599131a4bc2fa957338732a37c744cfe3e17b24",
            "isKey": false,
            "numCitedBy": 10840,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented. The technique is applicable to a wide variety of the classification functions, including Perceptrons, polynomials, and Radial Basis Functions. The effective number of parameters is adjusted automatically to match the complexity of the problem. The solution is expressed as a linear combination of supporting patterns. These are the subset of training patterns that are closest to the decision boundary. Bounds on the generalization performance based on the leave-one-out method and the VC-dimension are given. Experimental results on optical character recognition problems demonstrate the good generalization obtained when compared with other learning algorithms."
            },
            "slug": "A-training-algorithm-for-optimal-margin-classifiers-Boser-Guyon",
            "title": {
                "fragments": [],
                "text": "A training algorithm for optimal margin classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented, applicable to a wide variety of the classification functions, including Perceptrons, polynomials, and Radial Basis Functions."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '92"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2041866"
                        ],
                        "name": "L. Jackel",
                        "slug": "L.-Jackel",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Jackel",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Jackel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "122774836"
                        ],
                        "name": "A. Brunot",
                        "slug": "A.-Brunot",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Brunot",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Brunot"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152547641"
                        ],
                        "name": "Corinna Cortes",
                        "slug": "Corinna-Cortes",
                        "structuredName": {
                            "firstName": "Corinna",
                            "lastName": "Cortes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Corinna Cortes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747317"
                        ],
                        "name": "J. Denker",
                        "slug": "J.-Denker",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Denker",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Denker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2050470845"
                        ],
                        "name": "H. Drucker",
                        "slug": "H.-Drucker",
                        "structuredName": {
                            "firstName": "Harris",
                            "lastName": "Drucker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Drucker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743797"
                        ],
                        "name": "I. Guyon",
                        "slug": "I.-Guyon",
                        "structuredName": {
                            "firstName": "Isabelle",
                            "lastName": "Guyon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Guyon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145636949"
                        ],
                        "name": "Urs Muller",
                        "slug": "Urs-Muller",
                        "structuredName": {
                            "firstName": "Urs",
                            "lastName": "Muller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Urs Muller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "97914531"
                        ],
                        "name": "E. Sackinger",
                        "slug": "E.-Sackinger",
                        "structuredName": {
                            "firstName": "E.",
                            "lastName": "Sackinger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Sackinger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2812486"
                        ],
                        "name": "P. Simard",
                        "slug": "P.-Simard",
                        "structuredName": {
                            "firstName": "Patrice",
                            "lastName": "Simard",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Simard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 11259076,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d50dce749321301f0104689f2dc582303a83be65",
            "isKey": false,
            "numCitedBy": 631,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "COMPARISON OF LEARNINGALGORITHMS FOR HANDWRITTEN DIGITRECOGNITIONY. LeCun, L. Jackel, L. Bottou, A. Brunot, C. Cortes,J. Denker, H. Drucker, I. Guyon, U. M \u007fuller,E. S\u007fackinger, P. Simard, and V. VapnikBell Lab oratories, Holmdel, NJ 07733, USAEmail: yann@research.att.comAbstractThis pap er compares the p erformance of several classi er algorithmson a standard database of handwritten digits. We consider not only rawaccuracy, but also rejection, training time, recognition time, and memoryrequirements.1"
            },
            "slug": "Comparison-of-learning-algorithms-for-handwritten-LeCun-Jackel",
            "title": {
                "fragments": [],
                "text": "Comparison of learning algorithms for handwritten digit recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "This comparison of several learning algorithms for handwritten digits considers not only raw accuracy, but also rejection, training time, recognition time, and memory requirements."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404459229"
                        ],
                        "name": "J. Shawe-Taylor",
                        "slug": "J.-Shawe-Taylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shawe-Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shawe-Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685083"
                        ],
                        "name": "N. Cristianini",
                        "slug": "N.-Cristianini",
                        "structuredName": {
                            "firstName": "Nello",
                            "lastName": "Cristianini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cristianini"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16707671,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "4dde852a9852580b982f2dc5a833e733b79ac36c",
            "isKey": false,
            "numCitedBy": 55,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "A number of results have bounded generalization of a classifier in terms of its margin on the training points. There has been some debate about whether the minimum margin is the best measure of the distribution of training set margin values with which to estimate the generalization. Freund and Schapire have shown how a different function of the margin distribution can be used to bound the number of mistakes of an on-line learning algorithm for a perceptron, as well as an expected error bound. We show that a slight generalization of their construction can be used to give a pac style bound on the tail of the distribution of the generalization errors that arise from a given sample size. Algorithms arising from the approach are related to those of Cortes and Vapnik. We generalise the basic result to function classes with bounded fat- shattering dimension and the 1-norm of the slack variables which gives rise to Vapnik's box constraint algorithm. We also extend the results to the regression case and obtain bounds on the probability that a randomly chosen test point will have error greater than a given value. The bounds apply to the $\\epsilon$-insensitive loss function proposed by Vapnik for Support Vector Machine regression. A special case of this bound gives a bound on the probabilities in terms of the least squares error on the training set showing a quadratic decline in probability with margin."
            },
            "slug": "Robust-Bounds-on-Generalization-from-the-Margin-Shawe-Taylor-Cristianini",
            "title": {
                "fragments": [],
                "text": "Robust Bounds on Generalization from the Margin Distribution"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is shown that a slight generalization of their construction can be used to give a pac style bound on the tail of the distribution of the generalization errors that arise from a given sample size."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700597"
                        ],
                        "name": "Jyrki Kivinen",
                        "slug": "Jyrki-Kivinen",
                        "structuredName": {
                            "firstName": "Jyrki",
                            "lastName": "Kivinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jyrki Kivinen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794034"
                        ],
                        "name": "Manfred K. Warmuth",
                        "slug": "Manfred-K.-Warmuth",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Warmuth",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manfred K. Warmuth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15718458,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4dc175e8f6e7ca5c40ffd6fb9c6b92323bf7daf2",
            "isKey": false,
            "numCitedBy": 281,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider two algorithms for on-line prediction based on a linear model. The algorithms are the well-known Gradient Descent (GD) algorithm and a new algorithm, which we call EG *. They both maintain a weight vector using simple updates. For the GD algorithm, the weight vector is updated by subtracting from it the gradient of the squared error made on a prediction multiplied by a parameter called the learning rate. The EG* uses the components of the gradient in the exponents of factors that are used in updating the weight vector multiplicatively. We present worst-case on-line loss bounds for EG* and compare them to previously known bounds for the GD algorithm. The bounds suggest that although the on-line losses of the algorithms are in general incomparable, EG * has a much smaller loss if only few of the input variables are relevant for the predictions. Experiments show that the worst-case upper bounds are quite tight already on simple artificial data. Our main methodological idea is using a distance function between weight vectors both in motivating the algorithms and as a potential function in an amortized analysis that leads to worst-case loss bounds. Using squared Euclidean distance leads to the GD algorithm, and using the relative entropy leads to the EG* algorithm."
            },
            "slug": "Additive-versus-exponentiated-gradient-updates-for-Kivinen-Warmuth",
            "title": {
                "fragments": [],
                "text": "Additive versus exponentiated gradient updates for linear prediction"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The main methodological idea is using a distance function between weight vectors both in motivating the algorithms and as a potential function in an amortized analysis that leads to worst-case loss bounds."
            },
            "venue": {
                "fragments": [],
                "text": "STOC '95"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1388387856"
                        ],
                        "name": "N. Cesa-Bianchi",
                        "slug": "N.-Cesa-Bianchi",
                        "structuredName": {
                            "firstName": "Nicol\u00f2",
                            "lastName": "Cesa-Bianchi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cesa-Bianchi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703537"
                        ],
                        "name": "Y. Freund",
                        "slug": "Y.-Freund",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Freund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1772099"
                        ],
                        "name": "D. Helmbold",
                        "slug": "D.-Helmbold",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Helmbold",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Helmbold"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794034"
                        ],
                        "name": "Manfred K. Warmuth",
                        "slug": "Manfred-K.-Warmuth",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Warmuth",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manfred K. Warmuth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 124263,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "239ae23dc2f934cfa005261ade01023fe7950b82",
            "isKey": false,
            "numCitedBy": 624,
            "numCiting": 85,
            "paperAbstract": {
                "fragments": [],
                "text": "We analyze algorithms that predict a binary value by combining the predictions of several prediction strategies, called `experts''. Our analysis is for worst-case situations, i.e., we make no assumptions about the way the sequence of bits to be predicted is generated. We measure the performance of the algorithm by the difference between the expected number of mistakes it makes on the bit sequence and the expected number of mistakes made by the best expert on this sequence, where the expectation is taken with respect to the randomization in the predictions. We show that the minimum achievable difference is on the order of the square root of the number of mistakes of the best expert, and we give efficient algorithms that achieve this. Our upper and lower bounds have matching leading constants in most cases. We then show how this leads to certain kinds of pattern recognition/learning algorithms with performance bounds that improve on the best results currently known in this context. We also extend our analysis to the case in which log loss is used instead of the expected number of mistakes."
            },
            "slug": "How-to-use-expert-advice-Cesa-Bianchi-Freund",
            "title": {
                "fragments": [],
                "text": "How to use expert advice"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "This work analyzes algorithms that predict a binary value by combining the predictions of several prediction strategies, called `experts', and shows how this leads to certain kinds of pattern recognition/learning algorithms with performance bounds that improve on the best results currently known in this context."
            },
            "venue": {
                "fragments": [],
                "text": "STOC '93"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1772099"
                        ],
                        "name": "D. Helmbold",
                        "slug": "D.-Helmbold",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Helmbold",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Helmbold"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794034"
                        ],
                        "name": "Manfred K. Warmuth",
                        "slug": "Manfred-K.-Warmuth",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Warmuth",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manfred K. Warmuth"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 16694241,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c58afc1b7fd284351d6897f315617084087d4340",
            "isKey": false,
            "numCitedBy": 89,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "An algorithm is a weak learning algorithm if with some small probability it outputs a hypothesis with error slightly below 50%. This paper presents relationships between weak learning, weak prediction (where the probability of being correct is slightly larger than 50%), and consistency oracles (which decide whether or not a given set of examples is consistent with a concept in the class). Our main result is a simple polynomial prediction algorithm which makes only a single query to a consistency oracle and whose predictions have a polynomial edge over random guessing. We compare this prediction algorithm with several of the standard prediction techniques, deriving an improved worst case bound on Gibbs algorithm in the process. We use our algorithm to show that a concept class is polynomially learnable if and only if there is a polynomial probabilistic consistency oracle for the class. Since strong learning algorithms can be built from weak learning algorithms, our results also characterizes strong learnability."
            },
            "slug": "On-Weak-Learning-Helmbold-Warmuth",
            "title": {
                "fragments": [],
                "text": "On Weak Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper presents relationships between weak learning, weak prediction, and consistency oracles and uses an algorithm to show that a concept class is polynomially learnable if and only if there is a polynomial probabilistic consistency oracle for the class."
            },
            "venue": {
                "fragments": [],
                "text": "J. Comput. Syst. Sci."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2550989"
                        ],
                        "name": "Norbert Klasner",
                        "slug": "Norbert-Klasner",
                        "structuredName": {
                            "firstName": "Norbert",
                            "lastName": "Klasner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Norbert Klasner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723095"
                        ],
                        "name": "H. Simon",
                        "slug": "H.-Simon",
                        "structuredName": {
                            "firstName": "Hans",
                            "lastName": "Simon",
                            "middleNames": [
                                "Ulrich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Simon"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 5464604,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b8e18455c51bb6fcb9e00ca88b652e2561bdbf74",
            "isKey": false,
            "numCitedBy": 27,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "A simple method is presented which, loosely speaking, virtually removes noise or misfit from data, and thereby converts a \u201cnoise-free\u201d algorithm A, which on-line learns linear functions from data without noise or misfit, into a \u201cnoise-tolerant\u201d algorithm Ant which learns linear functions from data containing noise or misfit. Given some technical conditions, this conversion preserves optimality. For instance, the optimal noise-free algorithm B of Bernstein from [3] is converted into an optimal noisetolerant algorithm 13nt. The conversion also works properly for all function classes which are closed under addition and contain linear functions as a subclass. In the second part of the paper, we show that Bernstein\u2019s on-line learning algorithm B can ~e converted into a batch learning algorithm B which consumes an (almost) minimal number of random training examples. This is true for a whole class of \u201cpat-style\u201d batch learning models (including learning with an (c, ~)good model and learning with an e-bounded expected r-loss). The upper bound on the sample size is shown by applying a method of Littlestone from [9] which converts an online learning algorithm A into a batch learning algorithm A and relates the sample size of A to the total loss of A. For this purpose, Littlestone\u2019s method is extended from *The author gratefully acknowledges the support of Bundesministerium fur Forschung und Technologies grant olINlo2c/2. 1 Let O, l-valued functions to real-valued functions. The lower bound on the sample size is shown by generalizing and applying a method of Simon from [11]."
            },
            "slug": "From-noise-free-to-noise-tolerant-and-from-on-line-Klasner-Simon",
            "title": {
                "fragments": [],
                "text": "From noise-free to noise-tolerant and from on-line to batch learning"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A simple method is presented which virtually removes noise or misfit from data, and thereby converts a \u201cnoise-free\u201d algorithm A, which on-line learns linear functions from data without noise orMisfit, into a \u2018noises-tolerant\u2019 algorithm Ant which learns linearfunction from data containing noise ormisfit."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '95"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2961287"
                        ],
                        "name": "N. Littlestone",
                        "slug": "N.-Littlestone",
                        "structuredName": {
                            "firstName": "Nick",
                            "lastName": "Littlestone",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Littlestone"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 19753387,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a2606d97cf80839e7d223a0769529e2cc51a95bf",
            "isKey": false,
            "numCitedBy": 155,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "From-on-line-to-batch-learning-Littlestone",
            "title": {
                "fragments": [],
                "text": "From on-line to batch learning"
            },
            "venue": {
                "fragments": [],
                "text": "COLT '89"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34999823"
                        ],
                        "name": "T. Frie\u00df",
                        "slug": "T.-Frie\u00df",
                        "structuredName": {
                            "firstName": "Thilo-Thomas",
                            "lastName": "Frie\u00df",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Frie\u00df"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685083"
                        ],
                        "name": "N. Cristianini",
                        "slug": "N.-Cristianini",
                        "structuredName": {
                            "firstName": "Nello",
                            "lastName": "Cristianini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cristianini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153410082"
                        ],
                        "name": "I. Campbell",
                        "slug": "I.-Campbell",
                        "structuredName": {
                            "firstName": "I.",
                            "lastName": "Campbell",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Campbell"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1845,
                                "start": 29
                            }
                        ],
                        "text": "This method was suggested by Gallant (1986) who called it the pocket method. Littlestone (1989), suggested a two-phase method in which the performance of all of the rules is tested on a seperate test set and the rule with the least error is then used. Here we use a different method for converting the online perceptron algorithm into a batch learning algorithm; the method combines all of the rules generated by the online algorithm after it was run for just a single time through the training data. We now describe Helmbold and Warmuth\u2019s (1995) very simple \u201cleave-one-out\u201d method of converting an online learning algorithm into a batch learning algorithm. Our votedperceptron algorithm is a simple application of this general method. We start with the randomized version. Given a training set 3 x ' \" 6 ' )' 3 x ')\" 6 and an unlabeled instance x, we do the following. We select a number in #1 ' ' + uniformly at random. We then take the first examples in the training sequence and append the unlabeled instance to the end of this subsequence. We run the online algorithm on this sequence of length ( % , and use the prediction of the online algorithm on the last unlabeled instance. In the deterministic leave-one-out conversion, we modify the randomized leave-one-out conversion to make it deterministic in the obvious way by choosing the most likely prediction. That is, we compute the prediction that would result for all possible choices of in #1 ' ' + , and we take majority vote of these predictions. It is straightforward to show that taking a majority vote runs the risk of doubling the probability of mistake while it has the potential of significantly decreasing it. In this work we decided to focus primarily on deterministic voting rather than randomization. The following theorem follows directly from Helmbold and Warmuth (1995). (See also Kivinen and Warmuth (1997) and Cesa-Bianchi et al."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 547,
                                "start": 29
                            }
                        ],
                        "text": "This method was suggested by Gallant (1986) who called it the pocket method. Littlestone (1989), suggested a two-phase method in which the performance of all of the rules is tested on a seperate test set and the rule with the least error is then used. Here we use a different method for converting the online perceptron algorithm into a batch learning algorithm; the method combines all of the rules generated by the online algorithm after it was run for just a single time through the training data. We now describe Helmbold and Warmuth\u2019s (1995) very simple \u201cleave-one-out\u201d method of converting an online learning algorithm into a batch learning algorithm."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 29
                            }
                        ],
                        "text": "This method was suggested by Gallant (1986) who called it the pocket method."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 29
                            }
                        ],
                        "text": "This method was suggested by Gallant (1986) who called it the pocket method. Littlestone (1989), suggested a two-phase method in which the performance of all of the rules is tested on a seperate test set and the rule with the least error is then used."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60042307,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ec129af4c59e7813ea1e60a5a098b569afcdf4de",
            "isKey": true,
            "numCitedBy": 117,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The present invention is directed to non-hygroscopic, water-soluble sugar compositions which are prepared by grinding together in a dry, solid state, a white sugar component and a \"pulverizing aid\" in the form of a water-soluble maltodextrin having a measurable dextrose equivalent value not substantially above 20, said \"pulverizing aid\" being employed in amounts ranging from about 5 to about 20% by weight of said total composition, the resulting product having an average particle size such that 95% by weight of the composition passes through a 325 mesh, said composition being further characterized as having a ratio of weight average particle size to number average particle size of less than 2. The compositions are free-flowing powders useful in preparing icings, buttercreams and fudges."
            },
            "slug": "The-Kernel-Adatron-:-A-fast-and-simple-learning-for-Frie\u00df-Cristianini",
            "title": {
                "fragments": [],
                "text": "The Kernel-Adatron : A fast and simple learning procedure for support vector machines"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The present invention is directed to non-hygroscopic, water-soluble sugar compositions which are prepared by grinding together in a dry, solid state, a white sugar component and a \"pulverizing aid\" in the form of a water- soluble maltodextrin having a measurable dextrose equivalent value not substantially above 20."
            },
            "venue": {
                "fragments": [],
                "text": "ICML 1998"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 28637672,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "385197d4c02593e2823c71e4f90a0993b703620e",
            "isKey": false,
            "numCitedBy": 26321,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "A comprehensive look at learning and generalization theory. The statistical theory of learning and generalization concerns the problem of choosing desired functions on the basis of empirical data. Highly applicable to a variety of computer science and robotics fields, this book offers lucid coverage of the theory as a whole. Presenting a method for determining the necessary and sufficient conditions for consistency of learning process, the author covers function estimates from small data pools, applying these estimations to real-life problems, and much more."
            },
            "slug": "Statistical-learning-theory-Vapnik",
            "title": {
                "fragments": [],
                "text": "Statistical learning theory"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Presenting a method for determining the necessary and sufficient conditions for consistency of learning process, the author covers function estimates from small data pools, applying these estimations to real-life problems, and much more."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51083130"
                        ],
                        "name": "F. Rosenblatt",
                        "slug": "F.-Rosenblatt",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Rosenblatt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Rosenblatt"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12781225,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "5d11aad09f65431b5d3cb1d85328743c9e53ba96",
            "isKey": false,
            "numCitedBy": 9074,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "The first of these questions is in the province of sensory physiology, and is the only one for which appreciable understanding has been achieved. This article will be concerned primarily with the second and third questions, which are still subject to a vast amount of speculation, and where the few relevant facts currently supplied by neurophysiology have not yet been integrated into an acceptable theory. With regard to the second question, two alternative positions have been maintained. The first suggests that storage of sensory information is in the form of coded representations or images, with some sort of one-to-one mapping between the sensory stimulus"
            },
            "slug": "The-perceptron:-a-probabilistic-model-for-storage-Rosenblatt",
            "title": {
                "fragments": [],
                "text": "The perceptron: a probabilistic model for information storage and organization in the brain."
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This article will be concerned primarily with the second and third questions, which are still subject to a vast amount of speculation, and where the few relevant facts currently supplied by neurophysiology have not yet been integrated into an acceptable theory."
            },
            "venue": {
                "fragments": [],
                "text": "Psychological review"
            },
            "year": 1958
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1847175"
                        ],
                        "name": "M. Minsky",
                        "slug": "M.-Minsky",
                        "structuredName": {
                            "firstName": "Marvin",
                            "lastName": "Minsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Minsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2434678"
                        ],
                        "name": "S. Papert",
                        "slug": "S.-Papert",
                        "structuredName": {
                            "firstName": "Seymour",
                            "lastName": "Papert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Papert"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 5400596,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f74ded11f72099d16591a1191d72262ae6b5f14a",
            "isKey": false,
            "numCitedBy": 3040,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Cambridge, Mass.: MIT Press, 1972. 2nd. ed. The book's aim is to seek general results from the close study of abstract version of devices known as perceptrons"
            },
            "slug": "Perceptrons-an-introduction-to-computational-Minsky-Papert",
            "title": {
                "fragments": [],
                "text": "Perceptrons - an introduction to computational geometry"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "The aim of this book is to seek general results from the close study of abstract version of devices known as perceptrons."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1969
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2545803"
                        ],
                        "name": "M. Aizerman",
                        "slug": "M.-Aizerman",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Aizerman",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Aizerman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60493317,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c3caf34c1c86633b6e80dca29e3cb2b6367a0f93",
            "isKey": false,
            "numCitedBy": 1692,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Theoretical-Foundations-of-the-Potential-Function-Aizerman",
            "title": {
                "fragments": [],
                "text": "Theoretical Foundations of the Potential Function Method in Pattern Recognition Learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1964
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 187,
                                "start": 164
                            }
                        ],
                        "text": ". i (Vk, ck)) an unlabeled instance: x compute a predicted label 9 as follows: k S= C ci sign(vi \nx); Q = sign(s) . i=l Figure 1: The voted-perceptron algorithm. of Vapnik and Chervonenkis [17] for \nthe linearly separable case."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 122
                            }
                        ],
                        "text": "As we have recently learned, the performance of the final prediction vector has be en analyzed by Vapnik and Chervonenkis [19]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 249,
                                "start": 245
                            }
                        ],
                        "text": "On the other hand, the resulting algorithm is very simple and easy to implement, and the theoretical bounds on the expected ge neralization error of the new algorithm are almost identical to the bounds for SVM's given by Vapnik and Chervonenkis [19] in the linearly separable case."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 241,
                                "start": 218
                            }
                        ],
                        "text": "On the other hand, the resulting algorithmis very \nsimple and easy to implement, and the theo- retical bounds on the expected generalizationerror of the \nnew algorithm are almost identical to the bounds for SVM given by Vapnik and Chervonenkis [17] in the \nlinearly separable case."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 52
                            }
                        ],
                        "text": "We also quote a theorem of Vapni k and Chervonenkis [19] for the linearly separable case."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 11
                            }
                        ],
                        "text": "Theorem 6 (Vapnik and Chervonenkis) Assume \nall exam- ples are generated i.i.d. at random."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Chervonenkis.Theory of pattern recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Nauka, Moscow,"
            },
            "year": 1974
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 24
                            }
                        ],
                        "text": "perceptron algorithm of Rosenblatt (1958, 1962) and a trans formation of online learning algorithms to batch learning algorithms developed by He lmbold and Warmuth (1995). Moreover, following the work of Aizerman, Braverman and Roz onoer (1964), we show that kernel functions can be used with our algorithm so that w e can run our algorithm efficiently in very high dimensional spaces."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 839,
                                "start": 24
                            }
                        ],
                        "text": "perceptron algorithm of Rosenblatt (1958, 1962) and a trans formation of online learning algorithms to batch learning algorithms developed by He lmbold and Warmuth (1995). Moreover, following the work of Aizerman, Braverman and Roz onoer (1964), we show that kernel functions can be used with our algorithm so that w e can run our algorithm efficiently in very high dimensional spaces. Our algorithm and i ts analysis involve little more than combining these three known methods. On the other hand, the resulting algorithm is very simple and easy to implement, and the theoretical bound s o the expected generalization error of the new algorithm are almost identical to th e bounds for SVM\u2019s given by Vapnik and Chervonenkis (1974) in the linearly separable ca s . We repeated some of the experiments performed by Cortes and V pnik (1995) on the use of SVM on the problem of classifying handwritten digits."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1493,
                                "start": 24
                            }
                        ],
                        "text": "perceptron algorithm of Rosenblatt (1958, 1962) and a trans formation of online learning algorithms to batch learning algorithms developed by He lmbold and Warmuth (1995). Moreover, following the work of Aizerman, Braverman and Roz onoer (1964), we show that kernel functions can be used with our algorithm so that w e can run our algorithm efficiently in very high dimensional spaces. Our algorithm and i ts analysis involve little more than combining these three known methods. On the other hand, the resulting algorithm is very simple and easy to implement, and the theoretical bound s o the expected generalization error of the new algorithm are almost identical to th e bounds for SVM\u2019s given by Vapnik and Chervonenkis (1974) in the linearly separable ca s . We repeated some of the experiments performed by Cortes and V pnik (1995) on the use of SVM on the problem of classifying handwritten digits. We tested both the votedperceptron algorithm and a variant based on averaging rathe r than voting. These experiments indicate that the use of kernel functions with the per ceptron algorithm yields a dramatic improvement in performance, both in test accuracy and in computation time. In addition, we found that, when training time is limited, th e voted-perceptron algorithm performs better than the traditional way of using the percep t on algorithm (although all methods converge eventually to roughly the same level of per formance). Recently, Friess, Cristianini and Campbell (1998) have exp erimented with a different online learning algorithm called the adatron."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 732,
                                "start": 24
                            }
                        ],
                        "text": "perceptron algorithm of Rosenblatt (1958, 1962) and a trans formation of online learning algorithms to batch learning algorithms developed by He lmbold and Warmuth (1995). Moreover, following the work of Aizerman, Braverman and Roz onoer (1964), we show that kernel functions can be used with our algorithm so that w e can run our algorithm efficiently in very high dimensional spaces. Our algorithm and i ts analysis involve little more than combining these three known methods. On the other hand, the resulting algorithm is very simple and easy to implement, and the theoretical bound s o the expected generalization error of the new algorithm are almost identical to th e bounds for SVM\u2019s given by Vapnik and Chervonenkis (1974) in the linearly separable ca s ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On convergence proofs on perceptr"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1962
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 252,
                                "start": 221
                            }
                        ],
                        "text": "On the other hand, the resulting algorithm is very simple and easy to implement, and the theoretical bound s o the expected generalization error of the new algorithm are almost identical to th e bounds for SVM\u2019s given by Vapnik and Chervonenkis (1974) in the linearly separable ca s ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 359,
                                "start": 221
                            }
                        ],
                        "text": "On the other hand, the resulting algorithm is very simple and easy to implement, and the theoretical bound s o the expected generalization error of the new algorithm are almost identical to th e bounds for SVM\u2019s given by Vapnik and Chervonenkis (1974) in the linearly separable ca s . We repeated some of the experiments performed by Cortes and V pnik (1995) on the use of SVM on the problem of classifying handwritten digits."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 28
                            }
                        ],
                        "text": "See also the recent work of Shawe-Taylor and Cristianini (1998) who used this techni que to derive generalization error bounds for any large margin classifier."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1013,
                                "start": 221
                            }
                        ],
                        "text": "On the other hand, the resulting algorithm is very simple and easy to implement, and the theoretical bound s o the expected generalization error of the new algorithm are almost identical to th e bounds for SVM\u2019s given by Vapnik and Chervonenkis (1974) in the linearly separable ca s . We repeated some of the experiments performed by Cortes and V pnik (1995) on the use of SVM on the problem of classifying handwritten digits. We tested both the votedperceptron algorithm and a variant based on averaging rathe r than voting. These experiments indicate that the use of kernel functions with the per ceptron algorithm yields a dramatic improvement in performance, both in test accuracy and in computation time. In addition, we found that, when training time is limited, th e voted-perceptron algorithm performs better than the traditional way of using the percep t on algorithm (although all methods converge eventually to roughly the same level of per formance). Recently, Friess, Cristianini and Campbell (1998) have exp erimented with a different online learning algorithm called the adatron."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Robust bounds o"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 57
                            }
                        ],
                        "text": "The following \ntheorem follows directly from Helmbold and Warmuth [7]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 108
                            }
                        ],
                        "text": "[41 Nicolb Cesa-Bianchi, Yoav Freund, \nDavid Haussler, David P. Helmbold, Robert E. Schapire, and Manfred K. Warmuth."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 33
                            }
                        ],
                        "text": "[8] Jyrki Kivinen and Manfred K. Warmuth."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 10
                            }
                        ],
                        "text": "(See also Kivinen and Warmuth [8] and Cesa-Bianchi \net al. [4].)"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 196,
                                "start": 189
                            }
                        ],
                        "text": "In this paper, we propose to use a more sophisticated \nmethod of applying the on-line perceptron algorithm to batch learning, namely, a variation of the leave-one-out \nmethod of Helmbold and Warmuth [7]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 29
                            }
                        ],
                        "text": "We now describe \nHelmbold and Warmuth s [7] very sim- ple leave-one-out method of converting an online learn- ing algorithm \ninto a batch learning algorithm."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 335,
                                "start": 328
                            }
                        ],
                        "text": "Large Margin Classification Using the Perceptron Algorithm Yoav Freund Robert E. Schapire AT&#38;T \nLabs 180 Park Avenue Plorham Park, NJ 07932-0971 USA { yoav, schapire}@research.att.com Abstract We \nintroduce and analyze a new algorithm for lin- ear classification whichcombines Rosenblatt sper- ceptron \nalgorithm with Helmbold and Warmuth s leave-one-out method."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 203,
                                "start": 196
                            }
                        ],
                        "text": "The algo- rithm is based on the well known perceptron algorithm of Rosenblatt [ 14, 151 and \na transformation of online learning algorithms to batch learning algorithms developed by Helm- bold and \nWarmuth [7]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 31
                            }
                        ],
                        "text": "(Se e also Kivinen and Warmuth [10] and Cesa-Bianchi et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 49
                            }
                        ],
                        "text": "IEEE, 1986. r71 David P. Helmbold and Manfred K. Warmuth."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Additive versus exponentiated gradient upda  tes for linear prediction.Information and Computation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 85
                            }
                        ],
                        "text": "We repeated some of the experiments performed by Cortes and Vapnik [5] on the \nuse of SVM on the problem of classify- ing handwritten digits."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 205
                            }
                        ],
                        "text": "On the other hand, the resulting algorithmis very \nsimple and easy to implement, and the theo- retical bounds on the expected generalizationerror of the \nnew algorithm are almost identical to the bounds for SVM given by Vapnik and Chervonenkis [17] in the \nlinearly separable case."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 147
                            }
                        ],
                        "text": "One of the most influential developments in the theory of machine learning in the last f w years is Vapnik's work on support vector machines (SVM) [18]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 155
                            }
                        ],
                        "text": "INTRODUCTION One of the most influential developments \nin the theory of machine learning in the last few years is Vapnik s work on support vector machines (SVM) \n[16]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Estimation of Dependences"
            },
            "venue": {
                "fragments": [],
                "text": "Based on Empirical Data  . Springer-Verlag,"
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 137
                            }
                        ],
                        "text": "The use of kernel func- tions for classification \nproblems was suggested by Baser, Guyon and Vapnik [3], continuing the work of Aizerman, Braverman and \nRozonoer [ 11."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 115
                            }
                        ],
                        "text": "The use of kernel functions for classification problems was proposed by suggested Aizerman, Braverman and Rozonoer [1] who specifically described a method for combining kernal functions with the perceptron algorithm."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 66
                            }
                        ],
                        "text": "Moreover, following the work of Aizer man, Braverman and Rozonoer [1], we show that kernel functions can be used with our algorithm so that w e c n run our algorithm efficiently in very high dimensional spaces."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Theoretical foundati  ons of the potential function method in pattern recognition learning.  Automation and Remote Control"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1964
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102790204"
                        ],
                        "name": "A. Novikoff",
                        "slug": "A.-Novikoff",
                        "structuredName": {
                            "firstName": "Albert",
                            "lastName": "Novikoff",
                            "middleNames": [
                                "B.",
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Novikoff"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122810543,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "eba6b55f7d3302471579eedd5c1558b38f4ea8da",
            "isKey": false,
            "numCitedBy": 439,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "ON-CONVERGENCE-PROOFS-FOR-PERCEPTRONS-Novikoff",
            "title": {
                "fragments": [],
                "text": "ON CONVERGENCE PROOFS FOR PERCEPTRONS"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1963
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49637194"
                        ],
                        "name": "A. Mullin",
                        "slug": "A.-Mullin",
                        "structuredName": {
                            "firstName": "Albert",
                            "lastName": "Mullin",
                            "middleNames": [
                                "A."
                            ],
                            "suffix": ""
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Mullin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51083130"
                        ],
                        "name": "F. Rosenblatt",
                        "slug": "F.-Rosenblatt",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Rosenblatt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Rosenblatt"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61566132,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cccc0a4817fd5f6d8758c66b4065a23897d49f1d",
            "isKey": false,
            "numCitedBy": 2369,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Principles-of-neurodynamics-Mullin-Rosenblatt",
            "title": {
                "fragments": [],
                "text": "Principles of neurodynamics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1962
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143618092"
                        ],
                        "name": "H. D. Block",
                        "slug": "H.-D.-Block",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Block",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. D. Block"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 56720069,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "eddfd0b187a06c1742932a63d002ea767ce1cdbf",
            "isKey": false,
            "numCitedBy": 352,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-perceptron:-a-model-for-brain-functioning.-I-Block",
            "title": {
                "fragments": [],
                "text": "The perceptron: a model for brain functioning. I"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1962
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61480753,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "edb62d05f8eeaa7e1921c6c25c544935a2b6b131",
            "isKey": false,
            "numCitedBy": 415,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Theory-of-Pattern-Recognition-Amari",
            "title": {
                "fragments": [],
                "text": "A Theory of Pattern Recognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1968
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 32
                            }
                        ],
                        "text": "This algorithm was suggested by Anlauf and Biehl (1989) as a method for calculating the largest margin c lassifier (also called the \u201cmaximally stable perceptron\u201d)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The adatron: an adaptive pe"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Optimal linear discriminants"
            },
            "venue": {
                "fragments": [],
                "text": "Eighth International C&ference on Pattern Recognition"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "1998 (to appear))"
            },
            "venue": {
                "fragments": [],
                "text": "Statistical Learning Theory. Wiley"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "1974).Theory of pattern recognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1974
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Vapnik . Statistical Learning Theory"
            },
            "venue": {
                "fragments": [],
                "text": "Principles of Neurodynamics"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Robust bounds o"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The perceptron: A probabilistic mod"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1958
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[12] give a detailed comparison of algorithms on this dataset."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[12] have published a detailed com parison of the performance of some of the best digit classification systems in this setup."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Comparison of learning algor  ithms for handwritten digit recognition"
            },
            "venue": {
                "fragments": [],
                "text": "InInternational Conference on Artificial Neural Networks"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The kernladatron : A fast and simple learning procedure for support vector machines"
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning : Proceedings of the Fifteenth International Conference"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 43
                            }
                        ],
                        "text": "Recently, Friess, Cristianini and Campbell [7] have experimented with a different online learning algorithm called theadatron."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The kernel-adat  ron: A fast and simple learning procedure for support vector machines"
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning: Proceedings of the Fifteenth International Conference"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Additive versus expone"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Theory of pattern recognition. Nauka, Moscow"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1974
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Helmbold and Manfred K . Warmuth . On weak learning"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Computer and System Sciences"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Warmuth . How to use expert advice"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of the Association for Computing Machinery"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 67
                            }
                        ],
                        "text": "Journal of the Association for Computing Machinery, 44(3):427-485, May 1997. r51 Corinna Cortes and \nVladimir Vapnik."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Corinna Cortes and Vladimir Vapnik"
            },
            "venue": {
                "fragments": [],
                "text": "Support-vector networks. Machine Learning, 20(3):273-297, September 1995. S. I. Gallant. Optimal linear discriminants. In Eighth"
            },
            "year": 1997
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 6,
            "methodology": 6,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 38,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/Large-Margin-Classification-Using-the-Perceptron-Freund-Schapire/2479a5cf6cefefb83166c612564787414e47131f?sort=total-citations"
}