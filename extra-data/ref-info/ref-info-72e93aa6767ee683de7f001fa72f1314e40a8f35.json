{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144638694"
                        ],
                        "name": "Adam Coates",
                        "slug": "Adam-Coates",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Coates",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adam Coates"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697141"
                        ],
                        "name": "Honglak Lee",
                        "slug": "Honglak-Lee",
                        "structuredName": {
                            "firstName": "Honglak",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Honglak Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The actual images in the experiments are 2D, but for simplicity, our images in the visualization are in 1D."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 4
                            }
                        ],
                        "text": "For K-means, we downsample images to 40x40 in order to lower computational costs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 150
                            }
                        ],
                        "text": "We also compare our method against several other algorithms such as deep autoencoders (Hinton & Salakhutdinov, 2006; Bengio et al., 2007) and Kmeans (Coates et al., 2011)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 47
                            }
                        ],
                        "text": "J. Implementation details for autoencoders and K-means\nIn our implementation, deep autoencoders are also locally connected and use sigmoidal activation function."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 295,
                                "start": 276
                            }
                        ],
                        "text": "\u2026we use a large dataset generated by sampling random frames from random YouTube videos.1 Our input data are 200x200 images, much larger than typical 32x32 images used in deep learning and unsupervised feature learning (Krizhevsky, 2009; Ciresan et al., 2010; Le et al., 2010; Coates et al., 2011)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 49
                            }
                        ],
                        "text": "In our experiments, we used 30,000 centroids for Kmeans."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Copyright 2012 by the author(s)/owner(s)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 47
                            }
                        ],
                        "text": "We also varied the parameters of autoencoders, K-means and chose them to maximize performances given resource constraints."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 85
                            }
                        ],
                        "text": "For instance, we would like to understand if it is possible to build a face detector from only unlabeled images."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 293,
                                "start": 274
                            }
                        ],
                        "text": "Successful feature learning algorithms and their applications can be found in recent literature using a variety of approaches such as RBMs (Hinton et al., 2006), autoencoders (Hinton & Salakhutdinov, 2006; Bengio et al., 2007), sparse coding (Lee et al., 2007) and K-means (Coates et al., 2011)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 308212,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "be9a17321537d9289875fe475b71f4821457b435",
            "isKey": false,
            "numCitedBy": 2596,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "A great deal of research has focused on algorithms for learning features from unlabeled data. Indeed, much progress has been made on benchmark datasets like NORB and CIFAR by employing increasingly complex unsupervised learning algorithms and deep models. In this paper, however, we show that several simple factors, such as the number of hidden nodes in the model, may be more important to achieving high performance than the learning algorithm or the depth of the model. Specifically, we will apply several othe-shelf feature learning algorithms (sparse auto-encoders, sparse RBMs, K-means clustering, and Gaussian mixtures) to CIFAR, NORB, and STL datasets using only singlelayer networks. We then present a detailed analysis of the eect of changes in the model setup: the receptive field size, number of hidden nodes (features), the step-size (\u201cstride\u201d) between extracted features, and the eect of whitening. Our results show that large numbers of hidden nodes and dense feature extraction are critical to achieving high performance\u2014so critical, in fact, that when these parameters are pushed to their limits, we achieve state-of-the-art performance on both CIFAR-10 and NORB using only a single layer of features. More surprisingly, our best performance is based on K-means clustering, which is extremely fast, has no hyperparameters to tune beyond the model structure itself, and is very easy to implement. Despite the simplicity of our system, we achieve accuracy beyond all previously published results on the CIFAR-10 and NORB datasets (79.6% and 97.2% respectively)."
            },
            "slug": "An-Analysis-of-Single-Layer-Networks-in-Feature-Coates-Ng",
            "title": {
                "fragments": [],
                "text": "An Analysis of Single-Layer Networks in Unsupervised Feature Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "The results show that large numbers of hidden nodes and dense feature extraction are critical to achieving high performance\u2014so critical, in fact, that when these parameters are pushed to their limits, they achieve state-of-the-art performance on both CIFAR-10 and NORB using only a single layer of features."
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064160"
                        ],
                        "name": "A. Krizhevsky",
                        "slug": "A.-Krizhevsky",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Krizhevsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Krizhevsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 235,
                                "start": 219
                            }
                        ],
                        "text": "\u2026we use a large dataset generated by sampling random frames from random YouTube videos.1 Our input data are 200x200 images, much larger than typical 32x32 images used in deep learning and unsupervised feature learning (Krizhevsky, 2009; Ciresan et al., 2010; Le et al., 2010; Coates et al., 2011)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 122
                            }
                        ],
                        "text": "In Figure 10, we show how the weights are divided and stored in different \u201cpartitions,\u201d or more simply, machines (see also (Krizhevsky, 2009))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Copyright 2012 by the author(s)/owner(s)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 18268744,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5d90f06bb70a0a3dced62413346235c02b1aa086",
            "isKey": false,
            "numCitedBy": 17080,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Groups at MIT and NYU have collected a dataset of millions of tiny colour images from the web. It is, in principle, an excellent dataset for unsupervised training of deep generative models, but previous researchers who have tried this have found it dicult to learn a good set of lters from the images. We show how to train a multi-layer generative model that learns to extract meaningful features which resemble those found in the human visual cortex. Using a novel parallelization algorithm to distribute the work among multiple machines connected on a network, we show how training such a model can be done in reasonable time. A second problematic aspect of the tiny images dataset is that there are no reliable class labels which makes it hard to use for object recognition experiments. We created two sets of reliable labels. The CIFAR-10 set has 6000 examples of each of 10 classes and the CIFAR-100 set has 600 examples of each of 100 non-overlapping classes. Using these labels, we show that object recognition is signicantly improved by pre-training a layer of features on a large set of unlabeled tiny images."
            },
            "slug": "Learning-Multiple-Layers-of-Features-from-Tiny-Krizhevsky",
            "title": {
                "fragments": [],
                "text": "Learning Multiple Layers of Features from Tiny Images"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "It is shown how to train a multi-layer generative model that learns to extract meaningful features which resemble those found in the human visual cortex, using a novel parallelization algorithm to distribute the work among multiple machines connected on a network."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153440022"
                        ],
                        "name": "Ian J. Goodfellow",
                        "slug": "Ian-J.-Goodfellow",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Goodfellow",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ian J. Goodfellow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2827616"
                        ],
                        "name": "Quoc V. Le",
                        "slug": "Quoc-V.-Le",
                        "structuredName": {
                            "firstName": "Quoc",
                            "lastName": "Le",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Quoc V. Le"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34927843"
                        ],
                        "name": "Andrew M. Saxe",
                        "slug": "Andrew-M.-Saxe",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Saxe",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew M. Saxe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697141"
                        ],
                        "name": "Honglak Lee",
                        "slug": "Honglak-Lee",
                        "structuredName": {
                            "firstName": "Honglak",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Honglak Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "t consisting of faces and random distractors, we discovered a feature that is highly selective for faces. This further conrms the power of deep networks in learning invariant feature representations [7, 8]. 2 Dataset constructions Our dataset is constructed by sampling frames from 10 million YouTube videos. To avoid duplicates, each video contributes only one image to the dataset. Each image has 200x20"
                    },
                    "intents": []
                }
            ],
            "corpusId": 1808153,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3137bc367c61c0e507a5e3c1f8caeb26f292d79f",
            "isKey": false,
            "numCitedBy": 412,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "For many pattern recognition tasks, the ideal input feature would be invariant to multiple confounding properties (such as illumination and viewing angle, in computer vision applications). Recently, deep architectures trained in an unsupervised manner have been proposed as an automatic method for extracting useful features. However, it is difficult to evaluate the learned features by any means other than using them in a classifier. In this paper, we propose a number of empirical tests that directly measure the degree to which these learned features are invariant to different input transformations. We find that stacked autoencoders learn modestly increasingly invariant features with depth when trained on natural images. We find that convolutional deep belief networks learn substantially more invariant features in each layer. These results further justify the use of \"deep\" vs. \"shallower\" representations, but suggest that mechanisms beyond merely stacking one autoencoder on top of another may be important for achieving invariance. Our evaluation metrics can also be used to evaluate future work in deep learning, and thus help the development of future algorithms."
            },
            "slug": "Measuring-Invariances-in-Deep-Networks-Goodfellow-Le",
            "title": {
                "fragments": [],
                "text": "Measuring Invariances in Deep Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A number of empirical tests are proposed that directly measure the degree to which these learned features are invariant to different input transformations and find that stacked autoencoders learn modestly increasingly invariant features with depth when trained on natural images and convolutional deep belief networks learn substantially more invariant Features in each layer."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697141"
                        ],
                        "name": "Honglak Lee",
                        "slug": "Honglak-Lee",
                        "structuredName": {
                            "firstName": "Honglak",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Honglak Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1785346"
                        ],
                        "name": "Roger B. Grosse",
                        "slug": "Roger-B.-Grosse",
                        "structuredName": {
                            "firstName": "Roger",
                            "lastName": "Grosse",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Roger B. Grosse"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2615814"
                        ],
                        "name": "R. Ranganath",
                        "slug": "R.-Ranganath",
                        "structuredName": {
                            "firstName": "Rajesh",
                            "lastName": "Ranganath",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Ranganath"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 37
                            }
                        ],
                        "text": "1This is different from the work of (Lee et al., 2009) who trained their model on images from one class."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 47
                            }
                        ],
                        "text": "They also demonstrate that con-\nvolutional DBNs (Lee et al., 2009), trained on aligned images of faces, can learn a face detector."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 210
                            }
                        ],
                        "text": "\u2026in the brain is an area of active investigation, but current experimental evidence suggests the possibility that some neurons in the temporal cortex are highly selective for object categories such as faces or hands (Desimone et al., 1984), and perhaps even specific people (Quiroga et al., 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 143
                            }
                        ],
                        "text": "First, to scale the autoencoder to large images, we use a simple idea known as local receptive fields (LeCun et al., 1998; Raina et al., 2009; Lee et al., 2009; Le et al., 2010)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 109
                            }
                        ],
                        "text": "This is a stark difference between our approach and previous work (LeCun et al., 1998; Jarrett et al., 2009; Lee et al., 2009)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 130
                            }
                        ],
                        "text": "Addressing this issue is a focus of recent work in deep learning (Hinton et al., 2006; Bengio et al., 2007; Bengio & LeCun, 2007; Lee et al., 2008; 2009) which build hierarchies of feature representations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This experiment shows that patches, being detected as faces by the OpenCV face detector, account for less than 3% of the 100,000 sampled patches"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12008458,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1e80f755bcbf10479afd2338cec05211fdbd325c",
            "isKey": false,
            "numCitedBy": 2510,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "There has been much interest in unsupervised learning of hierarchical generative models such as deep belief networks. Scaling such models to full-sized, high-dimensional images remains a difficult problem. To address this problem, we present the convolutional deep belief network, a hierarchical generative model which scales to realistic image sizes. This model is translation-invariant and supports efficient bottom-up and top-down probabilistic inference. Key to our approach is probabilistic max-pooling, a novel technique which shrinks the representations of higher layers in a probabilistically sound way. Our experiments show that the algorithm learns useful high-level visual features, such as object parts, from unlabeled images of objects and natural scenes. We demonstrate excellent performance on several visual recognition tasks and show that our model can perform hierarchical (bottom-up and top-down) inference over full-sized images."
            },
            "slug": "Convolutional-deep-belief-networks-for-scalable-of-Lee-Grosse",
            "title": {
                "fragments": [],
                "text": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "The convolutional deep belief network is presented, a hierarchical generative model which scales to realistic image sizes and is translation-invariant and supports efficient bottom-up and top-down probabilistic inference."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '09"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2979876"
                        ],
                        "name": "R. Raina",
                        "slug": "R.-Raina",
                        "structuredName": {
                            "firstName": "Rajat",
                            "lastName": "Raina",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Raina"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2070938295"
                        ],
                        "name": "Anand Madhavan",
                        "slug": "Anand-Madhavan",
                        "structuredName": {
                            "firstName": "Anand",
                            "lastName": "Madhavan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anand Madhavan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 90
                            }
                        ],
                        "text": "To support parallelism on this cluster, we use the idea of local receptive fields, e.g., (Raina et al., 2009; Le et al., 2010; 2011b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 209,
                                "start": 205
                            }
                        ],
                        "text": "\u2026in the brain is an area of active investigation, but current experimental evidence suggests the possibility that some neurons in the temporal cortex are highly selective for object categories such as faces or hands (Desimone et al., 1984), and perhaps even specific people (Quiroga et al., 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Copyright 2012 by the author(s)/owner(s)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 123
                            }
                        ],
                        "text": "First, to scale the autoencoder to large images, we use a simple idea known as local receptive fields (LeCun et al., 1998; Raina et al., 2009; Lee et al., 2009; Le et al., 2010)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 392458,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e337c5e4c23999c36f64bcb33ebe6b284e1bcbf1",
            "isKey": false,
            "numCitedBy": 640,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "The promise of unsupervised learning methods lies in their potential to use vast amounts of unlabeled data to learn complex, highly nonlinear models with millions of free parameters. We consider two well-known unsupervised learning models, deep belief networks (DBNs) and sparse coding, that have recently been applied to a flurry of machine learning applications (Hinton & Salakhutdinov, 2006; Raina et al., 2007). Unfortunately, current learning algorithms for both models are too slow for large-scale applications, forcing researchers to focus on smaller-scale models, or to use fewer training examples.\n In this paper, we suggest massively parallel methods to help resolve these problems. We argue that modern graphics processors far surpass the computational capabilities of multicore CPUs, and have the potential to revolutionize the applicability of deep unsupervised learning methods. We develop general principles for massively parallelizing unsupervised learning tasks using graphics processors. We show that these principles can be applied to successfully scaling up learning algorithms for both DBNs and sparse coding. Our implementation of DBN learning is up to 70 times faster than a dual-core CPU implementation for large models. For example, we are able to reduce the time required to learn a four-layer DBN with 100 million free parameters from several weeks to around a single day. For sparse coding, we develop a simple, inherently parallel algorithm, that leads to a 5 to 15-fold speedup over previous methods."
            },
            "slug": "Large-scale-deep-unsupervised-learning-using-Raina-Madhavan",
            "title": {
                "fragments": [],
                "text": "Large-scale deep unsupervised learning using graphics processors"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is argued that modern graphics processors far surpass the computational capabilities of multicore CPUs, and have the potential to revolutionize the applicability of deep unsupervised learning methods."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '09"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761978"
                        ],
                        "name": "D. Erhan",
                        "slug": "D.-Erhan",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Erhan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Erhan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760871"
                        ],
                        "name": "Aaron C. Courville",
                        "slug": "Aaron-C.-Courville",
                        "structuredName": {
                            "firstName": "Aaron",
                            "lastName": "Courville",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aaron C. Courville"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145467703"
                        ],
                        "name": "Pascal Vincent",
                        "slug": "Pascal-Vincent",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Vincent",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pascal Vincent"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 111
                            }
                        ],
                        "text": "The second approach is to perform numerical optimization to find the optimal stimulus (Berkes & Wiskott, 2005; Erhan et al., 2009; Le et al., 2010)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In this section, we describe the algorithm that we use to learn features from the unlabeled training set."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15127402,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "65d994fb778a8d9e0f632659fb33a082949a50d3",
            "isKey": false,
            "numCitedBy": 990,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Deep architectures have demonstrated state-of-the-art results in a variety of settings, especially with vision datasets. Beyond the model definitions and the quantitative analyses, there is a need for qualitative comparisons of the solutions learned by various deep architectures. The goal of this paper is to find good qualitative interpretations of high level features represented by such models. To this end, we contrast and compare several techniques applied on Stacked Denoising Autoencoders and Deep Belief Networks, trained on several vision datasets. We show that, perhaps counter-intuitively, such interpretation is possible at the unit level, that it is simple to accomplish and that the results are consistent across various techniques. We hope that such techniques will allow researchers in deep architectures to understand more of how and why deep architectures work."
            },
            "slug": "Visualizing-Higher-Layer-Features-of-a-Deep-Network-Erhan-Bengio",
            "title": {
                "fragments": [],
                "text": "Visualizing Higher-Layer Features of a Deep Network"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper contrast and compare several techniques applied on Stacked Denoising Autoencoders and Deep Belief Networks, trained on several vision datasets, and shows that good qualitative interpretations of high level features represented by such models are possible at the unit level."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2827616"
                        ],
                        "name": "Quoc V. Le",
                        "slug": "Quoc-V.-Le",
                        "structuredName": {
                            "firstName": "Quoc",
                            "lastName": "Le",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Quoc V. Le"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145579706"
                        ],
                        "name": "A. Karpenko",
                        "slug": "A.-Karpenko",
                        "structuredName": {
                            "firstName": "Alexandre",
                            "lastName": "Karpenko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Karpenko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2020608"
                        ],
                        "name": "Jiquan Ngiam",
                        "slug": "Jiquan-Ngiam",
                        "structuredName": {
                            "firstName": "Jiquan",
                            "lastName": "Ngiam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiquan Ngiam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 94
                            }
                        ],
                        "text": "In the simplest implementation, before processing each mini-batch a\n4In (Bengio et al., 2007; Le et al., 2011a), the encoding weights and the decoding weights are tied: W1 = W2."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 126
                            }
                        ],
                        "text": "This optimization problem is also known as reconstruction Topographic Independent Component Analysis (Hyva\u0308rinen et al., 2009; Le et al., 2011a).4 The first term in the objective ensures the representations encode important information about the data, i.e., they can reconstruct input data; whereas\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This optimization problem is also known as reconstruction Topographic Independent Component Analysis [14]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 110
                            }
                        ],
                        "text": "To support parallelism on this cluster, we use the idea of local receptive fields, e.g., (Raina et al., 2009; Le et al., 2010; 2011b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "html In [14], the encoding weights and the decoding weights are tied: W1 = W2."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2714905,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "51e93552fe55be91a5711ff2aabc04b742503e68",
            "isKey": false,
            "numCitedBy": 329,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Independent Components Analysis (ICA) and its variants have been successfully used for unsupervised feature learning. However, standard ICA requires an orthonoramlity constraint to be enforced, which makes it difficult to learn overcomplete features. In addition, ICA is sensitive to whitening. These properties make it challenging to scale ICA to high dimensional data. In this paper, we propose a robust soft reconstruction cost for ICA that allows us to learn highly overcomplete sparse features even on unwhitened data. Our formulation reveals formal connections between ICA and sparse autoencoders, which have previously been observed only empirically. Our algorithm can be used in conjunction with off-the-shelf fast unconstrained optimizers. We show that the soft reconstruction cost can also be used to prevent replicated features in tiled convolutional neural networks. Using our method to learn highly overcomplete sparse features and tiled convolutional neural networks, we obtain competitive performances on a wide variety of object recognition tasks. We achieve state-of-the-art test accuracies on the STL-10 and Hollywood2 datasets."
            },
            "slug": "ICA-with-Reconstruction-Cost-for-Efficient-Feature-Le-Karpenko",
            "title": {
                "fragments": [],
                "text": "ICA with Reconstruction Cost for Efficient Overcomplete Feature Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A robust soft reconstruction cost for ICA is proposed that allows us to learn highly overcomplete sparse features even on unwhitened data and reveals formal connections between ICA and sparse autoencoders, which have previously been observed only empirically."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49959210"
                        ],
                        "name": "J. Dean",
                        "slug": "J.-Dean",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Dean",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Dean"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32131713"
                        ],
                        "name": "G. Corrado",
                        "slug": "G.-Corrado",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Corrado",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Corrado"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3089272"
                        ],
                        "name": "R. Monga",
                        "slug": "R.-Monga",
                        "structuredName": {
                            "firstName": "Rajat",
                            "lastName": "Monga",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Monga"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118440152"
                        ],
                        "name": "Kai Chen",
                        "slug": "Kai-Chen",
                        "structuredName": {
                            "firstName": "Kai",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145139947"
                        ],
                        "name": "Matthieu Devin",
                        "slug": "Matthieu-Devin",
                        "structuredName": {
                            "firstName": "Matthieu",
                            "lastName": "Devin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthieu Devin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2827616"
                        ],
                        "name": "Quoc V. Le",
                        "slug": "Quoc-V.-Le",
                        "structuredName": {
                            "firstName": "Quoc",
                            "lastName": "Le",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Quoc V. Le"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715548"
                        ],
                        "name": "Mark Z. Mao",
                        "slug": "Mark-Z.-Mao",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Mao",
                            "middleNames": [
                                "Z."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Z. Mao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706809"
                        ],
                        "name": "Marc'Aurelio Ranzato",
                        "slug": "Marc'Aurelio-Ranzato",
                        "structuredName": {
                            "firstName": "Marc'Aurelio",
                            "lastName": "Ranzato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marc'Aurelio Ranzato"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33666044"
                        ],
                        "name": "A. Senior",
                        "slug": "A.-Senior",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Senior",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Senior"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2080690"
                        ],
                        "name": "P. Tucker",
                        "slug": "P.-Tucker",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Tucker",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Tucker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143781496"
                        ],
                        "name": "Ke Yang",
                        "slug": "Ke-Yang",
                        "structuredName": {
                            "firstName": "Ke",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ke Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This experiment shows that patches, being detected as faces by the OpenCV face detector, account for less than 3% of the 100,000 sampled patches"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 372467,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3127190433230b3dc1abd0680bb58dced4bcd90e",
            "isKey": false,
            "numCitedBy": 3026,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance. In this paper, we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores. We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models. Within this framework, we have developed two algorithms for large-scale distributed training: (i) Downpour SGD, an asynchronous stochastic gradient descent procedure supporting a large number of model replicas, and (ii) Sandblaster, a framework that supports a variety of distributed batch optimization procedures, including a distributed implementation of L-BFGS. Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network training. We have successfully used our system to train a deep network 30x larger than previously reported in the literature, and achieves state-of-the-art performance on ImageNet, a visual object recognition task with 16 million images and 21k categories. We show that these same techniques dramatically accelerate the training of a more modestly- sized deep network for a commercial speech recognition service. Although we focus on and report performance of these methods as applied to training large neural networks, the underlying algorithms are applicable to any gradient-based machine learning algorithm."
            },
            "slug": "Large-Scale-Distributed-Deep-Networks-Dean-Corrado",
            "title": {
                "fragments": [],
                "text": "Large Scale Distributed Deep Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper considers the problem of training a deep network with billions of parameters using tens of thousands of CPU cores and develops two algorithms for large-scale distributed training, Downpour SGD and Sandblaster L-BFGS, which increase the scale and speed of deep network training."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2979876"
                        ],
                        "name": "R. Raina",
                        "slug": "R.-Raina",
                        "structuredName": {
                            "firstName": "Rajat",
                            "lastName": "Raina",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Raina"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2078284037"
                        ],
                        "name": "Alexis Battle",
                        "slug": "Alexis-Battle",
                        "structuredName": {
                            "firstName": "Alexis",
                            "lastName": "Battle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexis Battle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697141"
                        ],
                        "name": "Honglak Lee",
                        "slug": "Honglak-Lee",
                        "structuredName": {
                            "firstName": "Honglak",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Honglak Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1409971380"
                        ],
                        "name": "Ben Packer",
                        "slug": "Ben-Packer",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Packer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ben Packer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 110
                            }
                        ],
                        "text": "Using unlabeled data in the wild to learn features is the key idea behind the self-taught learning framework (Raina et al., 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6692382,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b3852f0113fcf8a3913c55ae92393ae6ccde347e",
            "isKey": false,
            "numCitedBy": 1610,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new machine learning framework called \"self-taught learning\" for using unlabeled data in supervised classification tasks. We do not assume that the unlabeled data follows the same class labels or generative distribution as the labeled data. Thus, we would like to use a large number of unlabeled images (or audio samples, or text documents) randomly downloaded from the Internet to improve performance on a given image (or audio, or text) classification task. Such unlabeled data is significantly easier to obtain than in typical semi-supervised or transfer learning settings, making self-taught learning widely applicable to many practical learning problems. We describe an approach to self-taught learning that uses sparse coding to construct higher-level features using the unlabeled data. These features form a succinct input representation and significantly improve classification performance. When using an SVM for classification, we further show how a Fisher kernel can be learned for this representation."
            },
            "slug": "Self-taught-learning:-transfer-learning-from-data-Raina-Battle",
            "title": {
                "fragments": [],
                "text": "Self-taught learning: transfer learning from unlabeled data"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "An approach to self-taught learning that uses sparse coding to construct higher-level features using the unlabeled data to form a succinct input representation and significantly improve classification performance."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '07"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706809"
                        ],
                        "name": "Marc'Aurelio Ranzato",
                        "slug": "Marc'Aurelio-Ranzato",
                        "structuredName": {
                            "firstName": "Marc'Aurelio",
                            "lastName": "Ranzato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marc'Aurelio Ranzato"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "13919023"
                        ],
                        "name": "F. Huang",
                        "slug": "F.-Huang",
                        "structuredName": {
                            "firstName": "Fu",
                            "lastName": "Huang",
                            "middleNames": [
                                "Jie"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "90841478"
                        ],
                        "name": "Y-Lan Boureau",
                        "slug": "Y-Lan-Boureau",
                        "structuredName": {
                            "firstName": "Y-Lan",
                            "lastName": "Boureau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y-Lan Boureau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 147
                            }
                        ],
                        "text": "Our work is inspired by recent successful algorithms in unsupervised feature learning and deep learning (Hinton et al., 2006; Bengio et al., 2007; Ranzato et al., 2007; Lee et al., 2007)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "ures from the unlabeled training set. 3.1. Previous work Our work is inspired by recent successful algorithms in unsupervised feature learning and deep learning (Hinton et al.,2006;Bengio et al.,2007;Ranzato et al., 2007;Lee et al.,2007). It is strongly in uenced by the work of (Olshausen &amp; Field,1996) on sparse coding. According to their study, sparse coding can be trained on unlabeled natural images to yield re"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11398758,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ccd52aff02b0f902f4ce7247c4fee7273014c41c",
            "isKey": false,
            "numCitedBy": 1089,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an unsupervised method for learning a hierarchy of sparse feature detectors that are invariant to small shifts and distortions. The resulting feature extractor consists of multiple convolution filters, followed by a feature-pooling layer that computes the max of each filter output within adjacent windows, and a point-wise sigmoid non-linearity. A second level of larger and more invariant features is obtained by training the same algorithm on patches of features from the first level. Training a supervised classifier on these features yields 0.64% error on MNIST, and 54% average recognition rate on Caltech 101 with 30 training samples per category. While the resulting architecture is similar to convolutional networks, the layer-wise unsupervised training procedure alleviates the over-parameterization problems that plague purely supervised learning procedures, and yields good performance with very few labeled training samples."
            },
            "slug": "Unsupervised-Learning-of-Invariant-Feature-with-to-Ranzato-Huang",
            "title": {
                "fragments": [],
                "text": "Unsupervised Learning of Invariant Feature Hierarchies with Applications to Object Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "An unsupervised method for learning a hierarchy of sparse feature detectors that are invariant to small shifts and distortions that alleviates the over-parameterization problems that plague purely supervised learning procedures, and yields good performance with very few labeled training samples."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2217144"
                        ],
                        "name": "Simon Osindero",
                        "slug": "Simon-Osindero",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Osindero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Simon Osindero"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725303"
                        ],
                        "name": "Y. Teh",
                        "slug": "Y.-Teh",
                        "structuredName": {
                            "firstName": "Yee",
                            "lastName": "Teh",
                            "middleNames": [
                                "Whye"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Teh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 105
                            }
                        ],
                        "text": "Our work is inspired by recent successful algorithms in unsupervised feature learning and deep learning (Hinton et al., 2006; Bengio et al., 2007; Ranzato et al., 2007; Lee et al., 2007)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 57
                            }
                        ],
                        "text": "In particular, Lee et al (2008) show that stacked sparse RBMs can model certain simple functions of the V2 area of the cortex."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Copyright 2012 by the author(s)/owner(s)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 66
                            }
                        ],
                        "text": "Addressing this issue is a focus of recent work in deep learning (Hinton et al., 2006; Bengio et al., 2007; Bengio & LeCun, 2007; Lee et al., 2008; 2009) which build hierarchies of feature representations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 140
                            }
                        ],
                        "text": "Successful feature learning algorithms and their applications can be found in recent literature using a variety of approaches such as RBMs (Hinton et al., 2006), autoencoders (Hinton & Salakhutdinov, 2006; Bengio et al., 2007), sparse coding (Lee et al., 2007) and K-means (Coates et al., 2011)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2309950,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8978cf7574ceb35f4c3096be768c7547b28a35d0",
            "isKey": true,
            "numCitedBy": 13407,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "We show how to use complementary priors to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind."
            },
            "slug": "A-Fast-Learning-Algorithm-for-Deep-Belief-Nets-Hinton-Osindero",
            "title": {
                "fragments": [],
                "text": "A Fast Learning Algorithm for Deep Belief Nets"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A fast, greedy algorithm is derived that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697141"
                        ],
                        "name": "Honglak Lee",
                        "slug": "Honglak-Lee",
                        "structuredName": {
                            "firstName": "Honglak",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Honglak Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1779306"
                        ],
                        "name": "Chaitanya Ekanadham",
                        "slug": "Chaitanya-Ekanadham",
                        "structuredName": {
                            "firstName": "Chaitanya",
                            "lastName": "Ekanadham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chaitanya Ekanadham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 259,
                                "start": 243
                            }
                        ],
                        "text": "Successful feature learning algorithms and their applications can be found in recent literature using a variety of approaches such as RBMs (Hinton et al., 2006), autoencoders (Hinton & Salakhutdinov, 2006; Bengio et al., 2007), sparse coding (Lee et al., 2007) and K-means (Coates et al., 2011)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 169
                            }
                        ],
                        "text": "Our work is inspired by recent successful algorithms in unsupervised feature learning and deep learning (Hinton et al., 2006; Bengio et al., 2007; Ranzato et al., 2007; Lee et al., 2007)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "sue is a focus of recent work in deep learning (Hinton et al.,2006;Bengio et al., 2007;Bengio &amp; LeCun,2007;Lee et al.,2008;2009) which build hierarchies of feature representations. In particular, Lee et al (2008) show that stacked sparse RBMs can model certain simple functions of the V2 area of the cortex. They also demonstrate that convolutional DBNs (Lee et al.,2009), trained on aligned Building high-level "
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12589862,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "202cbbf671743aefd380d2f23987bd46b9caaf97",
            "isKey": false,
            "numCitedBy": 1028,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Motivated in part by the hierarchical organization of the cortex, a number of algorithms have recently been proposed that try to learn hierarchical, or \"deep,\" structure from unlabeled data. While several authors have formally or informally compared their algorithms to computations performed in visual area V1 (and the cochlea), little attempt has been made thus far to evaluate these algorithms in terms of their fidelity for mimicking computations at deeper levels in the cortical hierarchy. This paper presents an unsupervised learning model that faithfully mimics certain properties of visual area V2. Specifically, we develop a sparse variant of the deep belief networks of Hinton et al. (2006). We learn two layers of nodes in the network, and demonstrate that the first layer, similar to prior work on sparse coding and ICA, results in localized, oriented, edge filters, similar to the Gabor functions known to model V1 cell receptive fields. Further, the second layer in our model encodes correlations of the first layer responses in the data. Specifically, it picks up both colinear (\"contour\") features as well as corners and junctions. More interestingly, in a quantitative comparison, the encoding of these more complex \"corner\" features matches well with the results from the Ito & Komatsu's study of biological V2 responses. This suggests that our sparse variant of deep belief networks holds promise for modeling more higher-order features."
            },
            "slug": "Sparse-deep-belief-net-model-for-visual-area-V2-Lee-Ekanadham",
            "title": {
                "fragments": [],
                "text": "Sparse deep belief net model for visual area V2"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "An unsupervised learning model is presented that faithfully mimics certain properties of visual area V2 and the encoding of these more complex \"corner\" features matches well with the results from the Ito & Komatsu's study of biological V2 responses, suggesting that this sparse variant of deep belief networks holds promise for modeling more higher-order features."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 108
                            }
                        ],
                        "text": "Addressing this issue is a focus of recent work in deep learning (Hinton et al., 2006; Bengio et al., 2007; Bengio & LeCun, 2007; Lee et al., 2008; 2009) which build hierarchies of feature representations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Copyright 2012 by the author(s)/owner(s)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 15559637,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6fdb77260fc83dff91c44fea0f31a2cb8ed13d04",
            "isKey": false,
            "numCitedBy": 1116,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": "One long-term goal of machine learning research is to produce methods that are applicable to highly complex tasks, such as perception (vision, audition), reasoning, intelligent control, and other artificially intelligent behaviors. We argue that in order to progress toward this goal, the Machine Learning community must endeavor to discover algorithms that can learn highly complex functions, with minimal need for prior knowledge, and with minimal human intervention. We present mathematical and empirical evidence suggesting that many popular approaches to non-parametric learning, particularly kernel methods, are fundamentally limited in their ability to learn complex high-dimensional functions. Our analysis focuses on two problems. First, kernel machines are shallow architectures, in which one large layer of simple template matchers is followed by a single layer of trainable coefficients. We argue that shallow architectures can be very inefficient in terms of required number of computational elements and examples. Second, we analyze a limitation of kernel machines with a local kernel, linked to the curse of dimensionality, that applies to supervised, unsupervised (manifold learning) and semi-supervised kernel machines. Using empirical results on invariant image recognition tasks, kernel methods are compared with deep architectures, in which lower-level features or concepts are progressively combined into more abstract and higher-level representations. We argue that deep architectures have the potential to generalize in non-local ways, i.e., beyond immediate neighbors, and that this is crucial in order to make progress on the kind of complex tasks required for artificial intelligence."
            },
            "slug": "Scaling-learning-algorithms-towards-AI-Bengio-LeCun",
            "title": {
                "fragments": [],
                "text": "Scaling learning algorithms towards AI"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is argued that deep architectures have the potential to generalize in non-local ways, i.e., beyond immediate neighbors, and that this is crucial in order to make progress on the kind of complex tasks required for artificial intelligence."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2827616"
                        ],
                        "name": "Quoc V. Le",
                        "slug": "Quoc-V.-Le",
                        "structuredName": {
                            "firstName": "Quoc",
                            "lastName": "Le",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Quoc V. Le"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2020608"
                        ],
                        "name": "Jiquan Ngiam",
                        "slug": "Jiquan-Ngiam",
                        "structuredName": {
                            "firstName": "Jiquan",
                            "lastName": "Ngiam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiquan Ngiam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2122924605"
                        ],
                        "name": "Zhenghao Chen",
                        "slug": "Zhenghao-Chen",
                        "structuredName": {
                            "firstName": "Zhenghao",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhenghao Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50566400"
                        ],
                        "name": "D. J. Chia",
                        "slug": "D.-J.-Chia",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Chia",
                            "middleNames": [
                                "Jin",
                                "hao"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. J. Chia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2572525"
                        ],
                        "name": "Pang Wei Koh",
                        "slug": "Pang-Wei-Koh",
                        "structuredName": {
                            "firstName": "Pang Wei",
                            "lastName": "Koh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pang Wei Koh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Our algorithm is built upon these ideas, especially [15]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In addition to being more biologically feasible, this lets us learn more invariant features other than translational invariances [15]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "To scale the model to large images, we use the idea of local receptive fields [23, 15] and many levels of parallelism: model parallelism (parameters are distributed across machines) and data parallelism (data are distributed across machines)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "To scale these autoencoders to large images, we use a simple idea known as local receptive fields [17, 23, 19, 15]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "There are many types of pooling such as mean pooling, max pooling [17] and square-squareroot pooling [15] but in this work we will use the last type of pooling because it allows the learning of invariant features [13, 15]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9573124,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "05cc38e249a6f642363b5a5cbd71cda67cea5893",
            "isKey": false,
            "numCitedBy": 307,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Convolutional neural networks (CNNs) have been successfully applied to many tasks such as digit and object recognition. Using convolutional (tied) weights significantly reduces the number of parameters that have to be learned, and also allows translational invariance to be hard-coded into the architecture. In this paper, we consider the problem of learning invariances, rather than relying on hard-coding. We propose tiled convolution neural networks (Tiled CNNs), which use a regular \"tiled\" pattern of tied weights that does not require that adjacent hidden units share identical weights, but instead requires only that hidden units k steps away from each other to have tied weights. By pooling over neighboring units, this architecture is able to learn complex invariances (such as scale and rotational invariance) beyond translational invariance. Further, it also enjoys much of CNNs' advantage of having a relatively small number of learned parameters (such as ease of learning and greater scalability). We provide an efficient learning algorithm for Tiled CNNs based on Topographic ICA, and show that learning complex invariant features allows us to achieve highly competitive results for both the NORB and CIFAR-10 datasets."
            },
            "slug": "Tiled-convolutional-neural-networks-Le-Ngiam",
            "title": {
                "fragments": [],
                "text": "Tiled convolutional neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper proposes tiled convolution neural networks (Tiled CNNs), which use a regular \"tiled\" pattern of tied weights that does not require that adjacent hidden units share identical weights, but instead requires only that hidden units k steps away from each other to have tied weights."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3087941"
                        ],
                        "name": "Pascal Lamblin",
                        "slug": "Pascal-Lamblin",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Lamblin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pascal Lamblin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32384143"
                        ],
                        "name": "D. Popovici",
                        "slug": "D.-Popovici",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Popovici",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Popovici"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777528"
                        ],
                        "name": "H. Larochelle",
                        "slug": "H.-Larochelle",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Larochelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Larochelle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14201947,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "355d44f53428b1ac4fb2ab468d593c720640e5bd",
            "isKey": false,
            "numCitedBy": 3427,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Complexity theory of circuits strongly suggests that deep architectures can be much more efficient (sometimes exponentially) than shallow architectures, in terms of computational elements required to represent some functions. Deep multi-layer neural networks have many levels of non-linearities allowing them to compactly represent highly non-linear and highly-varying functions. However, until recently it was not clear how to train such deep networks, since gradient-based optimization starting from random initialization appears to often get stuck in poor solutions. Hinton et al. recently introduced a greedy layer-wise unsupervised learning algorithm for Deep Belief Networks (DBN), a generative model with many layers of hidden causal variables. In the context of the above optimization problem, we study this algorithm empirically and explore variants to better understand its success and extend it to cases where the inputs are continuous or where the structure of the input distribution is not revealing enough about the variable to be predicted in a supervised task. Our experiments also confirm the hypothesis that the greedy layer-wise unsupervised training strategy mostly helps the optimization, by initializing weights in a region near a good local minimum, giving rise to internal distributed representations that are high-level abstractions of the input, bringing better generalization."
            },
            "slug": "Greedy-Layer-Wise-Training-of-Deep-Networks-Bengio-Lamblin",
            "title": {
                "fragments": [],
                "text": "Greedy Layer-Wise Training of Deep Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "These experiments confirm the hypothesis that the greedy layer-wise unsupervised training strategy mostly helps the optimization, by initializing weights in a region near a good local minimum, giving rise to internal distributed representations that are high-level abstractions of the input, bringing better generalization."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2827616"
                        ],
                        "name": "Quoc V. Le",
                        "slug": "Quoc-V.-Le",
                        "structuredName": {
                            "firstName": "Quoc",
                            "lastName": "Le",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Quoc V. Le"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2020608"
                        ],
                        "name": "Jiquan Ngiam",
                        "slug": "Jiquan-Ngiam",
                        "structuredName": {
                            "firstName": "Jiquan",
                            "lastName": "Ngiam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiquan Ngiam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144638694"
                        ],
                        "name": "Adam Coates",
                        "slug": "Adam-Coates",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Coates",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adam Coates"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47778994"
                        ],
                        "name": "A. Lahiri",
                        "slug": "A.-Lahiri",
                        "structuredName": {
                            "firstName": "Ahbik",
                            "lastName": "Lahiri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Lahiri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "41227297"
                        ],
                        "name": "B. Prochnow",
                        "slug": "B.-Prochnow",
                        "structuredName": {
                            "firstName": "Bobby",
                            "lastName": "Prochnow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Prochnow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6076653,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "053912e76e50c9f923a1fc1c173f1365776060cc",
            "isKey": false,
            "numCitedBy": 884,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "The predominant methodology in training deep learning advocates the use of stochastic gradient descent methods (SGDs). Despite its ease of implementation, SGDs are difficult to tune and parallelize. These problems make it challenging to develop, debug and scale up deep learning algorithms with SGDs. In this paper, we show that more sophisticated off-the-shelf optimization methods such as Limited memory BFGS (L-BFGS) and Conjugate gradient (CG) with line search can significantly simplify and speed up the process of pretraining deep algorithms. In our experiments, the difference between L-BFGS/CG and SGDs are more pronounced if we consider algorithmic extensions (e.g., sparsity regularization) and hardware extensions (e.g., GPUs or computer clusters). Our experiments with distributed optimization support the use of L-BFGS with locally connected networks and convolutional neural networks. Using L-BFGS, our convolutional network model achieves 0.69% on the standard MNIST dataset. This is a state-of-the-art result on MNIST among algorithms that do not use distortions or pretraining."
            },
            "slug": "On-optimization-methods-for-deep-learning-Le-Ngiam",
            "title": {
                "fragments": [],
                "text": "On optimization methods for deep learning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown that more sophisticated off-the-shelf optimization methods such as Limited memory BFGS (L-BFGS) and Conjugate gradient (CG) with line search can significantly simplify and speed up the process of pretraining deep algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144717963"
                        ],
                        "name": "Karol Gregor",
                        "slug": "Karol-Gregor",
                        "structuredName": {
                            "firstName": "Karol",
                            "lastName": "Gregor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karol Gregor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 103
                            }
                        ],
                        "text": "Next, to achieve invariance to local deformations, we employ local L2 pooling (Hyva\u0308rinen et al., 2009; Gregor & LeCun, 2010; Le et al., 2010) and local contrast normalization (Jarrett et al., 2009)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 2
                            }
                        ],
                        "text": "For example, to build a face detector, one needs a large collection of images labeled as containing faces, often with a bounding box around the face."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15372343,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "31f04f8f83365fabf7ba9c9be1179c0da6815128",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a new neural architecture and an unsupervised algorithm for learning invariant representations from temporal sequence of images. The system uses two groups of complex cells whose outputs are combined multiplicatively: one that represents the content of the image, constrained to be constant over several consecutive frames, and one that represents the precise location of features, which is allowed to vary over time but constrained to be sparse. The architecture uses an encoder to extract features, and a decoder to reconstruct the input from the features. The method was applied to patches extracted from consecutive movie frames and produces orientation and frequency selective units analogous to the complex cells in V1. An extension of the method is proposed to train a network composed of units with local receptive field spread over a large image of arbitrary size. A layer of complex cells, subject to sparsity constraints, pool feature units over overlapping local neighborhoods, which causes the feature units to organize themselves into pinwheel patterns of orientation-selective receptive fields, similar to those observed in the mammalian visual cortex. A feed-forward encoder efficiently computes the feature representation of full images."
            },
            "slug": "Emergence-of-Complex-Like-Cells-in-a-Temporal-with-Gregor-LeCun",
            "title": {
                "fragments": [],
                "text": "Emergence of Complex-Like Cells in a Temporal Product Network with Local Receptive Fields"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "A new neural architecture and an unsupervised algorithm for learning invariant representations from temporal sequence of images and an extension of the method is proposed to train a network composed of units with local receptive field spread over a large image of arbitrary size."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108477977"
                        ],
                        "name": "Weiwei Zhang",
                        "slug": "Weiwei-Zhang",
                        "structuredName": {
                            "firstName": "Weiwei",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Weiwei Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Jian Sun",
                        "slug": "Jian-Sun",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50295995"
                        ],
                        "name": "Xiaoou Tang",
                        "slug": "Xiaoou-Tang",
                        "structuredName": {
                            "firstName": "Xiaoou",
                            "lastName": "Tang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaoou Tang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "scribed in (Zhang et al., 2008)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 50
                            }
                        ],
                        "text": "See Appendix F for a sample sequence.\nscribed in (Zhang et al., 2008)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2441648,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "93b8b54c709a798f6958ce77b4652810d9027ce5",
            "isKey": false,
            "numCitedBy": 149,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we focus on the problem of detecting the head of cat-like animals, adopting cat as a test case. We show that the performance depends crucially on how to effectively utilize the shape and texture features jointly. Specifically, we propose a two step approach for the cat head detection. In the first step, we train two individual detectors on two training sets. One training set is normalized to emphasize the shape features and the other is normalized to underscore the texture features. In the second step, we train a joint shape and texture fusion classifier to make the final decision. We demonstrate that a significant improvement can be obtained by our two step approach. In addition, we also propose a set of novel features based on oriented gradients, which outperforms existing leading features, e. g., Haar, HoG, and EoH. We evaluate our approach on a well labeled cat head data set with 10,000 images and PASCAL 2007 cat data."
            },
            "slug": "Cat-Head-Detection-How-to-Effectively-Exploit-Shape-Zhang-Sun",
            "title": {
                "fragments": [],
                "text": "Cat Head Detection - How to Effectively Exploit Shape and Texture Features"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper proposes a two step approach for the cat head detection and proposes a set of novel features based on oriented gradients, which outperforms existing leading features, e."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153302678"
                        ],
                        "name": "Jia Deng",
                        "slug": "Jia-Deng",
                        "structuredName": {
                            "firstName": "Jia",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jia Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39668247"
                        ],
                        "name": "A. Berg",
                        "slug": "A.-Berg",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Berg",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Berg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "94451829"
                        ],
                        "name": "K. Li",
                        "slug": "K.-Li",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 53
                            }
                        ],
                        "text": "We followed the experimental protocols specified by (Deng et al., 2010; Sanchez & Perronnin, 2011), in which, the datasets are randomly split into two halves for training and validation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In this section, we describe the algorithm that we use to learn features from the unlabeled training set."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1274537,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9800e3c3394c569be83379ee2ebe3424e09c2919",
            "isKey": false,
            "numCitedBy": 528,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "Image classification is a critical task for both humans and computers. One of the challenges lies in the large scale of the semantic space. In particular, humans can recognize tens of thousands of object classes and scenes. No computer vision algorithm today has been tested at this scale. This paper presents a study of large scale categorization including a series of challenging experiments on classification with more than 10, 000 image classes. We find that a) computational issues become crucial in algorithm design; b) conventional wisdom from a couple of hundred image categories on relative performance of different classifiers does not necessarily hold when the number of categories increases; c) there is a surprisingly strong relationship between the structure of WordNet (developed for studying language) and the difficulty of visual categorization; d) classification can be improved by exploiting the semantic hierarchy. Toward the future goal of developing automatic vision algorithms to recognize tens of thousands or even millions of image categories, we make a series of observations and arguments about dataset scale, category density, and image hierarchy."
            },
            "slug": "What-Does-Classifying-More-Than-10,-000-Image-Tell-Deng-Berg",
            "title": {
                "fragments": [],
                "text": "What Does Classifying More Than 10, 000 Image Categories Tell Us?"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A study of large scale categorization including a series of challenging experiments on classification with more than 10,000 image classes finds that computational issues become crucial in algorithm design and conventional wisdom from a couple of hundred image categories does not necessarily hold when the number of categories increases."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2077257730"
                        ],
                        "name": "Kevin Jarrett",
                        "slug": "Kevin-Jarrett",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Jarrett",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin Jarrett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2645384"
                        ],
                        "name": "K. Kavukcuoglu",
                        "slug": "K.-Kavukcuoglu",
                        "structuredName": {
                            "firstName": "Koray",
                            "lastName": "Kavukcuoglu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kavukcuoglu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706809"
                        ],
                        "name": "Marc'Aurelio Ranzato",
                        "slug": "Marc'Aurelio-Ranzato",
                        "structuredName": {
                            "firstName": "Marc'Aurelio",
                            "lastName": "Ranzato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marc'Aurelio Ranzato"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 1
                            }
                        ],
                        "text": "(Jarrett et al., 2009)\n3For more details regarding connectivity patterns and parameter sensitivity, see Appendix B and E.\nthe following optimization problem\nminimize W1,W2 m\u2211 i=1 (\u2225\u2225W2WT1 x(i) \u2212 x(i)\u2225\u222522+ \u03bb\nk\u2211 j=1 \u221a +Hj(WT1 x (i))2 ) ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In this section, we describe the algorithm that we use to learn features from the unlabeled training set."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 87
                            }
                        ],
                        "text": "This is a stark difference between our approach and previous work (LeCun et al., 1998; Jarrett et al., 2009; Lee et al., 2009)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 196,
                                "start": 176
                            }
                        ],
                        "text": "Next, to achieve invariance to local deformations, we employ local L2 pooling (Hyva\u0308rinen et al., 2009; Gregor & LeCun, 2010; Le et al., 2010) and local contrast normalization (Jarrett et al., 2009)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 88
                            }
                        ],
                        "text": "This agrees with previous study showing the importance of local contrast normalization (Jarrett et al., 2009)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 196,
                                "start": 176
                            }
                        ],
                        "text": "The third sublayer performs local subtractive and divisive normalization and it is inspired by biological and computational models (Pinto et al., 2008;\nLyu & Simoncelli, 2008; Jarrett et al., 2009).2\nAs mentioned above, central to our approach is the use of local connectivity between neurons."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 45
                            }
                        ],
                        "text": "For example, to build a face detector, one needs a large collection of images labeled as containing faces, often with a bounding box around the face."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This experiment shows that patches, being detected as faces by the OpenCV face detector, account for less than 3% of the 100,000 sampled patches"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 2
                            }
                        ],
                        "text": "But perhaps more importantly, it answers an intriguing question as to whether the specificity of the \u201cgrandmother neuron\u201d could possibly be learned from unlabeled data."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 206769720,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1f88427d7aa8225e47f946ac41a0667d7b69ac52",
            "isKey": false,
            "numCitedBy": 2084,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "In many recent object recognition systems, feature extraction stages are generally composed of a filter bank, a non-linear transformation, and some sort of feature pooling layer. Most systems use only one stage of feature extraction in which the filters are hard-wired, or two stages where the filters in one or both stages are learned in supervised or unsupervised mode. This paper addresses three questions: 1. How does the non-linearities that follow the filter banks influence the recognition accuracy? 2. does learning the filter banks in an unsupervised or supervised manner improve the performance over random filters or hardwired filters? 3. Is there any advantage to using an architecture with two stages of feature extraction, rather than one? We show that using non-linearities that include rectification and local contrast normalization is the single most important ingredient for good accuracy on object recognition benchmarks. We show that two stages of feature extraction yield better accuracy than one. Most surprisingly, we show that a two-stage system with random filters can yield almost 63% recognition rate on Caltech-101, provided that the proper non-linearities and pooling layers are used. Finally, we show that with supervised refinement, the system achieves state-of-the-art performance on NORB dataset (5.6%) and unsupervised pre-training followed by supervised refinement produces good accuracy on Caltech-101 (\u226b 65%), and the lowest known error rate on the undistorted, unprocessed MNIST dataset (0.53%)."
            },
            "slug": "What-is-the-best-multi-stage-architecture-for-Jarrett-Kavukcuoglu",
            "title": {
                "fragments": [],
                "text": "What is the best multi-stage architecture for object recognition?"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "It is shown that using non-linearities that include rectification and local contrast normalization is the single most important ingredient for good accuracy on object recognition benchmarks and that two stages of feature extraction yield better accuracy than one."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE 12th International Conference on Computer Vision"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143995438"
                        ],
                        "name": "Jorge S\u00e1nchez",
                        "slug": "Jorge-S\u00e1nchez",
                        "structuredName": {
                            "firstName": "Jorge",
                            "lastName": "S\u00e1nchez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jorge S\u00e1nchez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723883"
                        ],
                        "name": "F. Perronnin",
                        "slug": "F.-Perronnin",
                        "structuredName": {
                            "firstName": "Florent",
                            "lastName": "Perronnin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Perronnin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 72
                            }
                        ],
                        "text": "We followed the experimental protocols specified by (Deng et al., 2010; Sanchez & Perronnin, 2011), in which, the datasets are randomly split into two halves for training and validation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In this section, we describe the algorithm that we use to learn features from the unlabeled training set."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16199577,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eefcc7bcc05436dac9881acb4ff4e4a0b730e175",
            "isKey": false,
            "numCitedBy": 298,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "We address image classification on a large-scale, i.e. when a large number of images and classes are involved. First, we study classification accuracy as a function of the image signature dimensionality and the training set size. We show experimentally that the larger the training set, the higher the impact of the dimensionality on the accuracy. In other words, high-dimensional signatures are important to obtain state-of-the-art results on large datasets. Second, we tackle the problem of data compression on very large signatures (on the order of 105 dimensions) using two lossy compression strategies: a dimensionality reduction technique known as the hash kernel and an encoding technique based on product quantizers. We explain how the gain in storage can be traded against a loss in accuracy and/or an increase in CPU cost. We report results on two large databases \u2014 ImageNet and a dataset of lM Flickr images \u2014 showing that we can reduce the storage of our signatures by a factor 64 to 128 with little loss in accuracy. Integrating the decompression in the classifier learning yields an efficient and scalable training algorithm. On ILSVRC2010 we report a 74.3% accuracy at top-5, which corresponds to a 2.5% absolute improvement with respect to the state-of-the-art. On a subset of 10K classes of ImageNet we report a top-1 accuracy of 16.7%, a relative improvement of 160% with respect to the state-of-the-art."
            },
            "slug": "High-dimensional-signature-compression-for-image-S\u00e1nchez-Perronnin",
            "title": {
                "fragments": [],
                "text": "High-dimensional signature compression for large-scale image classification"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work reports results on two large databases \u2014 ImageNet and a dataset of lM Flickr images \u2014 showing that it can reduce the storage of the authors' signatures by a factor 64 to 128 with little loss in accuracy and integrating the decompression in the classifier learning yields an efficient and scalable training algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR 2011"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144535526"
                        ],
                        "name": "G. Montavon",
                        "slug": "G.-Montavon",
                        "structuredName": {
                            "firstName": "Gr\u00e9goire",
                            "lastName": "Montavon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Montavon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2522031"
                        ],
                        "name": "M. Braun",
                        "slug": "M.-Braun",
                        "structuredName": {
                            "firstName": "Mikio",
                            "lastName": "Braun",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Braun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145034054"
                        ],
                        "name": "K. M\u00fcller",
                        "slug": "K.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Klaus-Robert",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. M\u00fcller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 221,
                                "start": 218
                            }
                        ],
                        "text": "\u2026in the brain is an area of active investigation, but current experimental evidence suggests the possibility that some neurons in the temporal cortex are highly selective for object categories such as faces or hands (Desimone et al., 1984), and perhaps even specific people (Quiroga et al., 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1584684,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "908040d7ebdd79d824d1a297214584d8b5a4bccc",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Deep networks can potentially express a learning problem more efficiently than local learning machines. While deep networks outperform local learning machines on some problems, it is still unclear how their nice representation emerges from their complex structure. We present an analysis based on Gaussian kernels that measures how the representation of the learning problem evolves layer after layer as the deep network builds higher-level abstract representations of the input. We use this analysis to show empirically that deep networks build progressively better representations of the learning problem and that the best representations are obtained when the deep network discriminates only in the last layers."
            },
            "slug": "Layer-wise-analysis-of-deep-networks-with-Gaussian-Montavon-Braun",
            "title": {
                "fragments": [],
                "text": "Layer-wise analysis of deep networks with Gaussian kernels"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This analysis uses Gaussian kernels to show empirically that deep networks build progressively better representations of the learning problem and that the best representations are obtained when the deep network discriminates only in the last layers."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721248"
                        ],
                        "name": "P. Haffner",
                        "slug": "P.-Haffner",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Haffner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Haffner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 37392765,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9a5ea367f0fb05805acaa84a402f5d036eea37dc",
            "isKey": false,
            "numCitedBy": 744,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Finding an appropriate set of features is an essential problem in the design of shape recognition systems. This paper attempts to show that for recognizing simple objects with high shape variability such as handwritten characters, it is possible, and even advantageous, to feed the system directly with minimally processed images and to rely on learning to extract the right set of features. Convolutional Neural Networks are shown to be particularly well suited to this task. We also show that these networks can be used to recognize multiple objects without requiring explicit segmentation of the objects from their surrounding. The second part of the paper presents the Graph Transformer Network model which extends the applicability of gradient-based learning to systems that use graphs to represents features, objects, and their combinations."
            },
            "slug": "Object-Recognition-with-Gradient-Based-Learning-LeCun-Haffner",
            "title": {
                "fragments": [],
                "text": "Object Recognition with Gradient-Based Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "This paper attempts to show that for recognizing simple objects with high shape variability such as handwritten characters, it is possible, and even advantageous, to feed the system directly with minimally processed images and to rely on learning to extract the right set of features."
            },
            "venue": {
                "fragments": [],
                "text": "Shape, Contour and Grouping in Computer Vision"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 204,
                                "start": 176
                            }
                        ],
                        "text": "Successful feature learning algorithms and their applications can be found in recent literature using a variety of approaches such as RBMs (Hinton et al., 2006), autoencoders (Hinton & Salakhutdinov, 2006; Bengio et al., 2007), sparse coding (Lee et al., 2007) and K-means (Coates et al., 2011)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 87
                            }
                        ],
                        "text": "We also compare our method against several other algorithms such as deep autoencoders (Hinton & Salakhutdinov, 2006; Bengio et al., 2007) and Kmeans (Coates et al., 2011)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 157
                            }
                        ],
                        "text": "Successful feature learning algorithms and their applications can be found in recent literature using a variety of approaches such as RBMs [4], autoencoders [5, 6], sparse coding [7] and K-means [8]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1658773,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "46eb79e5eec8a4e2b2f5652b66441e8a4c921c3e",
            "isKey": false,
            "numCitedBy": 14638,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such \u201cautoencoder\u201d networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data."
            },
            "slug": "Reducing-the-Dimensionality-of-Data-with-Neural-Hinton-Salakhutdinov",
            "title": {
                "fragments": [],
                "text": "Reducing the Dimensionality of Data with Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work describes an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153302678"
                        ],
                        "name": "Jia Deng",
                        "slug": "Jia-Deng",
                        "structuredName": {
                            "firstName": "Jia",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jia Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144847596"
                        ],
                        "name": "Wei Dong",
                        "slug": "Wei-Dong",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Dong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Dong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2166511"
                        ],
                        "name": "R. Socher",
                        "slug": "R.-Socher",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Socher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Socher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2040091191"
                        ],
                        "name": "Li-Jia Li",
                        "slug": "Li-Jia-Li",
                        "structuredName": {
                            "firstName": "Li-Jia",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li-Jia Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "94451829"
                        ],
                        "name": "K. Li",
                        "slug": "K.-Li",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 143
                            }
                        ],
                        "text": "The test set consists of 37,000 images sampled from two datasets: Labeled Faces In the Wild dataset (Huang et al., 2007) and ImageNet dataset (Deng et al., 2009)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 99
                            }
                        ],
                        "text": "We applied the feature learning method to the task of recognizing objects in the ImageNet dataset (Deng et al., 2009)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In this section, we describe the algorithm that we use to learn features from the unlabeled training set."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 57246310,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1b47265245e8db53a553049dcb27ed3e495fd625",
            "isKey": false,
            "numCitedBy": 27367,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called \u201cImageNet\u201d, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond."
            },
            "slug": "ImageNet:-A-large-scale-hierarchical-image-database-Deng-Dong",
            "title": {
                "fragments": [],
                "text": "ImageNet: A large-scale hierarchical image database"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A new database called \u201cImageNet\u201d is introduced, a large-scale ontology of images built upon the backbone of the WordNet structure, much larger in scale and diversity and much more accurate than the current image datasets."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708655"
                        ],
                        "name": "B. Olshausen",
                        "slug": "B.-Olshausen",
                        "structuredName": {
                            "firstName": "Bruno",
                            "lastName": "Olshausen",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Olshausen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49649079"
                        ],
                        "name": "D. Field",
                        "slug": "D.-Field",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Field",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Field"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 59
                            }
                        ],
                        "text": "One shortcoming of early approaches such as sparse coding (Olshausen & Field, 1996) is that their architectures are shallow and typically capture low-level concepts (e.g., edge \u201cGabor\u201d filters) and simple invariances."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 42
                            }
                        ],
                        "text": "It is strongly influenced by the work of (Olshausen & Field, 1996) on sparse coding."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Our work is inspired by recent successful algorithms in unsupervised feature learning and deep learning [20, 9, 1, 24, 18]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "A shortcoming of early approaches such as sparse coding [20] is that their architectures are shallow and typically shown to capture simple variations (or learned features such as edge detectors)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In particular, in their seminal work, Olshausen and Field [20] discovered sparse coding, an unsupervised learning principle, that yields receptive fields having properties like those of V1 simple cells."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4358477,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8012c4a1e2ca663f1a04e80cbb19631a00cbab27",
            "isKey": true,
            "numCitedBy": 5639,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "THE receptive fields of simple cells in mammalian primary visual cortex can be characterized as being spatially localized, oriented1\u20134 and bandpass (selective to structure at different spatial scales), comparable to the basis functions of wavelet transforms5,6. One approach to understanding such response properties of visual neurons has been to consider their relationship to the statistical structure of natural images in terms of efficient coding7\u201312. Along these lines, a number of studies have attempted to train unsupervised learning algorithms on natural images in the hope of developing receptive fields with similar properties13\u201318, but none has succeeded in producing a full set that spans the image space and contains all three of the above properties. Here we investigate the proposal8,12 that a coding strategy that maximizes sparseness is sufficient to account for these properties. We show that a learning algorithm that attempts to find sparse linear codes for natural scenes will develop a complete family of localized, oriented, bandpass receptive fields, similar to those found in the primary visual cortex. The resulting sparse image code provides a more efficient representation for later stages of processing because it possesses a higher degree of statistical independence among its outputs."
            },
            "slug": "Emergence-of-simple-cell-receptive-field-properties-Olshausen-Field",
            "title": {
                "fragments": [],
                "text": "Emergence of simple-cell receptive field properties by learning a sparse code for natural images"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is shown that a learning algorithm that attempts to find sparse linear codes for natural scenes will develop a complete family of localized, oriented, bandpass receptive fields, similar to those found in the primary visual cortex."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3142556"
                        ],
                        "name": "Pierre Sermanet",
                        "slug": "Pierre-Sermanet",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Sermanet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pierre Sermanet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 188,
                                "start": 166
                            }
                        ],
                        "text": "It has 1 billion trainable parameters, which is more than an order of magnitude larger than other large networks reported in literature, e.g., (Ciresan et al., 2010; Sermanet & LeCun, 2011) with around 10 million parameters."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7593950,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9ab0de951cc9cdf16887b1f841f8da6affc9c0de",
            "isKey": false,
            "numCitedBy": 647,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "We apply Convolutional Networks (ConvNets) to the task of traffic sign classification as part of the GTSRB competition. ConvNets are biologically-inspired multi-stage architectures that automatically learn hierarchies of invariant features. While many popular vision approaches use hand-crafted features such as HOG or SIFT, ConvNets learn features at every level from data that are tuned to the task at hand. The traditional ConvNet architecture was modified by feeding 1st stage features in addition to 2nd stage features to the classifier. The system yielded the 2nd-best accuracy of 98.97% during phase I of the competition (the best entry obtained 98.98%), above the human performance of 98.81%, using 32\u00d732 color input images. Experiments conducted after phase 1 produced a new record of 99.17% by increasing the network capacity, and by using greyscale images instead of color. Interestingly, random features still yielded competitive results (97.33%)."
            },
            "slug": "Traffic-sign-recognition-with-multi-scale-Networks-Sermanet-LeCun",
            "title": {
                "fragments": [],
                "text": "Traffic sign recognition with multi-scale Convolutional Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "This work applies Convolutional Networks (ConvNets) to the task of traffic sign classification as part of the GTSRB competition, and yields the 2nd-best accuracy above the human performance."
            },
            "venue": {
                "fragments": [],
                "text": "The 2011 International Joint Conference on Neural Networks"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3219900"
                        ],
                        "name": "Gary B. Huang",
                        "slug": "Gary-B.-Huang",
                        "structuredName": {
                            "firstName": "Gary",
                            "lastName": "Huang",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gary B. Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2985062"
                        ],
                        "name": "Marwan A. Mattar",
                        "slug": "Marwan-A.-Mattar",
                        "structuredName": {
                            "firstName": "Marwan",
                            "lastName": "Mattar",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marwan A. Mattar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685538"
                        ],
                        "name": "Tamara L. Berg",
                        "slug": "Tamara-L.-Berg",
                        "structuredName": {
                            "firstName": "Tamara",
                            "lastName": "Berg",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tamara L. Berg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404579703"
                        ],
                        "name": "Eric Learned-Miller",
                        "slug": "Eric-Learned-Miller",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Learned-Miller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eric Learned-Miller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": " However, for better parallelism and better features, we used the version without tied weights. 4 4 Test set The test set consists of 37000 images sampled from two datasets: Labeled Faces In the Wild [10] and ImageNet [4]. There are 13026 faces sampled from non-aligned Labeled Faces in The Wild.6 The rest are distractor objects randomly sampled from ImageNet. These images are resized to 80x80, padded "
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 88166,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c6b3ca4f939e36a9679a70e14ce8b1bbbc5618f3",
            "isKey": false,
            "numCitedBy": 4896,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "Most face databases have been created under controlled conditions to facilitate the study of specific parameters on the face recognition problem. These parameters include such variables as position, pose, lighting, background, camera quality, and gender. While there are many applications for face recognition technology in which one can control the parameters of image acquisition, there are also many applications in which the practitioner has little or no control over such parameters. This database, Labeled Faces in the Wild, is provided as an aid in studying the latter, unconstrained, recognition problem. The database contains labeled face photographs spanning the range of conditions typically encountered in everyday life. The database exhibits \u201cnatural\u201d variability in factors such as pose, lighting, race, accessories, occlusions, and background. In addition to describing the details of the database, we provide specific experimental paradigms for which the database is suitable. This is done in an effort to make research performed with the database as consistent and comparable as possible. We provide baseline results, including results of a state of the art face recognition system combined with a face alignment system. To facilitate experimentation on the database, we provide several parallel databases, including an aligned version."
            },
            "slug": "Labeled-Faces-in-the-Wild:-A-Database-forStudying-Huang-Mattar",
            "title": {
                "fragments": [],
                "text": "Labeled Faces in the Wild: A Database forStudying Face Recognition in Unconstrained Environments"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The database contains labeled face photographs spanning the range of conditions typically encountered in everyday life, and exhibits \u201cnatural\u201d variability in factors such as pose, lighting, race, accessories, occlusions, and background."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751569"
                        ],
                        "name": "Samy Bengio",
                        "slug": "Samy-Bengio",
                        "structuredName": {
                            "firstName": "Samy",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Samy Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746841"
                        ],
                        "name": "Nicolas Usunier",
                        "slug": "Nicolas-Usunier",
                        "structuredName": {
                            "firstName": "Nicolas",
                            "lastName": "Usunier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nicolas Usunier"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 180
                            }
                        ],
                        "text": "On ImageNet with 22K categories, it achieved a 70% relative improvement over the highest other result of which we are aware (including unpublished results known to the authors of (Weston et al., 2011))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In this section, we describe the algorithm that we use to learn features from the unlabeled training set."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1337776,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "51480ee8f067453c2878f0148ffcfa3a856a02dc",
            "isKey": false,
            "numCitedBy": 728,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "Image annotation datasets are becoming larger and larger, with tens of millions of images and tens of thousands of possible annotations. We propose a strongly performing method that scales to such datasets by simultaneously learning to optimize precision at the top of the ranked list of annotations for a given image and learning a low-dimensional joint embedding space for both images and annotations. Our method, called WSABIE, both outperforms several baseline methods and is faster and consumes less memory."
            },
            "slug": "WSABIE:-Scaling-Up-to-Large-Vocabulary-Image-Weston-Bengio",
            "title": {
                "fragments": [],
                "text": "WSABIE: Scaling Up to Large Vocabulary Image Annotation"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "This work proposes a strongly performing method that scales to image annotation datasets by simultaneously learning to optimize precision at the top of the ranked list of annotations for a given image and learning a low-dimensional joint embedding space for both images and annotations."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2428823"
                        ],
                        "name": "Leonid Karlinsky",
                        "slug": "Leonid-Karlinsky",
                        "structuredName": {
                            "firstName": "Leonid",
                            "lastName": "Karlinsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Leonid Karlinsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2888605"
                        ],
                        "name": "Michael Dinerstein",
                        "slug": "Michael-Dinerstein",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Dinerstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Dinerstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152572925"
                        ],
                        "name": "Daniel Harari",
                        "slug": "Daniel-Harari",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Harari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Harari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743045"
                        ],
                        "name": "S. Ullman",
                        "slug": "S.-Ullman",
                        "structuredName": {
                            "firstName": "Shimon",
                            "lastName": "Ullman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ullman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13983669,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "894705894b9d51ff933ddab964c4536fdcd47fb5",
            "isKey": false,
            "numCitedBy": 76,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Detecting an object part relies on two sources of information - the appearance of the part itself and the context supplied by surrounding parts. In this paper we consider problems in which a target part cannot be recognized reliably using its own appearance, such as detecting low-resolution hands, and must be recognized using the context of surrounding parts. We develop the \u2018chains model\u2019 which can locate parts of interest in a robust and precise manner, even when the surrounding context is highly variable and deformable. In the proposed model, the relation between context features and the target part is modeled in a non-parametric manner using an ensemble of feature chains leading from parts in the context to the detection target. The method uses the configuration of the features in the image directly rather than through fitting an articulated 3-D model of the object. In addition, the chains are composable, meaning that new chains observed in the test image can be composed of sub-chains seen during training. Consequently, the model is capable of handling object poses which are infrequent, even non-existent, during training. We test the approach in different settings, including object parts detection, as well as complete object detection. The results show the advantages of the chains model for detecting and localizing parts of complex deformable objects."
            },
            "slug": "The-chains-model-for-detecting-parts-by-their-Karlinsky-Dinerstein",
            "title": {
                "fragments": [],
                "text": "The chains model for detecting parts by their context"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The \u2018chains model\u2019 is developed which can locate parts of interest in a robust and precise manner, even when the surrounding context is highly variable and deformable, and is capable of handling object poses which are infrequent, even non-existent, during training."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697141"
                        ],
                        "name": "Honglak Lee",
                        "slug": "Honglak-Lee",
                        "structuredName": {
                            "firstName": "Honglak",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Honglak Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2078284037"
                        ],
                        "name": "Alexis Battle",
                        "slug": "Alexis-Battle",
                        "structuredName": {
                            "firstName": "Alexis",
                            "lastName": "Battle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexis Battle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2979876"
                        ],
                        "name": "R. Raina",
                        "slug": "R.-Raina",
                        "structuredName": {
                            "firstName": "Rajat",
                            "lastName": "Raina",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Raina"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 259,
                                "start": 243
                            }
                        ],
                        "text": "Successful feature learning algorithms and their applications can be found in recent literature using a variety of approaches such as RBMs (Hinton et al., 2006), autoencoders (Hinton & Salakhutdinov, 2006; Bengio et al., 2007), sparse coding (Lee et al., 2007) and K-means (Coates et al., 2011)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 169
                            }
                        ],
                        "text": "Our work is inspired by recent successful algorithms in unsupervised feature learning and deep learning (Hinton et al., 2006; Bengio et al., 2007; Ranzato et al., 2007; Lee et al., 2007)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 179
                            }
                        ],
                        "text": "Successful feature learning algorithms and their applications can be found in recent literature using a variety of approaches such as RBMs [4], autoencoders [5, 6], sparse coding [7] and K-means [8]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 303727,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e64a9960734215e2b1866ea3cb723ffa5585ac14",
            "isKey": false,
            "numCitedBy": 2683,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Sparse coding provides a class of algorithms for finding succinct representations of stimuli; given only unlabeled input data, it discovers basis functions that capture higher-level features in the data. However, finding sparse codes remains a very difficult computational problem. In this paper, we present efficient sparse coding algorithms that are based on iteratively solving two convex optimization problems: an L1-regularized least squares problem and an L2-constrained least squares problem. We propose novel algorithms to solve both of these optimization problems. Our algorithms result in a significant speedup for sparse coding, allowing us to learn larger sparse codes than possible with previously described algorithms. We apply these algorithms to natural images and demonstrate that the inferred sparse codes exhibit end-stopping and non-classical receptive field surround suppression and, therefore, may provide a partial explanation for these two phenomena in V1 neurons."
            },
            "slug": "Efficient-sparse-coding-algorithms-Lee-Battle",
            "title": {
                "fragments": [],
                "text": "Efficient sparse coding algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "These algorithms are applied to natural images and it is demonstrated that the inferred sparse codes exhibit end-stopping and non-classical receptive field surround suppression and, therefore, may provide a partial explanation for these two phenomena in V1 neurons."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30017846"
                        ],
                        "name": "N. Pinto",
                        "slug": "N.-Pinto",
                        "structuredName": {
                            "firstName": "Nicolas",
                            "lastName": "Pinto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Pinto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2042941"
                        ],
                        "name": "D. Cox",
                        "slug": "D.-Cox",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Cox",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Cox"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1865831"
                        ],
                        "name": "J. DiCarlo",
                        "slug": "J.-DiCarlo",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "DiCarlo",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. DiCarlo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5955557,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "688b6fbc3c5c06e254961f70de9d855d3d008d09",
            "isKey": false,
            "numCitedBy": 583,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "Progress in understanding the brain mechanisms underlying vision requires the construction of computational models that not only emulate the brain's anatomy and physiology, but ultimately match its performance on visual tasks. In recent years, \u201cnatural\u201d images have become popular in the study of vision and have been used to show apparently impressive progress in building such models. Here, we challenge the use of uncontrolled \u201cnatural\u201d images in guiding that progress. In particular, we show that a simple V1-like model\u2014a neuroscientist's \u201cnull\u201d model, which should perform poorly at real-world visual object recognition tasks\u2014outperforms state-of-the-art object recognition systems (biologically inspired and otherwise) on a standard, ostensibly natural image recognition test. As a counterpoint, we designed a \u201csimpler\u201d recognition test to better span the real-world variation in object pose, position, and scale, and we show that this test correctly exposes the inadequacy of the V1-like model. Taken together, these results demonstrate that tests based on uncontrolled natural images can be seriously misleading, potentially guiding progress in the wrong direction. Instead, we reexamine what it means for images to be natural and argue for a renewed focus on the core problem of object recognition\u2014real-world image variation."
            },
            "slug": "Why-is-Real-World-Visual-Object-Recognition-Hard-Pinto-Cox",
            "title": {
                "fragments": [],
                "text": "Why is Real-World Visual Object Recognition Hard?"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown that a simple V1-like model\u2014a neuroscientist's \u201cnull\u201d model\u2014outperforms state-of-the-art object recognition systems (biologically inspired and otherwise) on a standard, ostensibly natural image recognition test."
            },
            "venue": {
                "fragments": [],
                "text": "PLoS Comput. Biol."
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721248"
                        ],
                        "name": "P. Haffner",
                        "slug": "P.-Haffner",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Haffner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Haffner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 204,
                                "start": 200
                            }
                        ],
                        "text": "\u2026in the brain is an area of active investigation, but current experimental evidence suggests the possibility that some neurons in the temporal cortex are highly selective for object categories such as faces or hands (Desimone et al., 1984), and perhaps even specific people (Quiroga et al., 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 103
                            }
                        ],
                        "text": "First, to scale the autoencoder to large images, we use a simple idea known as local receptive fields (LeCun et al., 1998; Raina et al., 2009; Lee et al., 2009; Le et al., 2010)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 67
                            }
                        ],
                        "text": "This is a stark difference between our approach and previous work (LeCun et al., 1998; Jarrett et al., 2009; Lee et al., 2009)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This experiment shows that patches, being detected as faces by the OpenCV face detector, account for less than 3% of the 100,000 sampled patches"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 188,
                                "start": 170
                            }
                        ],
                        "text": "Our style of stacking a series of uniform modules, switching between selectivity and tolerance layers, is reminiscent of Neocognition and HMAX (Fukushima & Miyake, 1982; LeCun et al., 1998; Riesenhuber & Poggio, 1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14542261,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "162d958ff885f1462aeda91cd72582323fd6a1f4",
            "isKey": true,
            "numCitedBy": 35242,
            "numCiting": 248,
            "paperAbstract": {
                "fragments": [],
                "text": "Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day."
            },
            "slug": "Gradient-based-learning-applied-to-document-LeCun-Bottou",
            "title": {
                "fragments": [],
                "text": "Gradient-based learning applied to document recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task, and Convolutional neural networks are shown to outperform all other techniques."
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IEEE"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144936828"
                        ],
                        "name": "R. Quiroga",
                        "slug": "R.-Quiroga",
                        "structuredName": {
                            "firstName": "Rodrigo",
                            "lastName": "Quiroga",
                            "middleNames": [
                                "Quian"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Quiroga"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145297860"
                        ],
                        "name": "L. Reddy",
                        "slug": "L.-Reddy",
                        "structuredName": {
                            "firstName": "Leila",
                            "lastName": "Reddy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Reddy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1852992"
                        ],
                        "name": "G. Kreiman",
                        "slug": "G.-Kreiman",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Kreiman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Kreiman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145624227"
                        ],
                        "name": "C. Koch",
                        "slug": "C.-Koch",
                        "structuredName": {
                            "firstName": "Christof",
                            "lastName": "Koch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Koch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1774949"
                        ],
                        "name": "I. Fried",
                        "slug": "I.-Fried",
                        "structuredName": {
                            "firstName": "Itzhak",
                            "lastName": "Fried",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Fried"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 112
                            }
                        ],
                        "text": "Contrary to what appears to be a widely-held intuition, our experimental results reveal that it is possible to train a face detector without having to label images as containing a face or not."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 295,
                                "start": 275
                            }
                        ],
                        "text": "\u2026in the brain is an area of active investigation, but current experimental evidence suggests the possibility that some neurons in the temporal cortex are highly selective for object categories such as faces or hands (Desimone et al., 1984), and perhaps even specific people (Quiroga et al., 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1234637,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "4d6109548fab119d5edfb7a77b63eb10ca58f4a4",
            "isKey": false,
            "numCitedBy": 1566,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "It takes a fraction of a second to recognize a person or an object even when seen under strikingly different conditions. How such a robust, high-level representation is achieved by neurons in the human brain is still unclear. In monkeys, neurons in the upper stages of the ventral visual pathway respond to complex images such as faces and objects and show some degree of invariance to metric properties such as the stimulus size, position and viewing angle. We have previously shown that neurons in the human medial temporal lobe (MTL) fire selectively to images of faces, animals, objects or scenes. Here we report on a remarkable subset of MTL neurons that are selectively activated by strikingly different pictures of given individuals, landmarks or objects and in some cases even by letter strings with their names. These results suggest an invariant, sparse and explicit code, which might be important in the transformation of complex visual percepts into long-term and more abstract memories."
            },
            "slug": "Invariant-visual-representation-by-single-neurons-Quiroga-Reddy",
            "title": {
                "fragments": [],
                "text": "Invariant visual representation by single neurons in the human brain"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A remarkable subset of MTL neurons are selectively activated by strikingly different pictures of given individuals, landmarks or objects and in some cases even by letter strings with their names, which suggest an invariant, sparse and explicit code, which might be important in the transformation of complex visual percepts into long-term and more abstract memories."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3245558"
                        ],
                        "name": "P. Berkes",
                        "slug": "P.-Berkes",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Berkes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Berkes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736245"
                        ],
                        "name": "Laurenz Wiskott",
                        "slug": "Laurenz-Wiskott",
                        "structuredName": {
                            "firstName": "Laurenz",
                            "lastName": "Wiskott",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Laurenz Wiskott"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 87
                            }
                        ],
                        "text": "The second approach is to perform numerical optimization to find the optimal stimulus (Berkes & Wiskott, 2005; Erhan et al., 2009; Le et al., 2010)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In this section, we describe the algorithm that we use to learn features from the unlabeled training set."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1744548,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "2af18c7b3495955d1ffa4f1b4c4ae5ea786cdedb",
            "isKey": false,
            "numCitedBy": 293,
            "numCiting": 81,
            "paperAbstract": {
                "fragments": [],
                "text": "In this study we investigate temporal slowness as a learning principle for receptive fields using slow feature analysis, a new algorithm to determine functions that extract slowly varying signals from the input data. We find a good qualitative and quantitative match between the set of learned functions trained on image sequences and the population of complex cells in the primary visual cortex (V1). The functions show many properties found also experimentally in complex cells, such as direction selectivity, non-orthogonal inhibition, end-inhibition, and side-inhibition. Our results demonstrate that a single unsupervised learning principle can account for such a rich repertoire of receptive field properties."
            },
            "slug": "Slow-feature-analysis-yields-a-rich-repertoire-of-Berkes-Wiskott",
            "title": {
                "fragments": [],
                "text": "Slow feature analysis yields a rich repertoire of complex cell properties."
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "This study investigates temporal slowness as a learning principle for receptive fields using slow feature analysis, a new algorithm to determine functions that extract slowly varying signals from the input data."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of vision"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794837"
                        ],
                        "name": "Siwei Lyu",
                        "slug": "Siwei-Lyu",
                        "structuredName": {
                            "firstName": "Siwei",
                            "lastName": "Lyu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Siwei Lyu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689350"
                        ],
                        "name": "Eero P. Simoncelli",
                        "slug": "Eero-P.-Simoncelli",
                        "structuredName": {
                            "firstName": "Eero",
                            "lastName": "Simoncelli",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eero P. Simoncelli"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 84
                            }
                        ],
                        "text": "Practically, this provides an inexpensive way to develop features from unlabeled data."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 152
                            }
                        ],
                        "text": "The third sublayer performs local subtractive and divisive normalization and it is inspired by biological and computational models (Pinto et al., 2008;\nLyu & Simoncelli, 2008; Jarrett et al., 2009).2\nAs mentioned above, central to our approach is the use of local connectivity between neurons."
                    },
                    "intents": []
                }
            ],
            "corpusId": 7415524,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "25d92fe0ab92ae57bd8fed87c95fa9a207ec61aa",
            "isKey": false,
            "numCitedBy": 154,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we describe a nonlinear image representation based on divisive normalization that is designed to match the statistical properties of photographic images, as well as the perceptual sensitivity of biological visual systems. We decompose an image using a multi-scale oriented representation, and use studentpsilas t as a model of the dependencies within local clusters of coefficients. We then show that normalization of each coefficient by the square root of a linear combination of the amplitudes of the coefficients in the cluster reduces statistical dependencies. We further show that the resulting divisive normalization transform is invertible and provide an efficient iterative inversion algorithm. Finally, we probe the statistical and perceptual advantages of this image representation by examining its robustness to added noise, and using it to enhance image contrast."
            },
            "slug": "Nonlinear-image-representation-using-divisive-Lyu-Simoncelli",
            "title": {
                "fragments": [],
                "text": "Nonlinear image representation using divisive normalization"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper decomposes an image using a multi-scale oriented representation, and uses studentpsilas t as a model of the dependencies within local clusters of coefficients to show that normalization of each coefficient by the square root of a linear combination of the amplitudes of the coefficients in the cluster reduces statistical dependencies."
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31778202"
                        ],
                        "name": "C. G. Keller",
                        "slug": "C.-G.-Keller",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Keller",
                            "middleNames": [
                                "Gustav"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. G. Keller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1765022"
                        ],
                        "name": "M. Enzweiler",
                        "slug": "M.-Enzweiler",
                        "structuredName": {
                            "firstName": "Markus",
                            "lastName": "Enzweiler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Enzweiler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144854796"
                        ],
                        "name": "D. Gavrila",
                        "slug": "D.-Gavrila",
                        "structuredName": {
                            "firstName": "Dariu",
                            "lastName": "Gavrila",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Gavrila"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Negative and positive examples in our human body dataset are subsampled at random from a benchmark dataset (Keller et al., 2009)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15698103,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b2ce846cd30c942529af9d12268590ddace22d9a",
            "isKey": false,
            "numCitedBy": 82,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Pedestrian detection is a rapidly evolving area in the intelligent vehicles domain. Stereo vision is an attractive sensor for this purpose. But unlike for monocular vision, there are no realistic, large scale benchmarks available for stereo-based pedestrian detection, to provide a common point of reference for evaluation. This paper introduces the Daimler Stereo-Vision Pedestrian Detection benchmark, which consists of several thousands of pedestrians in the training set, and a 27-min test drive through urban environment and associated vehicle data. The data, including ground truth, is made publicly available for non-commercial purposes. The paper furthermore quantifies the benefit of stereo vision for ROI generation and localization; at equal detection rates, false positives are reduced by a factor of 4\u20135 with stereo over mono, using the same HOG/linSVM classification component."
            },
            "slug": "A-new-benchmark-for-stereo-based-pedestrian-Keller-Enzweiler",
            "title": {
                "fragments": [],
                "text": "A new benchmark for stereo-based pedestrian detection"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The paper furthermore quantifies the benefit of stereo vision for ROI generation and localization; at equal detection rates, false positives are reduced by a factor of 4\u20135 with stereo over mono, using the same HOG/linSVM classification component."
            },
            "venue": {
                "fragments": [],
                "text": "2011 IEEE Intelligent Vehicles Symposium (IV)"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1865831"
                        ],
                        "name": "J. DiCarlo",
                        "slug": "J.-DiCarlo",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "DiCarlo",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. DiCarlo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6072034"
                        ],
                        "name": "D. Zoccolan",
                        "slug": "D.-Zoccolan",
                        "structuredName": {
                            "firstName": "Davide",
                            "lastName": "Zoccolan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Zoccolan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2913957"
                        ],
                        "name": "N. Rust",
                        "slug": "N.-Rust",
                        "structuredName": {
                            "firstName": "Nicole",
                            "lastName": "Rust",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Rust"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 187,
                                "start": 167
                            }
                        ],
                        "text": "(1)\nHere, \u03bb is a tradeoff parameter between sparsity and reconstruction; m, k are the number of examples and pooling units in a layer respectively; Hj is the vector of weights of the j-th pooling unit."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 69
                            }
                        ],
                        "text": "It has also been argued to be an architecture employed by the brain (DiCarlo et al., 2012)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13366193,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "3bc8c19cd2257790f2c92c0b6b757c0550a8404b",
            "isKey": false,
            "numCitedBy": 1245,
            "numCiting": 221,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "How-Does-the-Brain-Solve-Visual-Object-Recognition-DiCarlo-Zoccolan",
            "title": {
                "fragments": [],
                "text": "How Does the Brain Solve Visual Object Recognition?"
            },
            "venue": {
                "fragments": [],
                "text": "Neuron"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1996960"
                        ],
                        "name": "M. Riesenhuber",
                        "slug": "M.-Riesenhuber",
                        "structuredName": {
                            "firstName": "Maximilian",
                            "lastName": "Riesenhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Riesenhuber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 216,
                                "start": 190
                            }
                        ],
                        "text": "Our style of stacking a series of uniform modules, switching between selectivity and tolerance layers, is reminiscent of Neocognition and HMAX (Fukushima & Miyake, 1982; LeCun et al., 1998; Riesenhuber & Poggio, 1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": " is squareroot. The overall network is constructed by replicating this architecture three times, resulting in 6 sublayers. This style of stacking a series of uniform structures is reminiscent of HMAX [25] and Neocognition [6] approaches. During parameter learning, the parameters of the second sublayers (H) are xed. There are two sets of parameters associated with the rst sublayer, encoding weights W 1"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8920227,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "85abadb689897997f1e37baa7b5fc6f7d497518b",
            "isKey": false,
            "numCitedBy": 3317,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "Visual processing in cortex is classically modeled as a hierarchy of increasingly sophisticated representations, naturally extending the model of simple to complex cells of Hubel and Wiesel. Surprisingly, little quantitative modeling has been done to explore the biological feasibility of this class of models to explain aspects of higher-level visual processing such as object recognition. We describe a new hierarchical model consistent with physiological data from inferotemporal cortex that accounts for this complex visual task and makes testable predictions. The model is based on a MAX-like operation applied to inputs to certain cortical neurons that may have a general role in cortical function."
            },
            "slug": "Hierarchical-models-of-object-recognition-in-cortex-Riesenhuber-Poggio",
            "title": {
                "fragments": [],
                "text": "Hierarchical models of object recognition in cortex"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A new hierarchical model consistent with physiological data from inferotemporal cortex that accounts for this complex visual task and makes testable predictions is described."
            },
            "venue": {
                "fragments": [],
                "text": "Nature Neuroscience"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144375727"
                        ],
                        "name": "R. Desimone",
                        "slug": "R.-Desimone",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Desimone",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Desimone"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2554248"
                        ],
                        "name": "T. Albright",
                        "slug": "T.-Albright",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Albright",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Albright"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2385092"
                        ],
                        "name": "C. Gross",
                        "slug": "C.-Gross",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Gross",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Gross"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "121670017"
                        ],
                        "name": "C. Bruce",
                        "slug": "C.-Bruce",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Bruce",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Bruce"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": " simulate a simpler version of important neuroscientic results known as \\grandmother cell.&quot; This class of cells have been argued to exist in the brain with the role of detecting faces and hands [5] or specic people [21] (e.g., famous actress Jennifer Aniston). Current computer vision practices typically advocate the use of labeled data to achieve these high-level concepts. For example, to buil"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 238,
                                "start": 217
                            }
                        ],
                        "text": "\u2026in the brain is an area of active investigation, but current experimental evidence suggests the possibility that some neurons in the temporal cortex are highly selective for object categories such as faces or hands (Desimone et al., 1984), and perhaps even specific people (Quiroga et al., 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9216978,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "2990a8414fcffdae2469d0d86eb45f584bd6b283",
            "isKey": false,
            "numCitedBy": 1379,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Previous studies have reported that some neurons in the inferior temporal (IT) cortex respond selectively to highly specific complex objects. In the present study, we conducted the first systematic survey of the responses of IT neurons to both simple stimuli, such as edges and bars, and highly complex stimuli, such as models of flowers, snakes, hands, and faces. If a neuron responded to any of these stimuli, we attempted to isolate the critical stimulus features underlying the response. We found that many of the responsive neurons responded well to virtually every stimulus tested. The remaining, stimulus-selective cells were often selective along the dimensions of shape, color, or texture of a stimulus, and this selectivity was maintained throughout a large receptive field. Although most IT neurons do not appear to be \u201cdetectors\u201d for complex objects, we did find a separate population of cells that responded selectively to faces. The responses of these cells were dependent on the configuration of specific face features, and their selectivity was maintained over changes in stimulus size and position. A particularly high incidence of such cells was found deep in the superior temporal sulcus. These results indicate that there may be specialized mechanisms for the analysis of faces in IT cortex."
            },
            "slug": "Stimulus-selective-properties-of-inferior-temporal-Desimone-Albright",
            "title": {
                "fragments": [],
                "text": "Stimulus-selective properties of inferior temporal neurons in the macaque"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The first systematic survey of the responses of IT neurons to both simple stimuli and highly complex stimuli indicates that there may be specialized mechanisms for the analysis of faces in IT cortex."
            },
            "venue": {
                "fragments": [],
                "text": "The Journal of neuroscience : the official journal of the Society for Neuroscience"
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2334226"
                        ],
                        "name": "D. Hubel",
                        "slug": "D.-Hubel",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Hubel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Hubel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2629471"
                        ],
                        "name": "T. Wiesel",
                        "slug": "T.-Wiesel",
                        "structuredName": {
                            "firstName": "Torsten",
                            "lastName": "Wiesel",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Wiesel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In this section, we describe the algorithm that we use to learn features from the unlabeled training set."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17055992,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "6b4fe4aa4d66fecc7b2869569002714d91d0b3f7",
            "isKey": false,
            "numCitedBy": 12429,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "What chiefly distinguishes cerebral cortex from other parts of the central nervous system is the great diversity of its cell types and interconnexions. It would be astonishing if such a structure did not profoundly modify the response patterns of fibres coming into it. In the cat's visual cortex, the receptive field arrangements of single cells suggest that there is indeed a degree of complexity far exceeding anything yet seen at lower levels in the visual system. In a previous paper we described receptive fields of single cortical cells, observing responses to spots of light shone on one or both retinas (Hubel & Wiesel, 1959). In the present work this method is used to examine receptive fields of a more complex type (Part I) and to make additional observations on binocular interaction (Part II). This approach is necessary in order to understand the behaviour of individual cells, but it fails to deal with the problem of the relationship of one cell to its neighbours. In the past, the technique of recording evoked slow waves has been used with great success in studies of functional anatomy. It was employed by Talbot & Marshall (1941) and by Thompson, Woolsey & Talbot (1950) for mapping out the visual cortex in the rabbit, cat, and monkey. Daniel & Whitteiidge (1959) have recently extended this work in the primate. Most of our present knowledge of retinotopic projections, binocular overlap, and the second visual area is based on these investigations. Yet the method of evoked potentials is valuable mainly for detecting behaviour common to large populations of neighbouring cells; it cannot differentiate functionally between areas of cortex smaller than about 1 mm2. To overcome this difficulty a method has in recent years been developed for studying cells separately or in small groups during long micro-electrode penetrations through nervous tissue. Responses are correlated with cell location by reconstructing the electrode tracks from histological material. These techniques have been applied to"
            },
            "slug": "Receptive-fields,-binocular-interaction-and-in-the-Hubel-Wiesel",
            "title": {
                "fragments": [],
                "text": "Receptive fields, binocular interaction and functional architecture in the cat's visual cortex"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This method is used to examine receptive fields of a more complex type and to make additional observations on binocular interaction and this approach is necessary in order to understand the behaviour of individual cells, but it fails to deal with the problem of the relationship of one cell to its neighbours."
            },
            "venue": {
                "fragments": [],
                "text": "The Journal of physiology"
            },
            "year": 1962
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1895356"
                        ],
                        "name": "D. Ciresan",
                        "slug": "D.-Ciresan",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Ciresan",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ciresan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2514691"
                        ],
                        "name": "U. Meier",
                        "slug": "U.-Meier",
                        "structuredName": {
                            "firstName": "Ueli",
                            "lastName": "Meier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Meier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6803671"
                        ],
                        "name": "L. Gambardella",
                        "slug": "L.-Gambardella",
                        "structuredName": {
                            "firstName": "Luca",
                            "lastName": "Gambardella",
                            "middleNames": [
                                "Maria"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gambardella"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 257,
                                "start": 237
                            }
                        ],
                        "text": "\u2026we use a large dataset generated by sampling random frames from random YouTube videos.1 Our input data are 200x200 images, much larger than typical 32x32 images used in deep learning and unsupervised feature learning (Krizhevsky, 2009; Ciresan et al., 2010; Le et al., 2010; Coates et al., 2011)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 124
                            }
                        ],
                        "text": "Recent studies observe that it is quite time intensive to train deep learning algorithms to yield state of the art results (Ciresan et al., 2010)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 144
                            }
                        ],
                        "text": "It has 1 billion trainable parameters, which is more than an order of magnitude larger than other large networks reported in literature, e.g., (Ciresan et al., 2010; Sermanet & LeCun, 2011) with around 10 million parameters."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1918673,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b98cd08b75ebf2bd1d1ec47c51ef75777a7e64bd",
            "isKey": false,
            "numCitedBy": 875,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Good old online backpropagation for plain multilayer perceptrons yields a very low 0.35 error rate on the MNIST handwritten digits benchmark. All we need to achieve this best result so far are many hidden layers, many neurons per layer, numerous deformed training images to avoid overfitting, and graphics cards to greatly speed up learning."
            },
            "slug": "Deep,-Big,-Simple-Neural-Nets-for-Handwritten-Digit-Ciresan-Meier",
            "title": {
                "fragments": [],
                "text": "Deep, Big, Simple Neural Nets for Handwritten Digit Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Good old online backpropagation for plain multilayer perceptrons yields a very low 0.35 error rate on the MNIST handwritten digits benchmark."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2334226"
                        ],
                        "name": "D. Hubel",
                        "slug": "D.-Hubel",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Hubel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Hubel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2629471"
                        ],
                        "name": "T. Wiesel",
                        "slug": "T.-Wiesel",
                        "structuredName": {
                            "firstName": "Torsten",
                            "lastName": "Wiesel",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Wiesel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 134
                            }
                        ],
                        "text": "According to their study, sparse coding can be trained on unlabeled natural images to yield receptive fields akin to V1 simple cells (Hubel & Wiesel, 1959)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14801990,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "6f20e254e3993538c79e0ff2b9b8f198d3359cb3",
            "isKey": false,
            "numCitedBy": 4211,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "In the central nervous system the visual pathway from retina to striate cortex provides an opportunity to observe and compare single unit responses at several distinct levels. Patterns of light stimuli most effective in influencing units at one level may no longer be the most effective at the next. From differences in responses at successive stages in the pathway one may hope to gain some understanding of the part each stage plays in visual perception. By shining small spots of light on the light-adapted cat retina Kuffler (1953) showed that ganglion cells have concentric receptive fields, with an 'on' centre and an 'off ' periphery, or vice versa. The 'on' and 'off' areas within a receptive field were found to be mutually antagonistic, and a spot restricted to the centre of the field was more effective than one covering the whole receptive field (Barlow, FitzHugh & Kuffler, 1957). In the freely moving lightadapted cat it was found that the great majority of cortical cells studied gave little or no response to light stimuli covering most of the animal's visual field, whereas small spots shone in a restricted retinal region often evoked brisk responses (Hubel, 1959). A moving spot of light often produced stronger responses than a stationary one, and sometimes a moving spot gave more activation for one direction than for the opposite. The present investigation, made in acute preparations, includes a study of receptive fields of cells in the cat's striate cortex. Receptive fields of the cells considered in this paper were divided into separate excitatory and inhibitory ('on' and 'off') areas. In this respect they resembled retinal ganglion-cell receptive fields. However, the shape and arrangement of excitatory and inhibitory areas differed strikingly from the concentric pattern found in retinal ganglion cells. An attempt was made to correlate responses to moving stimuli"
            },
            "slug": "Receptive-fields-of-single-neurones-in-the-cat's-Hubel-Wiesel",
            "title": {
                "fragments": [],
                "text": "Receptive fields of single neurones in the cat's striate cortex"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The present investigation, made in acute preparations, includes a study of receptive fields of cells in the cat's striate cortex, which resembled retinal ganglion-cell receptive fields, but the shape and arrangement of excitatory and inhibitory areas differed strikingly from the concentric pattern found in retinalganglion cells."
            },
            "venue": {
                "fragments": [],
                "text": "The Journal of physiology"
            },
            "year": 1959
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3160228"
                        ],
                        "name": "K. Fukushima",
                        "slug": "K.-Fukushima",
                        "structuredName": {
                            "firstName": "Kunihiko",
                            "lastName": "Fukushima",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Fukushima"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3126340"
                        ],
                        "name": "S. Miyake",
                        "slug": "S.-Miyake",
                        "structuredName": {
                            "firstName": "Sei",
                            "lastName": "Miyake",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Miyake"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This style of stacking a series of uniform structures is reminiscent of HMAX [25] and Neocognition [6] approaches."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 144
                            }
                        ],
                        "text": "Our style of stacking a series of uniform modules, switching between selectivity and tolerance layers, is reminiscent of Neocognition and HMAX (Fukushima & Miyake, 1982; LeCun et al., 1998; Riesenhuber & Poggio, 1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2357880,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9aec973227713cd45f156090d82a3056cca8060f",
            "isKey": false,
            "numCitedBy": 697,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Neocognitron:-A-new-algorithm-for-pattern-tolerant-Fukushima-Miyake",
            "title": {
                "fragments": [],
                "text": "Neocognitron: A new algorithm for pattern recognition tolerant of deformations and shifts in position"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5604842"
                        ],
                        "name": "B. Pakkenberg",
                        "slug": "B.-Pakkenberg",
                        "structuredName": {
                            "firstName": "Bente",
                            "lastName": "Pakkenberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Pakkenberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3948628"
                        ],
                        "name": "D. Pelvig",
                        "slug": "D.-Pelvig",
                        "structuredName": {
                            "firstName": "Dorte",
                            "lastName": "Pelvig",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Pelvig"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1905551"
                        ],
                        "name": "L. Marner",
                        "slug": "L.-Marner",
                        "structuredName": {
                            "firstName": "Lisbeth",
                            "lastName": "Marner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Marner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2097095145"
                        ],
                        "name": "M. J. Bundgaard",
                        "slug": "M.-J.-Bundgaard",
                        "structuredName": {
                            "firstName": "Mads",
                            "lastName": "Bundgaard",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. J. Bundgaard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144624949"
                        ],
                        "name": "H. Gundersen",
                        "slug": "H.-Gundersen",
                        "structuredName": {
                            "firstName": "Hans",
                            "lastName": "Gundersen",
                            "middleNames": [
                                "J\u00f8rgen",
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Gundersen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2097840026"
                        ],
                        "name": "J. Nyengaard",
                        "slug": "J.-Nyengaard",
                        "structuredName": {
                            "firstName": "Jens",
                            "lastName": "Nyengaard",
                            "middleNames": [
                                "Rondel"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Nyengaard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6466114"
                        ],
                        "name": "L. Regeur",
                        "slug": "L.-Regeur",
                        "structuredName": {
                            "firstName": "Lisbeth",
                            "lastName": "Regeur",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Regeur"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14647116,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "7c7c04323f6ba96c32c47fc3159cc30614c2e822",
            "isKey": false,
            "numCitedBy": 388,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Aging-and-the-human-neocortex-Pakkenberg-Pelvig",
            "title": {
                "fragments": [],
                "text": "Aging and the human neocortex"
            },
            "venue": {
                "fragments": [],
                "text": "Experimental Gerontology"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790646"
                        ],
                        "name": "P. Dayan",
                        "slug": "P.-Dayan",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Dayan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Dayan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145293717"
                        ],
                        "name": "L. Abbott",
                        "slug": "L.-Abbott",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Abbott",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Abbott"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": " mouse. Right: The averaged image of the top 100 stimuli. Both are shown without the padded zero areas. 7This visualization technique is also used to visualize the receptive elds of simple cells (see [3] for an example). 7 8 Invariance properties In the following, we choose a set of 10 face images and perform simple transformations to them, scaling and translating. Next we want to check how sensitive"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14072069,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "4eb35c07a0d6162ac09c0269b473ef464d920e0d",
            "isKey": false,
            "numCitedBy": 3295,
            "numCiting": 249,
            "paperAbstract": {
                "fragments": [],
                "text": "Theoretical neuroscience provides a quantitative basis for describing what nervous systems do, determining how they function, and uncovering the general principles by which they operate. This text introduces the basic mathematical and computational methods of theoretical neuroscience and presents applications in a variety of areas including vision, sensory-motor integration, development, learning, and memory. The book is divided into three parts. Part I discusses the relationship between sensory stimuli and neural responses, focusing on the representation of information by the spiking activity of neurons. Part II discusses the modeling of neurons and neural circuits on the basis of cellular and synaptic biophysics. Part III analyzes the role of plasticity in development and learning. An appendix covers the mathematical methods used, and exercises are available on the book's Web site."
            },
            "slug": "Theoretical-Neuroscience:-Computational-and-of-Dayan-Abbott",
            "title": {
                "fragments": [],
                "text": "Theoretical Neuroscience: Computational and Mathematical Modeling of Neural Systems"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This text introduces the basic mathematical and computational methods of theoretical neuroscience and presents applications in a variety of areas including vision, sensory-motor integration, development, learning, and memory."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49591043"
                        ],
                        "name": "G. Gergely",
                        "slug": "G.-Gergely",
                        "structuredName": {
                            "firstName": "Gy\u00f6rgy",
                            "lastName": "Gergely",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Gergely"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2989594"
                        ],
                        "name": "H. Bekkering",
                        "slug": "H.-Bekkering",
                        "structuredName": {
                            "firstName": "Harold",
                            "lastName": "Bekkering",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bekkering"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2742111"
                        ],
                        "name": "I. Kir\u00e1ly",
                        "slug": "I.-Kir\u00e1ly",
                        "structuredName": {
                            "firstName": "Ildik\u00f3",
                            "lastName": "Kir\u00e1ly",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Kir\u00e1ly"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 39665452,
            "fieldsOfStudy": [
                "Psychology",
                "Medicine"
            ],
            "id": "a31b7bc899fd23a8ffdd769b25abe59977f22f54",
            "isKey": false,
            "numCitedBy": 686,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Here we show that if an adult demonstrates a new way to execute a task to a group of infants aged 14 months, the children will use this action to achieve the same goal only if they consider it to be the most rational alternative. Our results indicate that imitation of goal-directed action by preverbal infants is a selective, interpretative process, rather than a simple re-enactment of the means used by a demonstrator, as was previously thought."
            },
            "slug": "Developmental-psychology:-Rational-imitation-in-Gergely-Bekkering",
            "title": {
                "fragments": [],
                "text": "Developmental psychology: Rational imitation in preverbal infants"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The results indicate that imitation of goal-directed action by preverbal infants is a selective, interpretative process, rather than a simple re-enactment of the means used by a demonstrator, as was previously thought."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4433870"
                        ],
                        "name": "P. Lennie",
                        "slug": "P.-Lennie",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Lennie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Lennie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 47
                            }
                        ],
                        "text": "These properties in turn were characterized by [11, 12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17950742,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "dc7d03d05a437aaf0cc02a4ed36339cc259d2583",
            "isKey": false,
            "numCitedBy": 306,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Receptive-fields-Lennie",
            "title": {
                "fragments": [],
                "text": "Receptive fields"
            },
            "venue": {
                "fragments": [],
                "text": "Current Biology"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 266,
                                "start": 251
                            }
                        ],
                        "text": "\u2026et al., 2009; Le et al., 2011a).4 The first term in the objective ensures the representations encode important information about the data, i.e., they can reconstruct input data; whereas the second term encourages pooling features to group similar features together to achieve invariances."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 131
                            }
                        ],
                        "text": "The second approach is to perform numerical optimization to find the optimal stimulus (Berkes & Wiskott, 2005; Erhan et al., 2009; Le et al., 2010)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 144
                            }
                        ],
                        "text": "In addition to being more biologically plausible, unshared weights allow the learning of more invariances other than translational invariances (Le et al., 2010)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 3
                            }
                        ],
                        "text": "In our experiments, we obtained neurons that function as detectors for faces, human bodies, and cat faces by training on random frames of YouTube videos."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 110
                            }
                        ],
                        "text": "To support parallelism on this cluster, we use the idea of local receptive fields, e.g., (Raina et al., 2009; Le et al., 2010; 2011b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 274,
                                "start": 259
                            }
                        ],
                        "text": "\u2026we use a large dataset generated by sampling random frames from random YouTube videos.1 Our input data are 200x200 images, much larger than typical 32x32 images used in deep learning and unsupervised feature learning (Krizhevsky, 2009; Ciresan et al., 2010; Le et al., 2010; Coates et al., 2011)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 161
                            }
                        ],
                        "text": "First, to scale the autoencoder to large images, we use a simple idea known as local receptive fields (LeCun et al., 1998; Raina et al., 2009; Lee et al., 2009; Le et al., 2010)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 125
                            }
                        ],
                        "text": "Next, to achieve invariance to local deformations, we employ local L2 pooling (Hyva\u0308rinen et al., 2009; Gregor & LeCun, 2010; Le et al., 2010) and local contrast normalization (Jarrett et al., 2009)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 94
                            }
                        ],
                        "text": "L2 pooling, in particular, allows the learning of invariant features (Hyva\u0308rinen et al., 2009; Le et al., 2010)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 68
                            }
                        ],
                        "text": "For instance, we would like to understand if it is possible to build a face detector from only unlabeled images."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Tiled convolutional neural networks. In NIPS"
            },
            "venue": {
                "fragments": [],
                "text": "Tiled convolutional neural networks. In NIPS"
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 144
                            }
                        ],
                        "text": "It has 1 billion trainable parameters, which is more than an order of magnitude larger than other large networks reported in literature, e.g., (Ciresan et al., 2010; Sermanet & LeCun, 2011) with around 10 million parameters."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 124
                            }
                        ],
                        "text": "Recent studies observe that it is quite time intensive to train deep learning algorithms to yield state of the art results (Ciresan et al., 2010)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 257,
                                "start": 237
                            }
                        ],
                        "text": "\u2026we use a large dataset generated by sampling random frames from random YouTube videos.1 Our input data are 200x200 images, much larger than typical 32x32 images used in deep learning and unsupervised feature learning (Krizhevsky, 2009; Ciresan et al., 2010; Le et al., 2010; Coates et al., 2011)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Copyright 2012 by the author(s)/owner(s)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 51
                            }
                        ],
                        "text": "To train the model, we implemented model parallelism by distributing the local weights W1, W2 and H to different machines."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 46
                            }
                        ],
                        "text": "For instance, we would like to understand if it is possible to build a face detector from only unlabeled images."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "and Building high-level features using large-scale unsupervised learning Schmidhuber, J. Deep big simple neural nets excel on handwritten digit recognition"
            },
            "venue": {
                "fragments": [],
                "text": "and Building high-level features using large-scale unsupervised learning Schmidhuber, J. Deep big simple neural nets excel on handwritten digit recognition"
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 91
                            }
                        ],
                        "text": "More details about our results and analyses are discussed in the full version of our paper [15]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 110
                            }
                        ],
                        "text": "To support parallelism on this cluster, we use the idea of local receptive fields, e.g., (Raina et al., 2009; Le et al., 2010; 2011b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 126
                            }
                        ],
                        "text": "This optimization problem is also known as reconstruction Topographic Independent Component Analysis (Hyva\u0308rinen et al., 2009; Le et al., 2011a).4 The first term in the objective ensures the representations encode important information about the data, i.e., they can reconstruct input data; whereas\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 94
                            }
                        ],
                        "text": "In the simplest implementation, before processing each mini-batch a\n4In (Bengio et al., 2007; Le et al., 2011a), the encoding weights and the decoding weights are tied: W1 = W2."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Building high-level features using large scale unsupervised learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2011
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 102
                            }
                        ],
                        "text": "This optimization problem is also known as reconstruction Topographic Independent Component Analysis (Hyva\u0308rinen et al., 2009; Le et al., 2011a).4 The first term in the objective ensures the representations encode important information about the data, i.e., they can reconstruct input data; whereas\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 58
                            }
                        ],
                        "text": "This optimization problem is also known as reconstruction Topographic Independent Component Analysis (Hyva\u0308rinen et al., 2009; Le et al., 2011a).4 The first term in the objective ensures the representations encode important information about the data, i.e., they can reconstruct input data; whereas the second term encourages pooling features to group similar features together to achieve invariances."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 79
                            }
                        ],
                        "text": "Next, to achieve invariance to local deformations, we employ local L2 pooling (Hyva\u0308rinen et al., 2009; Gregor & LeCun, 2010; Le et al., 2010) and local contrast normalization (Jarrett et al., 2009)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 70
                            }
                        ],
                        "text": "L2 pooling, in particular, allows the learning of invariant features (Hyva\u0308rinen et al., 2009; Le et al., 2010)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 134
                            }
                        ],
                        "text": "Contemporary computer vision methodology typically emphasizes the role of labeled data to obtain these class-specific feature detectors."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Natural Image Statistics"
            },
            "venue": {
                "fragments": [],
                "text": "Natural Image Statistics"
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 110
                            }
                        ],
                        "text": "To support parallelism on this cluster, we use the idea of local receptive fields, e.g., (Raina et al., 2009; Le et al., 2010; 2011b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 126
                            }
                        ],
                        "text": "This optimization problem is also known as reconstruction Topographic Independent Component Analysis (Hyva\u0308rinen et al., 2009; Le et al., 2011a).4 The first term in the objective ensures the representations encode important information about the data, i.e., they can reconstruct input data; whereas\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 94
                            }
                        ],
                        "text": "In the simplest implementation, before processing each mini-batch a\n4In (Bengio et al., 2007; Le et al., 2011a), the encoding weights and the decoding weights are tied: W1 = W2."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 140
                            }
                        ],
                        "text": "To answer this, we train a 9- layered locally connected sparse autoencoder with pooling and local contrast normalization on a large dataset of images (the model has 1 billion connections, the dataset has 10 million 200x200 pixel images downloaded from the Internet)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning hierarchical spatio-temporal features for action recognition with independent subspace analysis"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2151530451"
                        ],
                        "name": "Le Li",
                        "slug": "Le-Li",
                        "structuredName": {
                            "firstName": "Le",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Le Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1591128560"
                        ],
                        "name": "Yujin Zhang",
                        "slug": "Yujin-Zhang",
                        "structuredName": {
                            "firstName": "Yujin",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yujin Zhang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 120399270,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c9a80b7bce8276c978b323585fd3164a61b6ea13",
            "isKey": false,
            "numCitedBy": 212,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "SENSC:-a-Stable-and-Efficient-Algorithm-for-Sparse-Li-Zhang",
            "title": {
                "fragments": [],
                "text": "SENSC: a Stable and Efficient Algorithm for Nonnegative Sparse Coding: SENSC: a Stable and Efficient Algorithm for Nonnegative Sparse Coding"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1895356"
                        ],
                        "name": "D. Ciresan",
                        "slug": "D.-Ciresan",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Ciresan",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ciresan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2514691"
                        ],
                        "name": "U. Meier",
                        "slug": "U.-Meier",
                        "structuredName": {
                            "firstName": "Ueli",
                            "lastName": "Meier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Meier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6803671"
                        ],
                        "name": "L. Gambardella",
                        "slug": "L.-Gambardella",
                        "structuredName": {
                            "firstName": "Luca",
                            "lastName": "Gambardella",
                            "middleNames": [
                                "Maria"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gambardella"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 45157843,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9e2964d36f154cc13b0af39c5b36cb3d76b27da1",
            "isKey": false,
            "numCitedBy": 414,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Deep-Big-Simple-Neural-Nets-Excel-on-Handwritten-Ciresan-Meier",
            "title": {
                "fragments": [],
                "text": "Deep Big Simple Neural Nets Excel on Handwritten Digit Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Acknowledgements: We thank Samy Bengio"
            },
            "venue": {
                "fragments": [],
                "text": "005% accuracy for this dataset"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "UFLDL_Tutorial http://www.nytimes.com/2012/06/26/technology/in-a-big-network-of-computers-evidence- of-machine-learning.html?_r=0"
            },
            "venue": {
                "fragments": [],
                "text": "UFLDL_Tutorial http://www.nytimes.com/2012/06/26/technology/in-a-big-network-of-computers-evidence- of-machine-learning.html?_r=0"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Building high-level features using large-scale unsupervised learning"
            },
            "venue": {
                "fragments": [],
                "text": "Building high-level features using large-scale unsupervised learning"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 110
                            }
                        ],
                        "text": "Using unlabeled data in the wild to learn features is the key idea behind the self-taught learning framework (Raina et al., 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Copyright 2012 by the author(s)/owner(s)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Selftaught learning: Transfer learning from unlabelled data"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 188,
                                "start": 166
                            }
                        ],
                        "text": "It has 1 billion trainable parameters, which is more than an order of magnitude larger than other large networks reported in literature, e.g., (Ciresan et al., 2010; Sermanet & LeCun, 2011) with around 10 million parameters."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This experiment shows that patches, being detected as faces by the OpenCV face detector, account for less than 3% of the 100,000 sampled patches"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Traffic sign recognition with multiscale convolutional neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "IJCNN"
            },
            "year": 2011
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 134
                            }
                        ],
                        "text": "According to their study, sparse coding can be trained on unlabeled natural images to yield receptive fields akin to V1 simple cells (Hubel & Wiesel, 1959)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Receptive fields of single neurons in the the cat's visual cortex"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Physiology"
            },
            "year": 1959
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Building high-level features using large scale unsupervised learning"
            },
            "venue": {
                "fragments": [],
                "text": "ICML 2012"
            },
            "year": 2012
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Trueswell. \"Use of speaker's gaze and syntax in verb learning.\" Language Learning and Development"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 35,
            "methodology": 32,
            "result": 4
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 63,
        "totalPages": 7
    },
    "page_url": "https://www.semanticscholar.org/paper/Building-high-level-features-using-large-scale-Le-Ranzato/72e93aa6767ee683de7f001fa72f1314e40a8f35?sort=total-citations"
}