{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143936663"
                        ],
                        "name": "Thomas Hofmann",
                        "slug": "Thomas-Hofmann",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Hofmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Hofmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7605995,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e6dd83b2aa34c806596fc619ff3fbccf5f9830ab",
            "isKey": false,
            "numCitedBy": 2499,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a novel statistical method for factor analysis of binary and count data which is closely related to a technique known as Latent Semantic Analysis. In contrast to the latter method which stems from linear algebra and performs a Singular Value Decomposition of co-occurrence tables, the proposed technique uses a generative latent class model to perform a probabilistic mixture decomposition. This results in a more principled approach with a solid foundation in statistical inference. More precisely, we propose to make use of a temperature controlled version of the Expectation Maximization algorithm for model fitting, which has shown excellent performance in practice. Probabilistic Latent Semantic Analysis has many applications, most prominently in information retrieval, natural language processing, machine learning from text, and in related areas. The paper presents perplexity results for different types of text and linguistic data collections and discusses an application in automated document indexing. The experiments indicate substantial and consistent improvements of the probabilistic method over standard Latent Semantic Analysis."
            },
            "slug": "Unsupervised-Learning-by-Probabilistic-Latent-Hofmann",
            "title": {
                "fragments": [],
                "text": "Unsupervised Learning by Probabilistic Latent Semantic Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper proposes to make use of a temperature controlled version of the Expectation Maximization algorithm for model fitting, which has shown excellent performance in practice, and results in a more principled approach with a solid foundation in statistical inference."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9194763"
                        ],
                        "name": "T. Hesterberg",
                        "slug": "T.-Hesterberg",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Hesterberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Hesterberg"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 6552928,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "47ba51a5913a6f2be8ab874a5f2d246735c89614",
            "isKey": false,
            "numCitedBy": 1675,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "The strength of this book is in bringing together advanced Monte Carlo (MC) methods developed in many disciplines. This intent is clear from the outset: \u201cMany researchers in different scienti\u008e c areas have contributed to its development: : : communications among researchers in these \u008e elds are very limited. It is therefore desirable to develop a relatively general framework in which scientists in every \u008e eld: : : can compare their Monte Carlo techniques and learn from one another.\u201d Throughout the book are examples of techniques invented, or reinvented, in different \u008e elds that may be applied elsewhere. This is occasionally embarrassing to those of us who are statisticians. Consider this statement: \u201cUsing the HMC to solve statistical inference problems was \u008e rst introduced by Neil (1996). This effort was only 10 years behind that in physics and theoretical chemistry. In contrast, statisticians were 40 years late in using the Metropolis algorithm.\u201d The book serves \u201cthree audiences: researchers specializing in the study of Monte Carlo algorithms; scientists who are interested in using advanced Monte Carlo techniques; and graduate students...second-year graduate-level course on Monte Carlo methods.\u201d Chapter 1 gives an overview and a variety of applications. These include the Ising model, molecular structure simulation, bioinformatics, target tracking, hypothesis testing for astronomical observations, Bayesian inference of multilevel models, missing-data problems. Chapter 2 covers basic MC methods and begins sequential methods, including exact sampling for chain-structured models, and sequential importance sampling and rejection control, with applications in solving a linear system, missing data, and populations genetics. Chapter 3 expands on sequential methods. The common thread is that each observation from a multivariate distribution is generated sequentially from approximate conditional distributions. The ratio between the joint density (of dimensions generated so far) and the approximation is an importance sampling weight and is a martingale; for high-dimensional problems, this tends to diverge, with most observations having weights near 0 and a few having high weight. Remedies include a variety of pruning and enrichment (also known as Russian roulette and splitting) and resampling techniques. Applications include growing a polymer, missing data, nonlinear \u008e ltering, and (in Chap. 4) molecular simulation, population genetics, motif patterns in DNA sequences, counting 0\u20131 tables with \u008e xed margins, parametric Bayes analysis, approximating permanents, target tracking, and digital communications. Chapter 5 introduces Markov chain Monte Carlo (MCMC) methods, with Metropolis\u2013Hastings and a number of generalizations, including multipoint, reversible jumping, and dynamic weighting rules. Chapters 6\u20138 treat MCMC methods based on the Gibbs sampler, including data augmentation, cluster algorithms, partial resampling, slice sampler, metropolized Gibbs, hit-and-run, random-ray, collapsing and grouping, the Swendsen\u2013Wang algorithm as data augmentation, transformation groups, and generalized Gibbs. Applications include Gaussian random \u008e elds, texture synthesis Bayesian probit regression, stochastic differential equations, hierarchical Bayes, \u008e nding motifs in protein or DNA sequences, Ising and Potts models, inference with multivariate t distributions, and parameter expansion for data augmentation. Chapter 9 considers hybrid MC and a connection to molecular dynamics algorithms used in structural biology and theoretical chemistry. Also covered are some strategies for improving ef\u008e ciency, including surrogate transition, window, and multipoint methods, and applications in Bayesian analysis and stochastic volatility. Chapters 10 and 11 discuss recent methods for ef\u008e cient MC sampling, including temperature-based methods (simulated tempering, parallel tempering, and simulated annealing), reweighting methods (umbrella sampling and multicanonical sampling) and evolution-based methods (adaptive direction sampling and conjugate gradient MC). Chapters 12 and 13 cover theory for Markov chains and their convergence rates. The book focuses on relatively more dif\u008e cult MC applications where \u201cdirectly generating independent samples from the target distribution \u008f is not feasible.\u201d It omits discussion of some relatively simple MC techniques that are valuable in applications where direct generation is feasible and which could be adapted for other applications; e.g. strati\u008e ed sampling (the \u201cstrati\u008e ed sampling\u201d technique discussed here is unusual and of limited value) post-strati\u008e cation, and defensive mixture designs in importance sampling (Hesterberg 1995). The treatment of importance sampling (IS) could be improved. The book describes the original motivation for IS\u2014focusing attention on \u201cimportant\u201d regions\u2014then indicates:"
            },
            "slug": "Monte-Carlo-Strategies-in-Scientific-Computing-Hesterberg",
            "title": {
                "fragments": [],
                "text": "Monte Carlo Strategies in Scientific Computing"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "The strength of this book is in bringing together advanced Monte Carlo methods developed in many disciplines, including the Ising model, molecular structure simulation, bioinformatics, target tracking, hypothesis testing for astronomical observations, Bayesian inference of multilevel models, missing-data problems."
            },
            "venue": {
                "fragments": [],
                "text": "Technometrics"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52626911"
                        ],
                        "name": "T. Minka",
                        "slug": "T.-Minka",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Minka",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Minka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 175
                            }
                        ],
                        "text": "The integral in this expression is intractable, and \" is thus usually estimated by using sophisticated approximations, either variational Bayes (1) or expectation propagation (9)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 220,
                                "start": 217
                            }
                        ],
                        "text": "We applied our Gibbs sampling algorithm to this dataset, together with the two algorithms that have previously been used for inference in Latent Dirichlet Allocation: variational Bayes (1) and expectation propagation (9)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1768942,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "45b4dde8e0945912a39666f2715cdf10a4445b1c",
            "isKey": false,
            "numCitedBy": 537,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "The generative aspect model is an extension of the multinomial model for text that allows word probabilities to vary stochastically across documents. Previous results with aspect models have been promising, but hindered by the computational difficulty of carrying out inference and learning. This paper demonstrates that the simple variational methods of Blei et al. (2001) can lead to inaccurate inferences and biased learning for the generative aspect model. We develop an alternative approach that leads to higher accuracy at comparable cost. An extension of Expectation-Propagation is used for inference and then embedded in an EM algorithm for learning. Experimental results are presented for both synthetic and real data sets."
            },
            "slug": "Expectation-Propogation-for-the-Generative-Aspect-Minka-Lafferty",
            "title": {
                "fragments": [],
                "text": "Expectation-Propogation for the Generative Aspect Model"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper demonstrates that the simple variational methods of Blei et al. (2001) can lead to inaccurate inferences and biased learning for the generative aspect model, and develops an alternative approach that leads to higher accuracy at comparable cost."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3194361"
                        ],
                        "name": "S. Geman",
                        "slug": "S.-Geman",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Geman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Geman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707642"
                        ],
                        "name": "D. Geman",
                        "slug": "D.-Geman",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Geman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Geman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5837272,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "459b30a9a960080f3b313e41886b1aa0e51e882c",
            "isKey": false,
            "numCitedBy": 18711,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": "We make an analogy between images and statistical mechanics systems. Pixel gray levels and the presence and orientation of edges are viewed as states of atoms or molecules in a lattice-like physical system. The assignment of an energy function in the physical system determines its Gibbs distribution. Because of the Gibbs distribution, Markov random field (MRF) equivalence, this assignment also determines an MRF image model. The energy function is a more convenient and natural mechanism for embodying picture attributes than are the local characteristics of the MRF. For a range of degradation mechanisms, including blurring, nonlinear deformations, and multiplicative or additive noise, the posterior distribution is an MRF with a structure akin to the image model. By the analogy, the posterior distribution defines another (imaginary) physical system. Gradual temperature reduction in the physical system isolates low energy states (``annealing''), or what is the same thing, the most probable states under the Gibbs distribution. The analogous operation under the posterior distribution yields the maximum a posteriori (MAP) estimate of the image given the degraded observations. The result is a highly parallel ``relaxation'' algorithm for MAP estimation. We establish convergence properties of the algorithm and we experiment with some simple pictures, for which good restorations are obtained at low signal-to-noise ratios."
            },
            "slug": "Stochastic-Relaxation,-Gibbs-Distributions,-and-the-Geman-Geman",
            "title": {
                "fragments": [],
                "text": "Stochastic Relaxation, Gibbs Distributions, and the Bayesian Restoration of Images"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "The analogy between images and statistical mechanics systems is made and the analogous operation under the posterior distribution yields the maximum a posteriori (MAP) estimate of the image given the degraded observations, creating a highly parallel ``relaxation'' algorithm for MAP estimation."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2061349144"
                        ],
                        "name": "M. Newman",
                        "slug": "M.-Newman",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Newman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Newman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2175718"
                        ],
                        "name": "G. Barkema",
                        "slug": "G.-Barkema",
                        "structuredName": {
                            "firstName": "Gerard",
                            "lastName": "Barkema",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Barkema"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 60541487,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9ef3fefde105d06c4bcb44ad6f9f0cd31c0aefee",
            "isKey": false,
            "numCitedBy": 2276,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "This book provides an introduction to Monte Carlo simulations in classical statistical physics and is aimed both at students beginning work in the field and at more experienced researchers who wish to learn more about Monte Carlo methods. The material covered includes methods for both equilibrium and out of equilibrium systems, and common algorithms like the Metropolis and heat-bath algorithms are discussed in detail, as well as more sophisticated ones such as continuous time Monte Carlo, cluster algorithms, multigrid methods, entropic sampling and simulated tempering. Data analysis techniques are also explained starting with straightforward measurement and error-estimation techniques and progressing to topics such as the single and multiple histogram methods and finite size scaling. The last few chapters of the book are devoted to implementation issues, including discussions of such topics as lattice representations, efficient implementation of data structures, multispin coding, parallelization of Monte Carlo algorithms, and random number generation. At the end of the book the authors give a number of example programmes demonstrating the applications of these techniques to a variety of well-known models."
            },
            "slug": "Monte-Carlo-Methods-in-Statistical-Physics-Newman-Barkema",
            "title": {
                "fragments": [],
                "text": "Monte Carlo Methods in Statistical Physics"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This book provides an introduction to Monte Carlo simulations in classical statistical physics and is aimed both at students beginning work in the field and at more experienced researchers who wish to learn more about Monte Carlo methods."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2089883"
                        ],
                        "name": "W. Gilks",
                        "slug": "W.-Gilks",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Gilks",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Gilks"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50130827"
                        ],
                        "name": "S. Richardson",
                        "slug": "S.-Richardson",
                        "structuredName": {
                            "firstName": "Sylvia",
                            "lastName": "Richardson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Richardson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48616434"
                        ],
                        "name": "D. Spiegelhalter",
                        "slug": "D.-Spiegelhalter",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Spiegelhalter",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Spiegelhalter"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 221894711,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3fbcff06339605696423609c0f3c02737c9e91e4",
            "isKey": false,
            "numCitedBy": 4092,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "INTRODUCING MARKOV CHAIN MONTE CARLO Introduction The Problem Markov Chain Monte Carlo Implementation Discussion HEPATITIS B: A CASE STUDY IN MCMC METHODS Introduction Hepatitis B Immunization Modelling Fitting a Model Using Gibbs Sampling Model Elaboration Conclusion MARKOV CHAIN CONCEPTS RELATED TO SAMPLING ALGORITHMS Markov Chains Rates of Convergence Estimation The Gibbs Sampler and Metropolis-Hastings Algorithm INTRODUCTION TO GENERAL STATE-SPACE MARKOV CHAIN THEORY Introduction Notation and Definitions Irreducibility, Recurrence, and Convergence Harris Recurrence Mixing Rates and Central Limit Theorems Regeneration Discussion FULL CONDITIONAL DISTRIBUTIONS Introduction Deriving Full Conditional Distributions Sampling from Full Conditional Distributions Discussion STRATEGIES FOR IMPROVING MCMC Introduction Reparameterization Random and Adaptive Direction Sampling Modifying the Stationary Distribution Methods Based on Continuous-Time Processes Discussion IMPLEMENTING MCMC Introduction Determining the Number of Iterations Software and Implementation Output Analysis Generic Metropolis Algorithms Discussion INFERENCE AND MONITORING CONVERGENCE Difficulties in Inference from Markov Chain Simulation The Risk of Undiagnosed Slow Convergence Multiple Sequences and Overdispersed Starting Points Monitoring Convergence Using Simulation Output Output Analysis for Inference Output Analysis for Improving Efficiency MODEL DETERMINATION USING SAMPLING-BASED METHODS Introduction Classical Approaches The Bayesian Perspective and the Bayes Factor Alternative Predictive Distributions How to Use Predictive Distributions Computational Issues An Example Discussion HYPOTHESIS TESTING AND MODEL SELECTION Introduction Uses of Bayes Factors Marginal Likelihood Estimation by Importance Sampling Marginal Likelihood Estimation Using Maximum Likelihood Application: How Many Components in a Mixture? Discussion Appendix: S-PLUS Code for the Laplace-Metropolis Estimator MODEL CHECKING AND MODEL IMPROVEMENT Introduction Model Checking Using Posterior Predictive Simulation Model Improvement via Expansion Example: Hierarchical Mixture Modelling of Reaction Times STOCHASTIC SEARCH VARIABLE SELECTION Introduction A Hierarchical Bayesian Model for Variable Selection Searching the Posterior by Gibbs Sampling Extensions Constructing Stock Portfolios With SSVS Discussion BAYESIAN MODEL COMPARISON VIA JUMP DIFFUSIONS Introduction Model Choice Jump-Diffusion Sampling Mixture Deconvolution Object Recognition Variable Selection Change-Point Identification Conclusions ESTIMATION AND OPTIMIZATION OF FUNCTIONS Non-Bayesian Applications of MCMC Monte Carlo Optimization Monte Carlo Likelihood Analysis Normalizing-Constant Families Missing Data Decision Theory Which Sampling Distribution? Importance Sampling Discussion STOCHASTIC EM: METHOD AND APPLICATION Introduction The EM Algorithm The Stochastic EM Algorithm Examples GENERALIZED LINEAR MIXED MODELS Introduction Generalized Linear Models (GLMs) Bayesian Estimation of GLMs Gibbs Sampling for GLMs Generalized Linear Mixed Models (GLMMs) Specification of Random-Effect Distributions Hyperpriors and the Estimation of Hyperparameters Some Examples Discussion HIERARCHICAL LONGITUDINAL MODELLING Introduction Clinical Background Model Detail and MCMC Implementation Results Summary and Discussion MEDICAL MONITORING Introduction Modelling Medical Monitoring Computing Posterior Distributions Forecasting Model Criticism Illustrative Application Discussion MCMC FOR NONLINEAR HIERARCHICAL MODELS Introduction Implementing MCMC Comparison of Strategies A Case Study from Pharmacokinetics-Pharmacodynamics Extensions and Discussion BAYESIAN MAPPING OF DISEASE Introduction Hypotheses and Notation Maximum Likelihood Estimation of Relative Risks Hierarchical Bayesian Model of Relative Risks Empirical Bayes Estimation of Relative Risks Fully Bayesian Estimation of Relative Risks Discussion MCMC IN IMAGE ANALYSIS Introduction The Relevance of MCMC to Image Analysis Image Models at Different Levels Methodological Innovations in MCMC Stimulated by Imaging Discussion MEASUREMENT ERROR Introduction Conditional-Independence Modelling Illustrative examples Discussion GIBBS SAMPLING METHODS IN GENETICS Introduction Standard Methods in Genetics Gibbs Sampling Approaches MCMC Maximum Likelihood Application to a Family Study of Breast Cancer Conclusions MIXTURES OF DISTRIBUTIONS: INFERENCE AND ESTIMATION Introduction The Missing Data Structure Gibbs Sampling Implementation Convergence of the Algorithm Testing for Mixtures Infinite Mixtures and Other Extensions AN ARCHAEOLOGICAL EXAMPLE: RADIOCARBON DATING Introduction Background to Radiocarbon Dating Archaeological Problems and Questions Illustrative Examples Discussion Index"
            },
            "slug": "Markov-Chain-Monte-Carlo-in-Practice-Gilks-Richardson",
            "title": {
                "fragments": [],
                "text": "Markov Chain Monte Carlo in Practice"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "The Markov Chain Monte Carlo Implementation Results Summary and Discussion MEDICAL MONITORING Introduction Modelling Medical Monitoring Computing Posterior Distributions Forecasting Model Criticism Illustrative Application Discussion MCMC for NONLINEAR HIERARCHICAL MODELS."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144418438"
                        ],
                        "name": "Hinrich Sch\u00fctze",
                        "slug": "Hinrich-Sch\u00fctze",
                        "structuredName": {
                            "firstName": "Hinrich",
                            "lastName": "Sch\u00fctze",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hinrich Sch\u00fctze"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 91
                            }
                        ],
                        "text": "Perplexity is a standard measure of performance for statistical models of natural language (14) and is defined as exp{)log P(wtest\"\")#ntest}, where wtest and ntest indicate the identities and number of words in the test set, respectively."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 52800448,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "084c55d6432265785e3ff86a2e900a49d501c00a",
            "isKey": false,
            "numCitedBy": 7803,
            "numCiting": 294,
            "paperAbstract": {
                "fragments": [],
                "text": "Statistical approaches to processing natural language text have become dominant in recent years. This foundational text is the first comprehensive introduction to statistical natural language processing (NLP) to appear. The book contains all the theory and algorithms needed for building NLP tools. It provides broad but rigorous coverage of mathematical and linguistic foundations, as well as detailed discussion of statistical methods, allowing students and researchers to construct their own implementations. The book covers collocation finding, word sense disambiguation, probabilistic parsing, information retrieval, and other applications."
            },
            "slug": "Foundations-of-statistical-natural-language-Manning-Sch\u00fctze",
            "title": {
                "fragments": [],
                "text": "Foundations of statistical natural language processing"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "This foundational text is the first comprehensive introduction to statistical natural language processing (NLP) to appear and provides broad but rigorous coverage of mathematical and linguistic foundations, as well as detailed discussion of statistical methods, allowing students and researchers to construct their own implementations."
            },
            "venue": {
                "fragments": [],
                "text": "SGMD"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46341118"
                        ],
                        "name": "T. Kuhn",
                        "slug": "T.-Kuhn",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Kuhn",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kuhn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067794782"
                        ],
                        "name": "David Hawkins",
                        "slug": "David-Hawkins",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Hawkins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Hawkins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 125362885,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "0fc425a8004830fdd7f207efd4fa7a2331d56d3f",
            "isKey": false,
            "numCitedBy": 38534,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "A good book may have the power to change the way we see the world, but a great book actually becomes part of our daily consciousness, pervading our thinking to the point that we take it for granted, and we forget how provocative and challenging its ideas once were-and still are. \"The Structure of Scientific Revolutions\" is that kind of book. When it was first published in 1962, it was a landmark event in the history and philosophy of science. And fifty years later, it still has many lessons to teach. With \"The Structure of Scientific Revolutions\", Kuhn challenged long-standing linear notions of scientific progress, arguing that transformative ideas don't arise from the day-to-day, gradual process of experimentation and data accumulation, but that revolutions in science, those breakthrough moments that disrupt accepted thinking and offer unanticipated ideas, occur outside of \"normal science,\" as he called it. Though Kuhn was writing when physics ruled the sciences, his ideas on how scientific revolutions bring order to the anomalies that amass over time in research experiments are still instructive in our biotech age. This new edition of Kuhn's essential work in the history of science includes an insightful introductory essay by Ian Hacking that clarifies terms popularized by Kuhn, including paradigm and incommensurability, and applies Kuhn's ideas to the science of today. Usefully keyed to the separate sections of the book, Hacking's essay provides important background information as well as a contemporary context. Newly designed, with an expanded index, this edition will be eagerly welcomed by the next generation of readers seeking to understand the history of our perspectives on science."
            },
            "slug": "The-Structure-of-Scientific-Revolutions-Kuhn-Hawkins",
            "title": {
                "fragments": [],
                "text": "The Structure of Scientific Revolutions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1962
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49695512"
                        ],
                        "name": "C. Findlay",
                        "slug": "C.-Findlay",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Findlay",
                            "middleNames": [
                                "Scott"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Findlay"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 45100112,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "8bd8cf2c311c47eb86d9e2ec1be099326fd13b96",
            "isKey": false,
            "numCitedBy": 10,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "A generalized fundamental theorem of natural selection is derived for populations incorporating both genetic and cultural transmission. The phenotype is determined by an arbitrary number of multiallelic loci with two-factor epistasis and an arbitrary linkage map, as well as by cultural transmission from the parents. Generations are discrete but partially overlapping, and mating may be nonrandom at either the genotypic or the phenotypic level (or both). I show that cultural transmission has several important implications for the evolution of population fitness, most notably that there is a time lag in the response to selection such that the future evolution depends on the past selection history of the population."
            },
            "slug": "Fundamental-theorem-of-natural-selection-under-Findlay",
            "title": {
                "fragments": [],
                "text": "Fundamental theorem of natural selection under gene-culture transmission."
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown that cultural transmission has several important implications for the evolution of population fitness, most notably that there is a time lag in the response to selection such that the future evolution depends on the past selection history of the population."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences of the United States of America"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1727849"
                        ],
                        "name": "S. Hanson",
                        "slug": "S.-Hanson",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Hanson",
                            "middleNames": [
                                "Jose"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Hanson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60565534,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "69d7086300e7f5322c06f2f242a565b3a182efb5",
            "isKey": false,
            "numCitedBy": 4652,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Bill Baird { Publications References 1] B. Baird. Bifurcation analysis of oscillating neural network model of pattern recognition in the rabbit olfactory bulb. In D. 3] B. Baird. Bifurcation analysis of a network model of the rabbit olfactory bulb with periodic attractors stored by a sequence learning algorithm. 5] B. Baird. Bifurcation theory methods for programming static or periodic attractors and their bifurcations in dynamic neural networks."
            },
            "slug": "In-Advances-in-Neural-Information-Processing-Hanson",
            "title": {
                "fragments": [],
                "text": "In Advances in Neural Information Processing Systems"
            },
            "venue": {
                "fragments": [],
                "text": "NIPS 1990"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699205"
                        ],
                        "name": "S. Furui",
                        "slug": "S.-Furui",
                        "structuredName": {
                            "firstName": "Sadaoki",
                            "lastName": "Furui",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Furui"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143604406"
                        ],
                        "name": "B. Juang",
                        "slug": "B.-Juang",
                        "structuredName": {
                            "firstName": "Biing-Hwang",
                            "lastName": "Juang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Juang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145320076"
                        ],
                        "name": "W. Chou",
                        "slug": "W.-Chou",
                        "structuredName": {
                            "firstName": "Wu",
                            "lastName": "Chou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Chou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66495315"
                        ],
                        "name": "Understanding",
                        "slug": "Understanding",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Understanding",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Understanding"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 57979724,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1a0f97b920912d55458b51652d3023e63b70a343",
            "isKey": false,
            "numCitedBy": 4,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This workshop focuses on the recent progress and new ground-breaking paradigms of automatic speech recognition and understanding, with robust modeling as the main theme."
            },
            "slug": "1997-IEEE-Workshop-on-Automatic-Speech-Recognition-Furui-Juang",
            "title": {
                "fragments": [],
                "text": "1997 IEEE Workshop on Automatic Speech Recognition and Understanding : proceedings"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This workshop focuses on the recent progress and new ground-breaking paradigms of automatic speech recognition and understanding, with robust modeling as the main theme."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1956403"
                        ],
                        "name": "P. Kitcher",
                        "slug": "P.-Kitcher",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Kitcher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Kitcher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69524199"
                        ],
                        "name": "W. Salmon",
                        "slug": "W.-Salmon",
                        "structuredName": {
                            "firstName": "Wesley",
                            "lastName": "Salmon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Salmon"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 141198642,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "4f50d25a78adf557b6aef822bdb97408ac97ad6e",
            "isKey": false,
            "numCitedBy": 103,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Minnesota-studies-in-the-philosophy-of-science-Kitcher-Salmon",
            "title": {
                "fragments": [],
                "text": "Minnesota studies in the philosophy of science"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2100881"
                        ],
                        "name": "R. Lickley",
                        "slug": "R.-Lickley",
                        "structuredName": {
                            "firstName": "Robin",
                            "lastName": "Lickley",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lickley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2695814"
                        ],
                        "name": "E. Bard",
                        "slug": "E.-Bard",
                        "structuredName": {
                            "firstName": "Ellen",
                            "lastName": "Bard",
                            "middleNames": [
                                "Gurman"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Bard"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 63869139,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "4dd0826066d1a97d57c1c13e8dc6b6de4a492719",
            "isKey": false,
            "numCitedBy": 220,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Proceedings-of-the-International-Conference-on-Lickley-Bard",
            "title": {
                "fragments": [],
                "text": "Proceedings of the International Conference on Spoken Language Processing."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Carlo Strategies in Scientific Computing"
            },
            "venue": {
                "fragments": [],
                "text": "Carlo Strategies in Scientific Computing"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "J. Am. Stat. Assoc"
            },
            "venue": {
                "fragments": [],
                "text": "J. Am. Stat. Assoc"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Machine Intelligence"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Machine Intelligence"
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Carlo Methods in Statistical Physics"
            },
            "venue": {
                "fragments": [],
                "text": "Carlo Methods in Statistical Physics"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "J. Machine Learn. Res"
            },
            "venue": {
                "fragments": [],
                "text": "J. Machine Learn. Res"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Proc. Natl. Acad. Sci. USA 88"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. Natl. Acad. Sci. USA 88"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Machine Learn"
            },
            "venue": {
                "fragments": [],
                "text": "J"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "J. R. Stat. Soc. B"
            },
            "venue": {
                "fragments": [],
                "text": "J. R. Stat. Soc. B"
            },
            "year": 1977
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 98
                            }
                        ],
                        "text": "Subsequent samples are taken after an appropriate lag to ensure that their autocorrelation is low (10, 11)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 84
                            }
                        ],
                        "text": "We use Gibbs sampling (13), known as the heat bath algorithm in statistical physics (10), where the next state is reached by sequentially sampling all variables from their distribution when conditioned on the current values of all other variables and the data."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Monte Carlo Methods in Statistical Physics (Oxford"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 3,
            "methodology": 2
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 22,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Finding-scientific-topics-Griffiths-Steyvers/e99f196cf21e0781ef1e119d14e6db45cd71bf3b?sort=total-citations"
}