{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700994"
                        ],
                        "name": "R. Battiti",
                        "slug": "R.-Battiti",
                        "structuredName": {
                            "firstName": "Roberto",
                            "lastName": "Battiti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Battiti"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 3
                            }
                        ],
                        "text": "In Battiti (1989) it is shown that it is possible to use a secant approximation with O ( N ) computing and storage time that uses second-order information."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 133
                            }
                        ],
                        "text": "An heuristic method for modifying the learning rate is, for example, described in Lapedes and Farber (19861, Vogl et al. (1988), and Battiti (1989) (the bold driver (BD) method)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 66
                            }
                        ],
                        "text": "A similar method based on quadratic interpolation is presented in Battiti (1989)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17193734,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1861ba1d857984384e93dc7ab5658751099182ee",
            "isKey": false,
            "numCitedBy": 202,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Two methods for incr easing performance of th e backpropagat ion learning algorithm are present ed and their result s are compared with those obtained by optimi zing par ameters in the standard method . The first method requires adaptation of a scalar learning rat e in order to decrease th e energy value along the gradient direction in a close-to-optimal way. Th e second is derived from the conjugate gradient method with inexact linear searches . The strict locality requirement is relaxed but parallelism of computation is maintained, allowing efficient use of concurrent computation. For medium-size probl ems, typical speedups of one order of magnitude are obtained."
            },
            "slug": "Accelerated-Backpropagation-Learning:-Two-Methods-Battiti",
            "title": {
                "fragments": [],
                "text": "Accelerated Backpropagation Learning: Two Optimization Methods"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "Two methods for easing performance of the backpropagat ion learning algorithm are presented and their result s are compared with those obtained by optimi zing par ameters in the standard method."
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40371087"
                        ],
                        "name": "E. Johansson",
                        "slug": "E.-Johansson",
                        "structuredName": {
                            "firstName": "Erik",
                            "lastName": "Johansson",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Johansson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50247236"
                        ],
                        "name": "F. Dowla",
                        "slug": "F.-Dowla",
                        "structuredName": {
                            "firstName": "Farid",
                            "lastName": "Dowla",
                            "middleNames": [
                                "U."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Dowla"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46821262"
                        ],
                        "name": "D. Goodman",
                        "slug": "D.-Goodman",
                        "structuredName": {
                            "firstName": "Dennis",
                            "lastName": "Goodman",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Goodman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 40190917,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "74a063a505ec68d7e6558abbf8ead5b783074c90",
            "isKey": false,
            "numCitedBy": 312,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "In many applications, the number of interconnects or weights in a neural network is so large that the learning time for the conventional backpropagation algorithm can become excessively long. Numerical optimization theory offers a rich and robust set of techniques which can be applied to neural networks to improve learning rates. In particular, the conjugate gradient method is easily adapted to the backpropagation learning problem. This paper describes the conjugate gradient method, its application to the backpropagation learning problem and presents results of numerical tests which compare conventional backpropagation, steepest descent and the conjugate gradient methods. For the parity problem, we find that the conjugate gradient method is an order of magnitude faster than conventional backpropagation with momentum."
            },
            "slug": "Backpropagation-Learning-for-Multilayer-Neural-the-Johansson-Dowla",
            "title": {
                "fragments": [],
                "text": "Backpropagation Learning for Multilayer Feed-Forward Neural Networks Using the Conjugate Gradient Method"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "For the parity problem, it is found that the conjugate gradient method is an order of magnitude faster than conventional backpropagation with momentum."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Neural Syst."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145468098"
                        ],
                        "name": "M. M\u00f8ller",
                        "slug": "M.-M\u00f8ller",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "M\u00f8ller",
                            "middleNames": [
                                "Fodslette"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. M\u00f8ller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8029054,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2f4a097b2131784d7ac3fc3c47d1e9283e9ac207",
            "isKey": false,
            "numCitedBy": 3758,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-scaled-conjugate-gradient-algorithm-for-fast-M\u00f8ller",
            "title": {
                "fragments": [],
                "text": "A scaled conjugate gradient algorithm for fast supervised learning"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758714"
                        ],
                        "name": "S. Fahlman",
                        "slug": "S.-Fahlman",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Fahlman",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Fahlman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 83
                            }
                        ],
                        "text": "An heuristic technique (quickprop) \u201dloosely based\u2019\u2019 on Newton\u2018s method is presented in Fahlman (1988)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 110
                            }
                        ],
                        "text": "Tuning operations are, for example, the choice of appropriate parameters like the learning and momentum rate (Fahlman 1988), \"annealing\" schedules for the learning rate (that is progressively reduced) (Malferrari et al. 19901, updating schemes based on summing the contributions of related patterns\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 284549,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "722a3c365a134a9f9b9ae1511f018d9b1ecff3de",
            "isKey": false,
            "numCitedBy": 1016,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Most connectionist or \"neural network\" learning systems use some form of the back-propagation algorithm. However, back-propagation learning is too slow for many applications, and it scales up poorly as tasks become larger and more complex. The factors governing learning speed are poorly understood. I have begun a systematic, empirical study of learning speed in backprop-like algorithms, measured against a variety of benchmark problems. The goal is twofold: to develop faster learning algorithms and to contribute to the development of a methodology that will be of value in future studies of this kind. This paper is a progress report describing the results obtained during the first six months of this study. To date I have looked only at a limited set of benchmark problems, but the results on these are encouraging: I have developed a new learning algorithm that is faster than standard backprop by an order of magnitude or more and that appears to scale up very well as the problem size increases. This research was sponsored in part by the National Science Foundation under Contract Number EET-8716324 and by the Defense Advanced Research Projects Agency (DOD), ARPA Order No. 4976 under Contract F33615-87C-1499 and monitored by the Avionics Laboratory, Air Force Wright Aeronautical Laboratories, Aeronautical Systems Division (AFSC), Wright-Patterson AFB, OH 45433-6543. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of these agencies or of the U.S. Government."
            },
            "slug": "An-empirical-study-of-learning-speed-in-networks-Fahlman",
            "title": {
                "fragments": [],
                "text": "An empirical study of learning speed in back-propagation networks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A new learning algorithm is developed that is faster than standard backprop by an order of magnitude or more and that appears to scale up very well as the problem size increases."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700994"
                        ],
                        "name": "R. Battiti",
                        "slug": "R.-Battiti",
                        "structuredName": {
                            "firstName": "Roberto",
                            "lastName": "Battiti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Battiti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144493859"
                        ],
                        "name": "F. Masulli",
                        "slug": "F.-Masulli",
                        "structuredName": {
                            "firstName": "Francesco",
                            "lastName": "Masulli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Masulli"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 297,
                                "start": 273
                            }
                        ],
                        "text": "\u2026method does not require the choice of critical parameters, is guaranteed to converge to a point with zero gradient, and has been shown to accelerate the learning phase by many orders of magnitude with respect to batch BP if high precision in the output values is desired (Battiti and Masulli 1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 324,
                                "start": 298
                            }
                        ],
                        "text": "This OSS (one-step secant) method does not require the choice of critical parameters, is guaranteed to converge to a point with zero gradient, and has been shown to accelerate the learning phase by many orders of magnitude with respect to batch BP if high precision in the output values is desired (Battiti and Masulli 1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60014158,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "90c4cd3e7f74cb937ebe592ca38c21886e7ed44e",
            "isKey": false,
            "numCitedBy": 130,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Standard back-propagation learning (BP) is known to have slow convergence properties. Furthermore no general prescription is given for selecting the appropriate learning rate, so success is dependent on a trial and error process. In this work a well known optimization technique (the Broyden-Fletcher-Goldfarb-Shanno memoryless quasi-Newton method) is employed to speed up convergence and to select parameters. The strict locality requirement is relaxed but parallelism of computation is maintained, allowing efficient use of concurrent computation. While requiring only limited changes to BP, this method yields a speed-up factor of 100 \u2013 500 for the medium-size networks considered."
            },
            "slug": "BFGS-Optimization-for-Faster-and-Automated-Learning-Battiti-Masulli",
            "title": {
                "fragments": [],
                "text": "BFGS Optimization for Faster and Automated Supervised Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The Broyden-Fletcher-Goldfarb-Shanno memoryless quasi-Newton method is employed to speed up convergence and to select parameters, allowing efficient use of concurrent computation."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145556695"
                        ],
                        "name": "I. Kanter",
                        "slug": "I.-Kanter",
                        "structuredName": {
                            "firstName": "Ido",
                            "lastName": "Kanter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Kanter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759839"
                        ],
                        "name": "S. Solla",
                        "slug": "S.-Solla",
                        "structuredName": {
                            "firstName": "Sara",
                            "lastName": "Solla",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Solla"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The convergence of methods based on gradient descent (or approximations thereof) depends critically on the relative size of the maximum and minimum eigenvalues of the Hessian matrix [see  LeCun et al. (1991)  and equation 2.6 for the case of steepest descent].,The effects of the autocorrelation matrix of the inputs on the learning process (for a single linear unit) are discussed in  LeCun et al. (1991) ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18303822,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "0c43153a3627c7d98cc09f909c232f3899597204",
            "isKey": true,
            "numCitedBy": 92,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "The learning time of a simple neural network model is obtained through an analytic computation of the eigenvalue spectrum for the Hessian matrix, which describes the second order properties of the cost function in the space of coupling coefficients. The form of the eigenvalue distribution suggests new techniques for accelerating the learning process, and provides a theoretical justification for the choice of centered versus biased state variables."
            },
            "slug": "Second-Order-Properties-of-Error-Surfaces:-Learning-LeCun-Kanter",
            "title": {
                "fragments": [],
                "text": "Second Order Properties of Error Surfaces: Learning Time and Generalization"
            },
            "venue": {
                "fragments": [],
                "text": "NIPS 1990"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40153708"
                        ],
                        "name": "L. G. Allred",
                        "slug": "L.-G.-Allred",
                        "structuredName": {
                            "firstName": "Lloyd",
                            "lastName": "Allred",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. G. Allred"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "71147545"
                        ],
                        "name": "G. E. Kelly",
                        "slug": "G.-E.-Kelly",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Kelly",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. E. Kelly"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 31701866,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "629ab2cc9a3c8b683dd50a9a70c3750a3d723958",
            "isKey": false,
            "numCitedBy": 30,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "A discussion is presented of three techniques which offer significant improvement in training time. In the first, training is restricted to those samples for which the network fails to predict correctly. The training process is extended to the entire training data set as the performance of the network improves. In the second technique, an acceleration process is used for neurons which produce the same output class for the inputs provided by the training sample. In the third technique, the learning rate is optimized, on the fly, to get the optimal improvement for each training pass. A derivation is presented for an optimal matching of momentum and learning rate"
            },
            "slug": "Supervised-learning-techniques-for-backpropagation-Allred-Kelly",
            "title": {
                "fragments": [],
                "text": "Supervised learning techniques for backpropagation networks"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "A discussion is presented of three techniques which offer significant improvement in training time by using an acceleration process for neurons which produce the same output class for the inputs provided by the training sample."
            },
            "venue": {
                "fragments": [],
                "text": "1990 IJCNN International Joint Conference on Neural Networks"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This OSS (one-step secant) method does not require the choice of critical parameters, is guaranteed to converge to a point with zero gradient, and has been shown to accelerate the learning phase by many orders of magnitude with respect to batch BP if high precision in the output values is desired ( Battiti and Masulli 1990 )."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6635519,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4abd4e51705e74f1739bd3a1e47ac10e45f6468b",
            "isKey": false,
            "numCitedBy": 1171,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning an input-output mapping from a set of examples, of the type that many neural networks have been constructed to perform, can be regarded as synthesizing an approximation of a multidimensional function (that is, solving the problem of hypersurface reconstruction). From this point of view, this form of learning is closely related to classical approximation techniques, such as generalized splines and regularization theory. A theory is reported that shows the equivalence between regularization and a class of three-layer networks called regularization networks or hyper basis functions. These networks are not only equivalent to generalized splines but are also closely related to the classical radial basis functions used for interpolation tasks and to several pattern recognition and neural network algorithms. They also have an interesting interpretation in terms of prototypes that are synthesized and optimally combined during the learning stage."
            },
            "slug": "Regularization-Algorithms-for-Learning-That-Are-to-Poggio-Girosi",
            "title": {
                "fragments": [],
                "text": "Regularization Algorithms for Learning That Are Equivalent to Multilayer Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A theory is reported that shows the equivalence between regularization and a class of three-layer networks called regularization networks or hyper basis functions."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1822055"
                        ],
                        "name": "Raymond L. Watrous",
                        "slug": "Raymond-L.-Watrous",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Watrous",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Raymond L. Watrous"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 89
                            }
                        ],
                        "text": "Secant methods for learning in the multilayer perceptron have been used, for example, in Watrous (1987)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15329984,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "934e49dac717a924bfda841bf6e54c32e900f0d1",
            "isKey": false,
            "numCitedBy": 294,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of learning using connectionist networks, in which network connection strengths are modified systematically so that the response of the network increasingly approximates the desired response can be structured as an optimization problem. The widely used back propagation method of connectionist learning [19, 21, 18] is set in the context of nonlinear optimization. In this framework, the issues of stability, convergence and parallelism are considered. As a form of gradient descent with fixed step size, back propagation is known to be unstable, which is illustrated using Rosenbrock's function. This is contrasted with stable methods which involve a line search in the gradient direction. The convergence criterion for connectionist problems involving binary functions is discussed relative to the behavior of gradient descent in the vicinity of local minima. A minimax criterion is compared with the least squares criterion. The contribution of the momentum term [19, 18] to more rapid convergence is interpreted relative to the geometry of the weight space. It is shown that in plateau regions of relatively constant gradient, the momentum term acts to increase the step size by a factor of 1/1-\u03bc, where \u03bc is the momentum term. In valley regions with steep sides, the momentum constant acts to focus the search direction toward the local minimum by averaging oscillations in the gradient. Comments University of Pennsylvania Department of Computer and Information Science Technical Report No. MSCIS-88-62. This technical report is available at ScholarlyCommons: http://repository.upenn.edu/cis_reports/597 LEARNING ALGORITHMS FOR CONNECTIONIST NETWORKS: APPLIED GRADIENT METHODS OF NONLINEAR OPTIMIZATION"
            },
            "slug": "Learning-Algorithms-for-Connectionist-Networks:-of-Watrous",
            "title": {
                "fragments": [],
                "text": "Learning Algorithms for Connectionist Networks: Applied Gradient Methods of Nonlinear Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is shown that in plateau regions of relatively constant gradient, the momentum term acts to increase the step size by a factor of 1/1-\u03bc, where \u03bc is the momentumTerm, and in valley regions with steep sides,The momentum constant acts to focus the search direction toward the local minimum by averaging oscillations in the gradient."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "118180516"
                        ],
                        "name": "F. M. Silva",
                        "slug": "F.-M.-Silva",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Silva",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. M. Silva"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068289963"
                        ],
                        "name": "L. B. Almeida",
                        "slug": "L.-B.-Almeida",
                        "structuredName": {
                            "firstName": "Lu\u00eds",
                            "lastName": "Almeida",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. B. Almeida"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This method has for example been used in  Kramer and Sangiovanni-Vicentelli (1988)  for the digit recognition pr~blem.~ Even if the training,In Kollias and Anastassiou (1989) the Levenberg-Marquardt technique is combined with the acceleration techniques described in Jacobs (1988) and  Silva and Almeida (1990) .,A similar acceleration technique has been presented in  Silva and Almeida (1990) .,Most of the presented techniques are suitable for a parallel implementation, with a speed-up that is approximately proportional to the number of processors employed [see, for example,  Kramer and Sangiovanni-Vicentelli (1988) ; Battiti et al. (1990); Battiti and Straforini (1991 11."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 144
                            }
                        ],
                        "text": "In Kollias and Anastassiou (1989) the Levenberg-Marquardt technique is combined with the acceleration techniques described in Jacobs (1988) and Silva and Almeida (1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 55
                            }
                        ],
                        "text": "A similar acceleration technique has been presented in Silva and Almeida (1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 30453139,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a546f8798597fd2acbad81a9da358f1d11a0ad75",
            "isKey": false,
            "numCitedBy": 180,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "Like other gradient descent techniques, backpropagation converges slowly, even for medium sized network problems. This fact results from the usually large dimension of the weight space and from the particular shape of the error surface in each iteration point. Oscillation between the sides of deep and narrow valleys, for example, is a well known case where gradient descent provides poor convergence rates."
            },
            "slug": "Acceleration-Techniques-for-the-Backpropagation-Silva-Almeida",
            "title": {
                "fragments": [],
                "text": "Acceleration Techniques for the Backpropagation Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "Backpropagation converges slowly, even for medium sized network problems, because of the usually large dimension of the weight space and from the particular shape of the error surface in each iteration point."
            },
            "venue": {
                "fragments": [],
                "text": "EURASIP Workshop"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2213123"
                        ],
                        "name": "D. Shanno",
                        "slug": "D.-Shanno",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Shanno",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Shanno"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 96
                            }
                        ],
                        "text": "Now, if exact line searches are performed, equation 6.2 produces mutually conjugate directions (Shanno 1978)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Now, if exact line searches are performed, equation 6.2 produces mutually conjugate directions ( Shanno 1978 )."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9256912,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "27250b833d10ec7174c841171c5fc5e792c10a63",
            "isKey": true,
            "numCitedBy": 433,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Conjugate gradient methods are iterative methods for finding the minimizer of a scalar function fx of a vector variable x which do not update an approximation to the inverse Hessian matrix. This paper examines the effects of inexact linear searches on the methods and shows how the traditional Fletcher-Reeves and Polak-Ribiere algorithm may be modified in a form discovered by Perry to a sequence which can be interpreted as a memorytess BFGS algorithm. This algorithm may then be scaled optimally in the sense of Oren and Spedicalo. This scaling can be combined with Beale restarts and Powell's restart criterion. Computational results will show that this new method substantially outperforms known conjugate gradient methods on a wide class of problems."
            },
            "slug": "Conjugate-Gradient-Methods-with-Inexact-Searches-Shanno",
            "title": {
                "fragments": [],
                "text": "Conjugate Gradient Methods with Inexact Searches"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The traditional Fletcher-Reeves and Polak-Ribiere algorithm may be modified in a form discovered by Perry to a sequence which can be interpreted as a memorytess BFGS algorithm and this algorithm may then be scaled optimally in the sense of Oren and Spedicalo."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Oper. Res."
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144215175"
                        ],
                        "name": "R. Jacobs",
                        "slug": "R.-Jacobs",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Jacobs",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Jacobs"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "exponentially weighted sum caused by the momentum rate a converges to 1/(1 - a) [see  Jacobs (1988)  for details].' Furthermore, summing the momentum term to the one proportional to the negative gradient may produce an ascent direction, so that the error increases after the weight update.,In Kollias and Anastassiou (1989) the Levenberg-Marquardt technique is combined with the acceleration techniques described in  Jacobs (1988)  and Silva and Almeida (1990).,Suggestions for adapting both the search direction and the step along this direction are presented in Chan and Fallside (1987) and  Jacobs (1988) ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 288,
                                "start": 275
                            }
                        ],
                        "text": "\u2026of this \u201cquick and dirty\u201d version is close and usually better than the one obtained by appropriately choosing a fixed learning rate in batch BP.\nSuggestions for adapting both the search direction and the step along this direction are presented in Chan and Fallside (1987) and Jacobs (1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 3
                            }
                        ],
                        "text": "In Jacobs (1988) each individual weight has its own learning rate, that is modified in order to avoid oscillations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 126
                            }
                        ],
                        "text": "In Kollias and Anastassiou (1989) the Levenberg-Marquardt technique is combined with the acceleration techniques described in Jacobs (1988) and Silva and Almeida (1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 151
                            }
                        ],
                        "text": "For example, if all partial derivatives are equal to 1, then the\nexponentially weighted sum caused by the momentum rate a converges to 1/(1 - a ) [see Jacobs (1988) for details]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9947500,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a9ef2995e8e1bd57a74343073219364811c2ace0",
            "isKey": true,
            "numCitedBy": 1988,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Increased-rates-of-convergence-through-learning-Jacobs",
            "title": {
                "fragments": [],
                "text": "Increased rates of convergence through learning rate adaptation"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40380553"
                        ],
                        "name": "G. Drago",
                        "slug": "G.-Drago",
                        "structuredName": {
                            "firstName": "Gian",
                            "lastName": "Drago",
                            "middleNames": [
                                "Paolo"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Drago"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751728"
                        ],
                        "name": "S. Ridella",
                        "slug": "S.-Ridella",
                        "structuredName": {
                            "firstName": "Sandro",
                            "lastName": "Ridella",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ridella"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 146
                            }
                        ],
                        "text": "Among the researchers using conjugate gradient methods for the MLP are Barnard and Cole (1988), Johansson et al. (19901, Bengio and Moore (19891, Drago and Ridella (1991), Hinton's group in Toronto, the groups at CMU, Bell Labs, etc."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 116616879,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2d02a3f9bb407438452b8c76609d0f6ea6bda4df",
            "isKey": false,
            "numCitedBy": 7,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "An-optimum-weights-initialization-for-improving-in-Drago-Ridella",
            "title": {
                "fragments": [],
                "text": "An optimum weights initialization for improving scaling relationships in BP learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2149702798"
                        ],
                        "name": "H. White",
                        "slug": "H.-White",
                        "structuredName": {
                            "firstName": "Halbert",
                            "lastName": "White",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. White"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In the review by  White (1989)  the use of appropriate modifications of Newton\u2019s methods for learning is considered starting from a statistical perspective."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 17
                            }
                        ],
                        "text": "In the review by White (1989) the use of appropriate modifications of Newton\u2019s methods for learning is considered starting from a statistical perspective."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 43711678,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "656a33c1db546da8490d6eba259e2a849d73a001",
            "isKey": false,
            "numCitedBy": 1012,
            "numCiting": 83,
            "paperAbstract": {
                "fragments": [],
                "text": "The premise of this article is that learning procedures used to train artificial neural networks are inherently statistical techniques. It follows that statistical theory can provide considerable insight into the properties, advantages, and disadvantages of different network learning methods. We review concepts and analytical results from the literatures of mathematical statistics, econometrics, systems identification, and optimization theory relevant to the analysis of learning in artificial neural networks. Because of the considerable variety of available learning procedures and necessary limitations of space, we cannot provide a comprehensive treatment. Our focus is primarily on learning procedures for feedforward networks. However, many of the concepts and issues arising in this framework are also quite broadly relevant to other network learning paradigms. In addition to providing useful insights, the material reviewed here suggests some potentially useful new training methods for artificial neural networks."
            },
            "slug": "Learning-in-Artificial-Neural-Networks:-A-White",
            "title": {
                "fragments": [],
                "text": "Learning in Artificial Neural Networks: A Statistical Perspective"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Concepts and analytical results from the literatures of mathematical statistics, econometrics, systems identification, and optimization theory relevant to the analysis of learning in artificial neural networks are reviewed."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102264919"
                        ],
                        "name": "L. Toint",
                        "slug": "L.-Toint",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Toint",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Toint"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 191,
                                "start": 179
                            }
                        ],
                        "text": "Other versions are MINPACK and NL2SOL (standard nonlinear least-squares packages).25 Additional techniques that are usable for large-scale least-squares problems are presented in Toint (1987)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 121969978,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "4fcfa6f9ecd6f10f004cc4f5d6a6f3455a301207",
            "isKey": false,
            "numCitedBy": 53,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The nonlinear model fitting problem is analyzed in this paper, with special emphasis on the practical solution techniques when the number of parameters in the model is large. Classical approaches to small dimensional least squares calculations are reviewed and an extension of them to problems involving many variables is proposed. This extension uses the context of partially separable structures, which has already proved its applicability for large scale optimization. An adaptable algorithm is discussed, which chooses between various possible models of the objective function. Preliminary numerical experience is also presented, which shows that actual numerical solution of a large class of fitting problems involving several hundreds of nonlinear parameters is possible at a reasonable cost."
            },
            "slug": "On-Large-Scale-Nonlinear-Least-Squares-Calculations-Toint",
            "title": {
                "fragments": [],
                "text": "On Large Scale Nonlinear Least Squares Calculations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707243"
                        ],
                        "name": "S. Kollias",
                        "slug": "S.-Kollias",
                        "structuredName": {
                            "firstName": "Stefanos",
                            "lastName": "Kollias",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kollias"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1801788"
                        ],
                        "name": "D. Anastassiou",
                        "slug": "D.-Anastassiou",
                        "structuredName": {
                            "firstName": "Dimitris",
                            "lastName": "Anastassiou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Anastassiou"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 3
                            }
                        ],
                        "text": "In Kollias and Anastassiou (1989) the Levenberg-Marquardt technique is combined with the acceleration techniques described in Jacobs (1988) and Silva and Almeida (1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 220,
                                "start": 190
                            }
                        ],
                        "text": "They are based on adaptive modeling of the objective function and have been used for problems with up to thousands of variables.2b Additional references are Gawthrop and Sbarbaro (1990) and Kollias and Anastassiou (1989)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60480746,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a52e4fb46b0f995103ffd369253e69023433d0e6",
            "isKey": false,
            "numCitedBy": 204,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "A novel learning algorithm is developed for the training of multilayer feedforward neural networks, based on a modification of the Marquardt-Levenberg least-squares optimization method. The algorithm updates the input weights of each neuron in the network in an effective parallel way. An adaptive distributed selection of the convergence rate parameter is presented, using suitable optimization strategies. The algorithm has better convergence properties than the conventional backpropagation learning technique. Its performance is illustrated, using examples from digital image halftoning and logical operations such as the XOR function. >"
            },
            "slug": "An-adaptive-least-squares-algorithm-for-the-of-Kollias-Anastassiou",
            "title": {
                "fragments": [],
                "text": "An adaptive least squares algorithm for the efficient training of artificial neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "A novel learning algorithm is developed for the training of multilayer feedforward neural networks, based on a modification of the Marquardt-Levenberg least-squares optimization method, which has better convergence properties than the conventional backpropagation learning technique."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145768864"
                        ],
                        "name": "D. Broomhead",
                        "slug": "D.-Broomhead",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Broomhead",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Broomhead"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144159852"
                        ],
                        "name": "D. Lowe",
                        "slug": "D.-Lowe",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Lowe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lowe"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 117200472,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d5558a34dfd1dbb572895664d38fca04029a99cb",
            "isKey": false,
            "numCitedBy": 2933,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : The relationship between 'learning' in adaptive layered networks and the fitting of data with high dimensional surfaces is discussed. This leads naturally to a picture of 'generalization in terms of interpolation between known data points and suggests a rational approach to the theory of such networks. A class of adaptive networks is identified which makes the interpolation scheme explicit. This class has the property that learning is equivalent to the solution of a set of linear equations. These networks thus represent nonlinear relationships while having a guaranteed learning rule. Great Britain."
            },
            "slug": "Radial-Basis-Functions,-Multi-Variable-Functional-Broomhead-Lowe",
            "title": {
                "fragments": [],
                "text": "Radial Basis Functions, Multi-Variable Functional Interpolation and Adaptive Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "The relationship between 'learning' in adaptive layered networks and the fitting of data with high dimensional surfaces is discussed, leading naturally to a picture of 'generalization in terms of interpolation between known data points and suggests a rational approach to the theory of such networks."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2784955"
                        ],
                        "name": "J. Nocedal",
                        "slug": "J.-Nocedal",
                        "structuredName": {
                            "firstName": "Jorge",
                            "lastName": "Nocedal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Nocedal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9033333,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d2aaf56fb183ad66d099ac6c9110c5c365ab27f3",
            "isKey": false,
            "numCitedBy": 2414,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We study how to use the BFGS quasi-Newton matrices to precondition minimization methods for problems where the storage is critical. We give an update formula which generates matrices using information from the last m iterations, where m is any number supplied by the user. The quasi-Newton matrix is updated at every iteration by dropping the oldest information and replacing it by the newest informa- tion. It is shown that the matrices generated have some desirable properties. The resulting algorithms are tested numerically and compared with several well- known methods. 1. Introduction. For the problem of minimizing an unconstrained function / of n variables, quasi-Newton methods are widely employed (4). They construct a se- quence of matrices which in some way approximate the hessian of /(or its inverse). These matrices are symmetric; therefore, it is necessary to have n(n + l)/2 storage locations for each one. For large dimensional problems it will not be possible to re- tain the matrices in the high speed storage of a computer, and one has to resort to other kinds of algorithms. For example, one could use the methods (Toint (15), Shanno (12)) which preserve the sparsity structure of the hessian, or conjugate gradient methods (CG) which only have to store 3 or 4 vectors. Recently, some CG algorithms have been developed which use a variable amount of storage and which do not require knowledge about the sparsity structure of the problem (2), (7), (8). A disadvantage of these methods is that after a certain number of iterations the quasi-Newton matrix is discarded, and the algorithm is restarted using an initial matrix (usually a diagonal matrix). We describe an algorithm which uses a limited amount of storage and where the quasi-Newton matrix is updated continuously. At every step the oldest information contained in the matrix is discarded and replaced by new one. In this way we hope to have a more up to date model of our function. We will concentrate on the BFGS method since it is considered to be the most efficient. We believe that similar algo- rithms cannot be developed for the other members of the Broyden 0-class (1). Let / be the function to be nnnimized, g its gradient and h its hessian. We define"
            },
            "slug": "Updating-Quasi-Newton-Matrices-With-Limited-Storage-Nocedal",
            "title": {
                "fragments": [],
                "text": "Updating Quasi-Newton Matrices With Limited Storage"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "An update formula which generates matrices using information from the last m iterations, where m is any number supplied by the user, and the BFGS method is considered to be the most efficient."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790356"
                        ],
                        "name": "T. Heskes",
                        "slug": "T.-Heskes",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Heskes",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Heskes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792269"
                        ],
                        "name": "H. Kappen",
                        "slug": "H.-Kappen",
                        "structuredName": {
                            "firstName": "Hilbert",
                            "lastName": "Kappen",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Kappen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 117006159,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2ff324bfbf696e115251016696283cdb887effe9",
            "isKey": false,
            "numCitedBy": 106,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-line-learning-processes-in-artificial-neural-Heskes-Kappen",
            "title": {
                "fragments": [],
                "text": "On-line learning processes in artificial neural networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144281180"
                        ],
                        "name": "L. Chan",
                        "slug": "L.-Chan",
                        "structuredName": {
                            "firstName": "Lai-Wan",
                            "lastName": "Chan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Chan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61495821,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6de7db339400930840bc8bb34530885b3c41a790",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "The adaptive training algorithm, the delta-bar-delta method, and the conjugate gradient method are suggested to improve the training speed of the back-propagation network. Comparisons of these methods are made their practical effectiveness in terms of training speed, storage requirement and the possibility of hardware implementation is analyzed and discussed. It is shown that the adaptive training method is fastest and the conjugate gradient slowest when they are applied in two examples. The robustness of both the adaptive training and the delta-bar-delta method against parameters is also considered.<<ETX>>"
            },
            "slug": "Efficacy-of-different-learning-algorithms-of-the-Chan",
            "title": {
                "fragments": [],
                "text": "Efficacy of different learning algorithms of the back-propagation network"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "The adaptive training algorithm, the delta-bar-delta method, and the conjugate gradient method are suggested to improve the training speed of the back-propagation network and it is shown that the adaptive training method is fastest and the conjunction gradient slowest when they are applied."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE TENCON'90: 1990 IEEE Region 10 Conference on Computer and Communication Systems. Conference Proceedings"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40221291"
                        ],
                        "name": "A. Kramer",
                        "slug": "A.-Kramer",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Kramer",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kramer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1388394865"
                        ],
                        "name": "A. Sangiovanni-Vincentelli",
                        "slug": "A.-Sangiovanni-Vincentelli",
                        "structuredName": {
                            "firstName": "Alberto",
                            "lastName": "Sangiovanni-Vincentelli",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sangiovanni-Vincentelli"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8301037,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b6733d4705a41de1a8f5a5eccd75479b448d5247",
            "isKey": false,
            "numCitedBy": 168,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Parallelizable optimization techniques are applied to the problem of learning in feedforward neural networks. In addition to having superior convergence properties, optimization techniques such as the Polak-Ribiere method are also significantly more efficient than the Backpropagation algorithm. These results are based on experiments performed on small boolean learning problems and the noisy real-valued learning problem of hand-written character recognition."
            },
            "slug": "Efficient-Parallel-Learning-Algorithms-for-Neural-Kramer-Sangiovanni-Vincentelli",
            "title": {
                "fragments": [],
                "text": "Efficient Parallel Learning Algorithms for Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "Parallelizable optimization techniques such as the Polak-Ribiere method are significantly more efficient than the Backpropagation algorithm and the noisy real-valued learning problem of hand-written character recognition."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144657347"
                        ],
                        "name": "Z. Luo",
                        "slug": "Z.-Luo",
                        "structuredName": {
                            "firstName": "Zhi-Quan",
                            "lastName": "Luo",
                            "middleNames": [
                                "Tom"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Luo"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 93
                            }
                        ],
                        "text": "The convergence properties of the LMS algorithm with adaptive learning rate are presented in Luo (1991), together with a clear comparison of the LMS algorithm with stochastic gradient descent and adaptive filtering algorithms."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 41564682,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6948b341214721b2ea6ea48d8a4de21f9518fbbd",
            "isKey": false,
            "numCitedBy": 146,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of training a linear feedforward neural network by using a gradient descent-like LMS learning algorithm. The objective is to find a weight matrix for the network, by repeatedly presenting to it a finite set of examples, so that the sum of the squares of the errors is minimized. Kohonen showed that with a small but fixed learning rate (or stepsize) some subsequences of the weight matrices generated by the algorithm will converge to certain matrices close to the optimal weight matrix. In this paper, we show that, by dynamically decreasing the learning rate during each training cycle, the sequence of matrices generated by the algorithm will converge to the optimal weight matrix. We also show that for any given \u220a > 0 the LMS algorithm, with decreasing learning rates, will generate an \u220a-optimal weight matrix (i.e., a matrix of distance at most \u220a away from the optimal matrix) after O(1/\u220a) training cycles. This is in contrast to (1/\u220alog 1/\u220a) training cycles needed to generate an \u220a-optimal weight matrix when the learning rate is kept fixed. We also give a general condition for the learning rates under which the LMS learning algorithm is guaranteed to converge to the optimal weight matrix."
            },
            "slug": "On-the-Convergence-of-the-LMS-Algorithm-with-Rate-Luo",
            "title": {
                "fragments": [],
                "text": "On the Convergence of the LMS Algorithm with Adaptive Learning Rate for Linear Feedforward Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is shown that, by dynamically decreasing the learning rate during each training cycle, the sequence of matrices generated by the LMS algorithm will converge to the optimal weight matrix."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691847"
                        ],
                        "name": "P. Gawthrop",
                        "slug": "P.-Gawthrop",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Gawthrop",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Gawthrop"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1390057008"
                        ],
                        "name": "D. Sbarbaro-Hofer",
                        "slug": "D.-Sbarbaro-Hofer",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Sbarbaro-Hofer",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Sbarbaro-Hofer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "set is redundant, on-line BP may be slow relative to second-order methods for badly conditioned problems. The convergence of methods based on gradient descent (or approximations thereof) depends critically on the relative size of the maximum and minimum eigenvalues of the Hessian matrix [see LeCun et al. (1991) and equation 2."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 157
                            }
                        ],
                        "text": "They are based on adaptive modeling of the objective function and have been used for problems with up to thousands of variables.2b Additional references are Gawthrop and Sbarbaro (1990) and Kollias and Anastassiou (1989)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "exponentially weighted sum caused by the momentum rate a converges to 1/(1 - a ) [see Jacobs (1988) for details]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6394640,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7c91d4ad51a52d118ac6378132cb107eb5f44f7c",
            "isKey": false,
            "numCitedBy": 7,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "A standard general algorithm, the stochastic approximation algorithm of Albert and Gardner [1] , is applied in a new context to compute the weights of a multilayer per ceptron network. This leads to a new algorithm, the gain backpropagation algorithm, which is related to, but significantly different from, the standard backpropagat ion algorith m [2]. Some simulation examples show the potential and limitations of the proposed approach and provide comparisons with the conventional backpropagation algorithm."
            },
            "slug": "Stochastic-Approximation-and-Multilayer-The-Gain-Gawthrop-Sbarbaro-Hofer",
            "title": {
                "fragments": [],
                "text": "Stochastic Approximation and Multilayer Perceptrons: The Gain Backpropagation Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A new algorithm, the gain backpropagation algorithm, which is related to, but significantly different from, the standard back Propagation Algorithm, is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2201276"
                        ],
                        "name": "S. Thiria",
                        "slug": "S.-Thiria",
                        "structuredName": {
                            "firstName": "Sylvie",
                            "lastName": "Thiria",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Thiria"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143651841"
                        ],
                        "name": "C. Mejia",
                        "slug": "C.-Mejia",
                        "structuredName": {
                            "firstName": "Carlos",
                            "lastName": "Mejia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Mejia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3320135"
                        ],
                        "name": "F. Badran",
                        "slug": "F.-Badran",
                        "structuredName": {
                            "firstName": "Fouad",
                            "lastName": "Badran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Badran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2085767"
                        ],
                        "name": "M. Cr\u00e9pon",
                        "slug": "M.-Cr\u00e9pon",
                        "structuredName": {
                            "firstName": "Michel",
                            "lastName": "Cr\u00e9pon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Cr\u00e9pon"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 128543753,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "5b5416a71a9bfdd7123d81e930dbba1fdbd9ff1e",
            "isKey": false,
            "numCitedBy": 98,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "The present paper shows that a wide class of complex transfer functions encountered in geophysics can be efficiently modeled using neural networks. Neural networks can approximate numerical and nonnumerical transfer functions. They provide an optimum basis of nonlinear functions allowing a uniform approximation of any continuous function. Neural networks can also realize classification tasks. It is shown that the classifier mode is related to Bayes discriminant functions, which give the minimum error risk classification. This mode is useful for extracting information from an unknown process. These properties are applied to the ERS1 simulated scatterometer data. Compared to other methods, neural network solutions are the most skillful."
            },
            "slug": "A-neural-network-approach-for-modeling-nonlinear-Thiria-Mejia",
            "title": {
                "fragments": [],
                "text": "A neural network approach for modeling nonlinear transfer functions: Application for wind retrieval from spaceborne scatterometer data"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "It is shown that the classifier mode is related to Bayes discriminant functions, which give the minimum error risk classification, and this mode is useful for extracting information from an unknown process."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055557361"
                        ],
                        "name": "C. Peterson",
                        "slug": "C.-Peterson",
                        "structuredName": {
                            "firstName": "Carsten",
                            "lastName": "Peterson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Peterson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2588243"
                        ],
                        "name": "Thorsteinn S. R\u00f6gnvaldsson",
                        "slug": "Thorsteinn-S.-R\u00f6gnvaldsson",
                        "structuredName": {
                            "firstName": "Thorsteinn",
                            "lastName": "R\u00f6gnvaldsson",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thorsteinn S. R\u00f6gnvaldsson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2309947"
                        ],
                        "name": "L. L\u00f6nnblad",
                        "slug": "L.-L\u00f6nnblad",
                        "structuredName": {
                            "firstName": "Leif",
                            "lastName": "L\u00f6nnblad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. L\u00f6nnblad"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 61099988,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cf3406ade38bdf5187866f125a3c2bd6ecf24eab",
            "isKey": false,
            "numCitedBy": 202,
            "numCiting": 99,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "JETNET-3.0\u2014A-versatile-artificial-neural-network-Peterson-R\u00f6gnvaldsson",
            "title": {
                "fragments": [],
                "text": "JETNET 3.0\u2014A versatile artificial neural network package"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1800149"
                        ],
                        "name": "J. Dennis",
                        "slug": "J.-Dennis",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Dennis",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Dennis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33225083"
                        ],
                        "name": "D. Gay",
                        "slug": "D.-Gay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Gay",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Gay"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2242562"
                        ],
                        "name": "R. Welsch",
                        "slug": "R.-Welsch",
                        "structuredName": {
                            "firstName": "Roy",
                            "lastName": "Welsch",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Welsch"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 144
                            }
                        ],
                        "text": "If the gradient is available (as is the case for feedforward nets), one may use: H , = [VE(w, + h e ) - VE(w,)]/h,, with suitable h, steps [see Dennis et al. (1981)l. Note that N + 1 grad& computations are needed, so that the method is not suggested for large networks!\nfollowing linear system:\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 207651652,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c9ac7ab30f9de30b1e01b64352554b4fc8b00e65",
            "isKey": false,
            "numCitedBy": 388,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Reference [ 1] explains the algorithm realized by NL2SOL in detail. The algorithm amounts to a variation on Newton's method in which part of the Hessian matrix is computed exactly and part is approximated by a secant (quasi-Newton) updating method. Once the iterates come sufficiently close to a local solution, they usually converge quite rapidly. To promote convergence from poor starting guesses, NL2SOL uses a model/trust-region technique along with an adaptive"
            },
            "slug": "Algorithm-573:-NL2SOL\u2014An-Adaptive-Nonlinear-[E4]-Dennis-Gay",
            "title": {
                "fragments": [],
                "text": "Algorithm 573: NL2SOL\u2014An Adaptive Nonlinear Least-Squares Algorithm [E4]"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "The algorithm amounts to a variation on Newton's method in which part of the Hessian matrix is computed exactly and part is approximated by a secant (quasi-Newton) updating method to promote convergence from poor starting guesses."
            },
            "venue": {
                "fragments": [],
                "text": "TOMS"
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1800149"
                        ],
                        "name": "J. Dennis",
                        "slug": "J.-Dennis",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Dennis",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Dennis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1795397"
                        ],
                        "name": "Bobby Schnabel",
                        "slug": "Bobby-Schnabel",
                        "structuredName": {
                            "firstName": "Bobby",
                            "lastName": "Schnabel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bobby Schnabel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 39
                            }
                        ],
                        "text": "I2\nThe line-search method suggested in Dennis and Schnabel (1983) is well suited for multilayer perceptrons (where the gradient can be obtained with limited effort during the computation of the error) and requires only a couple of error and gradient evaluations per iteration, in the average."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 217,
                                "start": 191
                            }
                        ],
                        "text": "\u2026LDLT = H, + K where\nA suitable choice of P is described in Gill et al. (1981).15 The availability of the modified Cholesky factorization is the starting point for the algorithm described in Dennis and Schnabel (1983) to find pc 2 0 such that V2E(w,) + pcI is positive definite and well conditioned."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 154
                            }
                        ],
                        "text": "A proper value for /I, can be efficiently found using the modified Cholesky factorization described in Gill et al. (1981) and the heuristics described in Dennis and Schnabel (1983)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 107
                            }
                        ],
                        "text": "D:, where D, is a diagonal scaling matrix, such that the new variables ti = D,w are in the same range [see Dennis and Schnabel(1983)l."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 65
                            }
                        ],
                        "text": "Software routines for the Cholesky decomposition can be found in Dennis and Schnabel (1983) or in Dongarra et al. (1979)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 189,
                                "start": 164
                            }
                        ],
                        "text": "The method is based on quadratic and cubic interpolations and designed to use in an efficient way the available information about the function to be minimized [see Dennis and Schnabel(1983) for details]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 168
                            }
                        ],
                        "text": "We omit the proof and the usable techniques for finding b, leaving the topics as a suggested reading for those in search of elegance and inspiration [see, for example, Dennis and Schnabel (1983)l."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 34
                            }
                        ],
                        "text": "Here we follow the terminology of Dennis and Schnabel (1983), where the term quasi-Newton refers to all algorithms \"derived\" from Newton's method."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 232,
                                "start": 206
                            }
                        ],
                        "text": "O In general, if the initial point wo is sufficiently close to the minimizer w,, and V2E(w,) is positive definite, the sequence generated by repeating Newton\u2019s algorithm converges q-quudruticully to w* [see Dennis and Schnabel (1983) for details]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 27578127,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "e1053197256c6c3c0631377ec23a3f7dc1cb4781",
            "isKey": true,
            "numCitedBy": 7616,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface 1. Introduction. Problems to be considered Characteristics of 'real-world' problems Finite-precision arithmetic and measurement of error Exercises 2. Nonlinear Problems in One Variable. What is not possible Newton's method for solving one equation in one unknown Convergence of sequences of real numbers Convergence of Newton's method Globally convergent methods for solving one equation in one uknown Methods when derivatives are unavailable Minimization of a function of one variable Exercises 3. Numerical Linear Algebra Background. Vector and matrix norms and orthogonality Solving systems of linear equations-matrix factorizations Errors in solving linear systems Updating matrix factorizations Eigenvalues and positive definiteness Linear least squares Exercises 4. Multivariable Calculus Background Derivatives and multivariable models Multivariable finite-difference derivatives Necessary and sufficient conditions for unconstrained minimization Exercises 5. Newton's Method for Nonlinear Equations and Unconstrained Minimization. Newton's method for systems of nonlinear equations Local convergence of Newton's method The Kantorovich and contractive mapping theorems Finite-difference derivative methods for systems of nonlinear equations Newton's method for unconstrained minimization Finite difference derivative methods for unconstrained minimization Exercises 6. Globally Convergent Modifications of Newton's Method. The quasi-Newton framework Descent directions Line searches The model-trust region approach Global methods for systems of nonlinear equations Exercises 7. Stopping, Scaling, and Testing. Scaling Stopping criteria Testing Exercises 8. Secant Methods for Systems of Nonlinear Equations. Broyden's method Local convergence analysis of Broyden's method Implementation of quasi-Newton algorithms using Broyden's update Other secant updates for nonlinear equations Exercises 9. Secant Methods for Unconstrained Minimization. The symmetric secant update of Powell Symmetric positive definite secant updates Local convergence of positive definite secant methods Implementation of quasi-Newton algorithms using the positive definite secant update Another convergence result for the positive definite secant method Other secant updates for unconstrained minimization Exercises 10. Nonlinear Least Squares. The nonlinear least-squares problem Gauss-Newton-type methods Full Newton-type methods Other considerations in solving nonlinear least-squares problems Exercises 11. Methods for Problems with Special Structure. The sparse finite-difference Newton method Sparse secant methods Deriving least-change secant updates Analyzing least-change secant methods Exercises Appendix A. A Modular System of Algorithms for Unconstrained Minimization and Nonlinear Equations (by Robert Schnabel) Appendix B. Test Problems (by Robert Schnabel) References Author Index Subject Index."
            },
            "slug": "Numerical-methods-for-unconstrained-optimization-Dennis-Schnabel",
            "title": {
                "fragments": [],
                "text": "Numerical methods for unconstrained optimization and nonlinear equations"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Newton's Method for Nonlinear Equations and Unconstrained Minimization and methods for solving nonlinear least-squares problems with Special Structure."
            },
            "venue": {
                "fragments": [],
                "text": "Prentice Hall series in computational mathematics"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144281180"
                        ],
                        "name": "L. Chan",
                        "slug": "L.-Chan",
                        "structuredName": {
                            "firstName": "Lai-Wan",
                            "lastName": "Chan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Chan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1998157"
                        ],
                        "name": "F. Fallside",
                        "slug": "F.-Fallside",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Fallside",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Fallside"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 102
                            }
                        ],
                        "text": "Suggestions for adapting both the search direction and the step along this direction are presented in Chan and Fallside (1987) and Jacobs (1988). In Chan and Fallside (1987) the learning and momentum rates are adapted to the structure of the error surface, by considering the angle O k between the last step and the gradient direction and by avoiding \u201ddomination\u201d of the weight update by the momentum term (in order to avoid ascent directions)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 102
                            }
                        ],
                        "text": "Suggestions for adapting both the search direction and the step along this direction are presented in Chan and Fallside (1987) and Jacobs (1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 270,
                                "start": 246
                            }
                        ],
                        "text": "\u2026of this \u201cquick and dirty\u201d version is close and usually better than the one obtained by appropriately choosing a fixed learning rate in batch BP.\nSuggestions for adapting both the search direction and the step along this direction are presented in Chan and Fallside (1987) and Jacobs (1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 3
                            }
                        ],
                        "text": "In Chan and Fallside (1987) the learning and momentum rates are adapted to the structure of the error surface, by considering the angle O k between the last step and the gradient direction and by avoiding \u201ddomination\u201d of the weight update by the momentum term (in order to avoid ascent directions)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 30067612,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "418d84da8a12aa01a6928140c5ba0d1e866ab979",
            "isKey": true,
            "numCitedBy": 155,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "An-adaptive-training-algorithm-for-back-propagation-Chan-Fallside",
            "title": {
                "fragments": [],
                "text": "An adaptive training algorithm for back propagation networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143771522"
                        ],
                        "name": "C. S. Leung",
                        "slug": "C.-S.-Leung",
                        "structuredName": {
                            "firstName": "Chung",
                            "lastName": "Leung",
                            "middleNames": [
                                "Siu"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. S. Leung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697083"
                        ],
                        "name": "R. Setiono",
                        "slug": "R.-Setiono",
                        "structuredName": {
                            "firstName": "Rudy",
                            "lastName": "Setiono",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Setiono"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61558018,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "db578894c40aa5756343012e44304651e0b41d62",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "An efficient implementation of a quasi-Newton algorithm for feedforward neural network training on a Cray Y-MP is presented. The most time-consuming step of a neural network training using the quasi-Newton algorithm is the computation of the error function and its gradient. We describe in this paper how this step can be implemented so that the neural network training may take full advantage of the Cray vectorization capabilities."
            },
            "slug": "Efficient-neural-network-training-algorithm-for-the-Leung-Setiono",
            "title": {
                "fragments": [],
                "text": "Efficient neural network training algorithm for the Cray Y-MP supercomputer"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "The most time-consuming step of a neural network training using the quasi-Newton algorithm is the computation of the error function and its gradient, which is described in this paper to take full advantage of the Cray vectorization capabilities."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 1993 International Conference on Neural Networks (IJCNN-93-Nagoya, Japan)"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2020074"
                        ],
                        "name": "A. Lapedes",
                        "slug": "A.-Lapedes",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Lapedes",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Lapedes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2542113"
                        ],
                        "name": "R. Farber",
                        "slug": "R.-Farber",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Farber",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Farber"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 119862081,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7d51742bdb98221b3f42208baaa3df91ba06a617",
            "isKey": false,
            "numCitedBy": 76,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-self-optimizing,-nonsymmetrical-neural-net-for-Lapedes-Farber",
            "title": {
                "fragments": [],
                "text": "A self-optimizing, nonsymmetrical neural net for content addressable memory and pattern recognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145075623"
                        ],
                        "name": "M. Ceccarelli",
                        "slug": "M.-Ceccarelli",
                        "structuredName": {
                            "firstName": "Michele",
                            "lastName": "Ceccarelli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Ceccarelli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686028"
                        ],
                        "name": "A. Petrosino",
                        "slug": "A.-Petrosino",
                        "structuredName": {
                            "firstName": "Alfredo",
                            "lastName": "Petrosino",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Petrosino"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33255420"
                        ],
                        "name": "R. Vaccaro",
                        "slug": "R.-Vaccaro",
                        "structuredName": {
                            "firstName": "Roberto",
                            "lastName": "Vaccaro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Vaccaro"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 8943010,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "29a00502c73fb33e9e12479833d82d5ee9510a2c",
            "isKey": false,
            "numCitedBy": 13,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper reports two techniques for parallelizing on a MIMD multicomputer a class of learning algorithms (competitive learning) for artificial neural networks widely used in pattern recognition and understanding. The first technique presented, following the divide et impera strategy, achieves O(n/p + logP) time for n neurons and P processors interconnected as a tree. A modification of the algorithm allows the application of a systolic technique with the processors interconnected as a ring; this technique has the advantage that the communication time does not depend on the number of processors. The two techniques are also compared on the basis of predicted and measured performance on a transputer-based MIMD machine. As the number of processors grows the advantage of the systolic approach increases. On the contrary, the divide et impera approach is more advantageous in the retrieving phase."
            },
            "slug": "Competitive-neural-networks-on-message-passing-Ceccarelli-Petrosino",
            "title": {
                "fragments": [],
                "text": "Competitive neural networks on message-passing parallel computers"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "Two techniques for parallelizing on a MIMD multicomputer a class of learning algorithms (competitive learning) for artificial neural networks widely used in pattern recognition and understanding are reported."
            },
            "venue": {
                "fragments": [],
                "text": "Concurr. Pract. Exp."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4745449"
                        ],
                        "name": "D. Goldfarb",
                        "slug": "D.-Goldfarb",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Goldfarb",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Goldfarb"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "It is possible to update directly the Cholesky factors ( Goldfarb 1976 ), with a total complexity of O(P) [see the implementation in Dennis and Schnabel (198311."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 56
                            }
                        ],
                        "text": "It is possible to update directly the Cholesky factors (Goldfarb 1976), with a total complexity of O ( P ) [see the implementation in Dennis and Schnabel (198311."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 122040520,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2cc38270668f9ac270a0850a4ef51d2da33d839c",
            "isKey": false,
            "numCitedBy": 56,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Several effilcient methods are given for updating the Cholesky factors of a symmetric positive definite matrix when it is modified by a rank-two correction which maintains symmetry and positive definiteness. These ideas are applied to variable metric (quasi-Newton) methods to produce numerically stable algorithms."
            },
            "slug": "Factorized-variable-metric-methods-for-optimization-Goldfarb",
            "title": {
                "fragments": [],
                "text": "Factorized variable metric methods for unconstrained optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Several effilcient methods are given for updating the Cholesky factors of a symmetric positive definite matrix when it is modified by a rank-two correction which maintains symmetry and positive definiteness."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1976
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3140752"
                        ],
                        "name": "Charles R. Rosenberg",
                        "slug": "Charles-R.-Rosenberg",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Rosenberg",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charles R. Rosenberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 150
                            }
                        ],
                        "text": "\u2026the learning rate (that is progressively reduced) (Malferrari et al. 19901, updating schemes based on summing the contributions of related patterns (Sejnowski and Rosenberg 1986), \"small batches,\" \"selective\" corrections only if the error is larger than a threshold (that may be progressively\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13921532,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "406033f22b6a671b94bcbdfaf63070b7ce6f3e48",
            "isKey": false,
            "numCitedBy": 761,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "Unrestricted English text can be converted to speech by applying phonological rules and handling exceptions with a look-up table. However, this approach is highly labor intensive since each entry and rule must be hand-crafted. NETtalk is an alternative approach that is based on an automated learning procedure for a parallel network of deterministic processing units. ~ f t e r ' training on a corpus of informal continuous speech, it achieves good performance and generalizes to novel words. The distributed internal representations of the phonological regularities discovered by the network are damage resistant."
            },
            "slug": "NETtalk:-a-parallel-network-that-learns-to-read-Sejnowski-Rosenberg",
            "title": {
                "fragments": [],
                "text": "NETtalk: a parallel network that learns to read aloud"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "NETtalk is an alternative approach that is based on an automated learning procedure for a parallel network of deterministic processing units that achieves good performance and generalizes to novel words."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2062748962"
                        ],
                        "name": "J. J. Mor\u00e9",
                        "slug": "J.-J.-Mor\u00e9",
                        "structuredName": {
                            "firstName": "Jorge",
                            "lastName": "Mor\u00e9",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. J. Mor\u00e9"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745867"
                        ],
                        "name": "B. S. Garbow",
                        "slug": "B.-S.-Garbow",
                        "structuredName": {
                            "firstName": "Burton",
                            "lastName": "Garbow",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. S. Garbow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3158856"
                        ],
                        "name": "K. E. Hillstrom",
                        "slug": "K.-E.-Hillstrom",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Hillstrom",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. E. Hillstrom"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60914441,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "147bedcd8950d1ac8b8005aa5569e9e8652e3a34",
            "isKey": false,
            "numCitedBy": 765,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "MINPACK-1 is a pack of FORTRAN subprograms for the numerical solution of nonlinear equations and nonlinear least-squares problems. This report provides an overview of the algorithms and software in the package, and includes the documentation and program listings."
            },
            "slug": "User-Guide-for-Minpack-1-Mor\u00e9-Garbow",
            "title": {
                "fragments": [],
                "text": "User Guide for Minpack-1"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "A pack of FORTRAN subprograms for the numerical solution of nonlinear equations and nonlinear least-squares problems and this report provides an overview of the algorithms and software in the package."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744288"
                        ],
                        "name": "P. Gill",
                        "slug": "P.-Gill",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Gill",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Gill"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143873253"
                        ],
                        "name": "W. Murray",
                        "slug": "W.-Murray",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Murray",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Murray"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685738"
                        ],
                        "name": "M. H. Wright",
                        "slug": "M.-H.-Wright",
                        "structuredName": {
                            "firstName": "Margaret",
                            "lastName": "Wright",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. H. Wright"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 20611582,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b8d9abd1c078573188b13d36c1b1efb7cb2fa865",
            "isKey": false,
            "numCitedBy": 7627,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Practical Optimization MethodsFree eBook: Practical Aspects of Structural Optimization [1701.01450] Practical optimization for hybrid quantum Practical Optimization | 4e70c9cf5faf796993a61adc9f46f2c5Acces PDF Practical OptimizationPractical Bayesian Optimization of Machine Learning Particle Swarm Optimization (PSO) An Overview Practical Issues Optimization Algorithms in Physics Practical Mathematical Optimization Universit T BremenA Practical Price Optimization Approach for Omnichannel A Gentle Introduction to Stochastic Optimization AlgorithmsApplied Sciences | Free Full-Text | Evolutionary 0387986316 Practical Optimization Methods: with A Lecture on Model Predictive ControlPractical Optimization : Algorithms and Engineering Wiley Series in Discrete Mathematics and Optimization Ser PRACTICAL OPTIMIZATION uCozEvolutionary practical optimization | DeepDyveA Practical Guide To Hyperparameter Optimization.Blood platelet production: a novel approach for practical [PDF] Practical Bilevel Optimization Download and Read Stability and Sample-based Approximations of Composite Practical portfolio optimization in Python (2/3) machine (PDF) Practical Financial Optimization. Decision making A Multiobjective Optimization Model for Prevention and Particle swarm optimization WikipediaPractical Methods Of Optimization|RPractical Portfolio Optimization London Business SchoolBao: Making Learned Query Optimization PracticalApache Spark Core Practical Optimization DatabricksPractical Methods of Optimization by R. FletcherChapter 11 Nonlinear Optimization Examples4.7 Applied Optimization Problems \u2013 Calculus Volume 1Practical bayesian optimization using Goptuna | by Masashi Practical Optimization Methods For 4th Generation Cellular Facility location problems \u2014 Mathematical Optimization Practical optimization (2004 edition) | Open Library[J726.Ebook] PDF Download Practical Optimization of Multi-objective Exploration for Practical Optimization Practical Optimization: a Gentle Introduction has moved!?Practical Rod Pumping Optimization on Apple Books(PDF) Practical Optimization with MATLAB The Free StudyPractical portfolio optimization in Python (3/3) code (PDF) Practical, Fast and Robust Point Cloud Registration Numerical Optimization Stanford UniversityPractical Optimization Methods with Mathematica ApplicationsPractical Optimization | 4e70c9cf5faf796993a61adc9f46f2c5Search Engine Optimization: Practical Marketing TechniquesLagout.orgMeter Placement in Active Distribution System using Manual: Practical guide to optimization for mobiles Unity"
            },
            "slug": "Practical-optimization-Gill-Murray",
            "title": {
                "fragments": [],
                "text": "Practical optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "This ebook Practical Optimization by Philip E. Gill is presented in pdf format and the full version of this ebook in DjVu, ePub, doc, txt, PDF forms is presented."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5725917"
                        ],
                        "name": "A. Goldstein",
                        "slug": "A.-Goldstein",
                        "structuredName": {
                            "firstName": "Allen",
                            "lastName": "Goldstein",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Goldstein"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 144
                            }
                        ],
                        "text": "A formulation of the above requirements that is frequently used in optimization is based on the work of Armijo and Goldstein [see, for example, Goldstein (1967)l. Requirement (I) and (11) become\n(4.3)\nwhere cy is a fixed constant E ( 0 , l ) and X > 0,\nV E ( w , + & p ) T p 2."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 118977333,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "96d135fa93be24774a432d47d6095aa7f36742cd",
            "isKey": true,
            "numCitedBy": 199,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Constructive-Real-Analysis-Goldstein",
            "title": {
                "fragments": [],
                "text": "Constructive Real Analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1967
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708869"
                        ],
                        "name": "J. Dongarra",
                        "slug": "J.-Dongarra",
                        "structuredName": {
                            "firstName": "Jack",
                            "lastName": "Dongarra",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Dongarra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145263592"
                        ],
                        "name": "C. Moler",
                        "slug": "C.-Moler",
                        "structuredName": {
                            "firstName": "Cleve",
                            "lastName": "Moler",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Moler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34641136"
                        ],
                        "name": "J. Bunch",
                        "slug": "J.-Bunch",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Bunch",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bunch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "80605580"
                        ],
                        "name": "G. W. Stewart",
                        "slug": "G.-W.-Stewart",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Stewart",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. W. Stewart"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 53828000,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "56bc7ffd0184e697c5a00cf25edb899381add983",
            "isKey": false,
            "numCitedBy": 981,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "General matrices Band matrices Positive definite matrices Positive definite band matrices Symmetric Indefinite Matrices Triangular matrices Tridiagonal matrices The Cholesky decomposition The QR decomposition Updating QR and Cholesky decompositions The singular value decomposition References Basic linear algebra subprograms Timing data Program listings BLA Listings."
            },
            "slug": "LINPACK-Users'-Guide-Dongarra-Moler",
            "title": {
                "fragments": [],
                "text": "LINPACK Users' Guide"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "General matrices Band matrices positive definite matrices Positive definite band matrices Symmetric Indefinite Matrices Triangular matrices Tridiagonal matrices The Cholesky decomposition The QR decomposition up to and including the singular value decomposition is studied."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145479119"
                        ],
                        "name": "J. Bingham",
                        "slug": "J.-Bingham",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Bingham",
                            "middleNames": [
                                "A.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bingham"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 19
                            }
                        ],
                        "text": "[see, for example, Bingham (1988)l."
                    },
                    "intents": []
                }
            ],
            "corpusId": 59752044,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "f0a652a436f76466e9a4ca72f3959a7aa87925b0",
            "isKey": false,
            "numCitedBy": 103,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Modem Marketing Base-Band Transmission Pass-Band Transmission and Modulation Methods Receiver Strategies and Components Carrier Recovery Timing Recovery Linear Adaptive Equalizers Nonlinear Equalizers Coding Used for Forward-Acting Error Correction Full-Duplex Operation The Ancillary Functions Needed to Make a Full Data Set."
            },
            "slug": "Theory-and-Practice-of-Modem-Design-Bingham",
            "title": {
                "fragments": [],
                "text": "Theory and Practice of Modem Design"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Modem Marketing Base-Band Transmission Pass-Band transmission and Modulation Methods Receiver Strategies and Components Carrier Recovery Timing Recovery Linear Adaptive Equalizers Nonlinear Equalizers Coding used for Forward-Acting Error Correction Full-Duplex Operation The Ancillary Functions Needed to Make a Full Data Set."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790459"
                        ],
                        "name": "E. Barnard",
                        "slug": "E.-Barnard",
                        "structuredName": {
                            "firstName": "Etienne",
                            "lastName": "Barnard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Barnard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8089863"
                        ],
                        "name": "R. Cole",
                        "slug": "R.-Cole",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Cole",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Cole"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 457,
                                "start": 71
                            }
                        ],
                        "text": "Among the researchers using conjugate gradient methods for the MLP are Barnard and Cole (1988), Johansson et al. (19901, Bengio and Moore (19891, Drago and Ridella (1991), Hinton's group in Toronto, the groups at CMU, Bell Labs, etc. A version in which the one-dimensional minimization is substituted by a scaling of the step that depends on success in error reduction and goodness of a one-dimensional quadratic approximation is presented in Moller (1990) (SCG)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 777,
                                "start": 71
                            }
                        ],
                        "text": "Among the researchers using conjugate gradient methods for the MLP are Barnard and Cole (1988), Johansson et al. (19901, Bengio and Moore (19891, Drago and Ridella (1991), Hinton's group in Toronto, the groups at CMU, Bell Labs, etc. A version in which the one-dimensional minimization is substituted by a scaling of the step that depends on success in error reduction and goodness of a one-dimensional quadratic approximation is presented in Moller (1990) (SCG). This O ( N ) scheme incorporates ideas from the \"model-trust region\" methods (see Section 4.3) and \"safety\" procedures that are absent in the CG schemes, yielding convergence results that are comparable with the OSS method described in Section 6. Some modifications of the method are presented in Williams (1991). It is worth stressing that expensive one-dimensional searches are also discouraged by current results in optimization (see Section 4."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 71
                            }
                        ],
                        "text": "Among the researchers using conjugate gradient methods for the MLP are Barnard and Cole (1988), Johansson et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 71
                            }
                        ],
                        "text": "Among the researchers using conjugate gradient methods for the MLP are Barnard and Cole (1988), Johansson et al. (19901, Bengio and Moore (19891, Drago and Ridella (1991), Hinton's group in Toronto, the groups at CMU, Bell Labs, etc."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 44132238,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a2201f92d418e5d989473e5357746ad2a039d566",
            "isKey": true,
            "numCitedBy": 60,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-neural-net-training-program-based-on-optimization-Barnard-Cole",
            "title": {
                "fragments": [],
                "text": "A neural-net training program based on conjugate-radient optimization"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1964979"
                        ],
                        "name": "C. G. Broyden",
                        "slug": "C.-G.-Broyden",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Broyden",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. G. Broyden"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1800149"
                        ],
                        "name": "J. Dennis",
                        "slug": "J.-Dennis",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Dennis",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Dennis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144241870"
                        ],
                        "name": "J. J. Mor\u00e9",
                        "slug": "J.-J.-Mor\u00e9",
                        "structuredName": {
                            "firstName": "Jorge",
                            "lastName": "Mor\u00e9",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. J. Mor\u00e9"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Now, one can prove that H+ is symmetric and positive definite if and only if H+ = f+.f: for some nonsingular matrix f+. Using this fact, one update of this kind can be derived, expressing H+ = f+ f: and using Broyden's method to derive a suitable ]+.I9 The resulting update is historically known as the Broyden, Fletcher, Coldfarb, and Shanno (BFGS) update (by  Broyden et al. 1973 ) and is given by,156 Roberto Battiti in  Broyden et al. (1973) l."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 143
                            }
                        ],
                        "text": "\u2026method to derive a suitable ]+.I9 The resulting update is historically known as the Broyden, Fletcher, Coldfarb, and Shanno (BFGS) update (by Broyden et al. 1973) and is given by\n(5.5)\nThe BFGS positive definite secant update has been the most successful update in a number of studies\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 80
                            }
                        ],
                        "text": "', the matrix is considered as a \"long\" vector. performed (see Section 4.1).\nin Broyden et al. (1973)l."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 122933015,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "7814fbef48b17ef24d71e948b857c9f8f5a23765",
            "isKey": true,
            "numCitedBy": 444,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-the-Local-and-Superlinear-Convergence-of-Methods-Broyden-Dennis",
            "title": {
                "fragments": [],
                "text": "On the Local and Superlinear Convergence of Quasi-Newton Methods"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144323963"
                        ],
                        "name": "S. Alexander",
                        "slug": "S.-Alexander",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Alexander",
                            "middleNames": [
                                "Thomas"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Alexander"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 187,
                                "start": 162
                            }
                        ],
                        "text": "The on-line procedure has to be used if the patterns are not available before learning starts [see, for example, the perceptron used for adaptive equalization in Widrow and Stearns (1985)1, and a continuous adaptation to a stream of input-output signals is desired."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 292,
                                "start": 267
                            }
                        ],
                        "text": "The result is that the convergence of stochastic LMS is guaranteed if E   1/(N G), where N is the number of parameters being optimized and A,, is the largest eigenvalue of the autocorrelation function of the input.5 A detailed study of adaptive filters is presented in Widrow and Stearns (1985)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The on-line procedure has to be used if the patterns are not available before learning starts [see, for example, the perceptron used for adaptive equalization in  Widrow and Stearns (1985) 1, and a continuous adaptation to a stream of input-output signals is desired.,The result is that the convergence of stochastic LMS is guaranteed if E  Widrow and Stearns (1985) ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12374910,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a7d78b005150b873a1b72423cdc045267e03daa7",
            "isKey": true,
            "numCitedBy": 3372,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Adaptive-Signal-Processing-Alexander",
            "title": {
                "fragments": [],
                "text": "Adaptive Signal Processing"
            },
            "venue": {
                "fragments": [],
                "text": "Texts and Monographs in Computer Science"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 12
                            }
                        ],
                        "text": "(1988), and Battiti (1989) (the bold driver (BD) method)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 248,
                                "start": 226
                            }
                        ],
                        "text": "Most of the presented techniques are suitable for a parallel implementation, with a speed-up that is approximately proportional to the number of processors employed [see, for example, Kramer and Sangiovanni-Vicentelli (1988); Battiti et al. (1990); Battiti and Straforini (1991 11."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 3
                            }
                        ],
                        "text": "In Battiti (1989) it is shown that it is possible to use a secant approximation with O ( N ) computing and storage time that uses second-order information."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 66
                            }
                        ],
                        "text": "A similar method based on quadratic interpolation is presented in Battiti (1989)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 133
                            }
                        ],
                        "text": "An heuristic method for modifying the learning rate is, for example, described in Lapedes and Farber (19861, Vogl et al. (1988), and Battiti (1989) (the bold driver (BD) method)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Accelerated back-propagation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145732246"
                        ],
                        "name": "E. Polak",
                        "slug": "E.-Polak",
                        "structuredName": {
                            "firstName": "Elijah",
                            "lastName": "Polak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Polak"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122931192,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "95c23bb25a03d296a7eedb7c8dffe1748bb614c6",
            "isKey": false,
            "numCitedBy": 2291,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Introduction-to-linear-and-nonlinear-programming-Polak",
            "title": {
                "fragments": [],
                "text": "Introduction to linear and nonlinear programming"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "While the above method is suitable for batch learning, a proposal for a learning method usable in the on-line procedure has been presented in  LeCun (1989)  and Becker and LeCun (1989)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 268,
                                "start": 256
                            }
                        ],
                        "text": "\u2026maintains the \"safety\" properties even when the search is executed in a small number of one-dimensional trials.=\nWhile the above method is suitable for batch learning, a proposal for a learning method usable in the on-line procedure has been presented in LeCun (1989) and Becker and LeCun (1989)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 24
                            }
                        ],
                        "text": "According to Becker and LeCun (1989) the method does not bring a tremendous speed-up, but converges reliably without requiring extensive parameter adjustments."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 59861896,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "01b6affe3ea4eae1978aec54e87087feb76d9215",
            "isKey": false,
            "numCitedBy": 863,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Generalization-and-network-design-strategies-LeCun",
            "title": {
                "fragments": [],
                "text": "Generalization and network design strategies"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50333420"
                        ],
                        "name": "S. Becker",
                        "slug": "S.-Becker",
                        "structuredName": {
                            "firstName": "Suzanna",
                            "lastName": "Becker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Becker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 296,
                                "start": 273
                            }
                        ],
                        "text": "\u2026maintains the \"safety\" properties even when the search is executed in a small number of one-dimensional trials.=\nWhile the above method is suitable for batch learning, a proposal for a learning method usable in the on-line procedure has been presented in LeCun (1989) and Becker and LeCun (1989)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 13
                            }
                        ],
                        "text": "According to Becker and LeCun (1989) the method does not bring a tremendous speed-up, but converges reliably without requiring extensive parameter adjustments."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "While the above method is suitable for batch learning, a proposal for a learning method usable in the on-line procedure has been presented in LeCun (1989) and  Becker and LeCun (1989) .,According to  Becker and LeCun (1989)  the method does not bring a tremendous speed-up, but converges reliably without requiring extensive parameter adjustments."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 59695337,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "589d377b23e2bdae7ad161b36a5d6613bcfccdde",
            "isKey": false,
            "numCitedBy": 410,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Improving-the-convergence-of-back-propagation-with-Becker-LeCun",
            "title": {
                "fragments": [],
                "text": "Improving the convergence of back-propagation learning with second-order methods"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145768864"
                        ],
                        "name": "D. Broomhead",
                        "slug": "D.-Broomhead",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Broomhead",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Broomhead"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144159852"
                        ],
                        "name": "D. Lowe",
                        "slug": "D.-Lowe",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Lowe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lowe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3686496,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "b08ba914037af6d88d16e2657a65cd9dc5cf5da1",
            "isKey": false,
            "numCitedBy": 2307,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Multivariable-Functional-Interpolation-and-Adaptive-Broomhead-Lowe",
            "title": {
                "fragments": [],
                "text": "Multivariable Functional Interpolation and Adaptive Networks"
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst."
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 58
                            }
                        ],
                        "text": "Second-order methods in continuous time are considered in Parker (1987)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Optimal algorithms for adaptive networks : Second - order back propagation , second - order direct propagation , and second - order Hebbian learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 15
                            }
                        ],
                        "text": "In Kollias and Anastassiou (1989) the Levenberg-Marquardt technique is combined with the acceleration techniques described in Jacobs (1988) and Silva and Almeida (1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 220,
                                "start": 202
                            }
                        ],
                        "text": "They are based on adaptive modeling of the objective function and have been used for problems with up to thousands of variables.2b Additional references are Gawthrop and Sbarbaro (1990) and Kollias and Anastassiou (1989)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An adaptive least squares algorithm for the efficient training of multilayered networks"
            },
            "venue": {
                "fragments": [],
                "text": "I E E E Trans . CAS"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 50
                            }
                        ],
                        "text": "Some modifications of the method are presented in Williams (1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Morquordt algorithm for choosing the step-size in backpropagation learning with conjugate gradients"
            },
            "venue": {
                "fragments": [],
                "text": "Preprint of the School of Cognitive and Computing Sciences, University"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Morquordt algorithm for choosing the step-size in backpropagation learning with conjugate gradients. Preprint of the School of Cognitive and Computing Sciences"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A neural network application in geology: Identification of genetic facies"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. Int. Conf. Artificial Neural Networks (ICANN-91),"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A comparison of nonlinear optimization strategies for adaptive feed-forward layered networks"
            },
            "venue": {
                "fragments": [],
                "text": "RSRE MEMO 4157, Royal Signals and Radar Establishment, Malvern, England. White,"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 247,
                                "start": 226
                            }
                        ],
                        "text": "Most of the presented techniques are suitable for a parallel implementation, with a speed-up that is approximately proportional to the number of processors employed [see, for example, Kramer and Sangiovanni-Vicentelli (1988); Battiti et al. (1990); Battiti and Straforini (1991 11."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An application-oriented development environment for neural net models on the multiprocessor Emma-2"
            },
            "venue": {
                "fragments": [],
                "text": "Proc . IFIP Workshop Silicon Architectures Neural Nets"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Using neural networks for signal analysis in oil well drilling"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Comp"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "LINPACK Users's Guide"
            },
            "venue": {
                "fragments": [],
                "text": "Siam, Philadelphia. Drago, G. P., and Ridella, S. 1991. An optimum weights initialization for improving scaling relationships in BP learning."
            },
            "year": 1979
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 202
                            }
                        ],
                        "text": "\u2026summing the contributions of related patterns (Sejnowski and Rosenberg 1986), \"small batches,\" \"selective\" corrections only if the error is larger than a threshold (that may be progressively reduced) (Vincent 1991; Allred and Kelly 19901, randomization of the sequence of pattern presentation, etc."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Facial feature location in coarse resolution images by multi - layered perceptrons"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "TABU Search methods in artificial intelligence and operations research"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Acceleration of learning"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. GNCB-CNR School, Trento, Italy. Bingham, J. A. C. 1988. The Theory and Practice of Modem Design. Wiley, New"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Regularization algorithms for learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 79
                            }
                        ],
                        "text": "19901, updating schemes based on summing the contributions of related patterns (Sejnowski and Rosenberg 1986), \"small batches,\" \"selective\" corrections only if the error is larger than a threshold (that may be progressively reduced) (Vincent 1991; Allred and Kelly 19901, randomization of the sequence of pattern presentation, etc."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 150
                            }
                        ],
                        "text": "\u2026the learning rate (that is progressively reduced) (Malferrari et al. 19901, updating schemes based on summing the contributions of related patterns (Sejnowski and Rosenberg 1986), \"small batches,\" \"selective\" corrections only if the error is larger than a threshold (that may be progressively\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "NETtalk: A parallel network"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 127
                            }
                        ],
                        "text": "The fact that many patterns possess redundant information [see, for example, the case of hand-written character recognition in LeCun (1986)l has been cited as an argument in favor of on-line BP, because many of the contributions to the gradient are similar, so that waiting for all contributions\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "HLM A multilayer learning network"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. 2986 Connectionist Models Summer School, Pittsburgh 169-177."
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Parallel supervised learning with the memoryless quasi-Newton method"
            },
            "venue": {
                "fragments": [],
                "text": "In"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Multivariable functional interpolation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 109
                            }
                        ],
                        "text": "An heuristic method for modifying the learning rate is, for example, described in Lapedes and Farber (19861, Vogl et al. (1988), and Battiti (1989) (the bold driver (BD) method)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "celerating the convergence of the backpropagation method"
            },
            "venue": {
                "fragments": [],
                "text": "Biol . Cybernet ."
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Optimal algorithms for adaptive networks : Second - order back propagation , second - order direct propagation , and second - order Hebbian learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 14,
            "methodology": 28
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 65,
        "totalPages": 7
    },
    "page_url": "https://www.semanticscholar.org/paper/First-and-Second-Order-Methods-for-Learning:-and-Battiti/bbf6f07e699587c8d52faf829a289f8cbc7f11a5?sort=total-citations"
}