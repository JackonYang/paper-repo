{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "150165387"
                        ],
                        "name": "J. Urgen Schmidhuber",
                        "slug": "J.-Urgen-Schmidhuber",
                        "structuredName": {
                            "firstName": "J",
                            "lastName": "Urgen Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Urgen Schmidhuber"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14510132,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c69201d091dd92699fd90a17b9e3407319726791",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper addresses the problem of learning t\u00f2divide and conquer' by meaningful hierarchical adaptive decomposition of temporal sequences. This problem is relevant for time-series analysis as well as for goal-directed learning, particularily if event sequences tend to have hierarchical temporal structure. The rst neural systems for recursively chunking sequences are described. These systems are based on a principle called th\u00e8principle of history compression'. This principle essentially says: As long as a predictor is able to predict future environmental inputs from previous ones, no additional knowledge can be obtained by observing these inputs in reality. Only unexpected inputs deserve attention. A focus is on a class of 2-network systems which try to collapse a self-organizing (possibly multi-level) hierarchy of temporal predictors into a single recurrent network. Only those input events that were not expected by the rst recurrent net are transferred to the second recurrent net. Therefore the second net receives a reduced discription of the input history. It tries to develop internal representations for`higher-level' temporal structure. These internal representations in turn serve to create additional training signals for the rst net, thus helping the rst net to create longer and longer`chunks' for the second net. Experiments show that chunking systems can be superior to the conventional training algorithms for recurrent nets. 1 OUTLINE Section 2 motivates the search for sequence-composing systems by describing major drawbacks of`conventional' learning algorithms for recurrent networks with time-varying inputs and outputs. Section 3 describes a simple observation which is essential for the rest of this paper: It describes th\u00e8principle of history compression'. This principle essentially says: As long as a predictor is able to predict future environmental inputs from previous ones, no additional knowledge can be obtained by observing these inputs in reality. Only unexpected inputs deserve attention. This principle is of particular interest if typical event sequences have hierarchical temporal structure. Basic schemes for constructing sequence chunking systems based on the principle of history compression are described. Section 4 then describes on-line and o-line versions of a particular 2-network chunking system which tries to collapse a self-organizing (possibly multi-level) predictor hierarchy into a single recurrent network (the automatizer). The idea is to feed everything that is unexpected into a `higher-level' recurrent net (the chunker). Since the expected things can be derived from the unexpected things by the automatizer, the chunker is fed with a reduced description of the input history. The chunker has a \u2026"
            },
            "slug": "Neural-Sequence-Chunkers-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Neural Sequence Chunkers"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Experiments show that chunking systems can be superior to the conventional training algorithms for recurrent nets, and a focus is on a class of 2-network systems which try to collapse a self-organizing hierarchy of temporal predictors into a single recurrent network."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16683347,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bc22e87a26d020215afe91c751e5bdaddd8e4922",
            "isKey": false,
            "numCitedBy": 265,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Previous algorithms for supervised sequence learning are based on dynamic recurrent networks. This paper describes an alternative class of gradient-based systems consisting of two feedforward nets that learn to deal with temporal sequences using fast weights: The first net learns to produce context-dependent weight changes for the second net whose weights may vary very quickly. The method offers the potential for STM storage efficiency: A single weight (instead of a full-fledged unit) may be sufficient for storing temporal information. Various learning methods are derived. Two experiments with unknown time delays illustrate the approach. One experiment shows how the system can be used for adaptive temporary variable binding."
            },
            "slug": "Learning-to-Control-Fast-Weight-Memories:-An-to-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Learning to Control Fast-Weight Memories: An Alternative to Dynamic Recurrent Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "This paper describes an alternative class of gradient-based systems consisting of two feedforward nets that learn to deal with temporal sequences using fast weights: the first net learns to produce context-dependent weight changes for the second net whose weights may vary very quickly."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "150165387"
                        ],
                        "name": "J. Urgen Schmidhuber",
                        "slug": "J.-Urgen-Schmidhuber",
                        "structuredName": {
                            "firstName": "J",
                            "lastName": "Urgen Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Urgen Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 28
                            }
                        ],
                        "text": "See (Hochreiter, 1991) and (Schmidhuber, 1991c) for details."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 94
                            }
                        ],
                        "text": "This method is a simpliication and an improvement of the recent chunking method described by (Schmidhuber, 1991a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 35
                            }
                        ],
                        "text": "The chunking systems described in (Schmidhuber, 1991a), (Schmidhuber, 1991c) and the current paper try to detect temporal regularities and learn to use them for identifying relevant points in time."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 148
                            }
                        ],
                        "text": "We take P i to be a conventional dynamic recurrent neural network (Robinson and Fallside, 1987)(Williams and Zipser, 1989)(Williams and Peng, 1990)(Schmidhuber, 1991d); however, it might be some other adaptive sequence processing device as well"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 5
                            }
                        ],
                        "text": "See (Schmidhuber, 1991c) for various modiications."
                    },
                    "intents": []
                }
            ],
            "corpusId": 5420632,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ad201d0dabb7f58730a60181eb51a36642ac6ea5",
            "isKey": true,
            "numCitedBy": 14,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we introduce design principles for unsupervised detection of regularities (like causal relationships) in temporal sequences. One basic idea is to train an adaptive predictor module to predict future events from past events, and to train an additional condence module to model the reliability of the predictor's predictions. We select system states at those points in time where there are changes in prediction reliability, and use them recur-sively as inputs for higher-level predictors. This can be benecial for\u00e0daptive sub-goal generation' as well as for`conventional' goal-directed (supervised and reinforcement) learning: Systems based on these design principles were successfully tested on tasks where conventional training algorithms for recurrent nets fail. Finally we describe the principles of the rst neural sequenc\u00e8chunker' which collapses a self-organizing multi-level predictor hierarchy into a single recurrent network. This paper is based on th\u00e8principle of reduced history description': As long as an adaptive sequence processing dynamic system is able to predict future environmental inputs from previous ones, no additional knowledge can be obtained by observing these inputs in reality. Only unpredicted inputs deserve attention ([7][4][9]). This paper demonstrates that it can be very ecient to focus on unexpected inputs and ignore expected ones. First we motivate this work by describing a major problem of`conventional' learning algorithms for time-varying inputs, namely, the problem of long time lags between relevant inputs. Then we introduce a principle for unsupervised detection of causal chains in streams of input events. Short representations of`presumed causal chains' recursively serve as inputs for`higher-level' detec"
            },
            "slug": "Adaptive-Decomposition-Of-Time-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Adaptive Decomposition Of Time"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "Design principles for unsupervised detection of regularities (like causal relationships) in temporal sequences and the principles of the rst neural sequenc\u00e8chunker, which collapses a self-organizing multi-level predictor hierarchy into a single recurrent network are introduced."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1895771"
                        ],
                        "name": "D. Zipser",
                        "slug": "D.-Zipser",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Zipser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Zipser"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Pi denotes the ith level network, which is trained to predict its own next inputfrom its previous inputs.' We take Pi to be a conventional dynamic recurrent neural network (Robinson and Fallside 1987;  Williams and Zipser 1989;  Williams and Peng 1990); however, it might be some other adaptive sequence processing device as we1L2"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 96
                            }
                        ],
                        "text": "We take P i to be a conventional dynamic recurrent neural network (Robinson and Fallside, 1987)(Williams and Zipser, 1989)(Williams and Peng, 1990)(Schmidhuber, 1991d); however, it might be some other adaptive sequence processing device as well"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60666828,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "424710825d726e10b016204ed2bc979e2a342d10",
            "isKey": false,
            "numCitedBy": 336,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract The real-time recurrent learning algorithm is a gradient-following learning algorithm for completely recurrent networks running in continually sampled time. Here we use a series of simulation experiments to investigate the power and properties of this algorithm. In the recurrent networks studied here, any unit can be connected to any other, and any unit can receive external input. These networks run continually in the sense that they sample their inputs on every update cycle, and any unit can have a training target on any cycle. The storage required and computation time on each step are independent of time and are completely determined by the size of the network, so no prior knowledge of the temporal structure of the task being learned is required. The algorithm is nonlocal in the sense that each unit must have knowledge of the complete recurrent weight matrix and error vector. The algorithm is computationally intensive in sequential computers, requiring a storage capacity of the order of the thi..."
            },
            "slug": "Experimental-Analysis-of-the-Real-time-Recurrent-Williams-Zipser",
            "title": {
                "fragments": [],
                "text": "Experimental Analysis of the Real-time Recurrent Learning Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "A series of simulation experiments are used to investigate the power and properties of the real-time recurrent learning algorithm, a gradient-following learning algorithm for completely recurrent networks running in continually sampled time."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11761172,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "89b9a181801f32bf62c4237c4265ba036a79f9dc",
            "isKey": false,
            "numCitedBy": 136,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "The real-time recurrent learning (RTRL) algorithm (Robinson and Fallside 1987; Williams and Zipser 1989) requires O(n4) computations per time step, where n is the number of noninput units. I describe a method suited for on-line learning that computes exactly the same gradient and requires fixed-size storage of the same order but has an average time complexity per time step of O(n3)."
            },
            "slug": "A-Fixed-Size-Storage-O(n3)-Time-Complexity-Learning-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "A Fixed Size Storage O(n3) Time Complexity Learning Algorithm for Fully Recurrent Continually Running Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A method suited for on-line learning that computes exactly the same gradient and requires fixed-size storage of the same order but has an average time complexity per time step of O(n3)."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47918185"
                        ],
                        "name": "Jing Peng",
                        "slug": "Jing-Peng",
                        "structuredName": {
                            "firstName": "Jing",
                            "lastName": "Peng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jing Peng"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 55
                            }
                        ],
                        "text": "In particular, computationally inexpensive variants of BPTT (Williams and Peng, 1990) are interesting: There are tasks with hierarchical temporal structure where only a few iterations of`back-propagation back into time' per time step are in principle suucient to bridge arbitrary time lags (see section 5)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 27
                            }
                        ],
                        "text": "Both the IID-Algorithm and BPTT are appropriate."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 123
                            }
                        ],
                        "text": "We take P i to be a conventional dynamic recurrent neural network (Robinson and Fallside, 1987)(Williams and Zipser, 1989)(Williams and Peng, 1990)(Schmidhuber, 1991d); however, it might be some other adaptive sequence processing device as well"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 112
                            }
                        ],
                        "text": "But, a chunking system was able to solve the 20-step task rather quickly, using an eecient approximation of the BPTT-method where error was propagated a maximum of 3 steps into the past (although there was a 20 step time lag!)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 61
                            }
                        ],
                        "text": "In particular, computationally inexpensive variants of BPTT (Williams and Peng, 1990) are interesting: There are tasks with hierarchical temporal structure where only a few iterations of`back-propagation back into time' per time step are in principle suucient to bridge arbitrary time lags (see\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12979634,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "26bc0449360d7016f684eafae5b5d2feded32041",
            "isKey": true,
            "numCitedBy": 634,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "A novel variant of the familiar backpropagation-through-time approach to training recurrent networks is described. This algorithm is intended to be used on arbitrary recurrent networks that run continually without ever being reset to an initial state, and it is specifically designed for computationally efficient computer implementation. This algorithm can be viewed as a cross between epochwise backpropagation through time, which is not appropriate for continually running networks, and the widely used on-line gradient approximation technique of truncated backpropagation through time."
            },
            "slug": "An-Efficient-Gradient-Based-Algorithm-for-On-Line-Williams-Peng",
            "title": {
                "fragments": [],
                "text": "An Efficient Gradient-Based Algorithm for On-Line Training of Recurrent Network Trajectories"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "A novel variant of the familiar backpropagation-through-time approach to training recurrent networks is described, intended to be used on arbitrary recurrent networks that run continually without ever being reset to an initial state."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 42023620,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ac91740ae76ed9dbd853bddd6d1d9dc43bc55179",
            "isKey": false,
            "numCitedBy": 136,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "I propose a novel general principle for unsupervised learning of distributed nonredundant internal representations of input patterns. The principle is based on two opposing forces. For each representational unit there is an adaptive predictor, which tries to predict the unit from the remaining units. In turn, each unit tries to react to the environment such that it minimizes its predictability. This encourages each unit to filter \"abstract concepts\" out of the environmental input such that these concepts are statistically independent of those on which the other units focus. I discuss various simple yet potentially powerful implementations of the principle that aim at finding binary factorial codes (Barlow et al. 1989), i.e., codes where the probability of the occurrence of a particular input is simply the product of the probabilities of the corresponding code symbols. Such codes are potentially relevant for (1) segmentation tasks, (2) speeding up supervised learning, and (3) novelty detection. Methods for finding factorial codes automatically implement Occam's razor for finding codes using a minimal number of units. Unlike previous methods the novel principle has a potential for removing not only linear but also nonlinear output redundancy. Illustrative experiments show that algorithms based on the principle of predictability minimization are practically feasible. The final part of this paper describes an entirely local algorithm that has a potential for learning unique representations of extended input sequences."
            },
            "slug": "Learning-Factorial-Codes-by-Predictability-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Learning Factorial Codes by Predictability Minimization"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "A novel general principle for unsupervised learning of distributed nonredundant internal representations of input patterns based on two opposing forces that has a potential for removing not only linear but also nonlinear output redundancy."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47583877"
                        ],
                        "name": "C. Myers",
                        "slug": "C.-Myers",
                        "structuredName": {
                            "firstName": "Catherine",
                            "lastName": "Myers",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Myers"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "5This distinction between attended and automized events can also be found in the systems of  Myers (1990)  and of Ring (1991)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 207107772,
            "fieldsOfStudy": [
                "Computer Science",
                "Psychology"
            ],
            "id": "ff4edc33d3eeae6b8b08a04f290dee7da322be57",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning with delayed reinforcement refers to situations where the reinforcement to a learning system occurs only at the end of a string of actions or outputs, and it must then be assigned back to the relevant actions. A method for accomplishing this is presented which buffers a small number of past actions based on the unpredictability of or attention to each as it occurs. This approach allows for the buffer size to be small, and yet learning can reach indefinitely far back into the past; it also allows the system to learn when reinforcement is not only delayed but also reinforcements from other unrelated actions may arrive during this delay. An example of a simulated food-finding creature is used to show the system at work in a predictive application where reinforcements show this interleaving behaviour."
            },
            "slug": "Learning-with-Delayed-Reinforcement-Through-Myers",
            "title": {
                "fragments": [],
                "text": "Learning with Delayed Reinforcement Through Attention-Driven Buffering"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A method for accomplishing this is presented which buffers a small number of past actions based on the unpredictability of or attention to each as it occurs, which allows for the buffer size to be small, and yet learning can reach indefinitely far back into the past."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Neural Syst."
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 28
                            }
                        ],
                        "text": "See (Hochreiter, 1991) and (Schmidhuber, 1991c) for details."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 94
                            }
                        ],
                        "text": "This method is a simpliication and an improvement of the recent chunking method described by (Schmidhuber, 1991a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 35
                            }
                        ],
                        "text": "The chunking systems described in (Schmidhuber, 1991a), (Schmidhuber, 1991c) and the current paper try to detect temporal regularities and learn to use them for identifying relevant points in time."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 148
                            }
                        ],
                        "text": "We take P i to be a conventional dynamic recurrent neural network (Robinson and Fallside, 1987)(Williams and Zipser, 1989)(Williams and Peng, 1990)(Schmidhuber, 1991d); however, it might be some other adaptive sequence processing device as well"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 5
                            }
                        ],
                        "text": "See (Schmidhuber, 1991c) for various modiications."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning to control fast-weight memories: An alternative to recurrent nets"
            },
            "venue": {
                "fragments": [],
                "text": "Tech. Rep. FKI-147-91, Institut fur Informatik, Technische"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 28
                            }
                        ],
                        "text": "See (Hochreiter, 1991) and (Schmidhuber, 1991c) for details."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 94
                            }
                        ],
                        "text": "This method is a simpliication and an improvement of the recent chunking method described by (Schmidhuber, 1991a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 35
                            }
                        ],
                        "text": "The chunking systems described in (Schmidhuber, 1991a), (Schmidhuber, 1991c) and the current paper try to detect temporal regularities and learn to use them for identifying relevant points in time."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 148
                            }
                        ],
                        "text": "We take P i to be a conventional dynamic recurrent neural network (Robinson and Fallside, 1987)(Williams and Zipser, 1989)(Williams and Peng, 1990)(Schmidhuber, 1991d); however, it might be some other adaptive sequence processing device as well"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 5
                            }
                        ],
                        "text": "See (Schmidhuber, 1991c) for various modiications."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An O(n 3 ) learning algorithm for fully recurrent networks"
            },
            "venue": {
                "fragments": [],
                "text": "An O(n 3 ) learning algorithm for fully recurrent networks"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 55
                            }
                        ],
                        "text": "In particular, computationally inexpensive variants of BPTT (Williams and Peng, 1990) are interesting: There are tasks with hierarchical temporal structure where only a few iterations of`back-propagation back into time' per time step are in principle suucient to bridge arbitrary time lags (see section 5)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 27
                            }
                        ],
                        "text": "Both the IID-Algorithm and BPTT are appropriate."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 123
                            }
                        ],
                        "text": "We take P i to be a conventional dynamic recurrent neural network (Robinson and Fallside, 1987)(Williams and Zipser, 1989)(Williams and Peng, 1990)(Schmidhuber, 1991d); however, it might be some other adaptive sequence processing device as well"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 112
                            }
                        ],
                        "text": "But, a chunking system was able to solve the 20-step task rather quickly, using an eecient approximation of the BPTT-method where error was propagated a maximum of 3 steps into the past (although there was a 20 step time lag!)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 61
                            }
                        ],
                        "text": "In particular, computationally inexpensive variants of BPTT (Williams and Peng, 1990) are interesting: There are tasks with hierarchical temporal structure where only a few iterations of`back-propagation back into time' per time step are in principle suucient to bridge arbitrary time lags (see\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An e \u000e cient gradientbased algorithm for on - line training of recurrentnetwork trajectories"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning to control fastweight memories : An alternative to recurrent nets"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The ' moving targets ' training method"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of ' Distributed Adaptive Neural Information Processing '"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 136
                            }
                        ],
                        "text": "We take P i to be a conventional dynamic recurrent neural network (Robinson and Fallside, 1987)(Williams and Zipser, 1989)(Williams and Peng, 1990)(Schmidhuber, 1991d); however, it might be some other adaptive sequence processing device as well"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 74
                            }
                        ],
                        "text": "In particular, computationally inexpensive variants of BPTT (Williams and Peng, 1990) are interesting: There are tasks with hierarchical temporal structure where only a few iterations of`back-propagation back into time' per time step are in principle suucient to bridge arbitrary time lags (see\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An efficient gradient-based algorithm"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Th\u00e8moving targets' training method"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings ofof`Distributed Adaptive Neural Information Processing"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 62
                            }
                        ],
                        "text": "A hierarchical approach to sequence generation was pursued by Miyata (1988). See Ring (1991) for an alternative method of building \"chunking\" hierarchies."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 722,
                                "start": 62
                            }
                        ],
                        "text": "A hierarchical approach to sequence generation was pursued by Miyata (1988). See Ring (1991) for an alternative method of building \"chunking\" hierarchies. 2For instance, we might employ the more limited feedforward networks and a \"time window\" approach. In this case, the number of previous inputs to be considered as a basis for the next prediction will remain fixed. 3A unique time representation is theoretically necessary to provide Ps+l with unambiguous information about when the failure occurred (see also the last paragraph of Section 2). A unique representation of the time that went by since the last unpredicted input occurred will do as well. 41n contrast, the reduced descriptions referred to by Mozer (1990) are not unambiguous."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An unsupervised PDP learning model for action planning"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Tenth Annual Conference of the Cognitive Science Society"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An unsupervised PDP learning model for action planning"
            },
            "venue": {
                "fragments": [],
                "text": "Proc . of the Tenth AnnualConference of the Cognitive Science Society"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 94
                            }
                        ],
                        "text": "This method is a simplification and an improvement of the recent chunking method described by Schmidhuber (1991a). Often a multilevel predictor hierarchy will be the fastest way of learning to deal with sequences with multilevel temporal structure (e."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The utility driven dynamic error propagation network"
            },
            "venue": {
                "fragments": [],
                "text": "Tech. Rep. CUED/F-INFENG/TR.l, Cambridge University Engineering Department. Rohwer, R. 1989. The 'moving targets' training method. In"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 67
                            }
                        ],
                        "text": "We take P i to be a conventional dynamic recurrent neural network (Robinson and Fallside, 1987)(Williams and Zipser, 1989)(Williams and Peng, 1990)(Schmidhuber, 1991d); however, it might be some other adaptive sequence processing device as well"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The utility driven dynamic error propagation network"
            },
            "venue": {
                "fragments": [],
                "text": "The utility driven dynamic error propagation network"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 112
                            }
                        ],
                        "text": "5This distinction between attended and automized events can also be found in the systems of Myers (1990) and of Ring (1991)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Incremental development of complex behaviors through automatic construction of sensory-motor hierarchies"
            },
            "venue": {
                "fragments": [],
                "text": "In"
            },
            "year": 1991
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 8,
            "methodology": 10
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 20,
        "totalPages": 2
    },
    "page_url": "https://www.semanticscholar.org/paper/Learning-Complex,-Extended-Sequences-Using-the-of-Schmidhuber/50c770b425a5bb25c77387f687a9910a9d130722?sort=total-citations"
}