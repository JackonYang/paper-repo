{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2730654"
                        ],
                        "name": "Victor Chahuneau",
                        "slug": "Victor-Chahuneau",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Chahuneau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Victor Chahuneau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2824355"
                        ],
                        "name": "Eva Schlinger",
                        "slug": "Eva-Schlinger",
                        "structuredName": {
                            "firstName": "Eva",
                            "lastName": "Schlinger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eva Schlinger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144365875"
                        ],
                        "name": "Noah A. Smith",
                        "slug": "Noah-A.-Smith",
                        "structuredName": {
                            "firstName": "Noah",
                            "lastName": "Smith",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noah A. Smith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745899"
                        ],
                        "name": "Chris Dyer",
                        "slug": "Chris-Dyer",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Dyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris Dyer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 66
                            }
                        ],
                        "text": "A future task is thus to combine it with a system that can do so (Chahuneau et al., 2013)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1553957,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bac1b5d5767c4f78f8e7ec99d059bc82d5ca154a",
            "isKey": false,
            "numCitedBy": 85,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "Translation into morphologically rich languages is an important but recalcitrant problem in MT. We present a simple and effective approach that deals with the problem in two phases. First, a discriminative model is learned to predict inflections of target words from rich source-side annotations. Then, this model is used to create additional sentencespecific word- and phrase-level translations that are added to a standard translation model as \u201csynthetic\u201d phrases. Our approach relies on morphological analysis of the target language, but we show that an unsupervised Bayesian model of morphology can successfully be used in place of a supervised analyzer. We report significant improvements in translation quality when translating from English to Russian, Hebrew and Swahili."
            },
            "slug": "Translating-into-Morphologically-Rich-Languages-Chahuneau-Schlinger",
            "title": {
                "fragments": [],
                "text": "Translating into Morphologically Rich Languages with Synthetic Phrases"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The approach relies on morphological analysis of the target language, but it is shown that an unsupervised Bayesian model of morphology can successfully be used in place of a supervised analyzer."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2672644"
                        ],
                        "name": "Angeliki Lazaridou",
                        "slug": "Angeliki-Lazaridou",
                        "structuredName": {
                            "firstName": "Angeliki",
                            "lastName": "Lazaridou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Angeliki Lazaridou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48188880"
                        ],
                        "name": "M. Marelli",
                        "slug": "M.-Marelli",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Marelli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Marelli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2713535"
                        ],
                        "name": "Roberto Zamparelli",
                        "slug": "Roberto-Zamparelli",
                        "structuredName": {
                            "firstName": "Roberto",
                            "lastName": "Zamparelli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Roberto Zamparelli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145283199"
                        ],
                        "name": "Marco Baroni",
                        "slug": "Marco-Baroni",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Baroni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marco Baroni"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 90
                            }
                        ],
                        "text": "Accounting for linguistically derived infor-\nmation such as morphology (Luong et al., 2013; Lazaridou et al., 2013) or syntax (Hermann & Blunsom, 2013) has recently proved beneficial to learning vector representations of words."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 217,
                                "start": 195
                            }
                        ],
                        "text": "Our additive composition function can be regarded as an instantiation of the weighted addition strategy that performed well in a distributional compositional approach to derivational morphology (Lazaridou et al., 2013)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 105
                            }
                        ],
                        "text": "Conversely, compositional vector-space modelling has recently been applied to morphology to good effect (Lazaridou et al., 2013; Luong et al., 2013), but lacked the probabilistic basis necessary for use with a machine translation decoder."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 69
                            }
                        ],
                        "text": "Accounting for linguistically derived information such as morphology (Luong et al., 2013; Lazaridou et al., 2013) or syntax (Hermann & Blunsom, 2013) has recently proved beneficial to learning vector representations of words."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7371294,
            "fieldsOfStudy": [
                "Computer Science",
                "Linguistics"
            ],
            "id": "b4fcb3921b42d156a812484dcabf60c3f4410d42",
            "isKey": true,
            "numCitedBy": 102,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "Speakers of a language can construct an unlimited number of new words through morphological derivation. This is a major cause of data sparseness for corpus-based approaches to lexical semantics, such as distributional semantic models of word meaning. We adapt compositional methods originally developed for phrases to the task of deriving the distributional meaning of morphologically complex words from their parts. Semantic representations constructed in this way beat a strong baseline and can be of higher quality than representations directly constructed from corpus data. Our results constitute a novel evaluation of the proposed composition methods, in which the full additive model achieves the best performance, and demonstrate the usefulness of a compositional morphology component in distributional semantics."
            },
            "slug": "Compositional-ly-Derived-Representations-of-Complex-Lazaridou-Marelli",
            "title": {
                "fragments": [],
                "text": "Compositional-ly Derived Representations of Morphologically Complex Words in Distributional Semantics"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work adapts compositional methods originally developed for phrases to the task of deriving the distributional meaning of morphologically complex words from their parts, and demonstrates the usefulness of a compositional morphology component in distributional semantics."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2910877"
                        ],
                        "name": "K. Hermann",
                        "slug": "K.-Hermann",
                        "structuredName": {
                            "firstName": "Karl",
                            "lastName": "Hermann",
                            "middleNames": [
                                "Moritz"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Hermann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685771"
                        ],
                        "name": "P. Blunsom",
                        "slug": "P.-Blunsom",
                        "structuredName": {
                            "firstName": "Phil",
                            "lastName": "Blunsom",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Blunsom"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 125
                            }
                        ],
                        "text": "Accounting for linguistically derived infor-\nmation such as morphology (Luong et al., 2013; Lazaridou et al., 2013) or syntax (Hermann & Blunsom, 2013) has recently proved beneficial to learning vector representations of words."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17981782,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "79c0b2f44bbc2bc51de554b88ebe46204413f884",
            "isKey": false,
            "numCitedBy": 163,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "Modelling the compositional process by which the meaning of an utterance arises from the meaning of its parts is a fundamental task of Natural Language Processing. In this paper we draw upon recent advances in the learning of vector space representations of sentential semantics and the transparent interface between syntax and semantics provided by Combinatory Categorial Grammar to introduce Combinatory Categorial Autoencoders. This model leverages the CCG combinatory operators to guide a non-linear transformation of meaning within a sentence. We use this model to learn high dimensional embeddings for sentences and evaluate them in a range of tasks, demonstrating that the incorporation of syntax allows a concise model to learn representations that are both effective and general."
            },
            "slug": "The-Role-of-Syntax-in-Vector-Space-Models-of-Hermann-Blunsom",
            "title": {
                "fragments": [],
                "text": "The Role of Syntax in Vector Space Models of Compositional Semantics"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This model leverages the CCG combinatory operators to guide a non-linear transformation of meaning within a sentence and is used to learn high dimensional embeddings for sentences and evaluate them in a range of tasks, demonstrating that the incorporation of syntax allows a concise model to learn representations that are both effective and general."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40348417"
                        ],
                        "name": "Ashish Vaswani",
                        "slug": "Ashish-Vaswani",
                        "structuredName": {
                            "firstName": "Ashish",
                            "lastName": "Vaswani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ashish Vaswani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2651937"
                        ],
                        "name": "Yinggong Zhao",
                        "slug": "Yinggong-Zhao",
                        "structuredName": {
                            "firstName": "Yinggong",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yinggong Zhao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2372307"
                        ],
                        "name": "Victoria Fossum",
                        "slug": "Victoria-Fossum",
                        "structuredName": {
                            "firstName": "Victoria",
                            "lastName": "Fossum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Victoria Fossum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145287425"
                        ],
                        "name": "David Chiang",
                        "slug": "David-Chiang",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Chiang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Chiang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 71
                            }
                        ],
                        "text": "Aside from the choice of language pairs, this evaluation diverges from Vaswani et al. (2013) by using normalised probabilities, a process made tractable by the class-based decomposition and caching of context-specific normaliser terms."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 123
                            }
                        ],
                        "text": "Using unnormalised CSLMs during first-pass decoding has generated improvements in BLEU score for translation into English (Vaswani et al., 2013)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 0
                            }
                        ],
                        "text": "Vaswani et al. (2013) relied on unnormalised model scores for efficiency, but do not report on the performance impact of this assumption."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3065236,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "71480da09af638260801af1db8eff6acb4e1122f",
            "isKey": false,
            "numCitedBy": 265,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "We explore the application of neural language models to machine translation. We develop a new model that combines the neural probabilistic language model of Bengio et al., rectified linear units, and noise-contrastive estimation, and we incorporate it into a machine translation system both by reranking k-best lists and by direct integration into the decoder. Our large-scale, large-vocabulary experiments across four language pairs show that our neural language model improves translation quality by up to 1.1 Bleu."
            },
            "slug": "Decoding-with-Large-Scale-Neural-Language-Models-Vaswani-Zhao",
            "title": {
                "fragments": [],
                "text": "Decoding with Large-Scale Neural Language Models Improves Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "This work develops a new model that combines the neural probabilistic language model of Bengio et al., rectified linear units, and noise-contrastive estimation, and incorporates it into a machine translation system both by reranking k-best lists and by direct integration into the decoder."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2583391"
                        ],
                        "name": "Nal Kalchbrenner",
                        "slug": "Nal-Kalchbrenner",
                        "structuredName": {
                            "firstName": "Nal",
                            "lastName": "Kalchbrenner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nal Kalchbrenner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685771"
                        ],
                        "name": "P. Blunsom",
                        "slug": "P.-Blunsom",
                        "structuredName": {
                            "firstName": "Phil",
                            "lastName": "Blunsom",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Blunsom"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 290,
                                "start": 262
                            }
                        ],
                        "text": "Recent work has moved beyond monolingual vector-space modelling, incorporating phrase similarity ratings based on bilingual word embeddings as a translation model feature (Zou et al., 2013), or formulating translation purely in terms of continuous-space models (Kalchbrenner & Blunsom, 2013)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12639289,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "944a1cfd79dbfb6fef460360a0765ba790f4027a",
            "isKey": false,
            "numCitedBy": 1236,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a class of probabilistic continuous translation models called Recurrent Continuous Translation Models that are purely based on continuous representations for words, phrases and sentences and do not rely on alignments or phrasal translation units. The models have a generation and a conditioning aspect. The generation of the translation is modelled with a target Recurrent Language Model, whereas the conditioning on the source sentence is modelled with a Convolutional Sentence Model. Through various experiments, we show first that our models obtain a perplexity with respect to gold translations that is > 43% lower than that of stateof-the-art alignment-based translation models. Secondly, we show that they are remarkably sensitive to the word order, syntax, and meaning of the source sentence despite lacking alignments. Finally we show that they match a state-of-the-art system when rescoring n-best lists of translations."
            },
            "slug": "Recurrent-Continuous-Translation-Models-Kalchbrenner-Blunsom",
            "title": {
                "fragments": [],
                "text": "Recurrent Continuous Translation Models"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "A class of probabilistic continuous translation models called Recurrent Continuous Translation Models that are purely based on continuous representations for words, phrases and sentences and do not rely on alignments or phrasal translation units are introduced."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1821711"
                        ],
                        "name": "Thang Luong",
                        "slug": "Thang-Luong",
                        "structuredName": {
                            "firstName": "Thang",
                            "lastName": "Luong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thang Luong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2166511"
                        ],
                        "name": "R. Socher",
                        "slug": "R.-Socher",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Socher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Socher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 69
                            }
                        ],
                        "text": "Accounting for linguistically derived information such as morphology (Luong et al., 2013; Lazaridou et al., 2013) or syntax (Hermann & Blunsom, 2013) has recently proved beneficial to learning vector representations of words."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 70
                            }
                        ],
                        "text": "Accounting for linguistically derived infor-\nmation such as morphology (Luong et al., 2013; Lazaridou et al., 2013) or syntax (Hermann & Blunsom, 2013) has recently proved beneficial to learning vector representations of words."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 129
                            }
                        ],
                        "text": "Conversely, compositional vector-space modelling has recently been applied to morphology to good effect (Lazaridou et al., 2013; Luong et al., 2013), but lacked the probabilistic basis necessary for use with a machine translation decoder."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 46
                            }
                        ],
                        "text": "Unlike the recursive neural-network method of Luong et al. (2013), we do not impose a single tree structure over a word, which would ignore the ambiguity inherent in words like un[[lock]able] vs. [un[lock]]able."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 70
                            }
                        ],
                        "text": "We evaluate first using the English rare-word dataset (RW) created by Luong et al. (2013)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14276764,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "53ab89807caead278d3deb7b6a4180b277d3cb77",
            "isKey": true,
            "numCitedBy": 794,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "Vector-space word representations have been very successful in recent years at improving performance across a variety of NLP tasks. However, common to most existing work, words are regarded as independent entities without any explicit relationship among morphologically related words being modeled. As a result, rare and complex words are often poorly estimated, and all unknown words are represented in a rather crude way using only one or a few vectors. This paper addresses this shortcoming by proposing a novel model that is capable of building representations for morphologically complex words from their morphemes. We combine recursive neural networks (RNNs), where each morpheme is a basic unit, with neural language models (NLMs) to consider contextual information in learning morphologicallyaware word representations. Our learned models outperform existing word representations by a good margin on word similarity tasks across many datasets, including a new dataset we introduce focused on rare words to complement existing ones in an interesting way."
            },
            "slug": "Better-Word-Representations-with-Recursive-Neural-Luong-Socher",
            "title": {
                "fragments": [],
                "text": "Better Word Representations with Recursive Neural Networks for Morphology"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper combines recursive neural networks, where each morpheme is a basic unit, with neural language models to consider contextual information in learning morphologicallyaware word representations and proposes a novel model capable of building representations for morphologically complex words from their morphemes."
            },
            "venue": {
                "fragments": [],
                "text": "CoNLL"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2047446108"
                        ],
                        "name": "Tomas Mikolov",
                        "slug": "Tomas-Mikolov",
                        "structuredName": {
                            "firstName": "Tomas",
                            "lastName": "Mikolov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tomas Mikolov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144105277"
                        ],
                        "name": "Wen-tau Yih",
                        "slug": "Wen-tau-Yih",
                        "structuredName": {
                            "firstName": "Wen-tau",
                            "lastName": "Yih",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wen-tau Yih"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681543"
                        ],
                        "name": "G. Zweig",
                        "slug": "G.-Zweig",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Zweig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Zweig"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 108
                            }
                        ],
                        "text": "Maintaining a constant advantage over MKN requires also increasing the dimensionality d of representations (Mikolov et al., 2013a), but this was outside the scope of our experiment."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 84
                            }
                        ],
                        "text": "Such word representations have been found to capture some morphological regularity (Mikolov et al., 2013b), but we contend that there is a case for building a priori morphological awareness into\nProceedings of the 31 st International Conference on Machine Learning, Beijing, China, 2014."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7478738,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c4fd9c86b2b41df51a6fe212406dda81b1997fd4",
            "isKey": false,
            "numCitedBy": 3053,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Continuous space language models have recently demonstrated outstanding results across a variety of tasks. In this paper, we examine the vector-space word representations that are implicitly learned by the input-layer weights. We find that these representations are surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset. This allows vector-oriented reasoning based on the offsets between words. For example, the male/female relationship is automatically learned, and with the induced vector representations, \u201cKing Man + Woman\u201d results in a vector very close to \u201cQueen.\u201d We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions (provided with this paper), and are able to correctly answer almost 40% of the questions. We demonstrate that the word vectors capture semantic regularities by using the vector offset method to answer SemEval-2012 Task 2 questions. Remarkably, this method outperforms the best previous systems."
            },
            "slug": "Linguistic-Regularities-in-Continuous-Space-Word-Mikolov-Yih",
            "title": {
                "fragments": [],
                "text": "Linguistic Regularities in Continuous Space Word Representations"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "The vector-space word representations that are implicitly learned by the input-layer weights are found to be surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144518416"
                        ],
                        "name": "Holger Schwenk",
                        "slug": "Holger-Schwenk",
                        "structuredName": {
                            "firstName": "Holger",
                            "lastName": "Schwenk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Holger Schwenk"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 102
                            }
                        ],
                        "text": "Conventional back-off n-gram LMs (Chen & Goodman, 1998) and the increasingly popular vector-based LMs (Bengio et al., 2003; Schwenk et al., 2006; Mikolov et al., 2010) use parametrisations that do not explicitly encode morphological regularities among related forms, like abstract, abstraction and abstracted."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 58
                            }
                        ],
                        "text": "The primary outcomes are that (i) our morphology-guided CSLMs improve intrinsic language model performance when compared to baseline CSLMs and n-gram MKN models; (ii) word and morpheme representations learnt in the process compare favourably in terms of a word similarity task to a recent more complex model that used more data, while obtaining large gains on some languages; (iii) machine translation quality as measured by BLEU was improved consistently across six language pairs when using CSLMs during decoding, although the morphology-based representations led to further improvements beyond the level of optimiser variance only for English\u2192Czech."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 6
                            }
                        ],
                        "text": "The CSLMs are integrated directly into the decoder as an additional feature function, thus exercising a stronger influence on the search than in n-best list rescoring.11 Translation model feature weights are tuned with MERT (Och, 2003) on newstest2012."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 21
                            }
                        ],
                        "text": "Using unnormalised CSLMs during first-pass decoding has generated improvements in BLEU score for translation into English (Vaswani et al., 2013)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 28
                            }
                        ],
                        "text": "The key obstacle to using CSLMs in a decoder is the expensive normalisation over the vocabulary."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 69
                            }
                        ],
                        "text": "Log-bilinear (LBL) models (Mnih & Hinton, 2007) are an instance of CSLMs that make the same Markov assumption as n-gram language models."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 58
                            }
                        ],
                        "text": "A variety of strategies have been explored for bringing CSLMs to bear on machine translation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 124
                            }
                        ],
                        "text": "Rescoring lattices with a CSLM proved to be beneficial for ASR (Schwenk, 2004) and was subsequently applied to translation (Schwenk et al., 2006; Schwenk & Koehn, 2008), reaching training sizes of up to 500m words (Schwenk et al., 2012)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 270,
                                "start": 267
                            }
                        ],
                        "text": "To the best of our knowledge, this\n10 ES WS353 (Hassan & Mihalcea, 2009); Gur350 (Gurevych, 2005); RG65 (Rubenstein & Goodenough, 1965) with FR (Joubarne & Inkpen, 2011); ZG222 (Zesch & Gurevych, 2006).\nis the first study to investigate large vocabulary normalised CSLMs inside a decoder when translating into a range of morphologically rich languages."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 57
                            }
                        ],
                        "text": "This work focuses on continuous space language models (CSLMs), an umbrella term for the LMs that represent words with real-valued vectors."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 123
                            }
                        ],
                        "text": "Rescoring lattices with a CSLM proved to be beneficial for ASR (Schwenk, 2004) and was subsequently applied to translation (Schwenk et al., 2006; Schwenk & Koehn, 2008), reaching training sizes of up to 500m words (Schwenk et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 122
                            }
                        ],
                        "text": "The proliferation of word forms in morphologically rich languages presents challenges to the statistical language models (LMs) that play a key role in machine translation and speech recognition."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 124
                            }
                        ],
                        "text": "Conventional back-off n-gram LMs (Chen & Goodman, 1998) and the increasingly popular vector-based LMs (Bengio et al., 2003; Schwenk et al., 2006; Mikolov et al., 2010) use parametrisations that do not explicitly encode morphological regularities among related forms, like abstract, abstraction and\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 27
                            }
                        ],
                        "text": "Factored language models (FLMs) have been used to integrate morphological information into both discrete n-gram LMs (Bilmes & Kirchhoff, 2003) and CSLMs (Alexandrescu & Kirchhoff, 2006) by viewing a word as a set of factors."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1274371,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d4a258df43cc14e46988de9a4a7b2f0ea817529b",
            "isKey": true,
            "numCitedBy": 174,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Statistical machine translation systems are based on one or more translation models and a language model of the target language. While many different translation models and phrase extraction algorithms have been proposed, a standard word n-gram back-off language model is used in most systems. \n \nIn this work, we propose to use a new statistical language model that is based on a continuous representation of the words in the vocabulary. A neural network is used to perform the projection and the probability estimation. We consider the translation of European Parliament Speeches. This task is part of an international evaluation organized by the TC-STAR project in 2006. The proposed method achieves consistent improvements in the BLEU score on the development and test data. \n \nWe also present algorithms to improve the estimation of the language model probabilities when splitting long sentences into shorter chunks."
            },
            "slug": "Continuous-Space-Language-Models-for-Statistical-Schwenk",
            "title": {
                "fragments": [],
                "text": "Continuous Space Language Models for Statistical Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work proposes to use a new statistical language model that is based on a continuous representation of the words in the vocabulary, which achieves consistent improvements in the BLEU score on the development and test data."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40150953"
                        ],
                        "name": "E. Huang",
                        "slug": "E.-Huang",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Huang",
                            "middleNames": [
                                "Hsin-Chun"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2166511"
                        ],
                        "name": "R. Socher",
                        "slug": "R.-Socher",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Socher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Socher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 179
                            }
                        ],
                        "text": "Moreover, the csmRNNs were initialised with high-quality, publicly available word embeddings trained over weeks on much larger corpora of 630\u2013990m words (Collobert & Weston, 2008; Huang et al., 2012), in contrast to ours that are trained from scratch on much less data."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 372093,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2b669398c4cf2ebe04375c8b1beae20f4ac802fa",
            "isKey": false,
            "numCitedBy": 1184,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "Unsupervised word representations are very useful in NLP tasks both as inputs to learning algorithms and as extra word features in NLP systems. However, most of these models are built with only local context and one representation per word. This is problematic because words are often polysemous and global context can also provide useful information for learning word meanings. We present a new neural network architecture which 1) learns word embeddings that better capture the semantics of words by incorporating both local and global document context, and 2) accounts for homonymy and polysemy by learning multiple embeddings per word. We introduce a new dataset with human judgments on pairs of words in sentential context, and evaluate our model on it, showing that our model outperforms competitive baselines and other neural language models."
            },
            "slug": "Improving-Word-Representations-via-Global-Context-Huang-Socher",
            "title": {
                "fragments": [],
                "text": "Improving Word Representations via Global Context and Multiple Word Prototypes"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A new neural network architecture is presented which learns word embeddings that better capture the semantics of words by incorporating both local and global document context, and accounts for homonymy and polysemy by learning multiple embedDings per word."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745899"
                        ],
                        "name": "Chris Dyer",
                        "slug": "Chris-Dyer",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Dyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris Dyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144871732"
                        ],
                        "name": "Adam Lopez",
                        "slug": "Adam-Lopez",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Lopez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adam Lopez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2267212"
                        ],
                        "name": "Juri Ganitkevitch",
                        "slug": "Juri-Ganitkevitch",
                        "structuredName": {
                            "firstName": "Juri",
                            "lastName": "Ganitkevitch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Juri Ganitkevitch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143788080"
                        ],
                        "name": "Jonathan Weese",
                        "slug": "Jonathan-Weese",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Weese",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Weese"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2851411"
                        ],
                        "name": "Ferhan T\u00fcre",
                        "slug": "Ferhan-T\u00fcre",
                        "structuredName": {
                            "firstName": "Ferhan",
                            "lastName": "T\u00fcre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ferhan T\u00fcre"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685771"
                        ],
                        "name": "P. Blunsom",
                        "slug": "P.-Blunsom",
                        "structuredName": {
                            "firstName": "Phil",
                            "lastName": "Blunsom",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Blunsom"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144297428"
                        ],
                        "name": "Hendra Setiawan",
                        "slug": "Hendra-Setiawan",
                        "structuredName": {
                            "firstName": "Hendra",
                            "lastName": "Setiawan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hendra Setiawan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2332594"
                        ],
                        "name": "Vladimir Eidelman",
                        "slug": "Vladimir-Eidelman",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Eidelman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vladimir Eidelman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680292"
                        ],
                        "name": "P. Resnik",
                        "slug": "P.-Resnik",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Resnik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Resnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 13
                            }
                        ],
                        "text": "We use cdec (Dyer et al., 2010; 2013) to build symmetric word-alignments and extract rules for hierarchical phrasebased translation (Chiang, 2007)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16391184,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6053d1693c0fab0c21ebbea0ba5408b441a3542b",
            "isKey": false,
            "numCitedBy": 252,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We present cdec, an open source framework for decoding, aligning with, and training a number of statistical machine translation models, including word-based models, phrase-based models, and models based on synchronous context-free grammars. Using a single unified internal representation for translation forests, the decoder strictly separates model-specific translation logic from general rescoring, pruning, and inference algorithms. From this unified representation, the decoder can extract not only the 1or k-best translations, but also alignments to a reference, or the quantities necessary to drive discriminative training using gradient-based or gradient-free optimization techniques. Its efficient C++ implementation means that memory use and runtime performance are significantly better than comparable decoders."
            },
            "slug": "cdec:-A-Decoder,-Alignment,-and-Learning-Framework-Dyer-Lopez",
            "title": {
                "fragments": [],
                "text": "cdec: A Decoder, Alignment, and Learning Framework for Finite- State and Context-Free Translation Models"
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145287425"
                        ],
                        "name": "David Chiang",
                        "slug": "David-Chiang",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Chiang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Chiang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 108
                            }
                        ],
                        "text": ", 2010; 2013) to build symmetric word-alignments and extract rules for hierarchical phrasebased translation (Chiang, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 133
                            }
                        ],
                        "text": "We use cdec (Dyer et al., 2010; 2013) to build symmetric word-alignments and extract rules for hierarchical phrasebased translation (Chiang, 2007)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3505719,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0db6eb46ca9941660acc775e3ca39bf4434c18be",
            "isKey": false,
            "numCitedBy": 1299,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a statistical machine translation model that uses hierarchical phrasesphrases that contain subphrases. The model is formally a synchronous context-free grammar but is learned from a parallel text without any syntactic annotations. Thus it can be seen as combining fundamental ideas from both syntax-based translation and phrase-based translation. We describe our system's training and decoding methods in detail, and evaluate it for translation speed and translation accuracy. Using BLEU as a metric of translation accuracy, we find that our system performs significantly better than the Alignment Template System, a state-of-the-art phrase-based system."
            },
            "slug": "Hierarchical-Phrase-Based-Translation-Chiang",
            "title": {
                "fragments": [],
                "text": "Hierarchical Phrase-Based Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "A statistical machine translation model that uses hierarchical phrasesphrases that contain subphrasing that is formally a synchronous context-free grammar but is learned from a parallel text without any syntactic annotations is presented."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2047446108"
                        ],
                        "name": "Tomas Mikolov",
                        "slug": "Tomas-Mikolov",
                        "structuredName": {
                            "firstName": "Tomas",
                            "lastName": "Mikolov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tomas Mikolov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118440152"
                        ],
                        "name": "Kai Chen",
                        "slug": "Kai-Chen",
                        "structuredName": {
                            "firstName": "Kai",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32131713"
                        ],
                        "name": "G. Corrado",
                        "slug": "G.-Corrado",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Corrado",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Corrado"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49959210"
                        ],
                        "name": "J. Dean",
                        "slug": "J.-Dean",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Dean",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Dean"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Our contribution is to create morphological awareness in a probabilistic language model."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 108
                            }
                        ],
                        "text": "Maintaining a constant advantage over MKN requires also increasing the dimensionality d of representations (Mikolov et al., 2013a), but this was outside the scope of our experiment."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 84
                            }
                        ],
                        "text": "Such word representations have been found to capture some morphological regularity (Mikolov et al., 2013b), but we contend that there is a case for building a priori morphological awareness into\nProceedings of the 31 st International Conference on Machine Learning, Beijing, China, 2014."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5959482,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "330da625c15427c6e42ccfa3b747fb29e5835bf0",
            "isKey": false,
            "numCitedBy": 21906,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose two novel model architectures for computing continuous vector\nrepresentations of words from very large data sets. The quality of these\nrepresentations is measured in a word similarity task, and the results are\ncompared to the previously best performing techniques based on different types\nof neural networks. We observe large improvements in accuracy at much lower\ncomputational cost, i.e. it takes less than a day to learn high quality word\nvectors from a 1.6 billion words data set. Furthermore, we show that these\nvectors provide state-of-the-art performance on our test set for measuring\nsyntactic and semantic word similarities."
            },
            "slug": "Efficient-Estimation-of-Word-Representations-in-Mikolov-Chen",
            "title": {
                "fragments": [],
                "text": "Efficient Estimation of Word Representations in Vector Space"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "Two novel model architectures for computing continuous vector representations of words from very large data sets are proposed and it is shown that these vectors provide state-of-the-art performance on the authors' test set for measuring syntactic and semantic word similarities."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2219854"
                        ],
                        "name": "Mathias Creutz",
                        "slug": "Mathias-Creutz",
                        "structuredName": {
                            "firstName": "Mathias",
                            "lastName": "Creutz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mathias Creutz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2395884"
                        ],
                        "name": "K. Lagus",
                        "slug": "K.-Lagus",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Lagus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Lagus"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8819802,
            "fieldsOfStudy": [
                "Computer Science",
                "Linguistics"
            ],
            "id": "02711b7e8c73779db802cb00b6cbf407d0872b66",
            "isKey": false,
            "numCitedBy": 264,
            "numCiting": 84,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a model family called Morfessor for the unsupervised induction of a simple morphology from raw text data. The model is formulated in a probabilistic maximum a posteriori framework. Morfessor can handle highly inflecting and compounding languages where words can consist of lengthy sequences of morphemes. A lexicon of word segments, called morphs, is induced from the data. The lexicon stores information about both the usage and form of the morphs. Several instances of the model are evaluated quantitatively in a morpheme segmentation task on different sized sets of Finnish as well as English data. Morfessor is shown to perform very well compared to a widely known benchmark algorithm, in particular on Finnish data."
            },
            "slug": "Unsupervised-models-for-morpheme-segmentation-and-Creutz-Lagus",
            "title": {
                "fragments": [],
                "text": "Unsupervised models for morpheme segmentation and morphology learning"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "Morfessor can handle highly inflecting and compounding languages where words can consist of lengthy sequences of morphemes and is shown to perform very well compared to a widely known benchmark algorithm on Finnish data."
            },
            "venue": {
                "fragments": [],
                "text": "TSLP"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714004"
                        ],
                        "name": "A. Mnih",
                        "slug": "A.-Mnih",
                        "structuredName": {
                            "firstName": "Andriy",
                            "lastName": "Mnih",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Mnih"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 206,
                                "start": 187
                            }
                        ],
                        "text": "Other methods for achieving more drastic complexity reductions exist in the form of frequency-based truncation, shortlists (Schwenk, 2004), or casting the vocabulary as a full hierarchy (Mnih & Hinton, 2008) or partial hierarchy (Le et al., 2011)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We apply AdaGrad (Duchi et al., 2011) and tune the stepsize \u03be on development data.4 We halt training once the perplexity on the development data starts to increase."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10097073,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a9fc84f8abe740cdc7ee82e69444d1d00dbe0ceb",
            "isKey": false,
            "numCitedBy": 939,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Neural probabilistic language models (NPLMs) have been shown to be competitive with and occasionally superior to the widely-used n-gram language models. The main drawback of NPLMs is their extremely long training and testing times. Morin and Bengio have proposed a hierarchical language model built around a binary tree of words, which was two orders of magnitude faster than the non-hierarchical model it was based on. However, it performed considerably worse than its non-hierarchical counterpart in spite of using a word tree created using expert knowledge. We introduce a fast hierarchical language model along with a simple feature-based algorithm for automatic construction of word trees from the data. We then show that the resulting models can outperform non-hierarchical neural models as well as the best n-gram models."
            },
            "slug": "A-Scalable-Hierarchical-Distributed-Language-Model-Mnih-Hinton",
            "title": {
                "fragments": [],
                "text": "A Scalable Hierarchical Distributed Language Model"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A fast hierarchical language model along with a simple feature-based algorithm for automatic construction of word trees from the data are introduced and it is shown that the resulting models can outperform non-hierarchical neural models as well as the best n-gram models."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714004"
                        ],
                        "name": "A. Mnih",
                        "slug": "A.-Mnih",
                        "structuredName": {
                            "firstName": "Andriy",
                            "lastName": "Mnih",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Mnih"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725303"
                        ],
                        "name": "Y. Teh",
                        "slug": "Y.-Teh",
                        "structuredName": {
                            "firstName": "Yee",
                            "lastName": "Teh",
                            "middleNames": [
                                "Whye"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Teh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 93
                            }
                        ],
                        "text": "For the classless LBLs we use noise-contrastive estimation (NCE) (Gutmann & Hyva\u0308rinen, 2012; Mnih & Teh, 2012) to avoid normalisation during training."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6633369,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5b0d644f5c4b9880cbaf79932c0a4fa98996f068",
            "isKey": false,
            "numCitedBy": 520,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "In spite of their superior performance, neural probabilistic language models (NPLMs) remain far less widely used than n-gram models due to their notoriously long training times, which are measured in weeks even for moderately-sized datasets. Training NPLMs is computationally expensive because they are explicitly normalized, which leads to having to consider all words in the vocabulary when computing the log-likelihood gradients. \n \nWe propose a fast and simple algorithm for training NPLMs based on noise-contrastive estimation, a newly introduced procedure for estimating unnormalized continuous distributions. We investigate the behaviour of the algorithm on the Penn Treebank corpus and show that it reduces the training times by more than an order of magnitude without affecting the quality of the resulting models. The algorithm is also more efficient and much more stable than importance sampling because it requires far fewer noise samples to perform well. \n \nWe demonstrate the scalability of the proposed approach by training several neural language models on a 47M-word corpus with a 80K-word vocabulary, obtaining state-of-the-art results on the Microsoft Research Sentence Completion Challenge dataset."
            },
            "slug": "A-fast-and-simple-algorithm-for-training-neural-Mnih-Teh",
            "title": {
                "fragments": [],
                "text": "A fast and simple algorithm for training neural probabilistic language models"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work proposes a fast and simple algorithm for training NPLMs based on noise-contrastive estimation, a newly introduced procedure for estimating unnormalized continuous distributions and demonstrates the scalability of the proposed approach by training several neural language models on a 47M-word corpus with a 80K-word vocabulary."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36037226"
                        ],
                        "name": "R\u00e9jean Ducharme",
                        "slug": "R\u00e9jean-Ducharme",
                        "structuredName": {
                            "firstName": "R\u00e9jean",
                            "lastName": "Ducharme",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R\u00e9jean Ducharme"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120247189"
                        ],
                        "name": "Pascal Vincent",
                        "slug": "Pascal-Vincent",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Vincent",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pascal Vincent"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1909943744"
                        ],
                        "name": "Christian Janvin",
                        "slug": "Christian-Janvin",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Janvin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian Janvin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 29
                            }
                        ],
                        "text": "Conventional back-off n-gram LMs (Chen & Goodman, 1998) and the increasingly popular vector-based LMs (Bengio et al., 2003; Schwenk et al., 2006; Mikolov et al., 2010) use parametrisations that do not explicitly encode morphological regularities among related forms, like abstract, abstraction and abstracted."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 58
                            }
                        ],
                        "text": "The primary outcomes are that (i) our morphology-guided CSLMs improve intrinsic language model performance when compared to baseline CSLMs and n-gram MKN models; (ii) word and morpheme representations learnt in the process compare favourably in terms of a word similarity task to a recent more complex model that used more data, while obtaining large gains on some languages; (iii) machine translation quality as measured by BLEU was improved consistently across six language pairs when using CSLMs during decoding, although the morphology-based representations led to further improvements beyond the level of optimiser variance only for English\u2192Czech."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 6
                            }
                        ],
                        "text": "The CSLMs are integrated directly into the decoder as an additional feature function, thus exercising a stronger influence on the search than in n-best list rescoring.11 Translation model feature weights are tuned with MERT (Och, 2003) on newstest2012."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 21
                            }
                        ],
                        "text": "Using unnormalised CSLMs during first-pass decoding has generated improvements in BLEU score for translation into English (Vaswani et al., 2013)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 28
                            }
                        ],
                        "text": "The key obstacle to using CSLMs in a decoder is the expensive normalisation over the vocabulary."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 69
                            }
                        ],
                        "text": "Log-bilinear (LBL) models (Mnih & Hinton, 2007) are an instance of CSLMs that make the same Markov assumption as n-gram language models."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 58
                            }
                        ],
                        "text": "A variety of strategies have been explored for bringing CSLMs to bear on machine translation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 270,
                                "start": 267
                            }
                        ],
                        "text": "To the best of our knowledge, this\n10 ES WS353 (Hassan & Mihalcea, 2009); Gur350 (Gurevych, 2005); RG65 (Rubenstein & Goodenough, 1965) with FR (Joubarne & Inkpen, 2011); ZG222 (Zesch & Gurevych, 2006).\nis the first study to investigate large vocabulary normalised CSLMs inside a decoder when translating into a range of morphologically rich languages."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 57
                            }
                        ],
                        "text": "This work focuses on continuous space language models (CSLMs), an umbrella term for the LMs that represent words with real-valued vectors."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 122
                            }
                        ],
                        "text": "The proliferation of word forms in morphologically rich languages presents challenges to the statistical language models (LMs) that play a key role in machine translation and speech recognition."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 103
                            }
                        ],
                        "text": "Conventional back-off n-gram LMs (Chen & Goodman, 1998) and the increasingly popular vector-based LMs (Bengio et al., 2003; Schwenk et al., 2006; Mikolov et al., 2010) use parametrisations that do not explicitly encode morphological regularities among related forms, like abstract, abstraction and\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 27
                            }
                        ],
                        "text": "Factored language models (FLMs) have been used to integrate morphological information into both discrete n-gram LMs (Bilmes & Kirchhoff, 2003) and CSLMs (Alexandrescu & Kirchhoff, 2006) by viewing a word as a set of factors."
                    },
                    "intents": []
                }
            ],
            "corpusId": 221275765,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6c2b28f9354f667cd5bd07afc0471d8334430da7",
            "isKey": true,
            "numCitedBy": 6014,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts."
            },
            "slug": "A-Neural-Probabilistic-Language-Model-Bengio-Ducharme",
            "title": {
                "fragments": [],
                "text": "A Neural Probabilistic Language Model"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work proposes to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49610089"
                        ],
                        "name": "Andrei Alexandrescu",
                        "slug": "Andrei-Alexandrescu",
                        "structuredName": {
                            "firstName": "Andrei",
                            "lastName": "Alexandrescu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrei Alexandrescu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783839"
                        ],
                        "name": "Katrin Kirchhoff",
                        "slug": "Katrin-Kirchhoff",
                        "structuredName": {
                            "firstName": "Katrin",
                            "lastName": "Kirchhoff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Katrin Kirchhoff"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 56
                            }
                        ],
                        "text": "The primary outcomes are that (i) our morphology-guided CSLMs improve intrinsic language model performance when compared to baseline CSLMs and n-gram MKN models; (ii) word and morpheme representations learnt in the process compare favourably in terms of a word similarity task to a recent more complex model that used more data, while obtaining large gains on some languages; (iii) machine translation quality as measured by BLEU was improved consistently across six language pairs when using CSLMs during decoding, although the morphology-based representations led to further improvements beyond the level of optimiser variance only for English\u2192Czech."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 4
                            }
                        ],
                        "text": "The CSLMs are integrated directly into the decoder as an additional feature function, thus exercising a stronger influence on the search than in n-best list rescoring.11 Translation model feature weights are tuned with MERT (Och, 2003) on newstest2012."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 19
                            }
                        ],
                        "text": "Using unnormalised CSLMs during first-pass decoding has generated improvements in BLEU score for translation into English (Vaswani et al., 2013)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 26
                            }
                        ],
                        "text": "The key obstacle to using CSLMs in a decoder is the expensive normalisation over the vocabulary."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 67
                            }
                        ],
                        "text": "Log-bilinear (LBL) models (Mnih & Hinton, 2007) are an instance of CSLMs that make the same Markov assumption as n-gram language models."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 56
                            }
                        ],
                        "text": "A variety of strategies have been explored for bringing CSLMs to bear on machine translation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 0
                            }
                        ],
                        "text": "Alexandrescu & Kirchhoff (2006) demonstrated how factorising the representations of context-words can help deal with out-of-vocabulary words, but they did not evaluate the effect of factorising output words and did not conduct an extrinsic evaluation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 270,
                                "start": 265
                            }
                        ],
                        "text": "To the best of our knowledge, this\n10 ES WS353 (Hassan & Mihalcea, 2009); Gur350 (Gurevych, 2005); RG65 (Rubenstein & Goodenough, 1965) with FR (Joubarne & Inkpen, 2011); ZG222 (Zesch & Gurevych, 2006).\nis the first study to investigate large vocabulary normalised CSLMs inside a decoder when translating into a range of morphologically rich languages."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 55
                            }
                        ],
                        "text": "This work focuses on continuous space language models (CSLMs), an umbrella term for the LMs that represent words with real-valued vectors."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 184,
                                "start": 154
                            }
                        ],
                        "text": "Factored language models (FLMs) have been used to integrate morphological information into both discrete n-gram LMs (Bilmes & Kirchhoff, 2003) and CSLMs (Alexandrescu & Kirchhoff, 2006) by viewing a word as a set of factors."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 49
                            }
                        ],
                        "text": "This is in contrast to factored language models (Alexandrescu & Kirchhoff, 2006), which assume a fixed number of factors per word."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 937826,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a73ae2ce1cfafae61238b3fdb1bbb61093962b4d",
            "isKey": true,
            "numCitedBy": 98,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new type of neural probabilistic language model that learns a mapping from both words and explicit word features into a continuous space that is then used for word prediction. Additionally, we investigate several ways of deriving continuous word representations for unknown words from those of known words. The resulting model significantly reduces perplexity on sparse-data tasks when compared to standard backoff models, standard neural language models, and factored language models."
            },
            "slug": "Factored-Neural-Language-Models-Alexandrescu-Kirchhoff",
            "title": {
                "fragments": [],
                "text": "Factored Neural Language Models"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "A new type of neural probabilistic language model is presented that learns a mapping from both words and explicit word features into a continuous space that is then used for word prediction and significantly reduces perplexity on sparse-data tasks."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2860351"
                        ],
                        "name": "Will Y. Zou",
                        "slug": "Will-Y.-Zou",
                        "structuredName": {
                            "firstName": "Will",
                            "lastName": "Zou",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Will Y. Zou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2166511"
                        ],
                        "name": "R. Socher",
                        "slug": "R.-Socher",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Socher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Socher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46724030"
                        ],
                        "name": "Daniel Matthew Cer",
                        "slug": "Daniel-Matthew-Cer",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Cer",
                            "middleNames": [
                                "Matthew"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Matthew Cer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 188,
                                "start": 172
                            }
                        ],
                        "text": "Recent work has moved beyond monolingual vector-space modelling, incorporating phrase similarity ratings based on bilingual word embeddings as a translation model feature (Zou et al., 2013), or formulating translation purely in terms of continuous-space models (Kalchbrenner & Blunsom, 2013)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 931054,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0d3233d858660aff451a6c2561a05378ed09725a",
            "isKey": false,
            "numCitedBy": 535,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce bilingual word embeddings: semantic embeddings associated across two languages in the context of neural language models. We propose a method to learn bilingual embeddings from a large unlabeled corpus, while utilizing MT word alignments to constrain translational equivalence. The new embeddings significantly out-perform baselines in word semantic similarity. A single semantic similarity feature induced with bilingual embeddings adds near half a BLEU point to the results of NIST08 Chinese-English machine translation task."
            },
            "slug": "Bilingual-Word-Embeddings-for-Phrase-Based-Machine-Zou-Socher",
            "title": {
                "fragments": [],
                "text": "Bilingual Word Embeddings for Phrase-Based Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A method to learn bilingual embeddings from a large unlabeled corpus, while utilizing MT word alignments to constrain translational equivalence is proposed, which significantly out-perform baselines in word semantic similarity."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144518416"
                        ],
                        "name": "Holger Schwenk",
                        "slug": "Holger-Schwenk",
                        "structuredName": {
                            "firstName": "Holger",
                            "lastName": "Schwenk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Holger Schwenk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1755162"
                        ],
                        "name": "Philipp Koehn",
                        "slug": "Philipp-Koehn",
                        "structuredName": {
                            "firstName": "Philipp",
                            "lastName": "Koehn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philipp Koehn"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 146
                            }
                        ],
                        "text": "Rescoring lattices with a CSLM proved to be beneficial for ASR (Schwenk, 2004) and was subsequently applied to translation (Schwenk et al., 2006; Schwenk & Koehn, 2008), reaching training sizes of up to 500m words (Schwenk et al., 2012)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10426366,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "08d207e48b990265cfd1693b68574c193b080cd5",
            "isKey": false,
            "numCitedBy": 46,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents methods to combine large language models trained from diverse text sources and applies them to a state-ofart French\u2013English and Arabic\u2013English machine translation system. We show gains of over 2 BLEU points over a strong baseline by using continuous space language models in re-ranking."
            },
            "slug": "Large-and-Diverse-Language-Models-for-Statistical-Schwenk-Koehn",
            "title": {
                "fragments": [],
                "text": "Large and Diverse Language Models for Statistical Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "Methods to combine large language models trained from diverse text sources and applies them to a state-ofart French\u2013English and Arabic\u2013English machine translation system are presented."
            },
            "venue": {
                "fragments": [],
                "text": "IJCNLP"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144518416"
                        ],
                        "name": "Holger Schwenk",
                        "slug": "Holger-Schwenk",
                        "structuredName": {
                            "firstName": "Holger",
                            "lastName": "Schwenk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Holger Schwenk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34812618"
                        ],
                        "name": "A. Rousseau",
                        "slug": "A.-Rousseau",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Rousseau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Rousseau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1992242"
                        ],
                        "name": "M. Attik",
                        "slug": "M.-Attik",
                        "structuredName": {
                            "firstName": "Mohammed",
                            "lastName": "Attik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Attik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 76
                            }
                        ],
                        "text": ", 2006; Schwenk & Koehn, 2008), reaching training sizes of up to 500m words (Schwenk et al., 2012)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 235,
                                "start": 215
                            }
                        ],
                        "text": "Rescoring lattices with a CSLM proved to be beneficial for ASR (Schwenk, 2004) and was subsequently applied to translation (Schwenk et al., 2006; Schwenk & Koehn, 2008), reaching training sizes of up to 500m words (Schwenk et al., 2012)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7801390,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e1f27c00ead87b2bde0e1a4ef732677dd9b989ff",
            "isKey": false,
            "numCitedBy": 102,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Language models play an important role in large vocabulary speech recognition and statistical machine translation systems. The dominant approach since several decades are back-off language models. Some years ago, there was a clear tendency to build huge language models trained on hundreds of billions of words. Lately, this tendency has changed and recent works concentrate on data selection. Continuous space methods are a very competitive approach, but they have a high computational complexity and are not yet in widespread use. This paper presents an experimental comparison of all these approaches on a large statistical machine translation task. We also describe an open-source implementation to train and use continuous space language models (CSLM) for such large tasks. We describe an efficient implementation of the CSLM using graphical processing units from Nvidia. By these means, we are able to train an CSLM on more than 500 million words in 20 hours. This CSLM provides an improvement of up to 1.8 BLEU points with respect to the best back-off language model that we were able to build."
            },
            "slug": "Large,-Pruned-or-Continuous-Space-Language-Models-a-Schwenk-Rousseau",
            "title": {
                "fragments": [],
                "text": "Large, Pruned or Continuous Space Language Models on a GPU for Statistical Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "An experimental comparison of all the approaches to train and use continuous space language models (CSLM) on a large statistical machine translation task and an efficient implementation of the CSLM using graphical processing units from Nvidia is described."
            },
            "venue": {
                "fragments": [],
                "text": "WLM@NAACL-HLT"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2784529"
                        ],
                        "name": "H. Le",
                        "slug": "H.-Le",
                        "structuredName": {
                            "firstName": "Hai",
                            "lastName": "Le",
                            "middleNames": [
                                "Son"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Le"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3135839"
                        ],
                        "name": "I. Oparin",
                        "slug": "I.-Oparin",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Oparin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Oparin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2311059"
                        ],
                        "name": "A. Allauzen",
                        "slug": "A.-Allauzen",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Allauzen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Allauzen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685010"
                        ],
                        "name": "J. Gauvain",
                        "slug": "J.-Gauvain",
                        "structuredName": {
                            "firstName": "Jean-Luc",
                            "lastName": "Gauvain",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Gauvain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1846431"
                        ],
                        "name": "Fran\u00e7ois Yvon",
                        "slug": "Fran\u00e7ois-Yvon",
                        "structuredName": {
                            "firstName": "Fran\u00e7ois",
                            "lastName": "Yvon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fran\u00e7ois Yvon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 245,
                                "start": 230
                            }
                        ],
                        "text": "Other methods for achieving more drastic complexity reductions exist in the form of frequency-based truncation, shortlists (Schwenk, 2004), or casting the vocabulary as a full hierarchy (Mnih & Hinton, 2008) or partial hierarchy (Le et al., 2011)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14828669,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3aaa1e4974800767fcbd2c24c2f2af42bf412f97",
            "isKey": false,
            "numCitedBy": 157,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper introduces a new neural network language model (NNLM) based on word clustering to structure the output vocabulary: Structured Output Layer NNLM. This model is able to handle vocabularies of arbitrary size, hence dispensing with the design of short-lists that are commonly used in NNLMs. Several softmax layers replace the standard output layer in this model. The output structure depends on the word clustering which uses the continuous word representation induced by a NNLM. The GALE Mandarin data was used to carry out the speech-to-text experiments and evaluate the NNLMs. On this data the well tuned baseline system has a character error rate under 10%. Our model achieves consistent improvements over the combination of an n-gram model and classical short-list NNLMs both in terms of perplexity and recognition accuracy."
            },
            "slug": "Structured-Output-Layer-neural-network-language-Le-Oparin",
            "title": {
                "fragments": [],
                "text": "Structured Output Layer neural network language model"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "A new neural network language model (NNLM) based on word clustering to structure the output vocabulary: Structured Output Layer NNLM, able to handle vocabularies of arbitrary size, hence dispensing with the design of short-lists that are commonly used in NNLMs."
            },
            "venue": {
                "fragments": [],
                "text": "2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748118"
                        ],
                        "name": "J. Bilmes",
                        "slug": "J.-Bilmes",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Bilmes",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bilmes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783839"
                        ],
                        "name": "Katrin Kirchhoff",
                        "slug": "Katrin-Kirchhoff",
                        "structuredName": {
                            "firstName": "Katrin",
                            "lastName": "Kirchhoff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Katrin Kirchhoff"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 29
                            }
                        ],
                        "text": "Conventional back-off n-gram LMs (Chen & Goodman, 1998) and the increasingly popular vector-based LMs (Bengio et al., 2003; Schwenk et al., 2006; Mikolov et al., 2010) use parametrisations that do not explicitly encode morphological regularities among related forms, like abstract, abstraction and abstracted."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 58
                            }
                        ],
                        "text": "The primary outcomes are that (i) our morphology-guided CSLMs improve intrinsic language model performance when compared to baseline CSLMs and n-gram MKN models; (ii) word and morpheme representations learnt in the process compare favourably in terms of a word similarity task to a recent more complex model that used more data, while obtaining large gains on some languages; (iii) machine translation quality as measured by BLEU was improved consistently across six language pairs when using CSLMs during decoding, although the morphology-based representations led to further improvements beyond the level of optimiser variance only for English\u2192Czech."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 6
                            }
                        ],
                        "text": "The CSLMs are integrated directly into the decoder as an additional feature function, thus exercising a stronger influence on the search than in n-best list rescoring.11 Translation model feature weights are tuned with MERT (Och, 2003) on newstest2012."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 21
                            }
                        ],
                        "text": "Using unnormalised CSLMs during first-pass decoding has generated improvements in BLEU score for translation into English (Vaswani et al., 2013)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 28
                            }
                        ],
                        "text": "The key obstacle to using CSLMs in a decoder is the expensive normalisation over the vocabulary."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 69
                            }
                        ],
                        "text": "Log-bilinear (LBL) models (Mnih & Hinton, 2007) are an instance of CSLMs that make the same Markov assumption as n-gram language models."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 58
                            }
                        ],
                        "text": "A variety of strategies have been explored for bringing CSLMs to bear on machine translation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 270,
                                "start": 267
                            }
                        ],
                        "text": "To the best of our knowledge, this\n10 ES WS353 (Hassan & Mihalcea, 2009); Gur350 (Gurevych, 2005); RG65 (Rubenstein & Goodenough, 1965) with FR (Joubarne & Inkpen, 2011); ZG222 (Zesch & Gurevych, 2006).\nis the first study to investigate large vocabulary normalised CSLMs inside a decoder when translating into a range of morphologically rich languages."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 57
                            }
                        ],
                        "text": "This work focuses on continuous space language models (CSLMs), an umbrella term for the LMs that represent words with real-valued vectors."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 122
                            }
                        ],
                        "text": "The proliferation of word forms in morphologically rich languages presents challenges to the statistical language models (LMs) that play a key role in machine translation and speech recognition."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 117
                            }
                        ],
                        "text": "Factored language models (FLMs) have been used to integrate morphological information into both discrete n-gram LMs (Bilmes & Kirchhoff, 2003) and CSLMs (Alexandrescu & Kirchhoff, 2006) by viewing a word as a set of factors."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8457271,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c20b4068be640ebaffbc56382c3e4e0bcf62664e",
            "isKey": true,
            "numCitedBy": 337,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce factored language models (FLMs) and generalized parallel backoff (GPB). An FLM represents words as bundles of features (e.g., morphological classes, stems, data-driven clusters, etc.), and induces a probability model covering sequences of bundles rather than just words. GPB extends standard backoff to general conditional probability tables where variables might be heterogeneous types, where no obvious natural (temporal) backoff order exists, and where multiple dynamic backoff strategies are allowed. These methodologies were implemented during the JHU 2002 workshop as extensions to the SRI language modeling toolkit. This paper provides initial perplexity results on both CallHome Arabic and on Penn Treebank Wall Street Journal articles. Significantly, FLMs with GPB can produce bigrams with significantly lower perplexity, sometimes lower than highly-optimized baseline trigrams. In a multi-pass speech recognition context, where bigrams are used to create first-pass bigram lattices or N-best lists, these results are highly relevant."
            },
            "slug": "Factored-Language-Models-and-Generalized-Parallel-Bilmes-Kirchhoff",
            "title": {
                "fragments": [],
                "text": "Factored Language Models and Generalized Parallel Backoff"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Initial perplexity results on both CallHome Arabic and on Penn Treebank Wall Street Journal articles are provided, and FLMs with GPB can produce bigrams with significantly lower perplexity, sometimes lower than highly-optimized baseline trigrams."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714004"
                        ],
                        "name": "A. Mnih",
                        "slug": "A.-Mnih",
                        "structuredName": {
                            "firstName": "Andriy",
                            "lastName": "Mnih",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Mnih"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 60
                            }
                        ],
                        "text": "This is executed in the context of a log-bilinear (LBL) LM (Mnih & Hinton, 2007), which is sped up sufficiently by the use of word classing so that we can integrate the model into an open source machine translation decoder1 and evaluate its impact on translation into 6 languages, including the\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 27
                            }
                        ],
                        "text": "Log-bilinear (LBL) models (Mnih & Hinton, 2007) are an instance of CSLMs that make the same Markov assumption as n-gram language models."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 577005,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bd7d93193aad6c4b71cc8942e808753019e87706",
            "isKey": false,
            "numCitedBy": 606,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "The supremacy of n-gram models in statistical language modelling has recently been challenged by parametric models that use distributed representations to counteract the difficulties caused by data sparsity. We propose three new probabilistic language models that define the distribution of the next word in a sequence given several preceding words by using distributed representations of those words. We show how real-valued distributed representations for words can be learned at the same time as learning a large set of stochastic binary hidden features that are used to predict the distributed representation of the next word from previous distributed representations. Adding connections from the previous states of the binary hidden features improves performance as does adding direct connections between the real-valued distributed representations. One of our models significantly outperforms the very best n-gram models."
            },
            "slug": "Three-new-graphical-models-for-statistical-language-Mnih-Hinton",
            "title": {
                "fragments": [],
                "text": "Three new graphical models for statistical language modelling"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is shown how real-valued distributed representations for words can be learned at the same time as learning a large set of stochastic binary hidden features that are used to predict the distributed representation of the next word from previous distributed representations."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '07"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51012238"
                        ],
                        "name": "H. Schwenk",
                        "slug": "H.-Schwenk",
                        "structuredName": {
                            "firstName": "Holger",
                            "lastName": "Schwenk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Schwenk"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 123
                            }
                        ],
                        "text": "Other methods for achieving more drastic complexity reductions exist in the form of frequency-based truncation, shortlists (Schwenk, 2004), or casting the vocabulary as a full hierarchy (Mnih & Hinton, 2008) or partial hierarchy (Le et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 124
                            }
                        ],
                        "text": "Other methods for achieving more drastic complexity reductions exist in the form of frequency-based truncation, shortlists (Schwenk, 2004), or casting the vocabulary as a full hierarchy (Mnih & Hinton, 2008) or partial hierarchy (Le et al., 2011)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 63
                            }
                        ],
                        "text": "Rescoring lattices with a CSLM proved to be beneficial for ASR (Schwenk, 2004) and was subsequently applied to translation (Schwenk et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 64
                            }
                        ],
                        "text": "Rescoring lattices with a CSLM proved to be beneficial for ASR (Schwenk, 2004) and was subsequently applied to translation (Schwenk et al., 2006; Schwenk & Koehn, 2008), reaching training sizes of up to 500m words (Schwenk et al., 2012)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16930073,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d6fb7546a29320eadad868af66835059db93d99f",
            "isKey": true,
            "numCitedBy": 38,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently there has been increasing interest in using neural networks for language modeling. In contrast to the well-known backoff n-gram language models, the neural network approach tries to limit the data sparseness problem by performing the estimation in a continuous space, allowing by this means smooth interpolations. The complexity to train such a model and to calculate one n-gram probability is however several orders of magnitude higher than for the backoff models, making the new approach difficult to use in real applications. In this paper several techniques are presented that allow the use of a neural network language model in a large vocabulary speech recognition system, in particular very, fast lattice rescoring and efficient training of large neural networks on training corpora of over 10 million words. The described approach achieves significant word error reductions with respect to a carefully tuned 4-gram backoff language model in a state of the art conversational speech recognizer for the DARPA rich transcriptions evaluations."
            },
            "slug": "Efficient-training-of-large-neural-networks-for-Schwenk",
            "title": {
                "fragments": [],
                "text": "Efficient training of large neural networks for language modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The described approach achieves significant word error reductions with respect to a carefully tuned 4-gram backoff language model in a state of the art conversational speech recognizer for the DARPA rich transcriptions evaluations."
            },
            "venue": {
                "fragments": [],
                "text": "2004 IEEE International Joint Conference on Neural Networks (IEEE Cat. No.04CH37541)"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2939803"
                        ],
                        "name": "Ronan Collobert",
                        "slug": "Ronan-Collobert",
                        "structuredName": {
                            "firstName": "Ronan",
                            "lastName": "Collobert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronan Collobert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 153
                            }
                        ],
                        "text": "Moreover, the csmRNNs were initialised with high-quality, publicly available word embeddings trained over weeks on much larger corpora of 630\u2013990m words (Collobert & Weston, 2008; Huang et al., 2012), in contrast to ours that are trained from scratch on much less data."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2617020,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "57458bc1cffe5caa45a885af986d70f723f406b4",
            "isKey": false,
            "numCitedBy": 5025,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a single convolutional neural network architecture that, given a sentence, outputs a host of language processing predictions: part-of-speech tags, chunks, named entity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense (grammatically and semantically) using a language model. The entire network is trained jointly on all these tasks using weight-sharing, an instance of multitask learning. All the tasks use labeled data except the language model which is learnt from unlabeled text and represents a novel form of semi-supervised learning for the shared tasks. We show how both multitask learning and semi-supervised learning improve the generalization of the shared tasks, resulting in state-of-the-art-performance."
            },
            "slug": "A-unified-architecture-for-natural-language-deep-Collobert-Weston",
            "title": {
                "fragments": [],
                "text": "A unified architecture for natural language processing: deep neural networks with multitask learning"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "This work describes a single convolutional neural network architecture that, given a sentence, outputs a host of language processing predictions: part-of-speech tags, chunks, named entity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense using a language model."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '08"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32538203"
                        ],
                        "name": "P. Brown",
                        "slug": "P.-Brown",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Brown",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Brown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39944066"
                        ],
                        "name": "V. D. Pietra",
                        "slug": "V.-D.-Pietra",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Pietra",
                            "middleNames": [
                                "J.",
                                "Della"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. D. Pietra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144856857"
                        ],
                        "name": "P. D. Souza",
                        "slug": "P.-D.-Souza",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Souza",
                            "middleNames": [
                                "V.",
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. D. Souza"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3853032"
                        ],
                        "name": "J. Lai",
                        "slug": "J.-Lai",
                        "structuredName": {
                            "firstName": "Jennifer",
                            "lastName": "Lai",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2474650"
                        ],
                        "name": "R. Mercer",
                        "slug": "R.-Mercer",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Mercer",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mercer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 23
                            }
                        ],
                        "text": "Using Brownclustering (Brown et al., 1992),3 we partition the vocabulary into |C| classes, denoting as Cc the set of vocabulary items in class c, such that V = C1 \u222a \u00b7 \u00b7 \u00b7 \u222a C|C|."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10986188,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3de5d40b60742e3dfa86b19e7f660962298492af",
            "isKey": false,
            "numCitedBy": 3318,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the problem of predicting a word from previous words in a sample of text. In particular, we discuss n-gram models based on classes of words. We also discuss several statistical algorithms for assigning words to classes based on the frequency of their co-occurrence with other words. We find that we are able to extract classes that have the flavor of either syntactically based groupings or semantically based groupings, depending on the nature of the underlying statistics."
            },
            "slug": "Class-Based-n-gram-Models-of-Natural-Language-Brown-Pietra",
            "title": {
                "fragments": [],
                "text": "Class-Based n-gram Models of Natural Language"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "This work addresses the problem of predicting a word from previous words in a sample of text and discusses n-gram models based on classes of words, finding that these models are able to extract classes that have the flavor of either syntactically based groupings or semanticallybased groupings, depending on the nature of the underlying statistics."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2544109"
                        ],
                        "name": "Colette Joubarne",
                        "slug": "Colette-Joubarne",
                        "structuredName": {
                            "firstName": "Colette",
                            "lastName": "Joubarne",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Colette Joubarne"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697366"
                        ],
                        "name": "D. Inkpen",
                        "slug": "D.-Inkpen",
                        "structuredName": {
                            "firstName": "Diana",
                            "lastName": "Inkpen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Inkpen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 145
                            }
                        ],
                        "text": "To the best of our knowledge, this\n10 ES WS353 (Hassan & Mihalcea, 2009); Gur350 (Gurevych, 2005); RG65 (Rubenstein & Goodenough, 1965) with FR (Joubarne & Inkpen, 2011); ZG222 (Zesch & Gurevych, 2006).\nis the first study to investigate large vocabulary normalised CSLMs inside a decoder when\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6425853,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5d8b687058a0519a47f673dd88d1dc2856571286",
            "isKey": false,
            "numCitedBy": 46,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Despite the growth in digitization of data, there are still many languages without sufficient corpora to achieve valid measures of semantic similarity. If it could be shown that manually-assigned similarity scores from one language can be transferred to another language, then semantic similarity values could be used for languages with fewer resources. We test an automatic word similarity measure based on second-order co-occurrences in the Google ngram corpus, for English, German, and French. We show that the scores manually-assigned in the experiments of Rubenstein and Goodenough's for 65 English word pairs can be transferred directly into German and French. We do this by conducting human evaluation experiments for French word pairs (and by using similarly produced scores for German). We show that the correlation between the automatically-assigned semantic similarity scores and the scores assigned by human evaluators is not very different when using the Rubenstein and Goodenough's scores across language, compared to the language-specific scores."
            },
            "slug": "Comparison-of-Semantic-Similarity-for-Different-the-Joubarne-Inkpen",
            "title": {
                "fragments": [],
                "text": "Comparison of Semantic Similarity for Different Languages Using the Google n-gram Corpus and Second-Order Co-occurrence Measures"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "An automatic word similarity measure based on second-order co-occurrences in the Google ngram corpus is test, showing that the scores manually-assigned in the experiments of Rubenstein and Goodenough's for 65 English word pairs can be transferred directly into French."
            },
            "venue": {
                "fragments": [],
                "text": "Canadian Conference on AI"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1762744"
                        ],
                        "name": "A. Stolcke",
                        "slug": "A.-Stolcke",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Stolcke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Stolcke"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 72
                            }
                        ],
                        "text": "This includes a baseline 4-gram MKN language model, trained with SRILM (Stolcke, 2002) and queried efficiently using KenLM (Heafield, 2011)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1988103,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "399da68d3b97218b6c80262df7963baa89dcc71b",
            "isKey": false,
            "numCitedBy": 4998,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "SRILM is a collection of C++ libraries, executable programs, and helper scripts designed to allow both production of and experimentation with statistical language models for speech recognition and other applications. SRILM is freely available for noncommercial purposes. The toolkit supports creation and evaluation of a variety of language model types based on N-gram statistics, as well as several related tasks, such as statistical tagging and manipulation of N-best lists and word lattices. This paper summarizes the functionality of the toolkit and discusses its design and implementation, highlighting ease of rapid prototyping, reusability, and combinability of tools."
            },
            "slug": "SRILM-an-extensible-language-modeling-toolkit-Stolcke",
            "title": {
                "fragments": [],
                "text": "SRILM - an extensible language modeling toolkit"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "The functionality of the SRILM toolkit is summarized and its design and implementation is discussed, highlighting ease of rapid prototyping, reusability, and combinability of tools."
            },
            "venue": {
                "fragments": [],
                "text": "INTERSPEECH"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2002316"
                        ],
                        "name": "F. Och",
                        "slug": "F.-Och",
                        "structuredName": {
                            "firstName": "Franz",
                            "lastName": "Och",
                            "middleNames": [
                                "Josef"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Och"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 155
                            }
                        ],
                        "text": "On the other languages, the CLBL adds 0.5 to 1 BLEU points over the baseline, whereas additional improvement from the additive representations lies within MERT variance except for EN\u2192CS."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 234,
                                "start": 225
                            }
                        ],
                        "text": "The CSLMs are integrated directly into the decoder as an additional feature function, thus exercising a stronger influence on the search than in n-best list rescoring.11 Translation model feature weights are tuned with MERT (Och, 2003) on newstest2012."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 57
                            }
                        ],
                        "text": "11 Translation model feature weights are tuned with MERT (Och, 2003) on newstest2012."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5474833,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1f12451245667a85d0ee225a80880fc93c71cc8b",
            "isKey": true,
            "numCitedBy": 3304,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Often, the training procedure for statistical machine translation models is based on maximum likelihood or related criteria. A general problem of this approach is that there is only a loose relation to the final translation quality on unseen text. In this paper, we analyze various training criteria which directly optimize translation quality. These training criteria make use of recently proposed automatic evaluation metrics. We describe a new algorithm for efficient training an unsmoothed error count. We show that significantly better results can often be obtained if the final evaluation criterion is taken directly into account as part of the training procedure."
            },
            "slug": "Minimum-Error-Rate-Training-in-Statistical-Machine-Och",
            "title": {
                "fragments": [],
                "text": "Minimum Error Rate Training in Statistical Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown that significantly better results can often be obtained if the final evaluation criterion is taken directly into account as part of the training procedure."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702066"
                        ],
                        "name": "Kenneth Heafield",
                        "slug": "Kenneth-Heafield",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Heafield",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenneth Heafield"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 124
                            }
                        ],
                        "text": "This includes a baseline 4-gram MKN language model, trained with SRILM (Stolcke, 2002) and queried efficiently using KenLM (Heafield, 2011)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8313873,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "883d1d06d857a85a0e64bb19f0b17d56f2cc9d7b",
            "isKey": false,
            "numCitedBy": 1174,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and memory costs. The Probing data structure uses linear probing hash tables and is designed for speed. Compared with the widely-used SRILM, our Probing model is 2.4 times as fast while using 57% of the memory. The Trie data structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed at lower memory consumption. Trie simultaneously uses less memory than the smallest lossless baseline and less CPU than the fastest baseline. Our code is open-source, thread-safe, and integrated into the Moses, cdec, and Joshua translation systems. This paper describes the several performance techniques used and presents benchmarks against alternative implementations."
            },
            "slug": "KenLM:-Faster-and-Smaller-Language-Model-Queries-Heafield",
            "title": {
                "fragments": [],
                "text": "KenLM: Faster and Smaller Language Model Queries"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "KenLM is a library that implements two data structures for efficient language model queries, reducing both time and memory costs and is integrated into the Moses, cdec, and Joshua translation systems."
            },
            "venue": {
                "fragments": [],
                "text": "WMT@EMNLP"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730400"
                        ],
                        "name": "Iryna Gurevych",
                        "slug": "Iryna-Gurevych",
                        "structuredName": {
                            "firstName": "Iryna",
                            "lastName": "Gurevych",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Iryna Gurevych"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 46
                            }
                        ],
                        "text": "10 ES WS353 (Hassan & Mihalcea, 2009); Gur350 (Gurevych, 2005); RG65 (Rubenstein & Goodenough, 1965) with FR (Joubarne & Inkpen, 2011); ZG222 (Zesch & Gurevych, 2006)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 82
                            }
                        ],
                        "text": "To the best of our knowledge, this\n10 ES WS353 (Hassan & Mihalcea, 2009); Gur350 (Gurevych, 2005); RG65 (Rubenstein & Goodenough, 1965) with FR (Joubarne & Inkpen, 2011); ZG222 (Zesch & Gurevych, 2006).\nis the first study to investigate large vocabulary normalised CSLMs inside a decoder when\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 74
                            }
                        ],
                        "text": "To the best of our knowledge, this\n10 ES WS353 (Hassan & Mihalcea, 2009); Gur350 (Gurevych, 2005); RG65 (Rubenstein & Goodenough, 1965) with FR (Joubarne & Inkpen, 2011); ZG222 (Zesch & Gurevych, 2006).\nis the first study to investigate large vocabulary normalised CSLMs inside a decoder when translating into a range of morphologically rich languages."
                    },
                    "intents": []
                }
            ],
            "corpusId": 16584933,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2ebe88fab46e53e980b5aef28c414c04d133fc6f",
            "isKey": false,
            "numCitedBy": 130,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new method for computing semantic relatedness of concepts. The method relies solely on the structure of a conceptual network and eliminates the need for performing additional corpus analysis. The network structure is employed to generate artificial conceptual glosses. They replace textual definitions proper written by humans and are processed by a dictionary based metric of semantic relatedness [1]. We implemented the metric on the basis of GermaNet, the German counterpart of WordNet, and evaluated the results on a German dataset of 57 word pairs rated by human subjects for their semantic relatedness. Our approach can be easily applied to compute semantic relatedness based on alternative conceptual networks, e.g. in the domain of life sciences."
            },
            "slug": "Using-the-Structure-of-a-Conceptual-Network-in-Gurevych",
            "title": {
                "fragments": [],
                "text": "Using the Structure of a Conceptual Network in Computing Semantic Relatedness"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "The method relies solely on the structure of a conceptual network and eliminates the need for performing additional corpus analysis and can be easily applied to compute semantic relatedness based on alternative conceptual networks, e.g. in the domain of life sciences."
            },
            "venue": {
                "fragments": [],
                "text": "IJCNLP"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145920653"
                        ],
                        "name": "Samer Hassan",
                        "slug": "Samer-Hassan",
                        "structuredName": {
                            "firstName": "Samer",
                            "lastName": "Hassan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Samer Hassan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145557251"
                        ],
                        "name": "Rada Mihalcea",
                        "slug": "Rada-Mihalcea",
                        "structuredName": {
                            "firstName": "Rada",
                            "lastName": "Mihalcea",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rada Mihalcea"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 48
                            }
                        ],
                        "text": "To the best of our knowledge, this\n10 ES WS353 (Hassan & Mihalcea, 2009); Gur350 (Gurevych, 2005); RG65 (Rubenstein & Goodenough, 1965) with FR (Joubarne & Inkpen, 2011); ZG222 (Zesch & Gurevych, 2006).\nis the first study to investigate large vocabulary normalised CSLMs inside a decoder when\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 33
                            }
                        ],
                        "text": "But also on the standard English WS353 dataset (Finkelstein et al., 2002), we get a 26% better correlation with the human ratings."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 41
                            }
                        ],
                        "text": "To the best of our knowledge, this\n10 ES WS353 (Hassan & Mihalcea, 2009); Gur350 (Gurevych, 2005); RG65 (Rubenstein & Goodenough, 1965) with FR (Joubarne & Inkpen, 2011); ZG222 (Zesch & Gurevych, 2006).\nis the first study to investigate large vocabulary normalised CSLMs inside a decoder when translating into a range of morphologically rich languages."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1856431,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e4443628698e77b7c25f61410cc45384dee80a99",
            "isKey": false,
            "numCitedBy": 136,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we address the task of crosslingual semantic relatedness. We introduce a method that relies on the information extracted from Wikipedia, by exploiting the interlanguage links available between Wikipedia versions in multiple languages. Through experiments performed on several language pairs, we show that the method performs well, with a performance comparable to monolingual measures of relatedness."
            },
            "slug": "Cross-lingual-Semantic-Relatedness-Using-Knowledge-Hassan-Mihalcea",
            "title": {
                "fragments": [],
                "text": "Cross-lingual Semantic Relatedness Using Encyclopedic Knowledge"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "This paper introduces a method that relies on the information extracted from Wikipedia, by exploiting the interlanguage links available between Wikipedia versions in multiple languages, that performs well, with a performance comparable to monolingual measures of relatedness."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50126864"
                        ],
                        "name": "Joshua Goodman",
                        "slug": "Joshua-Goodman",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Goodman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joshua Goodman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 131
                            }
                        ],
                        "text": "Our approach to reducing the computational cost of normalisation is to use a class-based decomposition of the probabilistic model (Goodman, 2001; Mikolov et al., 2011)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7284722,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4af41f4d838daa7ca6995aeb4918b61989d1ed80",
            "isKey": false,
            "numCitedBy": 233,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Maximum entropy models are considered by many to be one of the most promising avenues of language modeling research. Unfortunately, long training times make maximum entropy research difficult. We present a speedup technique: we change the form of the model to use classes. Our speedup works by creating two maximum entropy models, the first of which predicts the class of each word, and the second of which predicts the word itself. This factoring of the model leads to fewer nonzero indicator functions, and faster normalization, achieving speedups of up to a factor of 35 over one of the best previous techniques. It also results in typically slightly lower perplexities. The same trick can be used to speed training of other machine learning techniques, e.g. neural networks, applied to any problem with a large number of outputs, such as language modeling."
            },
            "slug": "Classes-for-fast-maximum-entropy-training-Goodman",
            "title": {
                "fragments": [],
                "text": "Classes for fast maximum entropy training"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper presents a speedup technique: changing the form of the model to use classes, which leads to fewer nonzero indicator functions, and faster normalization, achieving speedups of up to a factor of 35 over one of the best previous techniques."
            },
            "venue": {
                "fragments": [],
                "text": "2001 IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings (Cat. No.01CH37221)"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745899"
                        ],
                        "name": "Chris Dyer",
                        "slug": "Chris-Dyer",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Dyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris Dyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2730654"
                        ],
                        "name": "Victor Chahuneau",
                        "slug": "Victor-Chahuneau",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Chahuneau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Victor Chahuneau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144365875"
                        ],
                        "name": "Noah A. Smith",
                        "slug": "Noah-A.-Smith",
                        "structuredName": {
                            "firstName": "Noah",
                            "lastName": "Smith",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noah A. Smith"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 13
                            }
                        ],
                        "text": "We use cdec (Dyer et al., 2010; 2013) to build symmetric word-alignments and extract rules for hierarchical phrasebased translation (Chiang, 2007)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8476273,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7b5e31257f01aba987f16e175a3e49e00a5bd3bb",
            "isKey": false,
            "numCitedBy": 819,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a simple log-linear reparameterization of IBM Model 2 that overcomes problems arising from Model 1\u2019s strong assumptions and Model 2\u2019s overparameterization. Efficient inference, likelihood evaluation, and parameter estimation algorithms are provided. Training the model is consistently ten times faster than Model 4. On three large-scale translation tasks, systems built using our alignment model outperform IBM Model 4. An open-source implementation of the alignment model described in this paper is available from http://github.com/clab/fast align ."
            },
            "slug": "A-Simple,-Fast,-and-Effective-Reparameterization-of-Dyer-Chahuneau",
            "title": {
                "fragments": [],
                "text": "A Simple, Fast, and Effective Reparameterization of IBM Model 2"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A simple log-linear reparameterization of IBM Model 2 that overcomes problems arising from Model 1\u2019's strong assumptions and Model 2\u2019s overparameterization is presented."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1795942"
                        ],
                        "name": "Reinhard Kneser",
                        "slug": "Reinhard-Kneser",
                        "structuredName": {
                            "firstName": "Reinhard",
                            "lastName": "Kneser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Reinhard Kneser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145322333"
                        ],
                        "name": "H. Ney",
                        "slug": "H.-Ney",
                        "structuredName": {
                            "firstName": "Hermann",
                            "lastName": "Ney",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 109
                            }
                        ],
                        "text": "In contrast, discrete n-gram models are estimated by smoothing and backing off over empirical distributions (Kneser & Ney, 1995; Chen & Goodman, 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9685476,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9548ac30c113562a51e603dbbc8e9fa651cfd3ab",
            "isKey": false,
            "numCitedBy": 1792,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "In stochastic language modeling, backing-off is a widely used method to cope with the sparse data problem. In case of unseen events this method backs off to a less specific distribution. In this paper we propose to use distributions which are especially optimized for the task of backing-off. Two different theoretical derivations lead to distributions which are quite different from the probability distributions that are usually used for backing-off. Experiments show an improvement of about 10% in terms of perplexity and 5% in terms of word error rate."
            },
            "slug": "Improved-backing-off-for-M-gram-language-modeling-Kneser-Ney",
            "title": {
                "fragments": [],
                "text": "Improved backing-off for M-gram language modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper proposes to use distributions which are especially optimized for the task of back-off, which are quite different from the probability distributions that are usually used for backing-off."
            },
            "venue": {
                "fragments": [],
                "text": "1995 International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2047446108"
                        ],
                        "name": "Tomas Mikolov",
                        "slug": "Tomas-Mikolov",
                        "structuredName": {
                            "firstName": "Tomas",
                            "lastName": "Mikolov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tomas Mikolov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2245567"
                        ],
                        "name": "M. Karafi\u00e1t",
                        "slug": "M.-Karafi\u00e1t",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Karafi\u00e1t",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Karafi\u00e1t"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1816892"
                        ],
                        "name": "L. Burget",
                        "slug": "L.-Burget",
                        "structuredName": {
                            "firstName": "Luk\u00e1\u0161",
                            "lastName": "Burget",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Burget"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1899242"
                        ],
                        "name": "J. Cernock\u00fd",
                        "slug": "J.-Cernock\u00fd",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Cernock\u00fd",
                            "middleNames": [
                                "Honza"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cernock\u00fd"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2803071"
                        ],
                        "name": "S. Khudanpur",
                        "slug": "S.-Khudanpur",
                        "structuredName": {
                            "firstName": "Sanjeev",
                            "lastName": "Khudanpur",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Khudanpur"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 102
                            }
                        ],
                        "text": "Conventional back-off n-gram LMs (Chen & Goodman, 1998) and the increasingly popular vector-based LMs (Bengio et al., 2003; Schwenk et al., 2006; Mikolov et al., 2010) use parametrisations that do not explicitly encode morphological regularities among related forms, like abstract, abstraction and abstracted."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 58
                            }
                        ],
                        "text": "The primary outcomes are that (i) our morphology-guided CSLMs improve intrinsic language model performance when compared to baseline CSLMs and n-gram MKN models; (ii) word and morpheme representations learnt in the process compare favourably in terms of a word similarity task to a recent more complex model that used more data, while obtaining large gains on some languages; (iii) machine translation quality as measured by BLEU was improved consistently across six language pairs when using CSLMs during decoding, although the morphology-based representations led to further improvements beyond the level of optimiser variance only for English\u2192Czech."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 6
                            }
                        ],
                        "text": "The CSLMs are integrated directly into the decoder as an additional feature function, thus exercising a stronger influence on the search than in n-best list rescoring.11 Translation model feature weights are tuned with MERT (Och, 2003) on newstest2012."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 21
                            }
                        ],
                        "text": "Using unnormalised CSLMs during first-pass decoding has generated improvements in BLEU score for translation into English (Vaswani et al., 2013)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 28
                            }
                        ],
                        "text": "The key obstacle to using CSLMs in a decoder is the expensive normalisation over the vocabulary."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 69
                            }
                        ],
                        "text": "Log-bilinear (LBL) models (Mnih & Hinton, 2007) are an instance of CSLMs that make the same Markov assumption as n-gram language models."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 58
                            }
                        ],
                        "text": "A variety of strategies have been explored for bringing CSLMs to bear on machine translation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 270,
                                "start": 267
                            }
                        ],
                        "text": "To the best of our knowledge, this\n10 ES WS353 (Hassan & Mihalcea, 2009); Gur350 (Gurevych, 2005); RG65 (Rubenstein & Goodenough, 1965) with FR (Joubarne & Inkpen, 2011); ZG222 (Zesch & Gurevych, 2006).\nis the first study to investigate large vocabulary normalised CSLMs inside a decoder when translating into a range of morphologically rich languages."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 57
                            }
                        ],
                        "text": "This work focuses on continuous space language models (CSLMs), an umbrella term for the LMs that represent words with real-valued vectors."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 122
                            }
                        ],
                        "text": "The proliferation of word forms in morphologically rich languages presents challenges to the statistical language models (LMs) that play a key role in machine translation and speech recognition."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 146
                            }
                        ],
                        "text": "Conventional back-off n-gram LMs (Chen & Goodman, 1998) and the increasingly popular vector-based LMs (Bengio et al., 2003; Schwenk et al., 2006; Mikolov et al., 2010) use parametrisations that do not explicitly encode morphological regularities among related forms, like abstract, abstraction and\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 27
                            }
                        ],
                        "text": "Factored language models (FLMs) have been used to integrate morphological information into both discrete n-gram LMs (Bilmes & Kirchhoff, 2003) and CSLMs (Alexandrescu & Kirchhoff, 2006) by viewing a word as a set of factors."
                    },
                    "intents": []
                }
            ],
            "corpusId": 17048224,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9819b600a828a57e1cde047bbe710d3446b30da5",
            "isKey": true,
            "numCitedBy": 4902,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "A new recurrent neural network based language model (RNN LM) with applications to speech recognition is presented. Results indicate that it is possible to obtain around 50% reduction of perplexity by using mixture of several RNN LMs, compared to a state of the art backoff language model. Speech recognition experiments show around 18% reduction of word error rate on the Wall Street Journal task when comparing models trained on the same amount of data, and around 5% on the much harder NIST RT05 task, even when the backoff model is trained on much more data than the RNN LM. We provide ample empirical evidence to suggest that connectionist language models are superior to standard n-gram techniques, except their high computational (training) complexity. Index Terms: language modeling, recurrent neural networks, speech recognition"
            },
            "slug": "Recurrent-neural-network-based-language-model-Mikolov-Karafi\u00e1t",
            "title": {
                "fragments": [],
                "text": "Recurrent neural network based language model"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Results indicate that it is possible to obtain around 50% reduction of perplexity by using mixture of several RNN LMs, compared to a state of the art backoff language model."
            },
            "venue": {
                "fragments": [],
                "text": "INTERSPEECH"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780779"
                        ],
                        "name": "Torsten Zesch",
                        "slug": "Torsten-Zesch",
                        "structuredName": {
                            "firstName": "Torsten",
                            "lastName": "Zesch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Torsten Zesch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730400"
                        ],
                        "name": "Iryna Gurevych",
                        "slug": "Iryna-Gurevych",
                        "structuredName": {
                            "firstName": "Iryna",
                            "lastName": "Gurevych",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Iryna Gurevych"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 149
                            }
                        ],
                        "text": "\u2026this\n10 ES WS353 (Hassan & Mihalcea, 2009); Gur350 (Gurevych, 2005); RG65 (Rubenstein & Goodenough, 1965) with FR (Joubarne & Inkpen, 2011); ZG222 (Zesch & Gurevych, 2006).\nis the first study to investigate large vocabulary normalised CSLMs inside a decoder when translating into a range of\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 171
                            }
                        ],
                        "text": "To the best of our knowledge, this\n10 ES WS353 (Hassan & Mihalcea, 2009); Gur350 (Gurevych, 2005); RG65 (Rubenstein & Goodenough, 1965) with FR (Joubarne & Inkpen, 2011); ZG222 (Zesch & Gurevych, 2006).\nis the first study to investigate large vocabulary normalised CSLMs inside a decoder when translating into a range of morphologically rich languages."
                    },
                    "intents": []
                }
            ],
            "corpusId": 7898806,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e69b28380b4efc695a60054f85e8c46a9731c35b",
            "isKey": false,
            "numCitedBy": 77,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Semantic relatedness is a special form of linguistic distance between words. Evaluating semantic relatedness measures is usually performed by comparison with human judgments. Previous test datasets had been created analytically and were limited in size. We propose a corpus-based system for automatically creating test datasets. Experiments with human subjects show that the resulting datasets cover all degrees of relatedness. As a result of the corpus-based approach, test datasets cover all types of lexical-semantic relations and contain domain-specific words naturally occurring in texts."
            },
            "slug": "Automatically-Creating-Datasets-for-Measures-of-Zesch-Gurevych",
            "title": {
                "fragments": [],
                "text": "Automatically Creating Datasets for Measures of Semantic Relatedness"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A corpus-based system for automatically creating test datasets that cover all types of lexical-semantic relations and contain domain-specific words naturally occurring in texts is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "ACL 2006"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2047446108"
                        ],
                        "name": "Tomas Mikolov",
                        "slug": "Tomas-Mikolov",
                        "structuredName": {
                            "firstName": "Tomas",
                            "lastName": "Mikolov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tomas Mikolov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2675251"
                        ],
                        "name": "Stefan Kombrink",
                        "slug": "Stefan-Kombrink",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Kombrink",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stefan Kombrink"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1816892"
                        ],
                        "name": "L. Burget",
                        "slug": "L.-Burget",
                        "structuredName": {
                            "firstName": "Luk\u00e1\u0161",
                            "lastName": "Burget",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Burget"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1899242"
                        ],
                        "name": "J. Cernock\u00fd",
                        "slug": "J.-Cernock\u00fd",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Cernock\u00fd",
                            "middleNames": [
                                "Honza"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cernock\u00fd"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2803071"
                        ],
                        "name": "S. Khudanpur",
                        "slug": "S.-Khudanpur",
                        "structuredName": {
                            "firstName": "Sanjeev",
                            "lastName": "Khudanpur",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Khudanpur"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 94
                            }
                        ],
                        "text": "(3)In preliminary experiments, Brown clusters gave better perplexities than frequency-binning (Mikolov et al., 2011)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 146
                            }
                        ],
                        "text": "Our approach to reducing the computational cost of normalisation is to use a class-based decomposition of the probabilistic model (Goodman, 2001; Mikolov et al., 2011)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 93
                            }
                        ],
                        "text": "3In preliminary experiments, Brown clusters gave better perplexities than frequency-binning (Mikolov et al., 2011).\nscore \u03c4(c) = p \u00b7 sc + tc and word score \u03bd(w), which are normalised separately:\nP (c|h) = exp (\u03c4(c))\u2211|C| c\u2032=1 exp (\u03c4(c \u2032)) (4)\nP (w|h, c) = exp (\u03bd(w))\u2211 v\u2032\u2208Cc exp (\u03bd(v \u2032)) ."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14850173,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "07ca885cb5cc4328895bfaec9ab752d5801b14cd",
            "isKey": false,
            "numCitedBy": 1423,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We present several modifications of the original recurrent neural network language model (RNN LM).While this model has been shown to significantly outperform many competitive language modeling techniques in terms of accuracy, the remaining problem is the computational complexity. In this work, we show approaches that lead to more than 15 times speedup for both training and testing phases. Next, we show importance of using a backpropagation through time algorithm. An empirical comparison with feedforward networks is also provided. In the end, we discuss possibilities how to reduce the amount of parameters in the model. The resulting RNN model can thus be smaller, faster both during training and testing, and more accurate than the basic one."
            },
            "slug": "Extensions-of-recurrent-neural-network-language-Mikolov-Kombrink",
            "title": {
                "fragments": [],
                "text": "Extensions of recurrent neural network language model"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "Several modifications of the original recurrent neural network language model are presented, showing approaches that lead to more than 15 times speedup for both training and testing phases and possibilities how to reduce the amount of parameters in the model."
            },
            "venue": {
                "fragments": [],
                "text": "2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40360495"
                        ],
                        "name": "Helmut Schmid",
                        "slug": "Helmut-Schmid",
                        "structuredName": {
                            "firstName": "Helmut",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Helmut Schmid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1805808"
                        ],
                        "name": "Florian Laws",
                        "slug": "Florian-Laws",
                        "structuredName": {
                            "firstName": "Florian",
                            "lastName": "Laws",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Florian Laws"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 42
                            }
                        ],
                        "text": "We used the decision tree-based tagger of Schmid & Laws (2008)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8958786,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fdceda7acf76cdeb587121110c8c795d721d81e7",
            "isKey": false,
            "numCitedBy": 191,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags. It is based on three ideas: (1) splitting of the POS tags into attribute vectors and decomposition of the contextual POS probabilities of the HMM into a product of attribute probabilities, (2) estimation of the contextual probabilities with decision trees, and (3) use of high-order HMMs. In experiments on German and Czech data, our tagger outperformed state-of-the-art POS taggers."
            },
            "slug": "Estimation-of-Conditional-Probabilities-With-Trees-Schmid-Laws",
            "title": {
                "fragments": [],
                "text": "Estimation of Conditional Probabilities With Decision Trees and an Application to Fine-Grained POS Tagging"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "A HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags based on splitting of the POS tags into attribute vectors and decomposition of the contextual POS probabilities of the HMM into a product of attribute probabilities."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47709773"
                        ],
                        "name": "H. Rubenstein",
                        "slug": "H.-Rubenstein",
                        "structuredName": {
                            "firstName": "Herbert",
                            "lastName": "Rubenstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Rubenstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1898344"
                        ],
                        "name": "J. Goodenough",
                        "slug": "J.-Goodenough",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Goodenough",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Goodenough"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 105
                            }
                        ],
                        "text": "To the best of our knowledge, this\n10 ES WS353 (Hassan & Mihalcea, 2009); Gur350 (Gurevych, 2005); RG65 (Rubenstein & Goodenough, 1965) with FR (Joubarne & Inkpen, 2011); ZG222 (Zesch & Gurevych, 2006).\nis the first study to investigate large vocabulary normalised CSLMs inside a decoder when\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 99
                            }
                        ],
                        "text": "To the best of our knowledge, this\n10 ES WS353 (Hassan & Mihalcea, 2009); Gur350 (Gurevych, 2005); RG65 (Rubenstein & Goodenough, 1965) with FR (Joubarne & Inkpen, 2011); ZG222 (Zesch & Gurevych, 2006).\nis the first study to investigate large vocabulary normalised CSLMs inside a decoder when translating into a range of morphologically rich languages."
                    },
                    "intents": []
                }
            ],
            "corpusId": 18309234,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "7ef3ac14cdb484aaa2b039850093febd5cf73a21",
            "isKey": false,
            "numCitedBy": 1460,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Experimentol corroboration was obtained for the hypothesis that the proportion of words common to the contexts of word A and to the contexts of word B is a function of the degree to which A and B are similar in meaning. The tests were carried out for variously defined contexts. The shapes of the functions, however, indicate that similarity of context is reliable as criterion only for detecting pairs of words that are very similar in meaning."
            },
            "slug": "Contextual-correlates-of-synonymy-Rubenstein-Goodenough",
            "title": {
                "fragments": [],
                "text": "Contextual correlates of synonymy"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The shapes of the functions indicate that similarity of context is reliable as criterion only for detecting pairs of words that are very similar in meaning."
            },
            "venue": {
                "fragments": [],
                "text": "CACM"
            },
            "year": 1965
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1643845523"
                        ],
                        "name": "F. ChenStanley",
                        "slug": "F.-ChenStanley",
                        "structuredName": {
                            "firstName": "F",
                            "lastName": "ChenStanley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. ChenStanley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1643789739"
                        ],
                        "name": "GoodmanJoshua",
                        "slug": "GoodmanJoshua",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "GoodmanJoshua",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "GoodmanJoshua"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 29
                            }
                        ],
                        "text": "Conventional back-off n-gram LMs (Chen & Goodman, 1998) and the increasingly popular vector-based LMs (Bengio et al., 2003; Schwenk et al., 2006; Mikolov et al., 2010) use parametrisations that do not explicitly encode morphological regularities among related forms, like abstract, abstraction and abstracted."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 58
                            }
                        ],
                        "text": "The primary outcomes are that (i) our morphology-guided CSLMs improve intrinsic language model performance when compared to baseline CSLMs and n-gram MKN models; (ii) word and morpheme representations learnt in the process compare favourably in terms of a word similarity task to a recent more complex model that used more data, while obtaining large gains on some languages; (iii) machine translation quality as measured by BLEU was improved consistently across six language pairs when using CSLMs during decoding, although the morphology-based representations led to further improvements beyond the level of optimiser variance only for English\u2192Czech."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 99
                            }
                        ],
                        "text": "In addition to contrasting the LBL variants, we also use modified Kneser-Ney n-gram models (MKNs) (Chen & Goodman, 1998) as baselines."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 6
                            }
                        ],
                        "text": "The CSLMs are integrated directly into the decoder as an additional feature function, thus exercising a stronger influence on the search than in n-best list rescoring.11 Translation model feature weights are tuned with MERT (Och, 2003) on newstest2012."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 129
                            }
                        ],
                        "text": "In contrast, discrete n-gram models are estimated by smoothing and backing off over empirical distributions (Kneser & Ney, 1995; Chen & Goodman, 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 21
                            }
                        ],
                        "text": "Using unnormalised CSLMs during first-pass decoding has generated improvements in BLEU score for translation into English (Vaswani et al., 2013)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 120
                            }
                        ],
                        "text": "Morpheme vectors from the CLBL++ enable handling OOV test words in a more nuanced way than using the global unknown word vector."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 28
                            }
                        ],
                        "text": "The key obstacle to using CSLMs in a decoder is the expensive normalisation over the vocabulary."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 69
                            }
                        ],
                        "text": "Log-bilinear (LBL) models (Mnih & Hinton, 2007) are an instance of CSLMs that make the same Markov assumption as n-gram language models."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 58
                            }
                        ],
                        "text": "A variety of strategies have been explored for bringing CSLMs to bear on machine translation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 270,
                                "start": 267
                            }
                        ],
                        "text": "To the best of our knowledge, this\n10 ES WS353 (Hassan & Mihalcea, 2009); Gur350 (Gurevych, 2005); RG65 (Rubenstein & Goodenough, 1965) with FR (Joubarne & Inkpen, 2011); ZG222 (Zesch & Gurevych, 2006).\nis the first study to investigate large vocabulary normalised CSLMs inside a decoder when translating into a range of morphologically rich languages."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 57
                            }
                        ],
                        "text": "This work focuses on continuous space language models (CSLMs), an umbrella term for the LMs that represent words with real-valued vectors."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 122
                            }
                        ],
                        "text": "The proliferation of word forms in morphologically rich languages presents challenges to the statistical language models (LMs) that play a key role in machine translation and speech recognition."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 34
                            }
                        ],
                        "text": "Conventional back-off n-gram LMs (Chen & Goodman, 1998) and the increasingly popular vector-based LMs (Bengio et al., 2003; Schwenk et al., 2006; Mikolov et al., 2010) use parametrisations that do not explicitly encode morphological regularities among related forms, like abstract, abstraction and\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 27
                            }
                        ],
                        "text": "Factored language models (FLMs) have been used to integrate morphological information into both discrete n-gram LMs (Bilmes & Kirchhoff, 2003) and CSLMs (Alexandrescu & Kirchhoff, 2006) by viewing a word as a set of factors."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Q and R imply that separate representations are used for conditioning and output."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 215842252,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d4e8bed3b50a035e1eabad614fe4218a34b3b178",
            "isKey": true,
            "numCitedBy": 2861,
            "numCiting": 87,
            "paperAbstract": {
                "fragments": [],
                "text": "We survey the most widely-used algorithms for smoothing models for language n -gram modeling. We then present an extensive empirical comparison of several of these smoothing techniques, including t..."
            },
            "slug": "An-empirical-study-of-smoothing-techniques-for-ChenStanley-GoodmanJoshua",
            "title": {
                "fragments": [],
                "text": "An empirical study of smoothing techniques for language modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "A survey of the most widely-used algorithms for smoothing models for language n -gram modeling and an extensive empirical comparison of several of these smoothing techniques are presented."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50064811"
                        ],
                        "name": "L. Finkelstein",
                        "slug": "L.-Finkelstein",
                        "structuredName": {
                            "firstName": "Lev",
                            "lastName": "Finkelstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Finkelstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1718798"
                        ],
                        "name": "E. Gabrilovich",
                        "slug": "E.-Gabrilovich",
                        "structuredName": {
                            "firstName": "Evgeniy",
                            "lastName": "Gabrilovich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Gabrilovich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745572"
                        ],
                        "name": "Y. Matias",
                        "slug": "Y.-Matias",
                        "structuredName": {
                            "firstName": "Y.",
                            "lastName": "Matias",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Matias"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747801"
                        ],
                        "name": "E. Rivlin",
                        "slug": "E.-Rivlin",
                        "structuredName": {
                            "firstName": "Ehud",
                            "lastName": "Rivlin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Rivlin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3316511"
                        ],
                        "name": "Zach Solan",
                        "slug": "Zach-Solan",
                        "structuredName": {
                            "firstName": "Zach",
                            "lastName": "Solan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zach Solan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2073936"
                        ],
                        "name": "G. Wolfman",
                        "slug": "G.-Wolfman",
                        "structuredName": {
                            "firstName": "Gadi",
                            "lastName": "Wolfman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wolfman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1779370"
                        ],
                        "name": "E. Ruppin",
                        "slug": "E.-Ruppin",
                        "structuredName": {
                            "firstName": "Eytan",
                            "lastName": "Ruppin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Ruppin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 48
                            }
                        ],
                        "text": "But also on the standard English WS353 dataset (Finkelstein et al., 2002), we get a 26% better correlation with the human ratings."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 12956853,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e0c01df98a6b633b25c96c1a99b713ac96f1c5be",
            "isKey": false,
            "numCitedBy": 1725,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "Keyword-based search engines are in widespread use today as a popular means for Web-based information retrieval. Although such systems seem deceptively simple, a considerable amount of skill is required in order to satisfy non-trivial information needs. This paper presents a new conceptual paradigm for performing search in context, that largely automates the search process, providing even non-professional users with highly relevant results. This paradigm is implemented in practice in the IntelliZap system, where search is initiated from a text query marked by the user in a document she views, and is guided by the text surrounding the marked query in that document (\"the context\"). The context-driven information retrieval process involves semantic keyword extraction and clustering to automatically generate new, augmented queries. The latter are submitted to a host of general and domain-specific search engines. Search results are then semantically reranked, using context. Experimental results testify that using context to guide search, effectively offers even inexperienced users an advanced search tool on the Web."
            },
            "slug": "Placing-search-in-context:-the-concept-revisited-Finkelstein-Gabrilovich",
            "title": {
                "fragments": [],
                "text": "Placing search in context: the concept revisited"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A new conceptual paradigm for performing search in context is presented, that largely automates the search process, providing even non-professional users with highly relevant results."
            },
            "venue": {
                "fragments": [],
                "text": "TOIS"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1803520"
                        ],
                        "name": "L. V. D. Maaten",
                        "slug": "L.-V.-D.-Maaten",
                        "structuredName": {
                            "firstName": "Laurens",
                            "lastName": "Maaten",
                            "middleNames": [
                                "van",
                                "der"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. V. D. Maaten"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5855042,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1c46943103bd7b7a2c7be86859995a4144d1938b",
            "isKey": false,
            "numCitedBy": 22376,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new technique called \u201ct-SNE\u201d that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images of objects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large datasets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of datasets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualizations produced by t-SNE are significantly better than those produced by the other techniques on almost all of the datasets."
            },
            "slug": "Visualizing-Data-using-t-SNE-Maaten-Hinton",
            "title": {
                "fragments": [],
                "text": "Visualizing Data using t-SNE"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "A new technique called t-SNE that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map, a variation of Stochastic Neighbor Embedding that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145992652"
                        ],
                        "name": "Michael U Gutmann",
                        "slug": "Michael-U-Gutmann",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Gutmann",
                            "middleNames": [
                                "U"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael U Gutmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791548"
                        ],
                        "name": "A. Hyv\u00e4rinen",
                        "slug": "A.-Hyv\u00e4rinen",
                        "structuredName": {
                            "firstName": "Aapo",
                            "lastName": "Hyv\u00e4rinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hyv\u00e4rinen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 66
                            }
                        ],
                        "text": "For the classless LBLs we use noise-contrastive estimation (NCE) (Gutmann & Hyva\u0308rinen, 2012; Mnih & Teh, 2012) to avoid normalisation during training."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11583904,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "23a99224c7b7d148675c1798c796ec1b0904620a",
            "isKey": false,
            "numCitedBy": 595,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the task of estimating, from observed data, a probabilistic model that is parameterized by a finite number of parameters. In particular, we are considering the situation where the model probability density function is unnormalized. That is, the model is only specified up to the partition function. The partition function normalizes a model so that it integrates to one for any choice of the parameters. However, it is often impossible to obtain it in closed form. Gibbs distributions, Markov and multi-layer networks are examples of models where analytical normalization is often impossible. Maximum likelihood estimation can then not be used without resorting to numerical approximations which are often computationally expensive. We propose here a new objective function for the estimation of both normalized and unnormalized models. The basic idea is to perform nonlinear logistic regression to discriminate between the observed data and some artificially generated noise. With this approach, the normalizing partition function can be estimated like any other parameter. We prove that the new estimation method leads to a consistent (convergent) estimator of the parameters. For large noise sample sizes, the new estimator is furthermore shown to behave like the maximum likelihood estimator. In the estimation of unnormalized models, there is a trade-off between statistical and computational performance. We show that the new method strikes a competitive trade-off in comparison to other estimation methods for unnormalized models. As an application to real data, we estimate novel two-layer models of natural image statistics with spline nonlinearities."
            },
            "slug": "Noise-Contrastive-Estimation-of-Unnormalized-with-Gutmann-Hyv\u00e4rinen",
            "title": {
                "fragments": [],
                "text": "Noise-Contrastive Estimation of Unnormalized Statistical Models, with Applications to Natural Image Statistics"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The basic idea is to perform nonlinear logistic regression to discriminate between the observed data and some artificially generated noise and it is shown that the new method strikes a competitive trade-off in comparison to other estimation methods for unnormalized models."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734693"
                        ],
                        "name": "John C. Duchi",
                        "slug": "John-C.-Duchi",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Duchi",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John C. Duchi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34840427"
                        ],
                        "name": "Elad Hazan",
                        "slug": "Elad-Hazan",
                        "structuredName": {
                            "firstName": "Elad",
                            "lastName": "Hazan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Elad Hazan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 17
                            }
                        ],
                        "text": "We apply AdaGrad (Duchi et al., 2011) and tune the stepsize \u03be on development data."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 18
                            }
                        ],
                        "text": "We apply AdaGrad (Duchi et al., 2011) and tune the stepsize \u03be on development data.4 We halt training once the perplexity on the development data starts to increase."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 538820,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "413c1142de9d91804d6d11c67ff3fed59c9fc279",
            "isKey": false,
            "numCitedBy": 8026,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms."
            },
            "slug": "Adaptive-Subgradient-Methods-for-Online-Learning-Duchi-Hazan",
            "title": {
                "fragments": [],
                "text": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work describes and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal functions that can be chosen in hindsight."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 29
                            }
                        ],
                        "text": "Conventional back-off n-gram LMs (Chen & Goodman, 1998) and the increasingly popular vector-based LMs (Bengio et al., 2003; Schwenk et al., 2006; Mikolov et al., 2010) use parametrisations that do not explicitly encode morphological regularities among related forms, like abstract, abstraction and abstracted."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 58
                            }
                        ],
                        "text": "The primary outcomes are that (i) our morphology-guided CSLMs improve intrinsic language model performance when compared to baseline CSLMs and n-gram MKN models; (ii) word and morpheme representations learnt in the process compare favourably in terms of a word similarity task to a recent more complex model that used more data, while obtaining large gains on some languages; (iii) machine translation quality as measured by BLEU was improved consistently across six language pairs when using CSLMs during decoding, although the morphology-based representations led to further improvements beyond the level of optimiser variance only for English\u2192Czech."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 6
                            }
                        ],
                        "text": "The CSLMs are integrated directly into the decoder as an additional feature function, thus exercising a stronger influence on the search than in n-best list rescoring.11 Translation model feature weights are tuned with MERT (Och, 2003) on newstest2012."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 21
                            }
                        ],
                        "text": "Using unnormalised CSLMs during first-pass decoding has generated improvements in BLEU score for translation into English (Vaswani et al., 2013)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 28
                            }
                        ],
                        "text": "The key obstacle to using CSLMs in a decoder is the expensive normalisation over the vocabulary."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 69
                            }
                        ],
                        "text": "Log-bilinear (LBL) models (Mnih & Hinton, 2007) are an instance of CSLMs that make the same Markov assumption as n-gram language models."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 58
                            }
                        ],
                        "text": "A variety of strategies have been explored for bringing CSLMs to bear on machine translation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 270,
                                "start": 267
                            }
                        ],
                        "text": "To the best of our knowledge, this\n10 ES WS353 (Hassan & Mihalcea, 2009); Gur350 (Gurevych, 2005); RG65 (Rubenstein & Goodenough, 1965) with FR (Joubarne & Inkpen, 2011); ZG222 (Zesch & Gurevych, 2006).\nis the first study to investigate large vocabulary normalised CSLMs inside a decoder when translating into a range of morphologically rich languages."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 57
                            }
                        ],
                        "text": "This work focuses on continuous space language models (CSLMs), an umbrella term for the LMs that represent words with real-valued vectors."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 122
                            }
                        ],
                        "text": "The proliferation of word forms in morphologically rich languages presents challenges to the statistical language models (LMs) that play a key role in machine translation and speech recognition."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 103
                            }
                        ],
                        "text": "Conventional back-off n-gram LMs (Chen & Goodman, 1998) and the increasingly popular vector-based LMs (Bengio et al., 2003; Schwenk et al., 2006; Mikolov et al., 2010) use parametrisations that do not explicitly encode morphological regularities among related forms, like abstract, abstraction and\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 27
                            }
                        ],
                        "text": "Factored language models (FLMs) have been used to integrate morphological information into both discrete n-gram LMs (Bilmes & Kirchhoff, 2003) and CSLMs (Alexandrescu & Kirchhoff, 2006) by viewing a word as a set of factors."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Neural Probabilistic Language Model. JMLR"
            },
            "venue": {
                "fragments": [],
                "text": "A Neural Probabilistic Language Model. JMLR"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 235,
                                "start": 215
                            }
                        ],
                        "text": "Rescoring lattices with a CSLM proved to be beneficial for ASR (Schwenk, 2004) and was subsequently applied to translation (Schwenk et al., 2006; Schwenk & Koehn, 2008), reaching training sizes of up to 500m words (Schwenk et al., 2012)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Pruned or Continuous Space Language Models on a GPU for Statistical Machine Translation"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. NAACL-HLT Workshop: On the Future of Language Modeling for HLT"
            },
            "year": 2012
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 22,
            "methodology": 30,
            "result": 2
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 47,
        "totalPages": 5
    },
    "page_url": "https://www.semanticscholar.org/paper/Compositional-Morphology-for-Word-Representations-Botha-Blunsom/46f418bf6fab132f193661226c5c27d67f870ea5?sort=total-citations"
}