{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145287425"
                        ],
                        "name": "David Chiang",
                        "slug": "David-Chiang",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Chiang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Chiang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 292,
                                "start": 280
                            }
                        ],
                        "text": "\u2026(if not optimal) results without careful tuning.\ncompared in terms of (1) time required for training; (2) alignment error rate (AER, lower is better);8 and (3) translation quality (BLEU, higher is better) of hierarchical phrase-based translation system that used the alignments (Chiang, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "\u2026below.\nequal to the value of h evaluated at the starting indices, j\u2191 and j\u2193; thus, the derivative we seek at each optimization iteration inside the M-step is:\n\u2207\u03bbL =Ep(ai|ei,f,m,n) [h(i, ai,m, n)]\n\u2212 1 Z\u03bb (tj\u2191(e \u03bbh(i,j\u2191,m,n), h(i, j\u2191,m, n), r, d) + tn\u2212j\u2193(e \u03bbh(i,j\u2193,m,n), h(i, j\u2191,m, n), r, d))."
                    },
                    "intents": []
                }
            ],
            "corpusId": 3505719,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0db6eb46ca9941660acc775e3ca39bf4434c18be",
            "isKey": false,
            "numCitedBy": 1299,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a statistical machine translation model that uses hierarchical phrasesphrases that contain subphrases. The model is formally a synchronous context-free grammar but is learned from a parallel text without any syntactic annotations. Thus it can be seen as combining fundamental ideas from both syntax-based translation and phrase-based translation. We describe our system's training and decoding methods in detail, and evaluate it for translation speed and translation accuracy. Using BLEU as a metric of translation accuracy, we find that our system performs significantly better than the Alignment Template System, a state-of-the-art phrase-based system."
            },
            "slug": "Hierarchical-Phrase-Based-Translation-Chiang",
            "title": {
                "fragments": [],
                "text": "Hierarchical Phrase-Based Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "A statistical machine translation model that uses hierarchical phrasesphrases that contain subphrasing that is formally a synchronous context-free grammar but is learned from a parallel text without any syntactic annotations is presented."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2002316"
                        ],
                        "name": "F. Och",
                        "slug": "F.-Och",
                        "structuredName": {
                            "firstName": "Franz",
                            "lastName": "Och",
                            "middleNames": [
                                "Josef"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Och"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145322333"
                        ],
                        "name": "H. Ney",
                        "slug": "H.-Ney",
                        "structuredName": {
                            "firstName": "Hermann",
                            "lastName": "Ney",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 0
                            }
                        ],
                        "text": "Och and Ney (2003) likewise remark on the overparameterization issue, removing a single variable of the original conditioning context, which only slightly improves matters."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 271,
                                "start": 12
                            }
                        ],
                        "text": "First, like Och and Ney (2003) who explored this issue for training Model 3, we found that EM tended to find poor values for p0, producing alignments that were overly sparse. By fixing the value at p0 = 0.08, we obtained minimal AER. Second, like Riley and Gildea (2012), we found that small values of \u03b1 improved the alignment error rate, although the impact was not particularly strong over large ranges of"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 192,
                                "start": 173
                            }
                        ],
                        "text": "We next compare the alignments produced by our model to the Giza++ implementation of the standard IBM models using the default training procedure and parameters reported in Och and Ney (2003). Our model is trained for 5 iterations using the procedure described above (\u00a73."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 12
                            }
                        ],
                        "text": "First, like Och and Ney (2003) who explored this issue for training Model 3, we found that EM tended to find poor values for p0, producing alignments that were overly sparse."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5219389,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "de2df29b0a0312de7270c3f5a0af6af5645cf91a",
            "isKey": true,
            "numCitedBy": 4472,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "We present and compare various methods for computing word alignments using statistical or heuristic models. We consider the five alignment models presented in Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov alignment model, smoothing techniques, and refinements. These statistical models are compared with two heuristic models based on the Dice coefficient. We present different methods for combining word alignments to perform a symmetrization of directed statistical alignment models. As evaluation criterion, we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment. We evaluate the models on the German-English Verbmobil task and the French-English Hansards task. We perform a detailed analysis of various design decisions of our statistical alignment system and evaluate these on training corpora of various sizes. An important result is that refined alignment models with a first-order dependence and a fertility model yield significantly better results than simple heuristic models. In the Appendix, we present an efficient training algorithm for the alignment models presented."
            },
            "slug": "A-Systematic-Comparison-of-Various-Statistical-Och-Ney",
            "title": {
                "fragments": [],
                "text": "A Systematic Comparison of Various Statistical Alignment Models"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "An important result is that refined alignment models with a first-order dependence and a fertility model yield significantly better results than simple heuristic models."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40045248"
                        ],
                        "name": "Darcey Riley",
                        "slug": "Darcey-Riley",
                        "structuredName": {
                            "firstName": "Darcey",
                            "lastName": "Riley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Darcey Riley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793218"
                        ],
                        "name": "D. Gildea",
                        "slug": "D.-Gildea",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Gildea",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Gildea"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 94
                            }
                        ],
                        "text": "01 and approximate the posterior distribution over the \u03b8f \u2019s using a mean-field approximation (Riley and Gildea, 2012)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "Vogel et al. (1996) hint at a similar reparameterization of Model 2; however, its likelihood and its gradient are not efficient to evaluate, making it impractical to train and use."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11989788,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6a730aff0b3e23423a00cb3407eb04e7f6e83878",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Bayesian approaches have been shown to reduce the amount of overfitting that occurs when running the EM algorithm, by placing prior probabilities on the model parameters. We apply one such Bayesian technique, variational Bayes, to the IBM models of word alignment for statistical machine translation. We show that using variational Bayes improves the performance of the widely used GIZA++ software, as well as improving the overall performance of the Moses machine translation system in terms of BLEU score."
            },
            "slug": "Improving-the-IBM-Alignment-Models-Using-Bayes-Riley-Gildea",
            "title": {
                "fragments": [],
                "text": "Improving the IBM Alignment Models Using Variational Bayes"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown that using variational Bayes improves the performance of the widely used GIZA++ software, as well as improving the overallperformance of the Moses machine translation system in terms of BLEU score."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145247319"
                        ],
                        "name": "S. Vogel",
                        "slug": "S.-Vogel",
                        "structuredName": {
                            "firstName": "Stephan",
                            "lastName": "Vogel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Vogel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145322333"
                        ],
                        "name": "H. Ney",
                        "slug": "H.-Ney",
                        "structuredName": {
                            "firstName": "Hermann",
                            "lastName": "Ney",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2324070"
                        ],
                        "name": "C. Tillmann",
                        "slug": "C.-Tillmann",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Tillmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Tillmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11644259,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "59c442932e9fcfcac6df5566c2bcd1ec331548c9",
            "isKey": false,
            "numCitedBy": 982,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we describe a new model for word alignment in statistical translation and present experimental results. The idea of the model is to make the alignment probabilities dependent on the differences in the alignment positions rather than on the absolute positions. To achieve this goal, the approach uses a first-order Hidden Markov model (HMM) for the word alignment problem as they are used successfully in speech recognition for the time alignment problem. The difference to the time alignment HMM is that there is no monotony constraint for the possible word orderings. We describe the details of the model and test the model on several bilingual corpora."
            },
            "slug": "HMM-Based-Word-Alignment-in-Statistical-Translation-Vogel-Ney",
            "title": {
                "fragments": [],
                "text": "HMM-Based Word Alignment in Statistical Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "A new model for word alignment in statistical translation using a first-order Hidden Markov model for the word alignment problem as they are used successfully in speech recognition for the time alignment problem."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32538203"
                        ],
                        "name": "P. Brown",
                        "slug": "P.-Brown",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Brown",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Brown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2714577"
                        ],
                        "name": "S. D. Pietra",
                        "slug": "S.-D.-Pietra",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Pietra",
                            "middleNames": [
                                "Della"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. D. Pietra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39944066"
                        ],
                        "name": "V. D. Pietra",
                        "slug": "V.-D.-Pietra",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Pietra",
                            "middleNames": [
                                "J.",
                                "Della"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. D. Pietra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2474650"
                        ],
                        "name": "R. Mercer",
                        "slug": "R.-Mercer",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Mercer",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mercer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 10
                            }
                        ],
                        "text": "|) operations, our formulation permits exact computation in \u0398(1), meaning our model can be applied even in applications where computational efficiency is paramount (e.g., MCMC simulations)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 71
                            }
                        ],
                        "text": "Our model is a variation of the lexical translation models proposed by Brown et al. (1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 71
                            }
                        ],
                        "text": "Word alignment is a fundamental problem in statistical machine translation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 113
                            }
                        ],
                        "text": "In the M-step, the lexical translation probabilities are updated by aggregating these\nas counts and normalizing (Brown et al., 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 19
                            }
                        ],
                        "text": "3We note here that Brown et al. (1993) derive their variant of this expression by starting with the joint probability of an alignment and translation, marginalizing, and then reorganizing common terms."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13259913,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ab7b5917515c460b90451e67852171a531671ab8",
            "isKey": true,
            "numCitedBy": 4745,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a series of five statistical models of the translation process and give algorithms for estimating the parameters of these models given a set of pairs of sentences that are translations of one another. We define a concept of word-by-word alignment between such pairs of sentences. For any given pair of such sentences each of our models assigns a probability to each of the possible word-by-word alignments. We give an algorithm for seeking the most probable of these alignments. Although the algorithm is suboptimal, the alignment thus obtained accounts well for the word-by-word relationships in the pair of sentences. We have a great deal of data in French and English from the proceedings of the Canadian Parliament. Accordingly, we have restricted our work to these two languages; but we feel that because our algorithms have minimal linguistic content they would work well on other pairs of languages. We also feel, again because of the minimal linguistic content of our algorithms, that it is reasonable to argue that word-by-word alignments are inherent in any sufficiently large bilingual corpus."
            },
            "slug": "The-Mathematics-of-Statistical-Machine-Translation:-Brown-Pietra",
            "title": {
                "fragments": [],
                "text": "The Mathematics of Statistical Machine Translation: Parameter Estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "It is reasonable to argue that word-by-word alignments are inherent in any sufficiently large bilingual corpus, given a set of pairs of sentences that are translations of one another."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Linguistics"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2730654"
                        ],
                        "name": "Victor Chahuneau",
                        "slug": "Victor-Chahuneau",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Chahuneau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Victor Chahuneau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144365875"
                        ],
                        "name": "Noah A. Smith",
                        "slug": "Noah-A.-Smith",
                        "structuredName": {
                            "firstName": "Noah",
                            "lastName": "Smith",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noah A. Smith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745899"
                        ],
                        "name": "Chris Dyer",
                        "slug": "Chris-Dyer",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Dyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris Dyer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 212
                            }
                        ],
                        "text": "org/wmt12 (6)While this computational effort is a small relative to the total cost in EM training, in algorithms where \u03bb changes more rapidly, for example in Bayesian posterior inference with Monte Carlo methods (Chahuneau et al., 2013), this savings can have substantial value."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 145
                            }
                        ],
                        "text": "\u2026to the total cost in EM training, in algorithms where \u03bb changes more rapidly, for example in Bayesian posterior inference with Monte Carlo methods (Chahuneau et al., 2013), this savings can have substantial value.\n\u03b1. Finally, we (perhaps surprisingly) found that the standard staged\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8602006,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "32d8688b89ddec363dbe1743dbf62c0cd3d77c99",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a morphology-aware nonparametric Bayesian model of language whose prior distribution uses manually constructed finitestate transducers to capture the word formation processes of particular languages. This relaxes the word independence assumption and enables sharing of statistical strength across, for example, stems or inflectional paradigms in different contexts. Our model can be used in virtually any scenario where multinomial distributions over words would be used. We obtain state-of-the-art results in language modeling, word alignment, and unsupervised morphological disambiguation for a variety of morphologically rich languages."
            },
            "slug": "Knowledge-Rich-Morphological-Priors-for-Bayesian-Chahuneau-Smith",
            "title": {
                "fragments": [],
                "text": "Knowledge-Rich Morphological Priors for Bayesian Language Models"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "A morphology-aware nonparametric Bayesian model of language whose prior distribution uses manually constructed finitestate transducers to capture the word formation processes of particular languages allows sharing of statistical strength across, for example, stems or inflectional paradigms in different contexts."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696645"
                        ],
                        "name": "Nizar Habash",
                        "slug": "Nizar-Habash",
                        "structuredName": {
                            "firstName": "Nizar",
                            "lastName": "Habash",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nizar Habash"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2652214"
                        ],
                        "name": "F. Sadat",
                        "slug": "F.-Sadat",
                        "structuredName": {
                            "firstName": "Fatiha",
                            "lastName": "Sadat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Sadat"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1586274,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c043b7516f32877f3f511e2d83605560797544f3",
            "isKey": false,
            "numCitedBy": 271,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we study the effect of different word-level preprocessing decisions for Arabic on SMT quality. Our results show that given large amounts of training data, splitting off only proclitics performs best. However, for small amounts of training data, it is best to apply English-like to-kenization using part-of-speech tags, and sophisticated morphological analysis and disambiguation. Moreover, choosing the appropriate preprocessing produces a significant increase in BLEU score if there is a change in genre between training and test data."
            },
            "slug": "Arabic-Preprocessing-Schemes-for-Statistical-Habash-Sadat",
            "title": {
                "fragments": [],
                "text": "Arabic Preprocessing Schemes for Statistical Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "The results show that given large amounts of training data, splitting off only proclitics performs best, and choosing the appropriate preprocessing produces a significant increase in BLEU score if there is a change in genre between training and test data."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52639766"
                        ],
                        "name": "R. R. Newell",
                        "slug": "R.-R.-Newell",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Newell",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. R. Newell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 70740449,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "08dfa089ab6cab6f1afa7a7aabff45a9e61ac84e",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "For most of us, mathematics is a pain in the neck, and we avoid it whenever we can. We hire an expert to figure our tax. We trust the clerk to make the right change. We buy meat weighed on a computing scale and gasoline from a computing pump. We learned algebra in school, but we've forgotten it! What wonder that most of us neither use nor understand the mathematics of statistics, which deals with general tendencies and random differences. We'd be glad to forget this, too, if we'd ever learned it. Or perhaps we wouldn't forget it, for there's a good likelihood that we'd find frequent occasion to use it, which is certainly not true of algebra. Statistics deal with chances. It attempts to tell, from looking at a few patients, what one could expect in 100 or 10,000\u2014from making a few measurements, what they would be if one hundred times repeated. Chances indicate the number of times one would encounter this or that in a large population. For me chances have no meaning unless there exists in reality, or in my i..."
            },
            "slug": "The-Mathematics-of-Statistics-Newell",
            "title": {
                "fragments": [],
                "text": "The Mathematics of Statistics"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Most of us neither use nor understand the mathematics of statistics, which deals with general tendencies and random differences, and most of us would be glad to forget this, too, if the authors'd ever learned it."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1952
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 230,
                                "start": 205
                            }
                        ],
                        "text": "To compute the value of the partition, we only need to evaluate the unnormalized probabilities at j\u2191 and j\u2193 and then use the following identity, which gives the sum of the first ` terms of a geometric series (Courant and Robbins, 1996):\ns`(g1, r) = \u2211\u0300 k=1 g1r k\u22121 = g1 1\u2212 r` 1\u2212 r ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 235,
                                "start": 208
                            }
                        ],
                        "text": "To compute the value of the partition, we only need to evaluate the unnormalized probabilities at j\u2191 and j\u2193 and then use the following identity, which gives the sum of the first ` terms of a geometric series (Courant and Robbins, 1996):"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The geometric progression"
            },
            "venue": {
                "fragments": [],
                "text": "What Is Mathematics?: An Elementary Approach to Ideas and Methods"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The mathematics of statistical"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 108
                            }
                        ],
                        "text": "This construction is referred to as an arithmetico-geometric series, and its sum may be computed as follows (Fernandez et al., 2006):"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 109
                            }
                        ],
                        "text": "This construction is referred to as an arithmetico-geometric series, and its sum may be computed as follows (Fernandez et al., 2006):\nt`(g1,a1, r, d) = \u2211\u0300 k=1 [a1 + d(k \u2212 1)] g1rk\u22121\n= a`g`+1 \u2212 a1g1 1\u2212 r + d (g`+1 \u2212 g1r) (1\u2212 r)2 ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Arithmetic-geometric series"
            },
            "venue": {
                "fragments": [],
                "text": "PlanetMath.org."
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The geometric progression"
            },
            "venue": {
                "fragments": [],
                "text": "What Is Mathematics ? : An Elementary Approach to Ideas and Methods"
            },
            "year": 1996
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 5,
            "methodology": 5
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 12,
        "totalPages": 2
    },
    "page_url": "https://www.semanticscholar.org/paper/A-Simple,-Fast,-and-Effective-Reparameterization-of-Dyer-Chahuneau/7b5e31257f01aba987f16e175a3e49e00a5bd3bb?sort=total-citations"
}