{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2718299"
                        ],
                        "name": "N. Wiberg",
                        "slug": "N.-Wiberg",
                        "structuredName": {
                            "firstName": "Niclas",
                            "lastName": "Wiberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Wiberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 48
                            }
                        ],
                        "text": "Recently, and independently of developments in the expert systems literature, Wiberget al. in [20] and Wiberg in his doctoral dissertation [21] have refocused attention on graphical models for codes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 115168171,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eb44d50bce92b4ce2c0ea53bd8ede95f628ee3cb",
            "isKey": false,
            "numCitedBy": 1007,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "Iterative decoding techniques have become a viable alternative for constructing high performance coding systems. In particular, the recent success of turbo codes indicates that performance close to the Shannon limit may be achieved. In this thesis, it is showed that many iterative decoding algorithms are special cases of two generic algorithms, the min-sum and sum-product algorithms, which also include non-iterative algorithms such as Viterbi decoding. The min-sum and sum-product algorithms are developed and presented as generalized trellis algorithms, where the time axis of the trellis is replaced by an arbitrary graph, the \u201cTanner graph\u201d. With cycle-free Tanner graphs, the resulting decoding algorithms (e.g., Viterbi decoding) are maximum-likelihood but suffer from an exponentially increasing complexity. Iterative decoding occurs when the Tanner graph has cycles (e.g., turbo codes); the resulting algorithms are in general suboptimal, but significant complexity reductions are possible compared to the cycle-free case. Several performance estimates for iterative decoding are developed, including a generalization of the union bound used with Viterbi decoding and a characterization of errors that are uncorrectable after infinitely many decoding iterations."
            },
            "slug": "Codes-and-Decoding-on-General-Graphs-Wiberg",
            "title": {
                "fragments": [],
                "text": "Codes and Decoding on General Graphs"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is showed that many iterative decoding algorithms are special cases of two generic algorithms, the min-sum and sum-product algorithms, which also include non-iterative algorithms such as Viterbi decoding."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46404423"
                        ],
                        "name": "J. Hagenauer",
                        "slug": "J.-Hagenauer",
                        "structuredName": {
                            "firstName": "Joachim",
                            "lastName": "Hagenauer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hagenauer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2549634"
                        ],
                        "name": "E. Offer",
                        "slug": "E.-Offer",
                        "structuredName": {
                            "firstName": "Elke",
                            "lastName": "Offer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Offer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38915382"
                        ],
                        "name": "L. Papke",
                        "slug": "L.-Papke",
                        "structuredName": {
                            "firstName": "Lutz",
                            "lastName": "Papke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Papke"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 80
                            }
                        ],
                        "text": "Other examples of compound codes include classical serially concatenated codes [2] (see also [3], [4]), Gallager\u2019s low-density parity-check codes [5], and various product codes [6], [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14954804,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d7b350b6d469ac30c02f10aa4e62f77f79c0106b",
            "isKey": false,
            "numCitedBy": 2523,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "Iterative decoding of two-dimensional systematic convolutional codes has been termed \"turbo\" (de)coding. Using log-likelihood algebra, we show that any decoder can be used which accepts soft inputs-including a priori values-and delivers soft outputs that can be split into three terms: the soft channel and a priori inputs, and the extrinsic value. The extrinsic value is used as an a priori value for the next iteration. Decoding algorithms in the log-likelihood domain are given not only for convolutional codes but also for any linear binary systematic block code. The iteration is controlled by a stop criterion derived from cross entropy, which results in a minimal number of iterations. Optimal and suboptimal decoders with reduced complexity are presented. Simulation results show that very simple component codes are sufficient, block codes are appropriate for high rates and convolutional codes for lower rates less than 2/3. Any combination of block and convolutional component codes is possible. Several interleaving techniques are described. At a bit error rate (BER) of 10/sup -4/ the performance is slightly above or around the bounds given by the cutoff rate for reasonably simple block/convolutional component codes, interleaver sizes less than 1000 and for three to six iterations."
            },
            "slug": "Iterative-decoding-of-binary-block-and-codes-Hagenauer-Offer",
            "title": {
                "fragments": [],
                "text": "Iterative decoding of binary block and convolutional codes"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "Using log-likelihood algebra, it is shown that any decoder can be used which accepts soft inputs-including a priori values-and delivers soft outputs that can be split into three terms: the soft channel and aPriori inputs, and the extrinsic value."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723150"
                        ],
                        "name": "R. McEliece",
                        "slug": "R.-McEliece",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "McEliece",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. McEliece"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2157745208"
                        ],
                        "name": "Jung-Fu Cheng",
                        "slug": "Jung-Fu-Cheng",
                        "structuredName": {
                            "firstName": "Jung-Fu",
                            "lastName": "Cheng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jung-Fu Cheng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14553992,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "26d953005dd08a863c157b528bbabdf5671d18b6",
            "isKey": false,
            "numCitedBy": 1004,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe the close connection between the now celebrated iterative turbo decoding algorithm of Berrou et al. (1993) and an algorithm that has been well known in the artificial intelligence community for a decade, but which is relatively unknown to information theorists: Pearl's (1982) belief propagation algorithm. We see that if Pearl's algorithm is applied to the \"belief network\" of a parallel concatenation of two or more codes, the turbo decoding algorithm immediately results. Unfortunately, however, this belief diagram has loops, and Pearl only proved that his algorithm works when there are no loops, so an explanation of the experimental performance of turbo decoding is still lacking. However, we also show that Pearl's algorithm can be used to routinely derive previously known iterative, but suboptimal, decoding algorithms for a number of other error-control systems, including Gallager's (1962) low-density parity-check codes, serially concatenated codes, and product codes. Thus, belief propagation provides a very attractive general methodology for devising low-complexity iterative decoding algorithms for hybrid coded systems."
            },
            "slug": "Turbo-Decoding-as-an-Instance-of-Pearl's-\"Belief-McEliece-Mackay",
            "title": {
                "fragments": [],
                "text": "Turbo Decoding as an Instance of Pearl's \"Belief Propagation\" Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is shown that Pearl's algorithm can be used to routinely derive previously known iterative, but suboptimal, decoding algorithms for a number of other error-control systems, including Gallager's low-density parity-check codes, serially concatenated codes, and product codes."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE J. Sel. Areas Commun."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2718299"
                        ],
                        "name": "N. Wiberg",
                        "slug": "N.-Wiberg",
                        "structuredName": {
                            "firstName": "Niclas",
                            "lastName": "Wiberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Wiberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143681410"
                        ],
                        "name": "H. Loeliger",
                        "slug": "H.-Loeliger",
                        "structuredName": {
                            "firstName": "Hans-Andrea",
                            "lastName": "Loeliger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Loeliger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7715701"
                        ],
                        "name": "R. Koetter",
                        "slug": "R.-Koetter",
                        "structuredName": {
                            "firstName": "Ralf",
                            "lastName": "Koetter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Koetter"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 36630145,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "848822f6c3446842730587cb4373a53f69e38720",
            "isKey": false,
            "numCitedBy": 350,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Until recently, most known decoding procedures for error-correcting codes were based either on algebraically calculating the error pattern or on some sort of tree or trellis search. With the advent of turbo coding, a third decoding principle has finally had its breakthrough: iterative decoding. With respect to Viterbi decoding, a code is most naturally described by means of a trellis diagram. The main thesis of the present paper is that, with respect to iterative decoding, the natural way of describing a code is by means of a Tanner graph, which may be viewed as a generalized trellis. More precisely, it is the \"time axis\" of a trellis that is generalized to a Tanner graph."
            },
            "slug": "Codes-and-iterative-decoding-on-general-graphs-Wiberg-Loeliger",
            "title": {
                "fragments": [],
                "text": "Codes and iterative decoding on general graphs"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The main thesis of the present paper is that, with respect to iterative decoding, the natural way of describing a code is by means of a Tanner graph, which may be viewed as a generalized trellis."
            },
            "venue": {
                "fragments": [],
                "text": "Eur. Trans. Telecommun."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3637824"
                        ],
                        "name": "R. M. Tanner",
                        "slug": "R.-M.-Tanner",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Tanner",
                            "middleNames": [
                                "Michael"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. M. Tanner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 33
                            }
                        ],
                        "text": "We now introduce Bayesian networks that, unlike MRF\u2019s and Tanner graphs, aredirected acyclicgraphs [12]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 4
                            }
                        ],
                        "text": "They show that a type of graphical model called a \u201cTanner graph\u201d (first introduced by Tanner [22] to describe a generalization of Gallager codes) provides a natural setting in which to describe and study iterative soft-decision decoding techniques, much as the code trellis [23] is an appropriate\u2026"
                    },
                    "intents": []
                }
            ],
            "corpusId": 754232,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "157218bae792b6ef550dfd0f73e688d83d98b3d7",
            "isKey": false,
            "numCitedBy": 2971,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "A method is described for constructing long error-correcting codes from one or more shorter error-correcting codes, referred to as subcodes, and a bipartite graph. A graph is shown which specifies carefully chosen subsets of the digits of the new codes that must be codewords in one of the shorter subcodes. Lower bounds to the rate and the minimum distance of the new code are derived in terms of the parameters of the graph and the subeodes. Both the encoders and decoders proposed are shown to take advantage of the code's explicit decomposition into subcodes to decompose and simplify the associated computational processes. Bounds on the performance of two specific decoding algorithms are established, and the asymptotic growth of the complexity of decoding for two types of codes and decoders is analyzed. The proposed decoders are able to make effective use of probabilistic information supplied by the channel receiver, e.g., reliability information, without greatly increasing the number of computations required. It is shown that choosing a transmission order for the digits that is appropriate for the graph and the subcodes can give the code excellent burst-error correction abilities. The construction principles"
            },
            "slug": "A-recursive-approach-to-low-complexity-codes-Tanner",
            "title": {
                "fragments": [],
                "text": "A recursive approach to low complexity codes"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is shown that choosing a transmission order for the digits that is appropriate for the graph and the subcodes can give the code excellent burst-error correction abilities."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1755571"
                        ],
                        "name": "J. Lodge",
                        "slug": "J.-Lodge",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lodge",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lodge"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2114079917"
                        ],
                        "name": "R. Young",
                        "slug": "R.-Young",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Young",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Young"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "107834783"
                        ],
                        "name": "P. Hoeher",
                        "slug": "P.-Hoeher",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Hoeher",
                            "middleNames": [
                                "Dipl",
                                "Ing"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hoeher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46404423"
                        ],
                        "name": "J. Hagenauer",
                        "slug": "J.-Hagenauer",
                        "structuredName": {
                            "firstName": "Joachim",
                            "lastName": "Hagenauer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hagenauer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 75
                            }
                        ],
                        "text": "Other examples of compound codes include classical serially concatenated codes [2] (see also [3], [4]), Gallager\u2019s low-density parity-check codes [5], and various product codes [6], [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 123613554,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "907f27e0152977187f2f7cdaa3ce3cd5b8833078",
            "isKey": false,
            "numCitedBy": 168,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "Very efficient signaling in radio channels requires the design of very powerful codes having special structure suitable for practical decoding schemes. Powerful codes are obtained by using simple block codes to construct multidimensional product codes. The decoding of multidimensional product codes, using separable symbol-by-symbol maximum a posteriori filters, is described. Simulation results are presented for three-dimensional product codes constructed with the (16,11) extended Hamming code. The extension of the concept to concatenated convolutional codes is given and some simulation results are presented. Potential applications are briefly discussed.<<ETX>>"
            },
            "slug": "Separable-MAP-\"filters\"-for-the-decoding-of-product-Lodge-Young",
            "title": {
                "fragments": [],
                "text": "Separable MAP \"filters\" for the decoding of product and concatenated codes"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The decoding of multidimensional product codes, using separable symbol-by-symbol maximum a posteriori filters, and the extension of the concept to concatenated convolutional codes is given and some simulation results are presented."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of ICC '93 - IEEE International Conference on Communications"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2081245302"
                        ],
                        "name": "CodesDavid",
                        "slug": "CodesDavid",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "CodesDavid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "CodesDavid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1410246062"
                        ],
                        "name": "C. J.",
                        "slug": "C.-J.",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "J.",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. J."
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2087926458"
                        ],
                        "name": "MacKayCavendish",
                        "slug": "MacKayCavendish",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "MacKayCavendish",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "MacKayCavendish"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1478845,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "27b1996dc57dd04f22a4b3aee4d0364855db4675",
            "isKey": false,
            "numCitedBy": 347,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We report the empirical performance of Gallager\u2019s low density parity check codes on Gaussian channels. We show that performance substantially better than that of standard convolutional and concatenated codes can be achieved; indeed the performance is almost as close to the Shannon limit as that of Turbo codes. A linear code may be described in terms of a generator matrix G or in terms of a parity check matrix H, which satisfies Hx = 0 for all codewords x. In 1962, Gallager reported work on binary codes defined in terms of low density parity check matrices (abbreviated \u2018GL codes\u2019) [5, 6]. The matrix H was defined in a non-systematic form; each column of H had a small weight (e.g., 3) and the weight per row was also uniform; the matrix H was constructed at random subject to these constraints. Gallager proved distance properties of these codes and described a probability-based decoding algorithm with promising empirical performance. However it appears that GL codes have been generally forgotten, the assumption perhaps being that concatenated codes [4] were superior for practical purposes (R.G. Gallager, personal communication). During our work on MN codes [8] we realised that it is possible to create \u2018good\u2019 codes from very sparse random matrices, and to decode them (even beyond their minimum distance) using approximate probabilistic algorithms. We eventually reinvented Gallager\u2019s decoding algorithm and GL codes. In this paper we report the empirical performance of these codes on Gaussian channels. We have proved theoretical properties of GL codes (essentially, that the channel coding theorem holds for them) elsewhere [9]. GL codes can also be defined over GF (q). We are currently implementing this generalization. We created sparse random parity check matrices in the following ways. Construction 1A. An M by N matrix (M rows, N columns) is created at random with weight per column t (e.g., t = 3), and weight per row as uniform as possible, and overlap between any two columns no greater than 1. (The weight of a column is the number of non-zero elements; the overlap between two columns is their inner product.)"
            },
            "slug": "Near-Shannon-Limit-Performance-of-Low-Density-Check-CodesDavid-C.",
            "title": {
                "fragments": [],
                "text": "Near Shannon Limit Performance of Low Density Parity Check Codes"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "It is shown that performance substantially better than that of standard convolutional and concatenated codes can be achieved; indeed the performance is almost as close to the Shannon limit as that of Turbo codes."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30050592"
                        ],
                        "name": "S. Benedetto",
                        "slug": "S.-Benedetto",
                        "structuredName": {
                            "firstName": "Sergio",
                            "lastName": "Benedetto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Benedetto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714533"
                        ],
                        "name": "G. Montorsi",
                        "slug": "G.-Montorsi",
                        "structuredName": {
                            "firstName": "Guido",
                            "lastName": "Montorsi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Montorsi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The turbo decoding algorithm, the \u201cseparable MAP filter\u201d algorithm [6], the new iterative decoding algorithm [4] used for decoding \u201cserially concatenated\u201d convolutional codes, and, as first pointed out by MacKay and Neal [16], Gallager\u2019s algorithm for decoding low-density parity check codes [5] are all a form of probability propagation in the compound code networks shown in Fig."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Other examples of compound codes include classical serially concatenated codes [2] (see also [3, 4]), Gallager\u2019s lowdensity parity-check codes [5], and various product codes [6,7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14586830,
            "fieldsOfStudy": [
                "Computer Science",
                "Business"
            ],
            "id": "c994706101df350cea61917aa204175e1c0a18a0",
            "isKey": false,
            "numCitedBy": 144,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "Serial concatenation of convolutional codes separated by an interleaver has recently been shown, through the use of upper bounds to the maximum likelihood performance, to be competitive with parallel concatenated coding schemes known in the literature as \u2018turbo codes\u2019. The most important feature of turbo codes consists in their relatively simple, yet high performance, iterative decoding algorithm. The authors propose a new iterative decoding algorithm for serial concatenation, and show that the new coding scheme can yield a significant advantage with respect to turbo codes."
            },
            "slug": "Iterative-decoding-of-serially-concatenated-codes-Benedetto-Montorsi",
            "title": {
                "fragments": [],
                "text": "Iterative decoding of serially concatenated convolutional codes"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A new iterative decoding algorithm for serial concatenation is proposed, and it is shown that the new coding scheme can yield a significant advantage with respect to turbo codes."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770859"
                        ],
                        "name": "R. Gallager",
                        "slug": "R.-Gallager",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Gallager",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Gallager"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 56
                            }
                        ],
                        "text": "The first to connect Pearl\u2019s \u201cbelief propagation\u201d algorithm with coding were MacKay and Neal [16]\u2013[18], who showed that Gallager\u2019s 35-year-old algorithm [5] for decoding lowdensity parity-check codes is essentially an instance of Pearl\u2019s algorithm."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 44
                            }
                        ],
                        "text": "Other examples of compound codes include classical serially concatenated codes [2] (see also [3], [4]), Gallager\u2019s low-density parity-check codes [5], and various product codes [6], [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12709402,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "206f827fad201506c315d40c1469b41a45141893",
            "isKey": false,
            "numCitedBy": 10568,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "A low-density parity-check code is a code specified by a parity-check matrix with the following properties: each column contains a small fixed number j \\geq 3 of l's and each row contains a small fixed number k > j of l's. The typical minimum distance of these codes increases linearly with block length for a fixed rate and fixed j . When used with maximum likelihood decoding on a sufficiently quiet binary-input symmetric channel, the typical probability of decoding error decreases exponentially with block length for a fixed rate and fixed j . A simple but nonoptimum decoding scheme operating directly from the channel a posteriori probabilities is described. Both the equipment complexity and the data-handling capacity in bits per second of this decoder increase approximately linearly with block length. For j > 3 and a sufficiently low rate, the probability of error using this decoder on a binary symmetric channel is shown to decrease at least exponentially with a root of the block length. Some experimental results show that the actual probability of decoding error is much smaller than this theoretical bound."
            },
            "slug": "Low-density-parity-check-codes-Gallager",
            "title": {
                "fragments": [],
                "text": "Low-density parity-check codes"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A simple but nonoptimum decoding scheme operating directly from the channel a posteriori probabilities is described and the probability of error using this decoder on a binary symmetric channel is shown to decrease at least exponentially with a root of the block length."
            },
            "venue": {
                "fragments": [],
                "text": "IRE Trans. Inf. Theory"
            },
            "year": 1962
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144572614"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 100
                            }
                        ],
                        "text": ") The first to connect Pearl\u2019s \u201cbelief propagation\u201d algorithm with coding were MacKay and Neal [16]\u2013[18], who showed that Gallager\u2019s 35-year-old algorithm [5] for decoding low-"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16406992,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "01c3188460d25219433c2dc28629d61b18970d54",
            "isKey": false,
            "numCitedBy": 2319,
            "numCiting": 85,
            "paperAbstract": {
                "fragments": [],
                "text": "We report theoretical and empirical properties of Gallager's (1963) low density parity check codes on Gaussian channels. It can be proved that, given an optimal decoder, these codes asymptotically approach the Shannon limit. With a practical 'belief propagation' decoder, performance substantially better than that of standard convolutional and concatenated codes can be achieved; indeed the performance is almost as close to the Shannon limit as that of turbo codes."
            },
            "slug": "Good-error-correcting-codes-based-on-very-sparse-Mackay",
            "title": {
                "fragments": [],
                "text": "Good Error-Correcting Codes Based on Very Sparse Matrices"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "It can be proved that, given an optimal decoder, Gallager's low density parity check codes asymptotically approach the Shannon limit."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2696176"
                        ],
                        "name": "L. Bahl",
                        "slug": "L.-Bahl",
                        "structuredName": {
                            "firstName": "Lalit",
                            "lastName": "Bahl",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bahl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144716964"
                        ],
                        "name": "J. Cocke",
                        "slug": "J.-Cocke",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Cocke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cocke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472759"
                        ],
                        "name": "F. Jelinek",
                        "slug": "F.-Jelinek",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Jelinek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jelinek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "16512130"
                        ],
                        "name": "J. Raviv",
                        "slug": "J.-Raviv",
                        "structuredName": {
                            "firstName": "Josef",
                            "lastName": "Raviv",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Raviv"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 28594190,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b51c6a5610be2c5648d1476b6f70e8037e0e8cb8",
            "isKey": false,
            "numCitedBy": 6485,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "The general problem of estimating the a posteriori probabilities of the states and transitions of a Markov source observed through a discrete memoryless channel is considered. The decoding of linear block and convolutional codes to minimize symbol error probability is shown to be a special case of this problem. An optimal decoding algorithm is derived."
            },
            "slug": "Optimal-decoding-of-linear-codes-for-minimizing-Bahl-Cocke",
            "title": {
                "fragments": [],
                "text": "Optimal decoding of linear codes for minimizing symbol error rate (Corresp.)"
            },
            "tldr": {
                "abstractSimilarityScore": 87,
                "text": "The general problem of estimating the a posteriori probabilities of the states and transitions of a Markov source observed through a discrete memoryless channel is considered and an optimal decoding algorithm is derived."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1833925"
                        ],
                        "name": "C. Berrou",
                        "slug": "C.-Berrou",
                        "structuredName": {
                            "firstName": "Claude",
                            "lastName": "Berrou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Berrou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1870588"
                        ],
                        "name": "A. Glavieux",
                        "slug": "A.-Glavieux",
                        "structuredName": {
                            "firstName": "Alain",
                            "lastName": "Glavieux",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Glavieux"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2952051"
                        ],
                        "name": "P. Thitimajshima",
                        "slug": "P.-Thitimajshima",
                        "structuredName": {
                            "firstName": "Punya",
                            "lastName": "Thitimajshima",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Thitimajshima"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 100
                            }
                        ],
                        "text": "Prominent among compound codes are the turbo codes introduced by Berrou, Glavieux and Thitimajshima [1], in which the constituent convolutional codes interact in \u201cparallel concatenation\u201d through an interleaver."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 133
                            }
                        ],
                        "text": "They provide a description of Pearl\u2019s algorithm, and make explicit the connection to the basic turbo decoding algorithm described in [1]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 46
                            }
                        ],
                        "text": "REFERENCES\n[1] C. Berrou, A. Glavieux, and P. Thitimajshima, \u201cNear Shannon limit error-correcting coding and decoding: Turbo codes,\u201d inProc."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 43
                            }
                        ],
                        "text": "The algorithm uses \u201cextrinsic information\u201d [1, 7] produced by the previous step, when processing the next trellis."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17770377,
            "fieldsOfStudy": [
                "Computer Science",
                "Business"
            ],
            "id": "3ba9baa534a8ea39a31c69e72ada959aaa6a4dc1",
            "isKey": true,
            "numCitedBy": 8239,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "A new class of convolutional codes called turbo-codes, whose performances in terms of bit error rate (BER) are close to the Shannon limit, is discussed. The turbo-code encoder is built using a parallel concatenation of two recursive systematic convolutional codes, and the associated decoder, using a feedback decoding rule, is implemented as P pipelined identical elementary decoders.<<ETX>>"
            },
            "slug": "Near-Shannon-limit-error-correcting-coding-and-1-Berrou-Glavieux",
            "title": {
                "fragments": [],
                "text": "Near Shannon limit error-correcting coding and decoding: Turbo-codes. 1"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A new class of convolutional codes called turbo-codes, whose performances in terms of bit error rate (BER) are close to the Shannon limit, is discussed."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of ICC '93 - IEEE International Conference on Communications"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723150"
                        ],
                        "name": "R. McEliece",
                        "slug": "R.-McEliece",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "McEliece",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. McEliece"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Initially, we assume that the codomain is the set of real numbers, but later, we will allow to be an arbitrary commutative semiring [21], [29]\u2010[ 31 ]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9207542,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2f6a15214a73abf7a05c604ecd08847614bf8a03",
            "isKey": false,
            "numCitedBy": 248,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "In this semi-tutorial paper, we will investigate the computational complexity of an abstract version of the Viterbi algorithm on a trellis, and show that if the trellis has e edges, the complexity of the Viterbi algorithm is /spl Theta/(e). This result suggests that the \"best\" trellis representation for a given linear block code is the one with the fewest edges. We will then show that, among all trellises that represent a given code, the original trellis introduced by Bahl, Cocke, Jelinek, and Raviv in 1974, and later rediscovered by Wolf (1978), Massey (1978), and Forney (1988), uniquely minimizes the edge count, as well as several other figures of merit. Following Forney and Kschischang and Sorokine (1995), we will also discuss \"trellis-oriented\" or \"minimal-span\" generator matrices, which facilitate the calculation of the size of the BCJR trellis, as well as the actual construction of it."
            },
            "slug": "On-the-BCJR-trellis-for-linear-block-codes-McEliece",
            "title": {
                "fragments": [],
                "text": "On the BCJR trellis for linear block codes"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that, among all trellises that represent a given code, the original trellis introduced by Bahl, Cocke, Jelinek, and Raviv in 1974, and later rediscovered by Wolf, Massey, and Forney, uniquely minimizes the edge count."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1805342"
                        ],
                        "name": "R. Blahut",
                        "slug": "R.-Blahut",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Blahut",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Blahut"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144845584"
                        ],
                        "name": "J. Massey",
                        "slug": "J.-Massey",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Massey",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Massey"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 118134864,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d94c9d2435b00d8266fd9b3ec11a1478fc0fa42f",
            "isKey": false,
            "numCitedBy": 135,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Block Coding. On a Problem of Persi Diaconis E. Berlekamp. A Finite Fourier Transform for Vectors of Arbitrary Length C.G. Gunther. Massey's Theorem and the Golay Codes R.E. Blahut. Spherical Codes from the Hexagonal Lattice T. Ericson, V. Zinoviev. On Group Codes generated by Finite Reflection Groups T. Mittelholzer. Using Redundancy to speed up Disk Arrays D.L. Cohn, R.L. Stevenson. A Comparison of Error Patterns corrected by Block Codes and Convolutional Codes J. Justesen. Coded MPSK Modulation for the AWGN and Rayleigh Fading Channels S. Lin, S, Rajpal, D.J. Rhee. On the Basic Averaging Arguments for Linear Codes H.A. Loeliger. Coding and Multiplexing H.J. Matt. Convolutional Coding. Duality of Linear Input-Output Maps S.K. Mitter. Inverses of Linear Sequential Circuits: On Beyond Poles and Zeros M.K. Sain. Trellises Old and New G.D. Forney, Jr. On Canonical Encoding Matrices and the Generalized Constraint Lengths of Convolutional Codes R. Johannesson, Z. Wan. On Code Linearity and Rotational Invariance for a Class of Trellis Codes for M-PSK L.H. Zetterberg. Progress towards Achieving Channel Capacity D.J. Costello, L. Perez. Soft is better than Hard J. Hagenauer. Charge Constrained Convolutional Codes M.A. Herro, R.H. Deng, Y.X. Li. Five Views of Differential MSK: a Unified Approach B. Rimoldi. Binary Convolutional Codes Revisited G. Ungerboeck. Cryptography. Development of Fast Multiplier Structures with Cryptographic Applications G. Agnew. On Repeated-Single-Root Constacyclic Codes V.C. da Rocha, Jr. Delay Estimation for Truly Random Binary Sequences or How to Measure the Length of Rip van Winkle's Sleep I.Ingemarsson. Low Weight Parity Checks for Linear Recurring Sequences G. Kuehn. Higher Order Derivatives and Differential Cryptanalysis X. Lai. The Strong Secret Key Rate of Discrete Random Triples U.M. Maurer. International Commercial Standards in Cryptography J. Omura. The Self-Shrinking Generator W. Meier, O. Staffelbach. Models for Adder Channels I. Bar-David. Coding for Adder Channels I.F. Blake. Information Theory. Orthogonal Checksets in the Plane and Enumerations of the Rationals modp P. Elias. An Inequality on the Capacity Region of Multiaccess Multipath Channels R.G. Gallager. On the Performance of Aperiodic Inverse Filter J. Ruprecht. Capacity of a Simple Stable Protocol for Short Message Service over a CDMA Network A.J. Viterbi. Random Time and Frequency Hopping for Infinite User Population S. Csibi. Multiple Access Collision Channel without Feedback and User Population L. Gyorfi. Messy Broadcasting in Networks R. Ahlswede, H.S. Haroutunian, L.H. Khachatrian. Stochastic Events H. Ohnsorge. Leaf-Average Node-Sum Interchanges in Rooted Trees with Applications R.A. Rueppel, J.L. Massey. Some Reflections on the Interference Channel E.C. van der Meulen. The Sliding-Window Lempel-Ziv Algorithm is Asymptotically Optimal A.D. Wyner, J. Ziv."
            },
            "slug": "Communications-and-Cryptography:-Two-Sides-of-One-Blahut-Massey",
            "title": {
                "fragments": [],
                "text": "Communications and Cryptography: Two Sides of One Tapestry"
            },
            "tldr": {
                "abstractSimilarityScore": 36,
                "text": "A comparison of Error Patterns corrected by Block Codes and Convolutional Codes and an Inequality on the Capacity Region of Multiaccess Multipath Channels."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749650"
                        ],
                        "name": "B. Frey",
                        "slug": "B.-Frey",
                        "structuredName": {
                            "firstName": "Brendan",
                            "lastName": "Frey",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Frey"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 116
                            }
                        ],
                        "text": "The first to connect Pearl\u2019s \u201cbelief propagation\u201d algorithm with coding were MacKay and Neal [16]\u2013[18], who showed that Gallager\u2019s 35-year-old algorithm [5] for decoding lowdensity parity-check codes is essentially an instance of Pearl\u2019s algorithm."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 59
                            }
                        ],
                        "text": "Nevertheless, the excellent performance of turbo codes and Gallager codes is testimony to the efficacy of these iterative decoding procedures."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 27
                            }
                        ],
                        "text": "9(c) shows an example of a Gallager low-density parity check code [5]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 128
                            }
                        ],
                        "text": "They show that a type of graphical model called a \u201cTanner graph\u201d (first introduced by Tanner [22] to describe a generalization of Gallager codes) provides a natural setting in which to describe and study iterative soft-decision decoding techniques, much as the code trellis [23] is an appropriate model in which to describe and study \u201cconventional\u201d maximum likelihood soft-decision decoding using the Viterbi algorithm."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 58
                            }
                        ],
                        "text": "Extensive simulation results of MacKay and Neal show that Gallager codes can perform nearly as well as turbo codes, indicating that we probably \u201csailed\u201d much closer to capacity 35 years ago than might have been appreciated in the interim."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 110
                            }
                        ],
                        "text": "Other examples of compound codes include classical serially concatenated codes [2] (see also [3], [4]), Gallager\u2019s low-density parity-check codes [5], and various product codes [6], [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 10
                            }
                        ],
                        "text": "[5] R. G. Gallager,Low-Density Parity-Check Codes.Cambridge, MA: MIT Press, 1963."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 48
                            }
                        ],
                        "text": "MacKay and Neal [16] were the first to describe Gallager\u2019s codes using Bayesian networks."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 229,
                                "start": 223
                            }
                        ],
                        "text": "The turbo decoding algorithm, the \u201cseparable MAP filter\u201d algorithm [6], the new iterative decoding algorithm [4] used for decoding \u201cserially concatenated\u201d convolutional codes, and, as first pointed out by MacKay and Neal [16], Gallager\u2019s algorithm for decoding low-density parity check codes [5] are all a form of probability propagation in the compound code networks shown in Fig."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 25
                            }
                        ],
                        "text": "[34] D. Bertsekas and R. Gallager,Data Networks, 2nd ed."
                    },
                    "intents": []
                }
            ],
            "corpusId": 57821331,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a4a34496869a2ef6dfd2ddb880ae5b5dc9cdf60f",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 82,
            "paperAbstract": {
                "fragments": [],
                "text": "Pattern classification, data compression, and channel coding are tasks that usually must deal with complex but structured natural or artificial systems. Patterns that we wish to classify are a consequence of a causal physical process. Images that we wish to compress are also a consequence of a causal physical process. Noisy outputs from a telephone line are corrupted versions of a signal produced by a structured man-made telephone modem. Not only are these tasks characterized by complex structure, but they also contain random elements. Graphical models such as Bayesian networks provide a way to describe the relationships between random variables in a stochastic system. \nIn this thesis, I use Bayesian networks as an overarching framework to describe and solve problems in the areas of pattern classification, data compression, and channel coding. Results on the classification of handwritten digits show that Bayesian network pattern classifiers outperform other standard methods, such as the k-nearest neighbor method. When Bayesian networks are used as source models for data compression, an exponentially large number of codewords are associated with each input pattern. It turns out that the code can still be used efficiently, if a new technique called \"bits-back coding\" is used. Several new error-correcting decoding algorithms are instances of \"probability propagation\" in various Bayesian networks. These new schemes are rapidly closing the gap between the performances of practical channel coding systems and Shannon's 50-year-old channel coding limit. The Bayesian network framework exposes the similarities between these codes and leads the way to a new class of \"trellis-constraint codes\" which also operate close to Shannon's limit."
            },
            "slug": "Bayesian-networks-for-pattern-classification,-data-Hinton-Frey",
            "title": {
                "fragments": [],
                "text": "Bayesian networks for pattern classification, data compression, and channel coding"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The Bayesian network framework exposes the similarities between these codes and leads the way to a new class of \"trellis-constraint codes\" which also operate close to Shannon's limit."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144572614"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 127
                            }
                        ],
                        "text": "Publisher Item Identifier S 0733-8716(98)00225-X.\npropagation algorithms and [15] for an extensive treatment of graphical models.)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17285553,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f9ae39a71308a0bfe12fd5c1ba13165547be3cbd",
            "isKey": false,
            "numCitedBy": 494,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new family of error-correcting codes for the binary symmetric channel. These codes are designed to encode a sparse source, and are defined in terms of very sparse invertible matrices, in such a way that the decoder can treat the signal and the noise symmetrically. The decoding problem involves only very sparse matrices and sparse vectors, and so is a promising candidate for practical decoding."
            },
            "slug": "Good-Codes-Based-on-Very-Sparse-Matrices-Mackay",
            "title": {
                "fragments": [],
                "text": "Good Codes Based on Very Sparse Matrices"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "A new family of error-correcting codes for the binary symmetric channel is presented, designed to encode a sparse source, and are defined in terms of very sparse invertible matrices, in such a way that the decoder can treat the signal and the noise symmetrically."
            },
            "venue": {
                "fragments": [],
                "text": "IMACC"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30050592"
                        ],
                        "name": "S. Benedetto",
                        "slug": "S.-Benedetto",
                        "structuredName": {
                            "firstName": "Sergio",
                            "lastName": "Benedetto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Benedetto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714533"
                        ],
                        "name": "G. Montorsi",
                        "slug": "G.-Montorsi",
                        "structuredName": {
                            "firstName": "Guido",
                            "lastName": "Montorsi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Montorsi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 93
                            }
                        ],
                        "text": "A \u201cserially concatenated\u201d convolutional compound code was proposed by Benedetto and Montorsi [3]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 93
                            }
                        ],
                        "text": "Other examples of compound codes include classical serially concatenated codes [2] (see also [3], [4]), Gallager\u2019s low-density parity-check codes [5], and various product codes [6], [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 24
                            }
                        ],
                        "text": "[3] S. Benedetto and G. Montorsi, \u201cSerial concatenation of block and convolutional codes,\u201dElectron."
                    },
                    "intents": []
                }
            ],
            "corpusId": 3041978,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c64f5af23218570f3745e25f3682db3d91b16e56",
            "isKey": false,
            "numCitedBy": 133,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "Parallel concatenated coding schemes employing convolutional codes as constituent codes linked by an interleaver have been proposed in the literature as \u2018turbo codes\u2019. They yield very good performance in connection with simple suboptimum decoding algorithms. The authors propose an alternative scheme consisting in the serial concatenation of block or convolutional codes and evaluate its average performance in terms of bit error probability."
            },
            "slug": "Serial-concatenation-of-block-and-convolutional-Benedetto-Montorsi",
            "title": {
                "fragments": [],
                "text": "Serial concatenation of block and convolutional codes"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The authors propose an alternative scheme consisting in the serial concatenation of block or convolutional codes and evaluate its average performance in terms of bit error probability."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144572614"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The first to connect Pearl\u2019s \u201cbelief propagation\u201d algorithm with coding were MacKay and Neal [16\u201318], who showed that Gallager\u2019s 35-year-old algorithm [5] for decoding low-density parity-check codes is essentially an instance of Pearl\u2019s algorithm."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 122801915,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "33f275df4188cf8d51f3a85bd95ed2afa64196e4",
            "isKey": false,
            "numCitedBy": 2785,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors report the empirical performance of Gallager's low density parity check codes on Gaussian channels. They show that performance substantially better than that of standard convolutional and concatenated codes can be achieved; indeed the performance is almost as close to the Shannon limit as that of turbo codes."
            },
            "slug": "Near-Shannon-limit-performance-of-low-density-check-Mackay-Neal",
            "title": {
                "fragments": [],
                "text": "Near Shannon limit performance of low density parity check codes"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "The authors report the empirical performance of Gallager's low density parity check codes on Gaussian channels, showing that performance substantially better than that of standard convolutional and concatenated codes can be achieved."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749650"
                        ],
                        "name": "B. Frey",
                        "slug": "B.-Frey",
                        "structuredName": {
                            "firstName": "Brendan",
                            "lastName": "Frey",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Frey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1889982"
                        ],
                        "name": "F. Kschischang",
                        "slug": "F.-Kschischang",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Kschischang",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Kschischang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745359"
                        ],
                        "name": "P. Gulak",
                        "slug": "P.-Gulak",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Gulak",
                            "middleNames": [
                                "Glenn"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Gulak"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In our simulations of the flooding schedule for a turbo decoder [ 33 ], we observed that, compared with the standard message-passing schedule, several orders of magnitude more messages are passed, but if the messages are passed concurrently, then several orders of magnitude fewer time-steps are required to achieve a given decoding performance."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14558086,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a19da7b886bab018de8bd1e4b9ade2f6f2b3e939",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "The turbo-decoding algorithm can be viewed as a message passing procedure on a graph. We contrast the standard \"forward-backward\" turbo-decoding algorithm with a new \"concurrent\" algorithm that is suited to parallel implementation."
            },
            "slug": "Concurrent-turbo-decoding-Frey-Kschischang",
            "title": {
                "fragments": [],
                "text": "Concurrent turbo-decoding"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This work contrasts the standard \"forward-backward\" turbo-decoding algorithm with a new \"concurrent\" algorithm that is suited to parallel implementation."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE International Symposium on Information Theory"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 92
                            }
                        ],
                        "text": "(See [12\u201314] for textbook treatments on probability or \u201cbelief\u201d propagation algorithms, and [15] for an extensive treatment of graphical models."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6286159,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e16a25faf7428e1fc5ed0a10b8196c0499c7fd0d",
            "isKey": false,
            "numCitedBy": 3412,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Statistical applications in fields such as bioinformatics, information retrieval, speech processing, image processing and communications often involve large-scale models in which thousands or millions of random variables are linked in complex ways. Graphical models provide a general methodology for approaching these problems, and indeed many of the models developed by researchers in these applied fields are instances of the general graphical model formalism. We review some of the basic ideas underlying graphical models, including the algorithmic ideas that allow graphical models to be deployed in large-scale data analysis problems. We also present examples of graphical models in bioinformatics, error-control coding and language processing."
            },
            "slug": "Graphical-Models-Jordan",
            "title": {
                "fragments": [],
                "text": "Graphical Models"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Some of the basic ideas underlying graphical models are reviewed, including the algorithmic ideas that allow graphical models to be deployed in large-scale data analysis problems and examples of graphical models in bioinformatics, error-control coding and language processing are presented."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145430701"
                        ],
                        "name": "J. Pearl",
                        "slug": "J.-Pearl",
                        "structuredName": {
                            "firstName": "Judea",
                            "lastName": "Pearl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Pearl"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 10
                            }
                        ],
                        "text": "These algorithms have been developed in the past decade in the expert systems literature, most notably by Pearl [10] and Lauritzen and Spiegelhalter [11]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13723620,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bb0419bccc2244ed33c9c42341f342511262daa3",
            "isKey": false,
            "numCitedBy": 2148,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "Belief networks are directed acyclic graphs in which the nodes represent propositions (or variables), the arcs signify direct dependencies between the linked propositions, and the strengths of these dependencies are quantified by conditional probabilities. A network of this sort can be used to represent the generic knowledge of a domain expert, and it turns into a computational architecture if the links are used not merely for storing factual knowledge but also for directing and activating the data flow in the computations which manipulate this knowledge. The first part of the paper deals with the task of fusing and propagating the impacts of new information through the networks in such a way that, when equilibrium is reached, each proposition will be assigned a measure of belief consistent with the axioms of probability theory. It is shown that if the network is singly connected (e.g. tree-structured), then probabilities can be updated by local propagation in an isomorphic network of parallel and autonomous processors and that the impact of new information can be imparted to all propositions in time proportional to the longest path in the network. The second part of the paper deals with the problem of finding a tree-structured representation for a collection of probabilistically coupled propositions using auxiliary (dummy) variables, colloquially called \"hidden causes.\" It is shown that if such a tree-structured representation exists, then it is possible to uniquely uncover the topology of the tree by observing pairwise dependencies among the available propositions (i.e., the leaves of the tree). The entire tree structure, including the strengths of all internal relationships, can be reconstructed in time proportional to n log n, where n is the number of leaves."
            },
            "slug": "Fusion,-Propagation,-and-Structuring-in-Belief-Pearl",
            "title": {
                "fragments": [],
                "text": "Fusion, Propagation, and Structuring in Belief Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that if the network is singly connected (e.g. tree-structured), then probabilities can be updated by local propagation in an isomorphic network of parallel and autonomous processors and that the impact of new information can be imparted to all propositions in time proportional to the longest path in the network."
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3231485"
                        ],
                        "name": "S. Aji",
                        "slug": "S.-Aji",
                        "structuredName": {
                            "firstName": "Srinivas",
                            "lastName": "Aji",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Aji"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723150"
                        ],
                        "name": "R. McEliece",
                        "slug": "R.-McEliece",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "McEliece",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. McEliece"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "A derivation along similar lines has also been carried out recently by Aji and McEliece [ 29 ], who also develop an algorithm for \u201cinformation distribution\u201d on a graph."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Hence, the distributed marginalization algorithm described will work over any commutative semiring (see, e.g., [30, Sec. 3.2] and [21,  29 , 31])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 41359839,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "61e625eb3ab78a2e00e941a85ed41e342d110e7c",
            "isKey": true,
            "numCitedBy": 16,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a general \"message-passing\" algorithm for distributing information in a graph. This algorithm may help us to understand the approximate correctness of both the Gallager-Tanner-Wiberg algorithm, and the turbo-decoding algorithm."
            },
            "slug": "A-general-algorithm-for-distributing-information-in-Aji-McEliece",
            "title": {
                "fragments": [],
                "text": "A general algorithm for distributing information in a graph"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "A general \"message-passing\" algorithm for distributing information in a graph that may help to understand the approximate correctness of both the Gallager-Tanner-Wiberg algorithm, and the turbo-decoding algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE International Symposium on Information Theory"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2820860"
                        ],
                        "name": "R. Neapolitan",
                        "slug": "R.-Neapolitan",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Neapolitan",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Neapolitan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 77
                            }
                        ],
                        "text": "These algorithms have been developed in the past decade in the expert systems literature, most notably by Pearl [10] and Lauritzen and Spiegelhalter [11]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5473785,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9e1c26d71c62120ecfa0784bdf0b417ba6c6a982",
            "isKey": false,
            "numCitedBy": 703,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "This text is a reprint of the seminal 1989 book Probabilistic Reasoning in Expert systems: Theory and Algorithms, which helped serve to create the field we now call Bayesian networks. It introduces the properties of Bayesian networks (called causal networks in the text), discusses algorithms for doing inference in Bayesian networks, covers abductive inference, and provides an introduction to decision analysis. Furthermore, it compares rule-base experts systems to ones based on Bayesian networks, and it introduces the frequentist and Bayesian approaches to probability. Finally, it provides a critique of the maximum entropy formalism. Probabilistic Reasoning in Expert Systems was written from the perspective of a mathematician with the emphasis being on the development of theorems and algorithms. Every effort was made to make the material accessible. There are ample examples throughout the text. This text is important reading for anyone interested in both the fundamentals of Bayesian networks and in the history of how they came to be. It also provides an insightful comparison of the two most prominent approaches to probability."
            },
            "slug": "Probabilistic-reasoning-in-expert-systems-theory-Neapolitan",
            "title": {
                "fragments": [],
                "text": "Probabilistic reasoning in expert systems - theory and algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "This text is a reprint of the seminal 1989 book Probabilistic Reasoning in Expert systems: Theory and Algorithms, which helped serve to create the field the authors now call Bayesian networks and provides an insightful comparison of the two most prominent approaches to probability."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144018201"
                        ],
                        "name": "G. Forney",
                        "slug": "G.-Forney",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Forney",
                            "middleNames": [
                                "David"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Forney"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "called a \u201cTanner graph\u201d (first introduced by Tanner [22] to describe a generalization of Gallager codes) provides a natural setting in which to describe and study iterative soft-decision decoding techniques, much as the code trellis [23] is an ap-"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 115629689,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c6fc9864d560ba750993b3afa5b1b453fd02e31b",
            "isKey": false,
            "numCitedBy": 6,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "The concept of a trellis and the proof of the optimality of the Viterbi algorithm grew out of early work at Codex Corporation, here published for the first time. A recently observed flaw in this proof is noted. Trellises for block codes and lattices are of current interest. An absolutely minimal trellis is exhibited for the E8 lattice. This trellis gives a succinct summary of the algebraic, geometrical, and dynamical structure of E8 and its sublattices."
            },
            "slug": "Trellises-Old-and-New-Forney",
            "title": {
                "fragments": [],
                "text": "Trellises Old and New"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This trellis gives a succinct summary of the algebraic, geometrical, and dynamical structure of E8 and its sublattices and is exhibited for the E8 lattice."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2050895"
                        ],
                        "name": "S. Lauritzen",
                        "slug": "S.-Lauritzen",
                        "structuredName": {
                            "firstName": "Steffen",
                            "lastName": "Lauritzen",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lauritzen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48616434"
                        ],
                        "name": "D. Spiegelhalter",
                        "slug": "D.-Spiegelhalter",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Spiegelhalter",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Spiegelhalter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 149
                            }
                        ],
                        "text": "These algorithms have been developed in the past decade in the expert systems literature, most notably by Pearl [10] and Lauritzen and Spiegelhalter [11]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 31
                            }
                        ],
                        "text": "[11] S. L. Lauritzen and D. J. Spiegelhalter, \u201cLocal computations with probabilities on graphical structures and their application to expert systems,\u201dJ. Roy."
                    },
                    "intents": []
                }
            ],
            "corpusId": 58792451,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "0a3767909649cf31d32e087693d93171af28ebe0",
            "isKey": false,
            "numCitedBy": 4303,
            "numCiting": 126,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Local-computations-with-probabilities-on-graphical-Lauritzen-Spiegelhalter",
            "title": {
                "fragments": [],
                "text": "Local computations with probabilities on graphical structures and their application to expert systems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4640201"
                        ],
                        "name": "V. Isham",
                        "slug": "V.-Isham",
                        "structuredName": {
                            "firstName": "Valerie",
                            "lastName": "Isham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Isham"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121854003,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "c5632b40077a9783b5afaa65f0bbe918eb63cec0",
            "isKey": false,
            "numCitedBy": 88,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "Binary-valued Markov random fields may be used as models for point processes with interactions (e.g. repulsion or attraction) between their points. This paper aims to provide a simple nontechnical introduction to Markov random fields in this context. The underlying spaces on which points occur are taken to be countable (e.g. lattice vertices) or continuous (Euclidean space). The role of Markov random fields as equilibrium processes for the temporal evolution of spatial processes is also discussed and various applications and examples are given."
            },
            "slug": "An-Introduction-to-Spatial-Point-Processes-and-Isham",
            "title": {
                "fragments": [],
                "text": "An Introduction to Spatial Point Processes and Markov Random Fields"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "The role of Markov random fields as equilibrium processes for the temporal evolution of spatial processes is discussed and various applications and examples are given."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749650"
                        ],
                        "name": "B. Frey",
                        "slug": "B.-Frey",
                        "structuredName": {
                            "firstName": "Brendan",
                            "lastName": "Frey",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Frey"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 11
                            }
                        ],
                        "text": "In [8] and [9], we observed that iterative decoding algorithms developed for these compound codes are often instances of probability propagation algorithms that operate in a graphical model of the code."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62488180,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "629cc74dcaf655feea40f64cd74617ac884ed0f8",
            "isKey": false,
            "numCitedBy": 621,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Probabilistic inference in graphical models pattern classification unsupervised learning data compression channel coding future research directions."
            },
            "slug": "Graphical-Models-for-Machine-Learning-and-Digital-Frey",
            "title": {
                "fragments": [],
                "text": "Graphical Models for Machine Learning and Digital Communication"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "Probabilistic inference in graphical models pattern classification unsupervised learning data compression channel coding future research directions and how this affects research directions is investigated."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145430701"
                        ],
                        "name": "J. Pearl",
                        "slug": "J.-Pearl",
                        "structuredName": {
                            "firstName": "Judea",
                            "lastName": "Pearl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Pearl"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 21
                            }
                        ],
                        "text": "The first to connect Pearl\u2019s \u201cbelief propagation\u201d algorithm with coding were MacKay and Neal [16]\u2013[18], who showed that Gallager\u2019s 35-year-old algorithm [5] for decoding lowdensity parity-check codes is essentially an instance of Pearl\u2019s algorithm."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 100
                            }
                        ],
                        "text": "We now introduce Bayesian networks that, unlike MRF\u2019s and Tanner graphs, are directed acyclicgraphs [12]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 106
                            }
                        ],
                        "text": "These algorithms have been developed in the past decade in the expert systems literature, most notably by Pearl [10] and Lauritzen and Spiegelhalter [11]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 5
                            }
                        ],
                        "text": "(See [12]\u2013[14] for textbook treatments on probability or \u201cbelief\u201d"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 8
                            }
                        ],
                        "text": "[12] J. Pearl,Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 189
                            }
                        ],
                        "text": "For example, replacing the real-valued product operation with summation and the summation operation with the max operator will yield a generalization of the Viterbi algorithm equivalent to Pearl\u2019s \u201cbelief revision\u201d algorithm [12, Sect."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 8
                            }
                        ],
                        "text": "[10] J. Pearl, \u201cFusion, propagation, and structuring in belief networks,\u201dArtif."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 10
                            }
                        ],
                        "text": "Following Pearl [12], we denote child-to-parent messages as\nmessages, and parent-to-child messages asmessages, so that, e.g., and , are, respectively, messages transmitted from vertex to its parent and to its child ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 30
                            }
                        ],
                        "text": "They provide a description of Pearl\u2019s algorithm, and make explicit the connection to the basic turbo decoding algorithm described in [1]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 31
                            }
                        ],
                        "text": "This observation is the key to Pearl\u2019s \u201cbelief propagation\u201d algorithm."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 31
                            }
                        ],
                        "text": "This observation is the key to Pearl\u2019s \u201cbelief\u201d or probability propagation algorithm, which computes thea posteriori distribution exactly in a cycle-free Bayesian network."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 88
                            }
                        ],
                        "text": "[19] R. J. McEliece, D. J. C. MacKay, and J.-F. Cheng, \u201cTurbo decoding as an instance of Pearl\u2019s \u2018belief propagation\u2019 algorithm,\u201d this issue, pp. 140\u2013152."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 16
                            }
                        ],
                        "text": "Following Pearl [12], we denote child-to-parent messages as messages, and parent-to-child messages as messages, so that, e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 32583695,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "70ef29e6f0ce082bb8a47fd85b9bfb7cc0f20c93",
            "isKey": true,
            "numCitedBy": 18219,
            "numCiting": 230,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nProbabilistic Reasoning in Intelligent Systems is a complete andaccessible account of the theoretical foundations and computational methods that underlie plausible reasoning under uncertainty. The author provides a coherent explication of probability as a language for reasoning with partial belief and offers a unifying perspective on other AI approaches to uncertainty, such as the Dempster-Shafer formalism, truth maintenance systems, and nonmonotonic logic. The author distinguishes syntactic and semantic approaches to uncertainty\u0097and offers techniques, based on belief networks, that provide a mechanism for making semantics-based systems operational. Specifically, network-propagation techniques serve as a mechanism for combining the theoretical coherence of probability theory with modern demands of reasoning-systems technology: modular declarative inputs, conceptually meaningful inferences, and parallel distributed computation. Application areas include diagnosis, forecasting, image interpretation, multi-sensor fusion, decision support systems, plan recognition, planning, speech recognition\u0097in short, almost every task requiring that conclusions be drawn from uncertain clues and incomplete information. \nProbabilistic Reasoning in Intelligent Systems will be of special interest to scholars and researchers in AI, decision theory, statistics, logic, philosophy, cognitive psychology, and the management sciences. Professionals in the areas of knowledge-based systems, operations research, engineering, and statistics will find theoretical and computational tools of immediate practical use. The book can also be used as an excellent text for graduate-level courses in AI, operations research, or applied probability."
            },
            "slug": "Probabilistic-reasoning-in-intelligent-systems-of-Pearl",
            "title": {
                "fragments": [],
                "text": "Probabilistic reasoning in intelligent systems - networks of plausible inference"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The author provides a coherent explication of probability as a language for reasoning with partial belief and offers a unifying perspective on other AI approaches to uncertainty, such as the Dempster-Shafer formalism, truth maintenance systems, and nonmonotonic logic."
            },
            "venue": {
                "fragments": [],
                "text": "Morgan Kaufmann series in representation and reasoning"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50167999"
                        ],
                        "name": "C. Preston",
                        "slug": "C.-Preston",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Preston",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Preston"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 124
                            }
                        ],
                        "text": "Given a directed graph , let theparents(or direct ancestors) of vertex be the set of vertices of that have directed edges connectingto ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 118732645,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "057cfbe5b54f91c6823205692c1f72fe7a0f65ab",
            "isKey": false,
            "numCitedBy": 324,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Gibbs-States-on-Countable-Sets-Preston",
            "title": {
                "fragments": [],
                "text": "Gibbs States on Countable Sets"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145980949"
                        ],
                        "name": "S. Verd\u00fa",
                        "slug": "S.-Verd\u00fa",
                        "structuredName": {
                            "firstName": "Sergio",
                            "lastName": "Verd\u00fa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Verd\u00fa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40228025"
                        ],
                        "name": "V. Poor",
                        "slug": "V.-Poor",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Poor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Poor"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Initially we assume that the codomain is the set of real numbers, but later we will allow to be an arbitrary commutative semiring [21, 29\u201331]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 121014287,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "0a0a5b8eff2a7039b867045bbb58392751d3e4d1",
            "isKey": false,
            "numCitedBy": 88,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The unifying purpose of the abstract dynamic programming models is to find sufficient conditions on the recursive definition of the objective function that guarantee the validity of the dynamic programming iteration. This paper presents backward, forward, and backward-forward models that weaken previous sufficient conditions and that include, but are not restricted to, optimization problems. The backward-forward model is devoted to the simultaneous solution of a collection of interrelated sequential problems based on the independent computation of a cost-to-arrive function and a cost-to-go function. Several extremization and nonextremization problems illustrate the applicability of the proposed models."
            },
            "slug": "Abstract-dynamic-programming-models-under-Verd\u00fa-Poor",
            "title": {
                "fragments": [],
                "text": "Abstract dynamic programming models under commutativity conditions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 58779360,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8592e46a5435d18bba70557846f47290b34c1aa5",
            "isKey": false,
            "numCitedBy": 1336,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter contains sections titled: Relaxation Searches, Easy and Hard Learning, The Boltzmann Machine Learning Algorithm, An Example of Hard Learning, Achieving Reliable Computation with Unreliable Hardware, An Example of the Effects of Damage, Conclusion, Acknowledgments, Appendix: Derivation of the Learning Algorithm, References"
            },
            "slug": "Learning-and-relearning-in-Boltzmann-machines-Hinton-Sejnowski",
            "title": {
                "fragments": [],
                "text": "Learning and relearning in Boltzmann machines"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "This chapter contains sections titled: Relaxation Searches, Easy and Hard learning, The Boltzmann Machine Learning Algorithm, An Example of Hard Learning, Achieving Reliable Computation with Unreliable Hardware, and an Example of the Effects of Damage."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786249"
                        ],
                        "name": "D. Bertsekas",
                        "slug": "D.-Bertsekas",
                        "structuredName": {
                            "firstName": "Dimitri",
                            "lastName": "Bertsekas",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Bertsekas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770859"
                        ],
                        "name": "R. Gallager",
                        "slug": "R.-Gallager",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Gallager",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Gallager"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 64631197,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ca9e3eae58899e256046b5be3ef5dd68b85b4b40",
            "isKey": false,
            "numCitedBy": 6796,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Data-Networks-Bertsekas-Gallager",
            "title": {
                "fragments": [],
                "text": "Data Networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39361090"
                        ],
                        "name": "L. Baum",
                        "slug": "L.-Baum",
                        "structuredName": {
                            "firstName": "Leonard",
                            "lastName": "Baum",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Baum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "101775270"
                        ],
                        "name": "T. Petrie",
                        "slug": "T.-Petrie",
                        "structuredName": {
                            "firstName": "Ted",
                            "lastName": "Petrie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Petrie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 65
                            }
                        ],
                        "text": "The turbo decoding algorithm uses the forward\u2013backward algorithm [35], [36] (or an approximation to it) to process each constituent trellis."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 120208815,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "603bdbb17ba1f909280405a076455ac4f878fbf3",
            "isKey": false,
            "numCitedBy": 2773,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Statistical-Inference-for-Probabilistic-Functions-Baum-Petrie",
            "title": {
                "fragments": [],
                "text": "Statistical Inference for Probabilistic Functions of Finite State Markov Chains"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1966
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786249"
                        ],
                        "name": "D. Bertsekas",
                        "slug": "D.-Bertsekas",
                        "structuredName": {
                            "firstName": "Dimitri",
                            "lastName": "Bertsekas",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Bertsekas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770859"
                        ],
                        "name": "R. Gallager",
                        "slug": "R.-Gallager",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Gallager",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Gallager"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 54142085,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "731b1a222cc0c57a56e4fe56bf26855263fc7c53",
            "isKey": false,
            "numCitedBy": 1122,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Data-networks-(2nd-ed.)-Bertsekas-Gallager",
            "title": {
                "fragments": [],
                "text": "Data networks (2nd ed.)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102278025"
                        ],
                        "name": "Ross Kindermann",
                        "slug": "Ross-Kindermann",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Kindermann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross Kindermann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34296841"
                        ],
                        "name": "J. Snell",
                        "slug": "J.-Snell",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Snell",
                            "middleNames": [
                                "Laurie"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Snell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 34
                            }
                        ],
                        "text": "Tanner graphs were introduced in [22] for the construction of good long error-correcting codes in terms of shorter codes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 117120661,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "870234e41be333eb8ab128cbd1ca1623838b8d7f",
            "isKey": false,
            "numCitedBy": 1314,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Markov-Random-Fields-and-Their-Applications-Kindermann-Snell",
            "title": {
                "fragments": [],
                "text": "Markov Random Fields and Their Applications"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749650"
                        ],
                        "name": "B. Frey",
                        "slug": "B.-Frey",
                        "structuredName": {
                            "firstName": "Brendan",
                            "lastName": "Frey",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Frey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1889982"
                        ],
                        "name": "F. Kschischang",
                        "slug": "F.-Kschischang",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Kschischang",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Kschischang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 88
                            }
                        ],
                        "text": "Other examples of compound codes include classical serially concatenated codes [2] (see also [3], [4]), Gallager\u2019s low-density parity-check codes [5], and various product codes [6], [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18822496,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1ee14128a8a604b00f8a0d872b60538d7277df7d",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Probability-Propagation-and-Iterative-Decoding-Frey-Kschischang",
            "title": {
                "fragments": [],
                "text": "Probability Propagation and Iterative Decoding"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Preston,Gibbs States on Countable Sets"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1974
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 45
                            }
                        ],
                        "text": "of the flooding schedule for a turbo decoder [33], we observed"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Concurrent turbodecoding"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. 1997 IEEE Int. Symp. Inform. Theory"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "for a photograph and biography, see this issue"
            },
            "venue": {
                "fragments": [],
                "text": "for a photograph and biography, see this issue"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Kschischang is an Associate Professor in the Department of Electrical and Computer Engineering at the University of Toronto"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Information Theory"
            },
            "year": 1974
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 142
                            }
                        ],
                        "text": "Initially, we assume that the codomain is the set of real numbers, but later, we will allowto be an arbitrary commutative semiring [21], [29]\u2013[31]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On the BJCR trellis for linear block codes"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inform. Theory"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 295,
                                "start": 292
                            }
                        ],
                        "text": "The turbo decoding algorithm, the \u201cseparable MAP filter\u201d algorithm [6], the new iterative decoding algorithm [4] used for decoding \u201cserially concatenated\u201d convolutional codes, and, as first pointed out by MacKay and Neal [16], Gallager\u2019s algorithm for decoding low-density parity check codes [5] are all a form of probability propagation in the compound code networks shown in Fig."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 155
                            }
                        ],
                        "text": ") The first to connect Pearl\u2019s \u201cbelief propagation\u201d algorithm with coding were MacKay and Neal [16]\u2013[18], who showed that Gallager\u2019s 35-year-old algorithm [5] for decoding low-"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 146
                            }
                        ],
                        "text": "Other examples of compound codes include classical serially concatenated codes [2] (see also [3], [4]), Gallager\u2019s low-density parity-check codes [5], and various product codes [6], [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Gallager,Low-Density Parity-Check Codes.Cambridge"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1963
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "in [20] and Wiberg in his doctoral dissertation [21] have refocused attention on graphical models for codes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[20], [21] were probably the first to describe turbo codes using this type of graphical model."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "otter, \u201cCodes and iterative decoding on general graphs,\u201dEur"
            },
            "venue": {
                "fragments": [],
                "text": "Trans. Telecommun.  ,"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Kschischang (S'83\u2013M'91), for a photograph and biography, see this issue"
            },
            "venue": {
                "fragments": [],
                "text": "Kschischang (S'83\u2013M'91), for a photograph and biography, see this issue"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 10
                            }
                        ],
                        "text": "(See [12]\u2013[14] for textbook treatments on probability or \u201cbelief\u201d"
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Introduction to Bayesian Networks.New"
            },
            "venue": {
                "fragments": [],
                "text": "York: Springer Verlag,"
            },
            "year": 1996
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 19,
            "methodology": 16
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 45,
        "totalPages": 5
    },
    "page_url": "https://www.semanticscholar.org/paper/Iterative-Decoding-of-Compound-Codes-by-Probability-Kschischang-Frey/bbd45449e1cdadbf1f0c06a9510b5ac247cb70b9?sort=total-citations"
}