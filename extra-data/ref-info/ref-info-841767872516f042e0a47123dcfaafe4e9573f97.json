{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790831"
                        ],
                        "name": "C. D. Santos",
                        "slug": "C.-D.-Santos",
                        "structuredName": {
                            "firstName": "C\u00edcero",
                            "lastName": "Santos",
                            "middleNames": [
                                "Nogueira",
                                "dos"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. D. Santos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735228"
                        ],
                        "name": "B. Zadrozny",
                        "slug": "B.-Zadrozny",
                        "structuredName": {
                            "firstName": "Bianca",
                            "lastName": "Zadrozny",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Zadrozny"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 10
                            }
                        ],
                        "text": "Following (Santos and Zadrozny, 2014), it consists in two stages that are jointly learnt."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 0
                            }
                        ],
                        "text": "(Santos and Zadrozny, 2014) applies a very similar model to the POS tagging of Portuguese and English."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 109
                            }
                        ],
                        "text": "For sequence labeling, a more appropriate solution relies on the approach of (Collobert, 2011), also used in (Santos and Zadrozny, 2014)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2834402,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f70391f9a6aeec75bc52b9fe38588b9d0b4d40c6",
            "isKey": true,
            "numCitedBy": 528,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Distributed word representations have recently been proven to be an invaluable resource for NLP. These representations are normally learned using neural networks and capture syntactic and semantic information about words. Information about word morphology and shape is normally ignored when learning word representations. However, for tasks like part-of-speech tagging, intra-word information is extremely useful, specially when dealing with morphologically rich languages. In this paper, we propose a deep neural network that learns character-level representation of words and associate them with usual word representations to perform POS tagging. Using the proposed approach, while avoiding the use of any handcrafted feature, we produce state-of-the-art POS taggers for two languages: English, with 97.32% accuracy on the Penn Treebank WSJ corpus; and Portuguese, with 97.47% accuracy on the Mac-Morpho corpus, where the latter represents an error reduction of 12.2% on the best previous known result."
            },
            "slug": "Learning-Character-level-Representations-for-Santos-Zadrozny",
            "title": {
                "fragments": [],
                "text": "Learning Character-level Representations for Part-of-Speech Tagging"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A deep neural network is proposed that learns character-level representation of words and associate them with usual word representations to perform POS tagging and produces state-of-the-art POS taggers for two languages."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2939803"
                        ],
                        "name": "Ronan Collobert",
                        "slug": "Ronan-Collobert",
                        "structuredName": {
                            "firstName": "Ronan",
                            "lastName": "Collobert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronan Collobert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "21432929"
                        ],
                        "name": "Michael Karlen",
                        "slug": "Michael-Karlen",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Karlen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Karlen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2645384"
                        ],
                        "name": "K. Kavukcuoglu",
                        "slug": "K.-Kavukcuoglu",
                        "structuredName": {
                            "firstName": "Koray",
                            "lastName": "Kavukcuoglu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kavukcuoglu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46283650"
                        ],
                        "name": "P. Kuksa",
                        "slug": "P.-Kuksa",
                        "structuredName": {
                            "firstName": "Pavel",
                            "lastName": "Kuksa",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Kuksa"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 77
                            }
                        ],
                        "text": "To infer a word embedding , we use a convolution layer (Waibel et al., 1990; Collobert et al., 2011), build as in (Santos and Zadrozny, 2014)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 351666,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bc1022b031dc6c7019696492e8116598097a8c12",
            "isKey": false,
            "numCitedBy": 6658,
            "numCiting": 108,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements."
            },
            "slug": "Natural-Language-Processing-(Almost)-from-Scratch-Collobert-Weston",
            "title": {
                "fragments": [],
                "text": "Natural Language Processing (Almost) from Scratch"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "A unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2939803"
                        ],
                        "name": "Ronan Collobert",
                        "slug": "Ronan-Collobert",
                        "structuredName": {
                            "firstName": "Ronan",
                            "lastName": "Collobert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronan Collobert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 77
                            }
                        ],
                        "text": "For sequence labeling, a more appropriate solution relies on the approach of (Collobert, 2011), also used in (Santos and Zadrozny, 2014)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11135765,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d6d931fe9bfe7223165b14ec85d79d56e406e38a",
            "isKey": false,
            "numCitedBy": 186,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a new fast purely discriminative algorithm for natural language parsing, based on a \u201cdeep\u201d recurrent convolutional graph transformer network (GTN). Assuming a decomposition of a parse tree into a stack of \u201clevels\u201d, the network predicts a level of the tree taking into account predictions of previous levels. Using only few basic text features which leverage word representations from Collobert and Weston (2008), we show similar performance (in F1 score) to existing pure discriminative parsers and existing \u201cbenchmark\u201d parsers (like Collins parser, probabilistic context-free grammars based), with a huge speed advantage."
            },
            "slug": "Deep-Learning-for-Efficient-Discriminative-Parsing-Collobert",
            "title": {
                "fragments": [],
                "text": "Deep Learning for Efficient Discriminative Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "A new fast purely discriminative algorithm for natural language parsing, based on a \u201cdeep\u201d recurrent convolutional graph transformer network (GTN) and assuming a decomposition of a parse tree into a stack of \"levels\u201d, the network predicts a level of the tree taking into account predictions of previous levels."
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1821711"
                        ],
                        "name": "Thang Luong",
                        "slug": "Thang-Luong",
                        "structuredName": {
                            "firstName": "Thang",
                            "lastName": "Luong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thang Luong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2166511"
                        ],
                        "name": "R. Socher",
                        "slug": "R.-Socher",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Socher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Socher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 1
                            }
                        ],
                        "text": "(Luong et al., 2013) also descends lower than the word level, using a dictionary of morphemes and recursive neural networks to model the structure of the words."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14276764,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "53ab89807caead278d3deb7b6a4180b277d3cb77",
            "isKey": false,
            "numCitedBy": 794,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "Vector-space word representations have been very successful in recent years at improving performance across a variety of NLP tasks. However, common to most existing work, words are regarded as independent entities without any explicit relationship among morphologically related words being modeled. As a result, rare and complex words are often poorly estimated, and all unknown words are represented in a rather crude way using only one or a few vectors. This paper addresses this shortcoming by proposing a novel model that is capable of building representations for morphologically complex words from their morphemes. We combine recursive neural networks (RNNs), where each morpheme is a basic unit, with neural language models (NLMs) to consider contextual information in learning morphologicallyaware word representations. Our learned models outperform existing word representations by a good margin on word similarity tasks across many datasets, including a new dataset we introduce focused on rare words to complement existing ones in an interesting way."
            },
            "slug": "Better-Word-Representations-with-Recursive-Neural-Luong-Socher",
            "title": {
                "fragments": [],
                "text": "Better Word Representations with Recursive Neural Networks for Morphology"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper combines recursive neural networks, where each morpheme is a basic unit, with neural language models to consider contextual information in learning morphologicallyaware word representations and proposes a novel model capable of building representations for morphologically complex words from their morphemes."
            },
            "venue": {
                "fragments": [],
                "text": "CoNLL"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40360495"
                        ],
                        "name": "Helmut Schmid",
                        "slug": "Helmut-Schmid",
                        "structuredName": {
                            "firstName": "Helmut",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Helmut Schmid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1805808"
                        ],
                        "name": "Florian Laws",
                        "slug": "Florian-Laws",
                        "structuredName": {
                            "firstName": "Florian",
                            "lastName": "Laws",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Florian Laws"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 90
                            }
                        ],
                        "text": "To give an idea of how a simple model would perform on such task, the reader can refer to (Schmid and Laws, 2008) and (Mueller et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8958786,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fdceda7acf76cdeb587121110c8c795d721d81e7",
            "isKey": false,
            "numCitedBy": 191,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags. It is based on three ideas: (1) splitting of the POS tags into attribute vectors and decomposition of the contextual POS probabilities of the HMM into a product of attribute probabilities, (2) estimation of the contextual probabilities with decision trees, and (3) use of high-order HMMs. In experiments on German and Czech data, our tagger outperformed state-of-the-art POS taggers."
            },
            "slug": "Estimation-of-Conditional-Probabilities-With-Trees-Schmid-Laws",
            "title": {
                "fragments": [],
                "text": "Estimation of Conditional Probabilities With Decision Trees and an Application to Fine-Grained POS Tagging"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "A HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags based on splitting of the POS tags into attribute vectors and decomposition of the contextual POS probabilities of the HMM into a product of attribute probabilities."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2277248"
                        ],
                        "name": "Alexander M. Fraser",
                        "slug": "Alexander-M.-Fraser",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Fraser",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander M. Fraser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40360495"
                        ],
                        "name": "Helmut Schmid",
                        "slug": "Helmut-Schmid",
                        "structuredName": {
                            "firstName": "Helmut",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Helmut Schmid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144844761"
                        ],
                        "name": "Rich\u00e1rd Farkas",
                        "slug": "Rich\u00e1rd-Farkas",
                        "structuredName": {
                            "firstName": "Rich\u00e1rd",
                            "lastName": "Farkas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rich\u00e1rd Farkas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108875668"
                        ],
                        "name": "Renjing Wang",
                        "slug": "Renjing-Wang",
                        "structuredName": {
                            "firstName": "Renjing",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Renjing Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144418438"
                        ],
                        "name": "Hinrich Sch\u00fctze",
                        "slug": "Hinrich-Sch\u00fctze",
                        "structuredName": {
                            "firstName": "Hinrich",
                            "lastName": "Sch\u00fctze",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hinrich Sch\u00fctze"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 37
                            }
                        ],
                        "text": "The corpus, de-\nscribed in details in (Fraser et al., 2013), contains a training set of 40472 sentences, a development and a test set of both 5000 sentences."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 22
                            }
                        ],
                        "text": "scribed in details in (Fraser et al., 2013), contains a training set of 40472 sentences, a development and a test set of both 5000 sentences."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7126458,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9046dcd5a0a16dbe6ccbcaeb873d8447de37f624",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 69,
            "paperAbstract": {
                "fragments": [],
                "text": "We study constituent parsing of German, a morphologically rich and less-configurational language. We use a probabilistic context-free grammar treebank grammar that has been adapted to the morphologically rich properties of German by markovization and special features added to its productions. We evaluate the impact of adding lexical knowledge. Then we examine both monolingual and bilingual approaches to parse reranking. Our reranking parser is the new state of the art in constituency parsing of the TIGER Treebank. We perform an analysis, concluding with lessons learned, which apply to parsing other morphologically rich and less-configurational languages."
            },
            "slug": "Knowledge-Sources-for-Constituent-Parsing-of-a-Rich-Fraser-Schmid",
            "title": {
                "fragments": [],
                "text": "Knowledge Sources for Constituent Parsing of German, a Morphologically Rich and Less-Configurational Language"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "This work uses a probabilistic context-free grammar treebank grammar that has been adapted to the morphologically rich properties of German by markovization and special features added to its productions to create a reranking parser that is the new state of the art in constituency parsing of the TIGER Treebank."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2047446108"
                        ],
                        "name": "Tomas Mikolov",
                        "slug": "Tomas-Mikolov",
                        "structuredName": {
                            "firstName": "Tomas",
                            "lastName": "Mikolov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tomas Mikolov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701686"
                        ],
                        "name": "Ilya Sutskever",
                        "slug": "Ilya-Sutskever",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Sutskever",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilya Sutskever"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713801"
                        ],
                        "name": "Anoop Deoras",
                        "slug": "Anoop-Deoras",
                        "structuredName": {
                            "firstName": "Anoop",
                            "lastName": "Deoras",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anoop Deoras"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2784529"
                        ],
                        "name": "H. Le",
                        "slug": "H.-Le",
                        "structuredName": {
                            "firstName": "Hai",
                            "lastName": "Le",
                            "middleNames": [
                                "Son"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Le"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2675251"
                        ],
                        "name": "Stefan Kombrink",
                        "slug": "Stefan-Kombrink",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Kombrink",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stefan Kombrink"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1899242"
                        ],
                        "name": "J. Cernock\u00fd",
                        "slug": "J.-Cernock\u00fd",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Cernock\u00fd",
                            "middleNames": [
                                "Honza"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cernock\u00fd"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 109
                            }
                        ],
                        "text": "While its raw application to language modeling did not achieve clear improvement over the word-based models (Mikolov et al., 2012), this approach shown impressive results for text generation (Sutskever et al., 2011; Graves, 2013)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 108
                            }
                        ],
                        "text": "While its raw application to language modeling did not achieve clear improvement over the word-based models (Mikolov et al., 2012), this approach shown impressive results for text generation (Sutskever et al."
                    },
                    "intents": []
                }
            ],
            "corpusId": 46542477,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1fd7fc06653723b05abe5f3d1de393ddcf6bdddb",
            "isKey": false,
            "numCitedBy": 180,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We explore the performance of several types of language mode ls on the word-level and the character-level language modelin g tasks. This includes two recently proposed recurrent neural netwo rk architectures, a feedforward neural network model, a maximum ent ropy model and the usual smoothed n-gram models. We then propose a simple technique for learning sub-word level units from th e data, and show that it combines advantages of both character and wo rdlevel models. Finally, we show that neural network based lan gu ge models can be order of magnitude smaller than compressed n-g ram models, at the same level of performance when applied to a Bro dcast news RT04 speech recognition task. By using sub-word un its, the size can be reduced even more."
            },
            "slug": "SUBWORD-LANGUAGE-MODELING-WITH-NEURAL-NETWORKS-Mikolov-Sutskever",
            "title": {
                "fragments": [],
                "text": "SUBWORD LANGUAGE MODELING WITH NEURAL NETWORKS"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A simple technique for learning sub-word level units from data is proposed, and it is shown that neural network based models can be order of magnitude smaller than compressed n-g ram models, at the same level of performance when applied to a Bro dcast news RT04 speech recognition task."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46447747"
                        ],
                        "name": "X. Zhang",
                        "slug": "X.-Zhang",
                        "structuredName": {
                            "firstName": "Xiang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 3
                            }
                        ],
                        "text": "In (Zhang and LeCun, 2015), the authors build a text understanding model that does not require any knowledge and uses hierarchical feature extraction."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14257320,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f1b0df6b2977d28e55df82576b108e4f5d87e044",
            "isKey": false,
            "numCitedBy": 484,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "This article demontrates that we can apply deep learning to text understanding from character-level inputs all the way up to abstract text concepts, using temporal convolutional networks (ConvNets). We apply ConvNets to various large-scale datasets, including ontology classification, sentiment analysis, and text categorization. We show that temporal ConvNets can achieve astonishing performance without the knowledge of words, phrases, sentences and any other syntactic or semantic structures with regards to a human language. Evidence shows that our models can work for both English and Chinese."
            },
            "slug": "Text-Understanding-from-Scratch-Zhang-LeCun",
            "title": {
                "fragments": [],
                "text": "Text Understanding from Scratch"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown that temporal ConvNets can achieve astonishing performance without the knowledge of words, phrases, sentences and any other syntactic or semantic structures with regards to a human language."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40150608"
                        ],
                        "name": "Thomas M\u00fcller",
                        "slug": "Thomas-M\u00fcller",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas M\u00fcller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40360495"
                        ],
                        "name": "Helmut Schmid",
                        "slug": "Helmut-Schmid",
                        "structuredName": {
                            "firstName": "Helmut",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Helmut Schmid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144418438"
                        ],
                        "name": "Hinrich Sch\u00fctze",
                        "slug": "Hinrich-Sch\u00fctze",
                        "structuredName": {
                            "firstName": "Hinrich",
                            "lastName": "Sch\u00fctze",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hinrich Sch\u00fctze"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 41
                            }
                        ],
                        "text": "CRF refers to state-of-the-art system of (Mueller et al., 2013)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 119
                            }
                        ],
                        "text": "To give an idea of how a simple model would perform on such task, the reader can refer to (Schmid and Laws, 2008) and (Mueller et al., 2013)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 79
                            }
                        ],
                        "text": "To the best of our knowledge, the best results on this task were published in (Mueller et al., 2013), who applied a high-order CRF that includes an intensive feature engineering to five different languages."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 91
                            }
                        ],
                        "text": "Results are reported in Table 1 along with with the state-of-the-art results published in (Mueller et al., 2013)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 113
                            }
                        ],
                        "text": "This shows that information from the context and from the morphology are complementary, which is conjectured in (Mueller et al., 2013) by using a morphological analyzer in complement of higher-order CRF."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7708374,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "58f0d54f2f3f1842c2b8687fc8eb7f8f4c5184e4",
            "isKey": true,
            "numCitedBy": 204,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "Training higher-order conditional random fields is prohibitive for huge tag sets. We present an approximated conditional random field using coarse-to-fine decoding and early updating. We show that our implementation yields fast and accurate morphological taggers across six languages with different morphological properties and that across languages higher-order models give significant improvements over 1-order models."
            },
            "slug": "Efficient-Higher-Order-CRFs-for-Morphological-M\u00fcller-Schmid",
            "title": {
                "fragments": [],
                "text": "Efficient Higher-Order CRFs for Morphological Tagging"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "This work presents an approximated conditional random field using coarse-to-fine decoding and early updating that yields fast and accurate morphological taggers across six languages with different morphological properties and that across languages higher-order models give significant improvements over 1- order models."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36037226"
                        ],
                        "name": "R\u00e9jean Ducharme",
                        "slug": "R\u00e9jean-Ducharme",
                        "structuredName": {
                            "firstName": "R\u00e9jean",
                            "lastName": "Ducharme",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R\u00e9jean Ducharme"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120247189"
                        ],
                        "name": "Pascal Vincent",
                        "slug": "Pascal-Vincent",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Vincent",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pascal Vincent"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1909943744"
                        ],
                        "name": "Christian Janvin",
                        "slug": "Christian-Janvin",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Janvin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian Janvin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 61
                            }
                        ],
                        "text": "To obtain word embeddings, the usual approach introduced by (Bengio et al., 2003) relies on a fixed vocabulary W and each word w \u2208 W is mapped to a vector of nf real valued features by a look-up matrix W \u2208 R|W|\u2217nf ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 221275765,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6c2b28f9354f667cd5bd07afc0471d8334430da7",
            "isKey": false,
            "numCitedBy": 6011,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts."
            },
            "slug": "A-Neural-Probabilistic-Language-Model-Bengio-Ducharme",
            "title": {
                "fragments": [],
                "text": "A Neural Probabilistic Language Model"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work proposes to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2354728"
                        ],
                        "name": "A. Karpathy",
                        "slug": "A.-Karpathy",
                        "structuredName": {
                            "firstName": "Andrej",
                            "lastName": "Karpathy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Karpathy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115231104"
                        ],
                        "name": "Justin Johnson",
                        "slug": "Justin-Johnson",
                        "structuredName": {
                            "firstName": "Justin",
                            "lastName": "Johnson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Justin Johnson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 137
                            }
                        ],
                        "text": "More recently, the character level was considered as more interpretable and convenient way to explore and understand recurrent networks (Karpathy et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 988348,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "40be3888daa5c2e5af4d36ae22f690bcc8caf600",
            "isKey": false,
            "numCitedBy": 911,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "Recurrent Neural Networks (RNNs), and specifically a variant with Long Short-Term Memory (LSTM), are enjoying renewed interest as a result of successful applications in a wide range of machine learning problems that involve sequential data. However, while LSTMs provide exceptional results in practice, the source of their performance and their limitations remain rather poorly understood. Using character-level language models as an interpretable testbed, we aim to bridge this gap by providing an analysis of their representations, predictions and error types. In particular, our experiments reveal the existence of interpretable cells that keep track of long-range dependencies such as line lengths, quotes and brackets. Moreover, our comparative analysis with finite horizon n-gram models traces the source of the LSTM improvements to long-range structural dependencies. Finally, we provide analysis of the remaining errors and suggests areas for further study."
            },
            "slug": "Visualizing-and-Understanding-Recurrent-Networks-Karpathy-Johnson",
            "title": {
                "fragments": [],
                "text": "Visualizing and Understanding Recurrent Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work uses character-level language models as an interpretable testbed to provide an analysis of LSTM representations, predictions and error types, and reveals the existence of interpretable cells that keep track of long-range dependencies such as line lengths, quotes and brackets."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701686"
                        ],
                        "name": "Ilya Sutskever",
                        "slug": "Ilya-Sutskever",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Sutskever",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilya Sutskever"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145704247"
                        ],
                        "name": "James Martens",
                        "slug": "James-Martens",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Martens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Martens"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 68
                            }
                        ],
                        "text": ", 2012), this approach shown impressive results for text generation (Sutskever et al., 2011; Graves, 2013)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 192
                            }
                        ],
                        "text": "While its raw application to language modeling did not achieve clear improvement over the word-based models (Mikolov et al., 2012), this approach shown impressive results for text generation (Sutskever et al., 2011; Graves, 2013)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 8843166,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e0e5dd8b206806372b3e20b9a2fbdbd0cf9ce1de",
            "isKey": false,
            "numCitedBy": 1254,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Recurrent Neural Networks (RNNs) are very powerful sequence models that do not enjoy widespread use because it is extremely difficult to train them properly. Fortunately, recent advances in Hessian-free optimization have been able to overcome the difficulties associated with training RNNs, making it possible to apply them successfully to challenging sequence problems. In this paper we demonstrate the power of RNNs trained with the new Hessian-Free optimizer (HF) by applying them to character-level language modeling tasks. The standard RNN architecture, while effective, is not ideally suited for such tasks, so we introduce a new RNN variant that uses multiplicative (or \"gated\") connections which allow the current input character to determine the transition matrix from one hidden state vector to the next. After training the multiplicative RNN with the HF optimizer for five days on 8 high-end Graphics Processing Units, we were able to surpass the performance of the best previous single method for character-level language modeling \u2013 a hierarchical non-parametric sequence model. To our knowledge this represents the largest recurrent neural network application to date."
            },
            "slug": "Generating-Text-with-Recurrent-Neural-Networks-Sutskever-Martens",
            "title": {
                "fragments": [],
                "text": "Generating Text with Recurrent Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The power of RNNs trained with the new Hessian-Free optimizer by applying them to character-level language modeling tasks is demonstrated, and a new RNN variant that uses multiplicative connections which allow the current input character to determine the transition matrix from one hidden state vector to the next is introduced."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2339384"
                        ],
                        "name": "Sabine Brants",
                        "slug": "Sabine-Brants",
                        "structuredName": {
                            "firstName": "Sabine",
                            "lastName": "Brants",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sabine Brants"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1889046"
                        ],
                        "name": "Stefanie Dipper",
                        "slug": "Stefanie-Dipper",
                        "structuredName": {
                            "firstName": "Stefanie",
                            "lastName": "Dipper",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stefanie Dipper"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2770771"
                        ],
                        "name": "Silvia Hansen",
                        "slug": "Silvia-Hansen",
                        "structuredName": {
                            "firstName": "Silvia",
                            "lastName": "Hansen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Silvia Hansen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2805308"
                        ],
                        "name": "Wolfgang Lezius",
                        "slug": "Wolfgang-Lezius",
                        "structuredName": {
                            "firstName": "Wolfgang",
                            "lastName": "Lezius",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wolfgang Lezius"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108191087"
                        ],
                        "name": "George Smith",
                        "slug": "George-Smith",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Smith",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "George Smith"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 122
                            }
                        ],
                        "text": "Experiments are carried out on the Part-of-Speech and Morphological tagging tasks using the German corpus TIGER Treebank (Brants et al., 2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6209052,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0a3713b0e86b282b1821e29688d3b28a7bfbc3e5",
            "isKey": false,
            "numCitedBy": 561,
            "numCiting": 88,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper reports on the TIGER Treebank, a corpus of currently 35.000 syntactically annotated German newspaper sentences. We describe what kind of information is encoded in the treebank and introduce the different representation formats that are used for the annotation and exploitation of the treebank. We explain the different methods used for the annotation: interactive annotation, using the tool Annotate, and LFG parsing. Furthermore, we give an account of the annotation scheme used for the TIGER treebank. This scheme is an extended and improved version of the NEGRA annotation scheme and we illustrate in detail the linguistic extensions that were made concerning the annotation in the TIGER project. The main differences are concerned with coordination, verb-subcategorization, expletives as well as proper nouns. In addition, the paper also presents the query tool TIGERSearch that was developed in the project to exploit the treebank in an adequate way. We describe the query language which was designed to facilitate a simple formulation of complex queries; furthermore, we shortly introduce TIGERin, a graphical user interface for query input. The paper concludes with a summary and some directions for future work."
            },
            "slug": "The-TIGER-Treebank-Brants-Dipper",
            "title": {
                "fragments": [],
                "text": "The TIGER Treebank"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "The TIGER Treebank, a corpus of currently 35.000 syntactically annotated German newspaper sentences, is reported on and what kind of information is encoded in the treebank is described and the different representation formats are introduced."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753223"
                        ],
                        "name": "A. Graves",
                        "slug": "A.-Graves",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Graves",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Graves"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3111912"
                        ],
                        "name": "Navdeep Jaitly",
                        "slug": "Navdeep-Jaitly",
                        "structuredName": {
                            "firstName": "Navdeep",
                            "lastName": "Jaitly",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Navdeep Jaitly"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40360972"
                        ],
                        "name": "Abdel-rahman Mohamed",
                        "slug": "Abdel-rahman-Mohamed",
                        "structuredName": {
                            "firstName": "Abdel-rahman",
                            "lastName": "Mohamed",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Abdel-rahman Mohamed"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 76
                            }
                        ],
                        "text": "We also experiment with a a bidirectional recurrent layer, as described in (Graves et al., 2013)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3338763,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1149888d75af4ed5dffc25731b875651c3ccdeb2",
            "isKey": false,
            "numCitedBy": 1300,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Deep Bidirectional LSTM (DBLSTM) recurrent neural networks have recently been shown to give state-of-the-art performance on the TIMIT speech database. However, the results in that work relied on recurrent-neural-network-specific objective functions, which are difficult to integrate with existing large vocabulary speech recognition systems. This paper investigates the use of DBLSTM as an acoustic model in a standard neural network-HMM hybrid system. We find that a DBLSTM-HMM hybrid gives equally good results on TIMIT as the previous work. It also outperforms both GMM and deep network benchmarks on a subset of the Wall Street Journal corpus. However the improvement in word error rate over the deep network is modest, despite a great increase in framelevel accuracy. We conclude that the hybrid approach with DBLSTM appears to be well suited for tasks where acoustic modelling predominates. Further investigation needs to be conducted to understand how to better leverage the improvements in frame-level accuracy towards better word error rates."
            },
            "slug": "Hybrid-speech-recognition-with-Deep-Bidirectional-Graves-Jaitly",
            "title": {
                "fragments": [],
                "text": "Hybrid speech recognition with Deep Bidirectional LSTM"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The hybrid approach with DBLSTM appears to be well suited for tasks where acoustic modelling predominates, and the improvement in word error rate over the deep network is modest, despite a great increase in framelevel accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE Workshop on Automatic Speech Recognition and Understanding"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753223"
                        ],
                        "name": "A. Graves",
                        "slug": "A.-Graves",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Graves",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Graves"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 68
                            }
                        ],
                        "text": ", 2012), this approach shown impressive results for text generation (Sutskever et al., 2011; Graves, 2013)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1697424,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "89b1f4740ae37fd04f6ac007577bdd34621f0861",
            "isKey": false,
            "numCitedBy": 3153,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles."
            },
            "slug": "Generating-Sequences-With-Recurrent-Neural-Networks-Graves",
            "title": {
                "fragments": [],
                "text": "Generating Sequences With Recurrent Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724972"
                        ],
                        "name": "A. Waibel",
                        "slug": "A.-Waibel",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Waibel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Waibel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40396597"
                        ],
                        "name": "Toshiyuki Hanazawa",
                        "slug": "Toshiyuki-Hanazawa",
                        "structuredName": {
                            "firstName": "Toshiyuki",
                            "lastName": "Hanazawa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Toshiyuki Hanazawa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9243990"
                        ],
                        "name": "K. Shikano",
                        "slug": "K.-Shikano",
                        "structuredName": {
                            "firstName": "Kiyohiro",
                            "lastName": "Shikano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Shikano"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49464494"
                        ],
                        "name": "Kevin J. Lang",
                        "slug": "Kevin-J.-Lang",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Lang",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin J. Lang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9563026,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cd62c9976534a6a2096a38244f6cbb03635a127e",
            "isKey": false,
            "numCitedBy": 2787,
            "numCiting": 92,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors present a time-delay neural network (TDNN) approach to phoneme recognition which is characterized by two important properties: (1) using a three-layer arrangement of simple computing units, a hierarchy can be constructed that allows for the formation of arbitrary nonlinear decision surfaces, which the TDNN learns automatically using error backpropagation; and (2) the time-delay arrangement enables the network to discover acoustic-phonetic features and the temporal relationships between them independently of position in time and therefore not blurred by temporal shifts in the input. As a recognition task, the speaker-dependent recognition of the phonemes B, D, and G in varying phonetic contexts was chosen. For comparison, several discrete hidden Markov models (HMM) were trained to perform the same task. Performance evaluation over 1946 testing tokens from three speakers showed that the TDNN achieves a recognition rate of 98.5% correct while the rate obtained by the best of the HMMs was only 93.7%. >"
            },
            "slug": "Phoneme-recognition-using-time-delay-neural-Waibel-Hanazawa",
            "title": {
                "fragments": [],
                "text": "Phoneme recognition using time-delay neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The authors present a time-delay neural network (TDNN) approach to phoneme recognition which is characterized by two important properties: using a three-layer arrangement of simple computing units, a hierarchy can be constructed that allows for the formation of arbitrary nonlinear decision surfaces, which the TDNN learns automatically using error backpropagation."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Acoust. Speech Signal Process."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734693"
                        ],
                        "name": "John C. Duchi",
                        "slug": "John-C.-Duchi",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Duchi",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John C. Duchi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34840427"
                        ],
                        "name": "Elad Hazan",
                        "slug": "Elad-Hazan",
                        "structuredName": {
                            "firstName": "Elad",
                            "lastName": "Hazan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Elad Hazan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 33
                            }
                        ],
                        "text": "For optimization, we use Adagrad (Duchi et al., 2011), with a learning rate of 0."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 34
                            }
                        ],
                        "text": "For optimization, we use Adagrad (Duchi et al., 2011), with a learning rate of 0.1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 538820,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "413c1142de9d91804d6d11c67ff3fed59c9fc279",
            "isKey": true,
            "numCitedBy": 8025,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms."
            },
            "slug": "Adaptive-Subgradient-Methods-for-Online-Learning-Duchi-Hazan",
            "title": {
                "fragments": [],
                "text": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work describes and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal functions that can be chosen in hindsight."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145500689"
                        ],
                        "name": "A. Viterbi",
                        "slug": "A.-Viterbi",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Viterbi",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Viterbi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 22
                            }
                        ],
                        "text": "The Viterbi algorithm (Viterbi, 1967) offers an exact solution to infer the path that gives the maximum score."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15843983,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "145c0b53514b02bdc3dadfb2e1cea124f2abd99b",
            "isKey": false,
            "numCitedBy": 5209,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "The probability of error in decoding an optimal convolutional code transmitted over a memoryless channel is bounded from above and below as a function of the constraint length of the code. For all but pathological channels the bounds are asymptotically (exponentially) tight for rates above R_{0} , the computational cutoff rate of sequential decoding. As a function of constraint length the performance of optimal convolutional codes is shown to be superior to that of block codes of the same length, the relative improvement increasing with rate. The upper bound is obtained for a specific probabilistic nonsequential decoding algorithm which is shown to be asymptotically optimum for rates above R_{0} and whose performance bears certain similarities to that of sequential decoding algorithms."
            },
            "slug": "Error-bounds-for-convolutional-codes-and-an-optimum-Viterbi",
            "title": {
                "fragments": [],
                "text": "Error bounds for convolutional codes and an asymptotically optimum decoding algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The upper bound is obtained for a specific probabilistic nonsequential decoding algorithm which is shown to be asymptotically optimum for rates above R_{0} and whose performance bears certain similarities to that of sequential decoding algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1967
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 137
                            }
                        ],
                        "text": "More recently, the character level was considered as more interpretable and convenient way to explore and understand recurrent networks (Karpathy et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Visualizing and understanding recurrent networks. CoRR, abs/1506"
            },
            "venue": {
                "fragments": [],
                "text": "Visualizing and understanding recurrent networks. CoRR, abs/1506"
            },
            "year": 2015
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Text understanding from scratch. CoRR, abs/1502"
            },
            "venue": {
                "fragments": [],
                "text": "Text understanding from scratch. CoRR, abs/1502"
            },
            "year": 1710
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 56
                            }
                        ],
                        "text": "To infer a word embedding , we use a convolution layer (Waibel et al., 1990; Collobert et al., 2011), build as in (Santos and Zadrozny, 2014)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Readings in Speech Recognition, chapter Phoneme Recognition Using Time-delay Neural Networks, pages 393\u2013404"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 56
                            }
                        ],
                        "text": "To infer a word embedding , we use a convolution layer (Waibel et al., 1990; Collobert et al., 2011), build as in (Santos and Zadrozny, 2014)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Readings in Speech Recognition, chapter Phoneme Recognition Using Time-delay Neural Networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Theano: a CPU and GPU math expression compiler"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Python for Scientific Computing Conference (SciPy)"
            }
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 6,
            "methodology": 13,
            "result": 2
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 23,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Non-lexical-neural-architecture-for-fine-grained-Labeau-L\u00f6ser/841767872516f042e0a47123dcfaafe4e9573f97?sort=total-citations"
}