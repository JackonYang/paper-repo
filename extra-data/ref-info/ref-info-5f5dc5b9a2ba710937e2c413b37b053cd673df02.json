{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2615814"
                        ],
                        "name": "R. Ranganath",
                        "slug": "R.-Ranganath",
                        "structuredName": {
                            "firstName": "Rajesh",
                            "lastName": "Ranganath",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Ranganath"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "21007048"
                        ],
                        "name": "S. Gerrish",
                        "slug": "S.-Gerrish",
                        "structuredName": {
                            "firstName": "Sean",
                            "lastName": "Gerrish",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Gerrish"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796335"
                        ],
                        "name": "D. Blei",
                        "slug": "D.-Blei",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Blei",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Blei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1580089,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6a667700100e228cb30a5d884258a0db921603fe",
            "isKey": false,
            "numCitedBy": 845,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Variational inference has become a widely used method to approximate posteriors in complex latent variables models. However, deriving a variational inference algorithm generally requires significant model-specific analysis, and these efforts can hinder and deter us from quickly developing and exploring a variety of models for a problem at hand. In this paper, we present a \"black box\" variational inference algorithm, one that can be quickly applied to many models with little additional derivation. Our method is based on a stochastic optimization of the variational objective where the noisy gradient is computed from Monte Carlo samples from the variational distribution. We develop a number of methods to reduce the variance of the gradient, always maintaining the criterion that we want to avoid difficult model-based derivations. We evaluate our method against the corresponding black box sampling based methods. We find that our method reaches better predictive likelihoods much faster than sampling methods. Finally, we demonstrate that Black Box Variational Inference lets us easily explore a wide space of models by quickly constructing and evaluating several models of longitudinal healthcare data."
            },
            "slug": "Black-Box-Variational-Inference-Ranganath-Gerrish",
            "title": {
                "fragments": [],
                "text": "Black Box Variational Inference"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper presents a \"black box\" variational inference algorithm, one that can be quickly applied to many models with little additional derivation, based on a stochastic optimization of the variational objective where the noisy gradient is computed from Monte Carlo samples from the Variational distribution."
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748523"
                        ],
                        "name": "Danilo Jimenez Rezende",
                        "slug": "Danilo-Jimenez-Rezende",
                        "structuredName": {
                            "firstName": "Danilo",
                            "lastName": "Jimenez Rezende",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Danilo Jimenez Rezende"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "14594344"
                        ],
                        "name": "S. Mohamed",
                        "slug": "S.-Mohamed",
                        "structuredName": {
                            "firstName": "Shakir",
                            "lastName": "Mohamed",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mohamed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688276"
                        ],
                        "name": "Daan Wierstra",
                        "slug": "Daan-Wierstra",
                        "structuredName": {
                            "firstName": "Daan",
                            "lastName": "Wierstra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daan Wierstra"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16935709,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f87247fb37f6b48da0757d7a1acf38da44510cdb",
            "isKey": false,
            "numCitedBy": 105,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition model to represent approximate posterior distributions, and that acts as a stochastic encoder of the data. We develop stochastic backpropagation \u2013 rules for back-propagation through stochastic variables \u2013 and use this to develop an algorithm that allows for joint optimisation of the parameters of both the generative and recognition model. We demonstrate on several real-world data sets that the model generates realistic samples, provides accurate imputations of missing data and is a useful tool for high-dimensional data visualisation."
            },
            "slug": "Stochastic-Back-propagation-and-Variational-in-Deep-Rezende-Mohamed",
            "title": {
                "fragments": [],
                "text": "Stochastic Back-propagation and Variational Inference in Deep Latent Gaussian Models"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "This work marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning that introduces a recognition model to represent approximate posterior distributions and that acts as a stochastic encoder of the data."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143855009"
                        ],
                        "name": "J. Paisley",
                        "slug": "J.-Paisley",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Paisley",
                            "middleNames": [
                                "William"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Paisley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796335"
                        ],
                        "name": "D. Blei",
                        "slug": "D.-Blei",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Blei",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Blei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "rectly in\ufb02uence the estimate through the samples z(l) \u02d8q \u02da(zjx), and it is impossible to differentiate through this sampling process. Existing work on stochastic variational bayes provide workarounds [BJP12], but not a solution to this problem. 2.3 Our estimator of the lower bound Under certain mild conditions outlined in section 2.4 for a chosen approximate posterior q \u02da(zjx) we can reparameterize its c"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1758804,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9ceb1dea15ac3df3d610fd0b3cc52b9a4e9141a3",
            "isKey": true,
            "numCitedBy": 384,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Mean-field variational inference is a method for approximate Bayesian posterior inference. It approximates a full posterior distribution with a factorized set of distributions by maximizing a lower bound on the marginal likelihood. This requires the ability to integrate a sum of terms in the log joint likelihood using this factorized distribution. Often not all integrals are in closed form, which is typically handled by using a lower bound. We present an alternative algorithm based on stochastic optimization that allows for direct optimization of the variational lower bound. This method uses control variates to reduce the variance of the stochastic search gradient, in which existing lower bounds can play an important role. We demonstrate the approach on two non-conjugate models: logistic regression and an approximation to the HDP."
            },
            "slug": "Variational-Bayesian-Inference-with-Stochastic-Paisley-Blei",
            "title": {
                "fragments": [],
                "text": "Variational Bayesian Inference with Stochastic Search"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work presents an alternative algorithm based on stochastic optimization that allows for direct optimization of the variational lower bound and demonstrates the approach on two non-conjugate models: logistic regression and an approximation to the HDP."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "28552618"
                        ],
                        "name": "M. Hoffman",
                        "slug": "M.-Hoffman",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Hoffman",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hoffman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796335"
                        ],
                        "name": "D. Blei",
                        "slug": "D.-Blei",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Blei",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Blei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108881999"
                        ],
                        "name": "Chong Wang",
                        "slug": "Chong-Wang",
                        "structuredName": {
                            "firstName": "Chong",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chong Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143855009"
                        ],
                        "name": "J. Paisley",
                        "slug": "J.-Paisley",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Paisley",
                            "middleNames": [
                                "William"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Paisley"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": ". An advantage of wake-sleep is that it also applies to models with discrete latent variables. Wake-Sleep has the same computational complexity as AEVB per datapoint. Stochastic variational inference [HBWP13] has recently received increasing interest. Recently, [BJP12] introduced a control variate schemes to reduce the high variance of the na\u00a8\u0131ve gradient estimator discussed in section 2.1, and applied to"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5652538,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bccb2f99a9d1c105699f5d88c479569085e2c7ba",
            "isKey": true,
            "numCitedBy": 2013,
            "numCiting": 133,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop stochastic variational inference, a scalable algorithm for approximating posterior distributions. We develop this technique for a large class of probabilistic models and we demonstrate it with two probabilistic topic models, latent Dirichlet allocation and the hierarchical Dirichlet process topic model. Using stochastic variational inference, we analyze several large collections of documents: 300K articles from Nature, 1.8M articles from The New York Times, and 3.8M articles from Wikipedia. Stochastic inference can easily handle data sets of this size and outperforms traditional variational inference, which can only handle a smaller subset. (We also show that the Bayesian nonparametric topic model outperforms its parametric counterpart.) Stochastic variational inference lets us apply complex Bayesian models to massive data sets."
            },
            "slug": "Stochastic-variational-inference-Hoffman-Blei",
            "title": {
                "fragments": [],
                "text": "Stochastic variational inference"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "Stochastic variational inference lets us apply complex Bayesian models to massive data sets, and it is shown that the Bayesian nonparametric topic model outperforms its parametric counterpart."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748523"
                        ],
                        "name": "Danilo Jimenez Rezende",
                        "slug": "Danilo-Jimenez-Rezende",
                        "structuredName": {
                            "firstName": "Danilo",
                            "lastName": "Jimenez Rezende",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Danilo Jimenez Rezende"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "14594344"
                        ],
                        "name": "S. Mohamed",
                        "slug": "S.-Mohamed",
                        "structuredName": {
                            "firstName": "Shakir",
                            "lastName": "Mohamed",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mohamed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688276"
                        ],
                        "name": "Daan Wierstra",
                        "slug": "Daan-Wierstra",
                        "structuredName": {
                            "firstName": "Daan",
                            "lastName": "Wierstra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daan Wierstra"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16895865,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "484ad17c926292fbe0d5211540832a8c8a8e958b",
            "isKey": false,
            "numCitedBy": 3900,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition model to represent approximate posterior distributions, and that acts as a stochastic encoder of the data. We develop stochastic back-propagation -- rules for back-propagation through stochastic variables -- and use this to develop an algorithm that allows for joint optimisation of the parameters of both the generative and recognition model. We demonstrate on several real-world data sets that the model generates realistic samples, provides accurate imputations of missing data and is a useful tool for high-dimensional data visualisation."
            },
            "slug": "Stochastic-Backpropagation-and-Approximate-in-Deep-Rezende-Mohamed",
            "title": {
                "fragments": [],
                "text": "Stochastic Backpropagation and Approximate Inference in Deep Generative Models"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "This work marries ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning that introduces a recognition model to represent approximate posterior distributions and that acts as a stochastic encoder of the data."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398746441"
                        ],
                        "name": "Eric Thibodeau-Laufer",
                        "slug": "Eric-Thibodeau-Laufer",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Thibodeau-Laufer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eric Thibodeau-Laufer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1815021"
                        ],
                        "name": "Guillaume Alain",
                        "slug": "Guillaume-Alain",
                        "structuredName": {
                            "firstName": "Guillaume",
                            "lastName": "Alain",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guillaume Alain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2965424"
                        ],
                        "name": "J. Yosinski",
                        "slug": "J.-Yosinski",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Yosinski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Yosinski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9494295,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5ffa8bf1bf3e39227be28de4ff6915d3b21eb52d",
            "isKey": false,
            "numCitedBy": 354,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a novel training principle for probabilistic models that is an alternative to maximum likelihood. The proposed Generative Stochastic Networks (GSN) framework is based on learning the transition operator of a Markov chain whose stationary distribution estimates the data distribution. The transition distribution of the Markov chain is conditional on the previous state, generally involving a small move, so this conditional distribution has fewer dominant modes, being unimodal in the limit of small moves. Thus, it is easier to learn because it is easier to approximate its partition function, more like learning to perform supervised function approximation, with gradients that can be obtained by backprop. We provide theorems that generalize recent work on the probabilistic interpretation of denoising autoencoders and obtain along the way an interesting justification for dependency networks and generalized pseudolikelihood, along with a definition of an appropriate joint distribution and sampling mechanism even when the conditionals are not consistent. GSNs can be used with missing inputs and can be used to sample subsets of variables given the rest. We validate these theoretical results with experiments on two image datasets using an architecture that mimics the Deep Boltzmann Machine Gibbs sampler but allows training to proceed with simple backprop, without the need for layerwise pretraining."
            },
            "slug": "Deep-Generative-Stochastic-Networks-Trainable-by-Bengio-Thibodeau-Laufer",
            "title": {
                "fragments": [],
                "text": "Deep Generative Stochastic Networks Trainable by Backprop"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Theorems that generalize recent work on the probabilistic interpretation of denoising autoencoders are provided and obtain along the way an interesting justification for dependency networks and generalized pseudolikelihood."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2887364"
                        ],
                        "name": "Tim Salimans",
                        "slug": "Tim-Salimans",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Salimans",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tim Salimans"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3250170"
                        ],
                        "name": "David A. Knowles",
                        "slug": "David-A.-Knowles",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Knowles",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David A. Knowles"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17433921,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "de58806bca096e0be83d74c353233898e37b69f3",
            "isKey": false,
            "numCitedBy": 211,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "textabstractWe propose a general algorithm for approximating nonstandard Bayesian posterior distributions. The algorithm minimizes the Kullback-Leibler divergence of an approximating distribution to the intractable posterior distribu- tion. Our method can be used to approximate any posterior distribution, provided that it is given in closed form up to the proportionality constant. The approxi- mation can be any distribution in the exponential family or any mixture of such distributions, which means that it can be made arbitrarily precise. Several exam- ples illustrate the speed and accuracy of our approximation method in practice."
            },
            "slug": "Fixed-Form-Variational-Posterior-Approximation-Salimans-Knowles",
            "title": {
                "fragments": [],
                "text": "Fixed-Form Variational Posterior Approximation through Stochastic Linear Regression"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "A general algorithm for approximating nonstandard Bayesian posterior distributions that minimizes the Kullback-Leibler divergence of an approximating distribution to the intractable posterior distribu- tion."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734693"
                        ],
                        "name": "John C. Duchi",
                        "slug": "John-C.-Duchi",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Duchi",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John C. Duchi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34840427"
                        ],
                        "name": "Elad Hazan",
                        "slug": "Elad-Hazan",
                        "structuredName": {
                            "firstName": "Elad",
                            "lastName": "Hazan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Elad Hazan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "oints (drawn from full dataset)  Random samples from noise distribution p() g r ;\u02daLe M(;\u02da;X ;) (Gradients of minibatch estimator (8)) ;\u02da Update parameters using gradients g (e.g. SGD or Adagrad [DHS10]) until convergence of parameters (;\u02da) return ;\u02da We apply this technique to the variational lower bound (eq. (2)), yielding our generic SGVB estimator LeA(;\u02da;x(i)) \u2019L(;\u02da;x(i)): LeA(;\u02da;x(i)) = 1 L"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "parameters, both variational and generative, were initialized by random sampling from N(0;0:01), and were jointly stochastically optimized using the MAP criterion. Stepsizes were adapted with Adagrad [DHS10]; the Adagrad global stepsize parameters were chosen from f0.01, 0.02, 0.1gbased on performance on the training set in the \ufb01rst few iterations. Minibatches of size M= 100 were used, with L= 1 samples "
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "ibatch size Mwas large enough, e.g. M = 100. Derivatives r ;\u02daLe(;XM) can be taken, and the resulting gradients can be used in conjunction with stochastic optimization methods such as SGD or Adagrad [DHS10]. See algorithm 1 for a basic approach to compute the stochastic gradients. A connection with auto-encoders becomes clear when looking at the objective function given at eq. (7). The \ufb01rst term is (the"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 538820,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "413c1142de9d91804d6d11c67ff3fed59c9fc279",
            "isKey": true,
            "numCitedBy": 8025,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms."
            },
            "slug": "Adaptive-Subgradient-Methods-for-Online-Learning-Duchi-Hazan",
            "title": {
                "fragments": [],
                "text": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work describes and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal functions that can be chosen in hindsight."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777528"
                        ],
                        "name": "H. Larochelle",
                        "slug": "H.-Larochelle",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Larochelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Larochelle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9383489,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "00cd1dab559a9671b692f39f14c1573ab2d1416b",
            "isKey": false,
            "numCitedBy": 343,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new approximate inference algorithm for Deep Boltzmann Machines (DBM\u2019s), a generative model with many layers of hidden variables. The algorithm learns a separate \u201crecognition\u201d model that is used to quickly initialize, in a single bottom-up pass, the values of the latent variables in all hidden layers. We show that using such a recognition model, followed by a combined top-down and bottom-up pass, it is possible to efficiently learn a good generative model of high-dimensional highly-structured sensory input. We show that the additional computations required by incorporating a top-down feedback plays a critical role in the performance of a DBM, both as a generative and discriminative model. Moreover, inference is only at most three times slower compared to the approximate inference in a Deep Belief Network (DBN), making large-scale learning of DBM\u2019s practical. Finally, we demonstrate that the DBM\u2019s trained using the proposed approximate inference algorithm perform well compared to DBN\u2019s and SVM\u2019s on the MNIST handwritten digit, OCR English letters, and NORB visual object recognition tasks."
            },
            "slug": "Efficient-Learning-of-Deep-Boltzmann-Machines-Salakhutdinov-Larochelle",
            "title": {
                "fragments": [],
                "text": "Efficient Learning of Deep Boltzmann Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A new approximate inference algorithm for Deep Boltzmann Machines (DBM\u2019s), a generative model with many layers of hidden variables, that learns a separate \u201crecognition\u201d model that is used to quickly initialize, in a single bottom-up pass, the values of the latent variables in all hidden layers."
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753223"
                        ],
                        "name": "A. Graves",
                        "slug": "A.-Graves",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Graves",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Graves"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14885866,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5a9ef216bf11f222438fff130c778267d39a9564",
            "isKey": false,
            "numCitedBy": 1074,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Variational methods have been previously explored as a tractable approximation to Bayesian inference for neural networks. However the approaches proposed so far have only been applicable to a few simple network architectures. This paper introduces an easy-to-implement stochastic variational method (or equivalently, minimum description length loss function) that can be applied to most neural networks. Along the way it revisits several common regularisers from a variational perspective. It also provides a simple pruning heuristic that can both drastically reduce the number of network weights and lead to improved generalisation. Experimental results are provided for a hierarchical multidimensional recurrent neural network applied to the TIMIT speech corpus."
            },
            "slug": "Practical-Variational-Inference-for-Neural-Networks-Graves",
            "title": {
                "fragments": [],
                "text": "Practical Variational Inference for Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper introduces an easy-to-implement stochastic variational method (or equivalently, minimum description length loss function) that can be applied to most neural networks and revisits several common regularisers from a variational perspective."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144717963"
                        ],
                        "name": "Karol Gregor",
                        "slug": "Karol-Gregor",
                        "structuredName": {
                            "firstName": "Karol",
                            "lastName": "Gregor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karol Gregor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1841008"
                        ],
                        "name": "Ivo Danihelka",
                        "slug": "Ivo-Danihelka",
                        "structuredName": {
                            "firstName": "Ivo",
                            "lastName": "Danihelka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ivo Danihelka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714004"
                        ],
                        "name": "A. Mnih",
                        "slug": "A.-Mnih",
                        "structuredName": {
                            "firstName": "Andriy",
                            "lastName": "Mnih",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Mnih"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723876"
                        ],
                        "name": "C. Blundell",
                        "slug": "C.-Blundell",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Blundell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Blundell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688276"
                        ],
                        "name": "Daan Wierstra",
                        "slug": "Daan-Wierstra",
                        "structuredName": {
                            "firstName": "Daan",
                            "lastName": "Wierstra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daan Wierstra"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14576846,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "695a2c95eacdbccb7a73d2f1e90e7b35b4b3d864",
            "isKey": false,
            "numCitedBy": 215,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a deep, generative autoencoder capable of learning hierarchies of distributed representations from data. Successive deep stochastic hidden layers are equipped with autoregressive connections, which enable the model to be sampled from quickly and exactly via ancestral sampling. We derive an efficient approximate parameter estimation method based on the minimum description length (MDL) principle, which can be seen as maximising a variational lower bound on the log-likelihood, with a feedforward neural network implementing approximate inference. We demonstrate state-of-the-art generative performance on a number of classic data sets, including several UCI data sets, MNIST and Atari 2600 games."
            },
            "slug": "Deep-AutoRegressive-Networks-Gregor-Danihelka",
            "title": {
                "fragments": [],
                "text": "Deep AutoRegressive Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "An efficient approximate parameter estimation method based on the minimum description length (MDL) principle is derived, which can be seen as maximising a variational lower bound on the log-likelihood, with a feedforward neural network implementing approximate inference."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760871"
                        ],
                        "name": "Aaron C. Courville",
                        "slug": "Aaron-C.-Courville",
                        "structuredName": {
                            "firstName": "Aaron",
                            "lastName": "Courville",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aaron C. Courville"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145467703"
                        ],
                        "name": "Pascal Vincent",
                        "slug": "Pascal-Vincent",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Vincent",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pascal Vincent"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "y the variational bound, a noisy data reconstruction term, exposing a novel connection between auto-encoders and stochastic variational inference. In contrast to a typical objective for auto-encoders [BCV13], all parameters updates, including those of the noise distribution, correspond to optimization of the variational lower bound on the marginal likelihood. From the learned generative model it is strai"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "der the autoencoding model [VLL+10], i.e. the negative reconstrution error. However, it is well known that this reconstruction criterion is in itself not suf\ufb01cient for learning useful representations [BCV13]. Regularization techniques have been proposed to make autoencoders learn useful representations, such as denoising, contractive and sparse autoencoder variants [BCV13]. Related are also encoder-decod"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 393948,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "184ac0766262312ba76bbdece4e7ffad0aa8180b",
            "isKey": false,
            "numCitedBy": 8748,
            "numCiting": 285,
            "paperAbstract": {
                "fragments": [],
                "text": "The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning."
            },
            "slug": "Representation-Learning:-A-Review-and-New-Bengio-Courville",
            "title": {
                "fragments": [],
                "text": "Representation Learning: A Review and New Perspectives"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Recent work in the area of unsupervised feature learning and deep learning is reviewed, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145467703"
                        ],
                        "name": "Pascal Vincent",
                        "slug": "Pascal-Vincent",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Vincent",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pascal Vincent"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777528"
                        ],
                        "name": "H. Larochelle",
                        "slug": "H.-Larochelle",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Larochelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Larochelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2453026"
                        ],
                        "name": "Isabelle Lajoie",
                        "slug": "Isabelle-Lajoie",
                        "structuredName": {
                            "firstName": "Isabelle",
                            "lastName": "Lajoie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Isabelle Lajoie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1798462"
                        ],
                        "name": "Pierre-Antoine Manzagol",
                        "slug": "Pierre-Antoine-Manzagol",
                        "structuredName": {
                            "firstName": "Pierre-Antoine",
                            "lastName": "Manzagol",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pierre-Antoine Manzagol"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17804904,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e2b7f37cd97a7907b1b8a41138721ed06a0b76cd",
            "isKey": false,
            "numCitedBy": 5610,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "We explore an original strategy for building deep networks, based on stacking layers of denoising autoencoders which are trained locally to denoise corrupted versions of their inputs. The resulting algorithm is a straightforward variation on the stacking of ordinary autoencoders. It is however shown on a benchmark of classification problems to yield significantly lower classification error, thus bridging the performance gap with deep belief networks (DBN), and in several cases surpassing it. Higher level representations learnt in this purely unsupervised fashion also help boost the performance of subsequent SVM classifiers. Qualitative experiments show that, contrary to ordinary autoencoders, denoising autoencoders are able to learn Gabor-like edge detectors from natural image patches and larger stroke detectors from digit images. This work clearly establishes the value of using a denoising criterion as a tractable unsupervised objective to guide the learning of useful higher level representations."
            },
            "slug": "Stacked-Denoising-Autoencoders:-Learning-Useful-in-Vincent-Larochelle",
            "title": {
                "fragments": [],
                "text": "Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work clearly establishes the value of using a denoising criterion as a tractable unsupervised objective to guide the learning of useful higher level representations."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2660689"
                        ],
                        "name": "O. Papaspiliopoulos",
                        "slug": "O.-Papaspiliopoulos",
                        "structuredName": {
                            "firstName": "Omiros",
                            "lastName": "Papaspiliopoulos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Papaspiliopoulos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3264000"
                        ],
                        "name": "G. Roberts",
                        "slug": "G.-Roberts",
                        "structuredName": {
                            "firstName": "Gareth",
                            "lastName": "Roberts",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Roberts"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2094212288"
                        ],
                        "name": "Martin Skold",
                        "slug": "Martin-Skold",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Skold",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Martin Skold"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "ked a reparameterization trick that is perhaps best known in literature for a different application , namely an ef\ufb01cient Gibbs sampling technique going under the name of non-centered parameterization [PRS07] or ancillary augmentation (AA) [YM11]. The essential parameterization trick is quite simple. Let q \u02da(zjx) be some conditional distribution parameterized by \u02da. It is then often possible to express the"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 30623460,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "63235595bd90cc41840795d4530ae53d69a56f06",
            "isKey": false,
            "numCitedBy": 226,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we describe centering and noncentering methodology as complementary techniques for use in parametrization of broad classes of hierarchical models, with a view to the construction of effective MCMC algorithms for exploring posterior distributions from these models. We give a clear qualitative understanding as to when centering and noncentering work well, and introduce theory concerning the convergence time complexity of Gibbs samplers using centered and noncentered parametrizations. We give general recipes for the construction of noncentered parametrizations, including an auxiliary variable technique called the state-space expansion technique. We also describe partially noncentered methods, and demonstrate their use in constructing robust Gibbs sampler algorithms whose convergence properties are not overly sensitive to the data."
            },
            "slug": "A-General-Framework-for-the-Parametrization-of-Papaspiliopoulos-Roberts",
            "title": {
                "fragments": [],
                "text": "A General Framework for the Parametrization of Hierarchical Models"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "This paper describes centering and noncentering methodology as complementary techniques for use in parametrization of broad classes of hierarchical models, with a view to the construction of effective MCMC algorithms for exploring posterior distributions from these models."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2645384"
                        ],
                        "name": "K. Kavukcuoglu",
                        "slug": "K.-Kavukcuoglu",
                        "structuredName": {
                            "firstName": "Koray",
                            "lastName": "Kavukcuoglu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kavukcuoglu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706809"
                        ],
                        "name": "Marc'Aurelio Ranzato",
                        "slug": "Marc'Aurelio-Ranzato",
                        "structuredName": {
                            "firstName": "Marc'Aurelio",
                            "lastName": "Ranzato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marc'Aurelio Ranzato"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": ". The SGVB objective contains a regularization term dictated by the variational bound (e.g. eq. (10)). Related are also encoder-decoder architectures such as the predictive sparse decomposition (PSD) [KRL08], from which we drew some inspiration. Also relevant are the recently introduced Generative Stochastic Networks [BTL13] where noisy auto-encoders learn the transition operator of a Markov chain that s"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5931210,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c63ef05c5f9c424b5cfeeed90dbe35eedf6cb8ec",
            "isKey": false,
            "numCitedBy": 241,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Adaptive sparse coding methods learn a possibly overcomplete set of basis functions, such that natural image patches can be reconstructed by linearly combining a small subset of these bases. The applicability of these methods to visual object recognition tasks has been limited because of the prohibitive cost of the optimization algorithms required to compute the sparse representation. In this work we propose a simple and efficient algorithm to learn basis functions. After training, this model also provides a fast and smooth approximator to the optimal representation, achieving even better accuracy than exact sparse coding algorithms on visual object recognition tasks."
            },
            "slug": "Fast-Inference-in-Sparse-Coding-Algorithms-with-to-Kavukcuoglu-Ranzato",
            "title": {
                "fragments": [],
                "text": "Fast Inference in Sparse Coding Algorithms with Applications to Object Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "This work proposes a simple and efficient algorithm to learn basis functions, which provides a fast and smooth approximator to the optimal representation, achieving even better accuracy than exact sparse coding algorithms on visual object recognition tasks."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9330607"
                        ],
                        "name": "S. Roweis",
                        "slug": "S.-Roweis",
                        "structuredName": {
                            "firstName": "Sam",
                            "lastName": "Roweis",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roweis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1939401,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f9cf9b6291aded2a82652002511aea36b6c5057c",
            "isKey": false,
            "numCitedBy": 922,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "I present an expectation-maximization (EM) algorithm for principal component analysis (PCA). The algorithm allows a few eigenvectors and eigenvalues to be extracted from large collections of high dimensional data. It is computationally very efficient in space and time. It also naturally accommodates missing information. I also introduce a new variant of PCA called sensible principal component analysis (SPCA) which defines a proper density model in the data space. Learning for SPCA is also done with an EM algorithm. I report results on synthetic and real data showing that these EM algorithms correctly and efficiently find the leading eigenvectors of the covariance of datasets in a few iterations using up to hundreds of thousands of datapoints in thousands of dimensions."
            },
            "slug": "EM-Algorithms-for-PCA-and-SPCA-Roweis",
            "title": {
                "fragments": [],
                "text": "EM Algorithms for PCA and SPCA"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "An expectation-maximization (EM) algorithm for principal component analysis (PCA) which allows a few eigenvectors and eigenvalues to be extracted from large collections of high dimensional data and defines a proper density model in the data space."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2443546"
                        ],
                        "name": "Yaming Yu",
                        "slug": "Yaming-Yu",
                        "structuredName": {
                            "firstName": "Yaming",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yaming Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2097256"
                        ],
                        "name": "Xiao-Li Meng",
                        "slug": "Xiao-Li-Meng",
                        "structuredName": {
                            "firstName": "Xiao-Li",
                            "lastName": "Meng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiao-Li Meng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5216852,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fba92da28358893996cd60755ee680166f856d02",
            "isKey": false,
            "numCitedBy": 193,
            "numCiting": 138,
            "paperAbstract": {
                "fragments": [],
                "text": "For a broad class of multilevel models, there exist two well-known competing parameterizations, the centered parameterization (CP) and the non-centered parameterization (NCP), for effective MCMC implementation. Much literature has been devoted to the questions of when to use which and how to compromise between them via partial CP/NCP. This article introduces an alternative strategy for boosting MCMC efficiency via simply interweaving\u2014but not alternating\u2014the two parameterizations. This strategy has the surprising property that failure of both the CP and NCP chains to converge geometrically does not prevent the interweaving algorithm from doing so. It achieves this seemingly magical property by taking advantage of the discordance of the two parameterizations, namely, the sufficiency of CP and the ancillarity of NCP, to substantially reduce the Markovian dependence, especially when the original CP and NCP form a \u201cbeauty and beast\u201d pair (i.e., when one chain mixes far more rapidly than the other). The ancillarity\u2013sufficiency reformulation of the CP\u2013NCP dichotomy allows us to borrow insight from the well-known Basu\u2019s theorem on the independence of (complete) sufficient and ancillary statistics, albeit a Bayesian version of Basu\u2019s theorem is currently lacking. To demonstrate the competitiveness and versatility of this ancillarity\u2013sufficiency interweaving strategy (ASIS) for real-world problems, we apply it to fit (1) a Cox process model for detecting changes in source intensity of photon counts observed by the Chandra X-ray telescope from a (candidate) neutron/quark star, which was the problem that motivated the ASIS strategy as it defeated other methods we initially tried; (2) a probit model for predicting latent membranous lupus nephritis; and (3) an interval-censored normal model for studying the lifetime of fluorescent lights. A bevy of open questions are presented, from the mysterious but exceedingly suggestive connections between ASIS and fiducial/structural inferences to nested ASIS for further boosting MCMC efficiency. This article has supplementary material online."
            },
            "slug": "To-Center-or-Not-to-Center:-That-Is-Not-the-(ASIS)-Yu-Meng",
            "title": {
                "fragments": [],
                "text": "To Center or Not to Center: That Is Not the Question\u2014An Ancillarity\u2013Sufficiency Interweaving Strategy (ASIS) for Boosting MCMC Efficiency"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This article introduces an alternative strategy for boosting MCMC efficiency via simply interweaving\u2014but not alternating\u2014the two parameterizations, and applies it to fit a Cox process model for detecting changes in source intensity of photon counts observed by the Chandra X-ray telescope from a (candidate) neutron/quark star."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065228513"
                        ],
                        "name": "H. Bourlard",
                        "slug": "H.-Bourlard",
                        "structuredName": {
                            "firstName": "Herv\u00e9",
                            "lastName": "Bourlard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bourlard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2396055"
                        ],
                        "name": "Y. Kamp",
                        "slug": "Y.-Kamp",
                        "structuredName": {
                            "firstName": "Yves",
                            "lastName": "Kamp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Kamp"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 206775335,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f5821548720901c89b3b7481f7500d7cd64e99bd",
            "isKey": false,
            "numCitedBy": 967,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "The multilayer perceptron, when working in auto-association mode, is sometimes considered as an interesting candidate to perform data compression or dimensionality reduction of the feature space in information processing applications. The present paper shows that, for auto-association, the nonlinearities of the hidden units are useless and that the optimal parameter values can be derived directly by purely linear techniques relying on singular value decomposition and low rank matrix approximation, similar in spirit to the well-known Karhunen-Lo\u00e8ve transform. This approach appears thus as an efficient alternative to the general error back-propagation algorithm commonly used for training multilayer perceptrons. Moreover, it also gives a clear interpretation of the r\u00f4le of the different parameters."
            },
            "slug": "Auto-association-by-multilayer-perceptrons-and-Bourlard-Kamp",
            "title": {
                "fragments": [],
                "text": "Auto-association by multilayer perceptrons and singular value decomposition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is shown that, for auto-association, the nonlinearities of the hidden units are useless and that the optimal parameter values can be derived directly by purely linear techniques relying on singular value decomposition and low rank matrix approximation, similar in spirit to the well-known Karhunen-Lo\u00e8ve transform."
            },
            "venue": {
                "fragments": [],
                "text": "Biological Cybernetics"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2577641"
                        ],
                        "name": "R. Linsker",
                        "slug": "R.-Linsker",
                        "structuredName": {
                            "firstName": "Ralph",
                            "lastName": "Linsker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Linsker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "mall . In relevant recent work on autoencoders [VLL+10] it was shown that the training criterion of unregularized autoencoders corresponds to maximization of a lower bound (see the infomax principle [Lin89]) of the mutual information between input Xand latent representation Z. Maximizing (w.r.t. parameters) of the mutual information is equivalent to maximizing the conditional entropy, which is lower bou"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18127428,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "efd2c547114d5296b38fbbdd5bff0acd324132f0",
            "isKey": false,
            "numCitedBy": 187,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper addresses the problem of determining the weights for a set of linear filters (model \"cells\") so as to maximize the ensemble-averaged information that the cells' output values jointly convey about their input values, given the statistical properties of the ensemble of input vectors. The quantity that is maximized is the Shannon information rate, or equivalently the average mutual information between input and output. Several models for the role of processing noise are analyzed, and the biological motivation for considering them is described. For simple models in which nearby input signal values (in space or time) are correlated, the cells resulting from this optimization process include center-surround cells and cells sensitive to temporal variations in input signal."
            },
            "slug": "An-Application-of-the-Principle-of-Maximum-to-Linsker",
            "title": {
                "fragments": [],
                "text": "An Application of the Principle of Maximum Information Preservation to Linear Systems"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "This paper addresses the problem of determining the weights for a set of linear filters so as to maximize the ensemble-averaged information that the cells' output values jointly convey about their input values, given the statistical properties of the ensemble of input vectors."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2477489"
                        ],
                        "name": "L. Devroye",
                        "slug": "L.-Devroye",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Devroye",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Devroye"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "ates), Beta, Chi-Squared, and F distributions. When all three approaches fail, good approximations to the inverse CDF exist requiring computations with time complexity comparable to the PDF (see e.g. [Dev86] for some methods). 3 Example: Variational Auto-Encoder In this section we\u2019ll give an example where we use a neural network for the probabilistic encoder q \u02da(zjx) (the approximation to the posterior o"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18412546,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "15aac8f8aaaa57e9bf9738117eac47becad2fcf2",
            "isKey": false,
            "numCitedBy": 186,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "A sample of n lid random variables with a given unknown density is given. We discuss several issues related to the problem or generating a new sample of lid random variables with almost the same density. In particular, we look at sample independence, consistency, sample indistinguishability, moment matching and generator efficiency. We also introduce the notion of a replacement number, the minimum number of observations in a given sample that have to be replaced to obtain a sample with a given density."
            },
            "slug": "Sample-based-non-uniform-random-variate-generation-Devroye",
            "title": {
                "fragments": [],
                "text": "Sample-based non-uniform random variate generation"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "Several issues related to the problem or generating a new sample of lid random variables with almost the same density are discussed, including sample independence, consistency, sample indistinguishability, moment matching and generator efficiency."
            },
            "venue": {
                "fragments": [],
                "text": "WSC '86"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46831169"
                        ],
                        "name": "G. Hinton",
                        "slug": "G.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790646"
                        ],
                        "name": "P. Dayan",
                        "slug": "P.-Dayan",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Dayan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Dayan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749650"
                        ],
                        "name": "B. Frey",
                        "slug": "B.-Frey",
                        "structuredName": {
                            "firstName": "Brendan",
                            "lastName": "Frey",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Frey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145572884"
                        ],
                        "name": "R. Neal",
                        "slug": "R.-Neal",
                        "structuredName": {
                            "firstName": "R",
                            "lastName": "Neal",
                            "middleNames": [
                                "M"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Neal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 871473,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6dd01cd9c17d1491ead8c9f97597fbc61dead8ea",
            "isKey": false,
            "numCitedBy": 1001,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "An unsupervised learning algorithm for a multilayer network of stochastic neurons is described. Bottom-up \"recognition\" connections convert the input into representations in successive hidden layers, and top-down \"generative\" connections reconstruct the representation in one layer from the representation in the layer above. In the \"wake\" phase, neurons are driven by recognition connections, and generative connections are adapted to increase the probability that they would reconstruct the correct activity vector in the layer below. In the \"sleep\" phase, neurons are driven by generative connections, and recognition connections are adapted to increase the probability that they would produce the correct activity vector in the layer above."
            },
            "slug": "The-\"wake-sleep\"-algorithm-for-unsupervised-neural-Hinton-Dayan",
            "title": {
                "fragments": [],
                "text": "The \"wake-sleep\" algorithm for unsupervised neural networks."
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "An unsupervised learning algorithm for a multilayer network of stochastic neurons is described, where bottom-up \"recognition\" connections convert the input into representations in successive hidden layers, and top-down \"generative\" connections reconstruct the representation in one layer from the representations in the layer above."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145401345"
                        ],
                        "name": "A. Kennedy",
                        "slug": "A.-Kennedy",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Kennedy",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kennedy"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "ts, and 3 latent variables; for higher dimensional latent space the estimates became unreliable. The AEVB and Wake-Sleep methods were compared to Monte Carlo EM (MCEM) with a Hybrid Monte Carlo (HMC) [DKPR87] sampler; details are in the appendix. We compared the convergence speed for the three algorithms, for a small and large training set size. Results are in \ufb01gure 2. 6 Conclusion We have introduced a no"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 121101759,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "22ea20339015130099017185e7f36e87933c6a43",
            "isKey": false,
            "numCitedBy": 2579,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Hybrid-Monte-Carlo-Kennedy",
            "title": {
                "fragments": [],
                "text": "Hybrid Monte Carlo"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Model generates remaining classes with same style"
            },
            "venue": {
                "fragments": [],
                "text": "D.P. Kingma  Conclusion"
            },
            "year": 2014
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Visualisations See figures 4 and 5 for visualisations of latent space and corresponding observed space of models learned with SGVB"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The\u201d wakesleep\u201d algorithm for unsupervised neural networks. SCIENCE"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 6,
            "methodology": 3
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 25,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Auto-Encoding-Variational-Bayes-Kingma-Welling/5f5dc5b9a2ba710937e2c413b37b053cd673df02?sort=total-citations"
}