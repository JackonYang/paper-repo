{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759124"
                        ],
                        "name": "C. Goller",
                        "slug": "C.-Goller",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Goller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Goller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34940447"
                        ],
                        "name": "A. K\u00fcchler",
                        "slug": "A.-K\u00fcchler",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "K\u00fcchler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. K\u00fcchler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6536466,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c58dd287a476b4722c5b6b1316629e2874682219",
            "isKey": false,
            "numCitedBy": 578,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "While neural networks are very successfully applied to the processing of fixed-length vectors and variable-length sequences, the current state of the art does not allow the efficient processing of structured objects of arbitrary shape (like logical terms, trees or graphs). We present a connectionist architecture together with a novel supervised learning scheme which is capable of solving inductive inference tasks on complex symbolic structures of arbitrary size. The most general structures that can be handled are labeled directed acyclic graphs. The major difference of our approach compared to others is that the structure-representations are exclusively tuned for the intended inference task. Our method is applied to tasks consisting in the classification of logical terms. These range from the detection of a certain subterm to the satisfaction of a specific unification pattern. Compared to previously known approaches we obtained superior results in that domain."
            },
            "slug": "Learning-task-dependent-distributed-representations-Goller-K\u00fcchler",
            "title": {
                "fragments": [],
                "text": "Learning task-dependent distributed representations by backpropagation through structure"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A connectionist architecture together with a novel supervised learning scheme which is capable of solving inductive inference tasks on complex symbolic structures of arbitrary size is presented."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of International Conference on Neural Networks (ICNN'96)"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145539951"
                        ],
                        "name": "J. Pollack",
                        "slug": "J.-Pollack",
                        "structuredName": {
                            "firstName": "Jordan",
                            "lastName": "Pollack",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Pollack"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 112
                            }
                        ],
                        "text": "Concrete examples of distributed reduced descriptors are the recursive autoassociative memory (RAAM) by Pollack [12] and the holographic reduced representations by Plate [13]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 11
                            }
                        ],
                        "text": "[12] J. B. Pollack, \u201cRecursive distributed representations,\u201dArtificial Intell., vol. 46, no. 1\u20132, pp. 77\u2013106, 1990."
                    },
                    "intents": []
                }
            ],
            "corpusId": 770011,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6a835df43fdc2f79126319f6fa033bb42147c6f6",
            "isKey": false,
            "numCitedBy": 948,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Recursive-Distributed-Representations-Pollack",
            "title": {
                "fragments": [],
                "text": "Recursive Distributed Representations"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749815"
                        ],
                        "name": "A. Sperduti",
                        "slug": "A.-Sperduti",
                        "structuredName": {
                            "firstName": "Alessandro",
                            "lastName": "Sperduti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sperduti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678493"
                        ],
                        "name": "A. Starita",
                        "slug": "A.-Starita",
                        "structuredName": {
                            "firstName": "Antonina",
                            "lastName": "Starita",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Starita"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 222,
                                "start": 218
                            }
                        ],
                        "text": "Note that if a DOAG does not possess a supersource, it is still possible to de ne a convention for adding an extra vertex s (with a minimal number of outgoing edges), such that s is a supersource for the expanded DOAG [19]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 112
                            }
                        ],
                        "text": "Another improvement in this direction has been the development of the Cascade-Correlation Network for structure [49, 19] (a generalization of recurrent Cascade-Correlation for sequences [50]) which obtained better results with respect to the LRAAM-based networks on a subset of the classi cation problems reported in [48]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 5942593,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3e33eca03933caaec671e20692e79d1acc9527e1",
            "isKey": false,
            "numCitedBy": 486,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "Standard neural networks and statistical methods are usually believed to be inadequate when dealing with complex structures because of their feature-based approach. In fact, feature-based approaches usually fail to give satisfactory solutions because of the sensitivity of the approach to the a priori selection of the features, and the incapacity to represent any specific information on the relationships among the components of the structures. However, we show that neural networks can, in fact, represent and classify structured patterns. The key idea underpinning our approach is the use of the so called \"generalized recursive neuron\", which is essentially a generalization to structures of a recurrent neuron. By using generalized recursive neurons, all the supervised networks developed for the classification of sequences, such as backpropagation through time networks, real-time recurrent networks, simple recurrent networks, recurrent cascade correlation networks, and neural trees can, on the whole, be generalized to structures. The results obtained by some of the above networks (with generalized recursive neurons) on the classification of logic terms are presented."
            },
            "slug": "Supervised-neural-networks-for-the-classification-Sperduti-Starita",
            "title": {
                "fragments": [],
                "text": "Supervised neural networks for the classification of structures"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is shown that neural networks can, in fact, represent and classify structured patterns and all the supervised networks developed for the classification of sequences can, on the whole, be generalized to structures."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749815"
                        ],
                        "name": "A. Sperduti",
                        "slug": "A.-Sperduti",
                        "structuredName": {
                            "firstName": "Alessandro",
                            "lastName": "Sperduti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sperduti"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 6422558,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e6dfc50eeb234d3458b13c055a57b0119c5c9435",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Labeling recursive auto-associative memory (LRAAM) is an extension of the RAAM model by Pollack (1990) to obtain distributed reduced representations of labeled directed graphs. In this paper some mathematical properties of LRAAM are discussed. Specifically, sufficient conditions on the asymptotical stability of the decoding process along a cycle of the encoded structure are given. LRAAM can be transformed into an analog Hopfield network with hidden units and an asymmetric connections matrix by connecting the output units with the input units. In this architecture encoded data can be accessed by content and different access procedures can be defined depending on the access key. Each access procedure corresponds to a particular constrained version of the recurrent network. The authors give sufficient conditions under which the property of asymptotical stability of a fixed point in one particular constrained version of the recurrent network can be extended to related fixed points in different constrained versions of the network. An example of encoding of a labeled directed graph on which the theoretical results are applied is given and discussed."
            },
            "slug": "Stability-properties-of-labeling-recursive-memory-Sperduti",
            "title": {
                "fragments": [],
                "text": "Stability properties of labeling recursive auto-associative memory"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The authors give sufficient conditions under which the property of asymptotical stability of a fixed point in one particular constrained version of the recurrent network can be extended to related fixed points in different constrained versions of the network."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688235"
                        ],
                        "name": "P. Frasconi",
                        "slug": "P.-Frasconi",
                        "structuredName": {
                            "firstName": "Paolo",
                            "lastName": "Frasconi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Frasconi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145467467"
                        ],
                        "name": "M. Gori",
                        "slug": "M.-Gori",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Gori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gori"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749815"
                        ],
                        "name": "A. Sperduti",
                        "slug": "A.-Sperduti",
                        "structuredName": {
                            "firstName": "Alessandro",
                            "lastName": "Sperduti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sperduti"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 65
                            }
                        ],
                        "text": "A detailed analysis on optimal convergence issues is proposed in [42]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11369016,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "c51ec71b5bec9b90fb1cbf1c037c0f27d714cac4",
            "isKey": false,
            "numCitedBy": 21,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "In the last few years it has been shown that recurrent neural networks are adequate for processing general data structures like trees and graphs, which opens the doors to a number of new interesting applications previously unexplored. In this paper, we analyze the efficiency of learning the membership of DO AGs (Directed Ordered Acyclic Graphs) in terms of local minima of the error surface by relying on the principle that their absence is a guarantee of efficient learning. We give sufficient conditions under which the error surface is local minima free. Specifically, we define a topological index associated wi th a collection of DOAGs that makes it possible to design the architecture so as to avoid local minima."
            },
            "slug": "On-the-Efficient-Classification-of-Data-Structures-Frasconi-Gori",
            "title": {
                "fragments": [],
                "text": "On the Efficient Classification of Data Structures by Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper analyzes the efficiency of learning the membership of DO AGs (Directed Ordered Acyclic Graphs) in terms of local minima of the error surface by relying on the principle that their absence is a guarantee of efficient learning."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688235"
                        ],
                        "name": "P. Frasconi",
                        "slug": "P.-Frasconi",
                        "structuredName": {
                            "firstName": "Paolo",
                            "lastName": "Frasconi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Frasconi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "neural networks (RNN\u2019s) [22] and probabilistic models such as hidden Markov models (HMM\u2019s) [23] or input\u2013output HMM\u2019s (IOHMM\u2019s) [24]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "8 Input\u2013output HMM\u2019s (IOHMM\u2019s) [24] are recent extension of HMM\u2019s for supervised learning on temporal domains."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "many learning systems, such as Boltzmann machines [29], multilayered perceptrons [30], (input\u2013output) hidden Markov models [23], [24], (just to mention some of them) can be easily regarded as particular graphical models."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Recently, numerous researchers have approached grammatical inference using adaptive models such as recurrent neural networks [22]10 or IOHMM\u2019s [24]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14298205,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "25bdc473f8377c1200adbd691fbb3cc77fa7bf70",
            "isKey": false,
            "numCitedBy": 338,
            "numCiting": 96,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider problems of sequence processing and propose a solution based on a discrete-state model in order to represent past context. We introduce a recurrent connectionist architecture having a modular structure that associates a subnetwork to each state. The model has a statistical interpretation we call input-output hidden Markov model (IOHMM). It can be trained by the estimation-maximization (EM) or generalized EM (GEM) algorithms, considering state trajectories as missing data, which decouples temporal credit assignment and actual parameter estimation. The model presents similarities to hidden Markov models (HMMs), but allows us to map input sequences to output sequences, using the same processing style as recurrent neural networks. IOHMMs are trained using a more discriminant learning paradigm than HMMs, while potentially taking advantage of the EM algorithm. We demonstrate that IOHMMs are well suited for solving grammatical inference problems on a benchmark problem. Experimental results are presented for the seven Tomita grammars, showing that these adaptive models can attain excellent generalization."
            },
            "slug": "Input-output-HMMs-for-sequence-processing-Bengio-Frasconi",
            "title": {
                "fragments": [],
                "text": "Input-output HMMs for sequence processing"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is demonstrated that IOHMMs are well suited for solving grammatical inference problems on a benchmark problem and able to map input sequences to output sequences, using the same processing style as recurrent neural networks."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688235"
                        ],
                        "name": "P. Frasconi",
                        "slug": "P.-Frasconi",
                        "structuredName": {
                            "firstName": "Paolo",
                            "lastName": "Frasconi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Frasconi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145467467"
                        ],
                        "name": "M. Gori",
                        "slug": "M.-Gori",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Gori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gori"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2540925"
                        ],
                        "name": "G. Soda",
                        "slug": "G.-Soda",
                        "structuredName": {
                            "firstName": "Giovanni",
                            "lastName": "Soda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Soda"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1057037,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f8e3da72acbe4fb13031f503b9151ea44d80c2ea",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Recurrent-neural-networks-and-prior-knowledge-for-a-Frasconi-Gori",
            "title": {
                "fragments": [],
                "text": "Recurrent neural networks and prior knowledge for sequence processing: a constrained nondeterministic approach"
            },
            "venue": {
                "fragments": [],
                "text": "Knowl. Based Syst."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749815"
                        ],
                        "name": "A. Sperduti",
                        "slug": "A.-Sperduti",
                        "structuredName": {
                            "firstName": "Alessandro",
                            "lastName": "Sperduti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sperduti"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "LRAAM\u2019s make it possible to carry out the synthesis of distributed reduced descriptors for fixed outdegree directed labeled graphs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 40
                            }
                        ],
                        "text": "More recently, the labeling RAAM model (LRAAM) [14]\u2013[16] has been proposed as an extension of RAAM\u2019s, while some advances on the LRAAM access by content capabilities has been discussed in [17]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 68
                            }
                        ],
                        "text": "A preliminary move toward this direction is reported in [48], where LRAAM-based networks were successfully used to perform classification of symbolic recursive structures encoding logical terms."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 249,
                                "start": 244
                            }
                        ],
                        "text": "Another improvement in this\ndirection has been the development of the cascade-correlation\nnetwork for structure [48], [19] (a generalization of recurrent\ncascade-correlation for sequences [50]) which obtained better results with respect to the LRAAM-based networks on a subset of the classification problems reported in [48]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 151
                            }
                        ],
                        "text": "In the field of natural language processing, very good results on the classification of distributed representations of syntactical trees devised by an LRAAM according to the typology of dialogue acts were obtained by Cadoret [18]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 40
                            }
                        ],
                        "text": "The experimental comparison between the LRAAM-based approach and the backpropagation through structure algorithm reported in [47] shows that the latter algorithm obtains slightly better results for all examples solvable by the LRAAM-based approach, but with smaller networks and training times."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 47
                            }
                        ],
                        "text": "More recently, the labeling RAAM model (LRAAM) [14]\u2013[16] has been proposed as an extension of"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 169
                            }
                        ],
                        "text": "Moreover, the backpropagation through structure algorithm was able to successfully learn classification tasks that could not be solved in a reasonable amount of time\nby LRAAM-based networks."
                    },
                    "intents": []
                }
            ],
            "corpusId": 9868915,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cb204afe3e9237f50ef8ea3b8dbf751cc096e2a0",
            "isKey": false,
            "numCitedBy": 48,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we propose an extension of the Recursive Auto-Associative Memory (RAAM) by Pollack. This extension, the Labeling RAAM (LRAAM), is able to encode labeled graphs with cycles by representing pointers explicitly. A theoretical analysis of the constraints imposed on the weights by the learning task under the hypothesis of perfect learning and linear output units is presented. Cycles and connuent pointers result to be particularly eeective in imposing constraints on the weights. Some technical problems encountered in the RAAM, such as the termination problem in the learning and decoding processes, are solved more naturally in the LRAAM framework. The representations developed for the pointers seem to be robust to recurrent decoding along a cycle. Data encoded in a LRAAM can be ac-cessed by pointer as well as by content. The direct access by content can be achieved by transforming the encoder network of the LRAAM in a Bidirectional Associative Memory (BAM). Diierent access procedures can be deened according to the access key. The access procedures are not wholly reliable, however they seem to have a high likelihood of success. A geometric interpretation of the decoding process is given and the representations developed in the pointer space of a two hidden units LRAAM are presented and discussed. In particular, the pointer space results to be partitioned in a fractal-like fashion. Some eeects on the representations induced by the Hoppeld-like dynamics of the pointer decoding process are discussed and an encoding scheme able to retain the richness of representation devised by the decoding function is outlined. The application of the LRAAM model to the control of the dynamics of recurrent high-order networks is brieey sketched as well."
            },
            "slug": "Labeling-Raam-Sperduti",
            "title": {
                "fragments": [],
                "text": "Labeling Raam"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "This extension of the Recursive Auto-Associative Memory (RAAM), the Labeling RAAM, is able to encode labeled graphs with cycles by representing pointers explicitly, and an encoding scheme able to retain the richness of representation devised by the decoding function is outlined."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749815"
                        ],
                        "name": "A. Sperduti",
                        "slug": "A.-Sperduti",
                        "structuredName": {
                            "firstName": "Alessandro",
                            "lastName": "Sperduti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sperduti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678493"
                        ],
                        "name": "A. Starita",
                        "slug": "A.-Starita",
                        "structuredName": {
                            "firstName": "Antonina",
                            "lastName": "Starita",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Starita"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 190
                            }
                        ],
                        "text": "More recently, the labeling RAAM model (LRAAM) [14, 15, 16] has been proposed as an extension of RAAMs, while some advances on the LRAAM access by content capabilities has been discussed in [17]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 62196243,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "353a9ab8360e4f6f302f0e3d9ffbc0cb66b8c93e",
            "isKey": false,
            "numCitedBy": 6,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "We show how labeling recursive auto-associative memory (LRAAM) can be exploited to generate 'on the fly' neural networks for associative access of labeled structures. The topology of these networks, that we call generalized Hopfield networks, depends on the topology of the query used to retrieve information, and the weights on the network connections are the weights of the LRAAM encoding the structures. A method for incremental discovering of multiple solutions to a given query is presented. This method is based on terminal repellers, which are used to 'delete' known solutions from the set of admissible solutions to a query. Terminal repellers are also used to implement exceptions at query level, i.e., when a solution to a query must satisfy some negative constraints on the labels and/or substructures. Besides, the proposed model solves very naturally the connectionist variable binding problem at query level. Some results for a tree-like query are presented."
            },
            "slug": "A-memory-model-based-on-LRAAM-for-associative-of-Sperduti-Starita",
            "title": {
                "fragments": [],
                "text": "A memory model based on LRAAM for associative access of structures"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "It is shown how labeling recursive auto-associative memory (LRAAM) can be exploited to generate 'on the fly' neural networks for associative access of labeled structures to solve very naturally the connectionist variable binding problem at query level."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of International Conference on Neural Networks (ICNN'96)"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749815"
                        ],
                        "name": "A. Sperduti",
                        "slug": "A.-Sperduti",
                        "structuredName": {
                            "firstName": "Alessandro",
                            "lastName": "Sperduti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sperduti"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 35327752,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7411b86dfcff0151c30f095150e029b3bce9fbdf",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract In this paper, we propose an extension to the recursive auto-associative memory (RAAM) by Pollack. This extension, the labelling RAAM (LRAAM), can encode labelled graphs with cycles by representing pointers explicitly. Some technical problems encountered in the RAAM, such as the termination problem in the learning and decoding processes, are solved more naturally in the LRAAM framework. The representations developed for the pointers seem to be robust to recurrent decoding along a cycle. Theoretical and experimental results show that the performances of the proposed learning scheme depend on the way the graphs are represented in the training set. Critical features for the representation are cycles and confluent pointers. Data encoded in a LRAAM can be accessed by a pointer as well as by content. Direct access by content can be achieved by transforming the encoder network of the LRAAM into a particular bidirectional associative memory (BAM). Statistics performed on different instances of LRAAM show..."
            },
            "slug": "Labelling-Recursive-Auto-associative-Memory-Sperduti",
            "title": {
                "fragments": [],
                "text": "Labelling Recursive Auto-associative Memory"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Theoretical and experimental results show that the performances of the proposed learning scheme depend on the way the graphs are represented in the training set, and the representations developed for the pointers seem to be robust to recurrent decoding along a cycle."
            },
            "venue": {
                "fragments": [],
                "text": "Connect. Sci."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2884373"
                        ],
                        "name": "J. Elman",
                        "slug": "J.-Elman",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Elman",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Elman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 2
                            }
                        ],
                        "text": ", [21]) and adaptive models for dealing with sequentially ordered data are now well known."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2763403,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "668087f0ae7ce1de6e0bd0965dbb480c08103260",
            "isKey": false,
            "numCitedBy": 9861,
            "numCiting": 111,
            "paperAbstract": {
                "fragments": [],
                "text": "Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves; the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands; indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context-dependent while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction."
            },
            "slug": "Finding-Structure-in-Time-Elman",
            "title": {
                "fragments": [],
                "text": "Finding Structure in Time"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory and suggests a method for representing lexical categories and the type/token distinction is developed."
            },
            "venue": {
                "fragments": [],
                "text": "Cogn. Sci."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749815"
                        ],
                        "name": "A. Sperduti",
                        "slug": "A.-Sperduti",
                        "structuredName": {
                            "firstName": "Alessandro",
                            "lastName": "Sperduti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sperduti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678493"
                        ],
                        "name": "A. Starita",
                        "slug": "A.-Starita",
                        "structuredName": {
                            "firstName": "Antonina",
                            "lastName": "Starita",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Starita"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759124"
                        ],
                        "name": "C. Goller",
                        "slug": "C.-Goller",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Goller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Goller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 321,
                                "start": 317
                            }
                        ],
                        "text": "Another improvement in this direction has been the development of the Cascade-Correlation Network for structure [49, 19] (a generalization of recurrent Cascade-Correlation for sequences [50]) which obtained better results with respect to the LRAAM-based networks on a subset of the classi cation problems reported in [48]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 57
                            }
                        ],
                        "text": "A preliminary move towards this direction is reported in [48], where LRAAM-based networks were successfully used to perform classi cation of symbolic recursive structures encoding logical terms."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8785487,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "14a9a814a54dbab99388fafbd96a1c5fe249e376",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper is a study on LRAAM-based (Labeling Recursive Auto-Associative Memory)classification of symbolic recursive structures encoding terms. The results reported here have been obtained by combining an LRAAM network with an analog perceptron. The approach used was to interleave the development of representations (unsupervised learning of the LRAAM) with the learning of the classification task. In this way, the representations are optimized with respect to the classification task. The intended applications of the approach described in this paper are hybrid (symbolic/connectionist) systems, where the connectionist part has to solve logic-oriented inductive learning tasks similar to the term-classification problems used in our experiments. These problems range from the detection of a specific subterm to the satisfaction of a specific unification pattern, and they can get a very satisfactory solution by our approach."
            },
            "slug": "Learning-Distributed-Representations-for-the-of-Sperduti-Starita",
            "title": {
                "fragments": [],
                "text": "Learning Distributed Representations for the Classification of Terms"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The intended applications of the approach described in this paper are hybrid (symbolic/connectionist) systems, where the connectionist part has to solve logic-oriented inductive learning tasks similar to the term-classification problems used in the experiments."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70219052"
                        ],
                        "name": "Wray L. Buntine",
                        "slug": "Wray-L.-Buntine",
                        "structuredName": {
                            "firstName": "Wray",
                            "lastName": "Buntine",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wray L. Buntine"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11672931,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fa7a32d9ce76cd016cf21d4f956e19d90e87b0dc",
            "isKey": false,
            "numCitedBy": 654,
            "numCiting": 143,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper is a multidisciplinary review of empirical, statistical learning from a graphical model perspective. Well-known examples of graphical models include Bayesian networks, directed graphs representing a Markov chain, and undirected networks representing a Markov field. These graphical models are extended to model data analysis and empirical learning using the notation of plates. Graphical operations for simplifying and manipulating a problem are provided including decomposition, differentiation, andthe manipulation of probability models from the exponential family. Two standard algorithm schemas for learning are reviewed in a graphical framework: Gibbs sampling and the expectation maximizationalgorithm. Using these operations and schemas, some popular algorithms can be synthesized from their graphical specification. This includes versions of linear regression, techniques for feed-forward networks, and learning Gaussian and discrete Bayesian networks from data. The paper concludes by sketching some implications for data analysis and summarizing how some popular algorithms fall within the framework presented. The main original contributions here are the decompositiontechniques and the demonstration that graphical models provide a framework for understanding and developing complex learning algorithms."
            },
            "slug": "Operations-for-Learning-with-Graphical-Models-Buntine",
            "title": {
                "fragments": [],
                "text": "Operations for Learning with Graphical Models"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The main original contributions here are the decompositiontechniques and the demonstration that graphical models provide a framework for understanding and developing complex learning algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "J. Artif. Intell. Res."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749815"
                        ],
                        "name": "A. Sperduti",
                        "slug": "A.-Sperduti",
                        "structuredName": {
                            "firstName": "Alessandro",
                            "lastName": "Sperduti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sperduti"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In [56] computational results on three different recursive"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 30545225,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "43e6972c4ab7c9a9a4d71b3c860dd76865b77960",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-the-Computational-Power-of-Recurrent-Neural-for-Sperduti",
            "title": {
                "fragments": [],
                "text": "On the Computational Power of Recurrent Neural Networks for Structures"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153170210"
                        ],
                        "name": "Clifford B. Miller",
                        "slug": "Clifford-B.-Miller",
                        "structuredName": {
                            "firstName": "Clifford",
                            "lastName": "Miller",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Clifford B. Miller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145157784"
                        ],
                        "name": "C. Lee Giles",
                        "slug": "C.-Lee-Giles",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Giles",
                            "middleNames": [
                                "Lee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lee Giles"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 121
                            }
                        ],
                        "text": "High-order neural networks, proposed mainly by Giles and associates for both static networks [40] and recurrent networks [41], are very interesting models especially for dealing with symbolic tasks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6095811,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "34f8c5769899dfd9450bb13c3f52c18c88444515",
            "isKey": false,
            "numCitedBy": 64,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "There has been much interest in increasing the computational power of neural networks. In addition there has been much interest in \u201cdesigning\u201d neural networks better suited to particular problems. Increasing the \u201corder\u201d of the connectivity of a neural network permits both. Though order has played a significant role in feedforward neural networks, its role in dynamically driven recurrent networks is still being understood. This work explores the effect of order in learning grammars. We present an experimental comparison of first order and second order recurrent neural networks, as applied to the task of grammatical inference. We show that for the small grammars studied these two neural net architectures have comparable learning and generalization power, and that both are reasonably capable of extracting the correct finite state automata for the language in question. However, for a larger randomly-generated ten-state grammar, second order networks significantly outperformed the first order networks, both in convergence time and generalization capability. We show that these networks learn faster the more neurons they have (our experiments used up to 10 hidden neurons), but that the solutions found by smaller networks are usually of better quality (in terms of generalization performance after training). Second order nets have the advantage that they converge more quickly to a solution and can find it more reliably than first order nets, but that the second order solutions tend to be of poorer quality than those of the first order if both architectures are trained to the same error tolerance. Despite this, second order nets can more successfully extract finite state machines using heuristic clustering techniques applied to the internal state representations. We speculate that this may be due to restrictions on the ability of first order architecture to fully make use of its internal state representation power and that this may have implications for the performance of the two architectures when scaled up to larger problems."
            },
            "slug": "Experimental-Comparison-of-the-Effect-of-Order-in-Miller-Giles",
            "title": {
                "fragments": [],
                "text": "Experimental Comparison of the Effect of Order in Recurrent Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work presents an experimental comparison of first order and second order recurrent neural networks, as applied to the task of grammatical inference and shows that for the small grammars studied these two neural net architectures have comparable learning and generalization power, and that both are reasonably capable of extracting the correct finite state automata for the language in question."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Pattern Recognit. Artif. Intell."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749815"
                        ],
                        "name": "A. Sperduti",
                        "slug": "A.-Sperduti",
                        "structuredName": {
                            "firstName": "Alessandro",
                            "lastName": "Sperduti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sperduti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1988653"
                        ],
                        "name": "D. Majidi",
                        "slug": "D.-Majidi",
                        "structuredName": {
                            "firstName": "Darya",
                            "lastName": "Majidi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Majidi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678493"
                        ],
                        "name": "A. Starita",
                        "slug": "A.-Starita",
                        "structuredName": {
                            "firstName": "Antonina",
                            "lastName": "Starita",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Starita"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 112
                            }
                        ],
                        "text": "Another improvement in this direction has been the development of the Cascade-Correlation Network for structure [49, 19] (a generalization of recurrent Cascade-Correlation for sequences [50]) which obtained better results with respect to the LRAAM-based networks on a subset of the classi cation problems reported in [48]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 45890351,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "66732991ae44d0b10e3909e2adb6c02da7680852",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Automatic inference is one of the main problems that syntactic and structural pattern recognition must solve for successful applications. Neural networks are artificial intelligence tools which already support automatic inference for successful applications of statistical pattern recognition. In this paper, we suggest that neural networks, and specifically Cascade-Correlation, can be used for automatic inference in syntactic and structural pattern recognition, as well. An extended version of a standard neuron which is able to deal with structures is presented and the Cascade-Correlation algorithm generalized to structured domains. The computational complexity of the proposed algorithm as well as experimental results obtained on problems involving logic terms are presented."
            },
            "slug": "Extended-Cascade-Correlation-for-Syntactic-and-Sperduti-Majidi",
            "title": {
                "fragments": [],
                "text": "Extended Cascade-Correlation for Syntactic and Structural Pattern Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "An extended version of a standard neuron which is able to deal with structures is presented and the Cascade-Correlation algorithm generalized to structured domains is presented."
            },
            "venue": {
                "fragments": [],
                "text": "SSPR"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3097049"
                        ],
                        "name": "Francois Gingras",
                        "slug": "Francois-Gingras",
                        "structuredName": {
                            "firstName": "Francois",
                            "lastName": "Gingras",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Francois Gingras"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10287053,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3311fba973367747bc4ff7783479b83998bf32e6",
            "isKey": false,
            "numCitedBy": 80,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we propose recurrent neural networks with feedback into the input units for handling two types of data analysis problems. On the one hand, this scheme can be used for static data when some of the input variables are missing. On the other hand, it can also be used for sequential data, when some of the input variables are missing or are available at different frequencies. Unlike in the case of probabilistic models (e.g. Gaussian) of the missing variables, the network does not attempt to model the distribution of the missing variables given the observed variables. Instead it is a more \"discriminant\" approach that fills in the missing variables for the sole purpose of minimizing a learning criterion (e.g., to minimize an output error)."
            },
            "slug": "Recurrent-Neural-Networks-for-Missing-or-Data-Bengio-Gingras",
            "title": {
                "fragments": [],
                "text": "Recurrent Neural Networks for Missing or Asynchronous Data"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "Recurrent neural networks with feedback into the input units for handling two types of data analysis problems, including static data and sequential data, when some of the input variables are missing or are available at different frequencies."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145157784"
                        ],
                        "name": "C. Lee Giles",
                        "slug": "C.-Lee-Giles",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Giles",
                            "middleNames": [
                                "Lee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lee Giles"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740471"
                        ],
                        "name": "C. Omlin",
                        "slug": "C.-Omlin",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Omlin",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Omlin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 125
                            }
                        ],
                        "text": "Recently, numerous researchers have approached grammatical inference using adaptive models such as recurrent neural networks [22]10 or input-output HMMs [24]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 95
                            }
                        ],
                        "text": "The most signi cant examples are neural architectures such as recurrent neural networks (RNNs) [22] and probabilistic models such as hidden Markov models (HMMs) [23] or input-output HMMs (IOHMMs) [24]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62215117,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7126c9df4f43579b3ccd8c7d989d3a5d689ab706",
            "isKey": false,
            "numCitedBy": 105,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract Recurrent neural networks readily process, learn and generate temporal sequences. In addition, they have been shown to have impressive computational power. Recurrent neural networks can be trained with symbolic string examples encoded as temporal sequences to behave like sequential finite slate recognizers. We discuss methods for extracting, inserting and refining symbolic grammatical rules for recurrent networks. This paper discusses various issues: how rules are inserted into recurrent networks, how they affect training and generalization, and how those rules can be checked and corrected. The capability of exchanging information between a symbolic representation (grammatical rules)and a connectionist representation (trained weights) has interesting implications. After partially known rules are inserted, recurrent networks can be trained to preserve inserted rules that were correct and to correct through training inserted rules that were \u2018incorrec\u2019\u2014rules inconsistent with the training data."
            },
            "slug": "Extraction,-Insertion-and-Refinement-of-Symbolic-in-Giles-Omlin",
            "title": {
                "fragments": [],
                "text": "Extraction, Insertion and Refinement of Symbolic Rules in Dynamically Driven Recurrent Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper discusses various issues: how rules are inserted into recurrent networks, how they affect training and generalization, and how those rules can be checked and corrected."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145157784"
                        ],
                        "name": "C. Lee Giles",
                        "slug": "C.-Lee-Giles",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Giles",
                            "middleNames": [
                                "Lee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lee Giles"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153170210"
                        ],
                        "name": "Clifford B. Miller",
                        "slug": "Clifford-B.-Miller",
                        "structuredName": {
                            "firstName": "Clifford",
                            "lastName": "Miller",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Clifford B. Miller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2158193072"
                        ],
                        "name": "Dong Chen",
                        "slug": "Dong-Chen",
                        "structuredName": {
                            "firstName": "Dong",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dong Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115401300"
                        ],
                        "name": "Hsing-Hen Chen",
                        "slug": "Hsing-Hen-Chen",
                        "structuredName": {
                            "firstName": "Hsing-Hen",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hsing-Hen Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34922532"
                        ],
                        "name": "Guo-Zheng Sun",
                        "slug": "Guo-Zheng-Sun",
                        "structuredName": {
                            "firstName": "Guo-Zheng",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guo-Zheng Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2552960"
                        ],
                        "name": "Yee-Chun Lee",
                        "slug": "Yee-Chun-Lee",
                        "structuredName": {
                            "firstName": "Yee-Chun",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yee-Chun Lee"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 19666035,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "872cdc269f3cb59f8a227818f35041415091545f",
            "isKey": false,
            "numCitedBy": 489,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We show that a recurrent, second-order neural network using a real-time, forward training algorithm readily learns to infer small regular grammars from positive and negative string training samples. We present simulations that show the effect of initial conditions, training set size and order, and neural network architecture. All simulations were performed with random initial weight strengths and usually converge after approximately a hundred epochs of training. We discuss a quantization algorithm for dynamically extracting finite state automata during and after training. For a well-trained neural net, the extracted automata constitute an equivalence class of state machines that are reducible to the minimal machine of the inferred grammar. We then show through simulations that many of the neural net state machines are dynamically stable, that is, they correctly classify many long unseen strings. In addition, some of these extracted automata actually outperform the trained neural network for classification of unseen strings."
            },
            "slug": "Learning-and-Extracting-Finite-State-Automata-with-Giles-Miller",
            "title": {
                "fragments": [],
                "text": "Learning and Extracting Finite State Automata with Second-Order Recurrent Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "It is shown that a recurrent, second-order neural network using a real-time, forward training algorithm readily learns to infer small regular grammars from positive and negative string training samples, and many of the neural net state machines are dynamically stable, that is, they correctly classify many long unseen strings."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3327108"
                        ],
                        "name": "B. D. Vries",
                        "slug": "B.-D.-Vries",
                        "structuredName": {
                            "firstName": "Bert",
                            "lastName": "Vries",
                            "middleNames": [
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. D. Vries"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143961030"
                        ],
                        "name": "J. Pr\u00edncipe",
                        "slug": "J.-Pr\u00edncipe",
                        "structuredName": {
                            "firstName": "Jos\u00e9",
                            "lastName": "Pr\u00edncipe",
                            "middleNames": [
                                "Carlos"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Pr\u00edncipe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 32
                            }
                        ],
                        "text": "For example, the gamma operator [25] is de ned by = q 1+ , being a constant between 0 and 1."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 958138,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a00986a3a7385020d9c50cdb76cb6a6b106b217f",
            "isKey": false,
            "numCitedBy": 276,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-gamma-model--A-new-neural-model-for-temporal-Vries-Pr\u00edncipe",
            "title": {
                "fragments": [],
                "text": "The gamma model--A new neural model for temporal processing"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9175683"
                        ],
                        "name": "Michael L. Baird",
                        "slug": "Michael-L.-Baird",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Baird",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael L. Baird"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14939484,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "15db30ddcd46d8f3dfb611d6f437d14660807d2b",
            "isKey": false,
            "numCitedBy": 285,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "recognition. Classic applications are illustrated such as Shaw's picture description language (1969) applied to bubble chamber photographs, and Ledley's chromosome grammar ( 1965). The student with no particular interest in pattern recognition, who nevertheless desires to grasp a quick overview of what formal language theory is all about, could read this chapter. A minimum of preliminary definitions and notation must be waded through before learning about-types of grammars (regular, context-free, context-sensitive), equivalency of grammars, syntaxdirected translations, and deterministic, nondeterministic, and stochastic systems. Higher dimensional grammars are treated in Chapter Three, i.e., those allowing more complex primitive description and interconnection capabilities. Tree, web, plex, and shape grammars are each described with accompanying examples which make the material very easy to absorb. Chapter Four, on recognition and translation of syntactic structures, presents that integral part of formal language theory known as automata theory. The one-to-one correspondences between types of string grammars and automata are presented here, namely finite automation/regular grammar, push-down automaton/context-free grammar, linear-bounded automaton/ context-sensitive grammar, and Turing machine/unrestricted grammar. Quite properly, the authors restrict their attention to the finite and push-down automata used to accept or reject input strings and to automata for tree recognition, since these are the models thus far proved to be most useful for syntactic pattern recognition tasks. Chapter Five introduces stocastic grammars, languages, and recognizers. The material is well covered and brings the reader up to date (1978). As the authors mention, the impact of this theory on syntactic pattern processing has not been thoroughly realized, and references are cited for one interested in pursuing the topic further. The final chapter deals with grammatical inference and exemplifies the principalideas underlying the problem of obtaining a pattern grammar from a set of samples. Again, the material in this chapter is presented in a logical and easy to understand format. By this time the reader is beginning to understand the limitations of the linguistic aspect of syntactic pattern recognition (Reviewer's opinion), and will draw his or her attention to some of the material which came out of this work in the late sixties, which can be described as structural pattern recognition, and which is the topic of the book review which follows."
            },
            "slug": "Structural-Pattern-Recognition-Baird",
            "title": {
                "fragments": [],
                "text": "Structural Pattern Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The authors restrict their attention to the finite and push-down automata used to accept or reject input strings and to automata for tree recognition, since these are the models thus far proved to be most useful for syntactic pattern recognition tasks."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749815"
                        ],
                        "name": "A. Sperduti",
                        "slug": "A.-Sperduti",
                        "structuredName": {
                            "firstName": "Alessandro",
                            "lastName": "Sperduti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sperduti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678493"
                        ],
                        "name": "A. Starita",
                        "slug": "A.-Starita",
                        "structuredName": {
                            "firstName": "Antonina",
                            "lastName": "Starita",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Starita"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 63871841,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9557accd4e4a9dab69fb9a76c7e64c48ee24f5d2",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "The Labeling RAAM model is a neural network able to encode labeled graphs in fixed size representations. In order to speed up the training procedure and for reducing in size the developed compressed representations, we propose a modular Labeling RAAM. In order to develop the modular system, we face two main problems: the mapping problem, i.e., how to map components of the structures into modules; and the membership problem, i.e., discovering which module must be used for decoding a compressed representation. The mapping between components and modules can be decided on the basis of the strongly connected components of the structures. The membership problem is solved by resorting to the BAMs derived from each LRAAM module. Preliminary results on the modular system are encouraging."
            },
            "slug": "Modular-Labeling-RAAM-Sperduti-Starita",
            "title": {
                "fragments": [],
                "text": "Modular Labeling RAAM"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "A modular Labeling RAAM system is proposed to speed up the training procedure and for reducing in size the developed compressed representations, by resorting to the BAMs derived from each LRAAM module."
            },
            "venue": {
                "fragments": [],
                "text": "ICANNGA"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3140752"
                        ],
                        "name": "Charles R. Rosenberg",
                        "slug": "Charles-R.-Rosenberg",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Rosenberg",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charles R. Rosenberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12926318,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "de996c32045df6f7b404dda2a753b6a9becf3c08",
            "isKey": false,
            "numCitedBy": 1885,
            "numCiting": 229,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes NETtalk, a class of massively-parallel network systems that learn to convert English text to speech. The memory representations for pronunciations are learned by practice and are shared among many processing units. The performance of NETtalk has some similarities with observed human performance. (i) The learning follows a power law. (ii) The more words the network learns, the better it is at generalizing and correctly pronouncing new words, (iii) The performance of the network degrades very slowly as connections in the network are damaged: no single link or processing unit is essential. (iv) Relearning after damage is much faster than learning during the original training. (v) Distributed or spaced practice is more effective for long-term retention than massed practice. Network models can be constructed that have the same performance and learning characteristics on a particular task, but differ completely at the levels of synaptic strengths and single-unit responses. However, hierarchical clustering techniques applied to NETtalk reveal that these different networks have similar internal representations of letter-to-sound correspondences within groups of processing units. This suggests that invariant internal representations may be found in assemblies of neurons intermediate in size between highly localized and completely distributed representations."
            },
            "slug": "Parallel-Networks-that-Learn-to-Pronounce-English-Sejnowski-Rosenberg",
            "title": {
                "fragments": [],
                "text": "Parallel Networks that Learn to Pronounce English Text"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "H hierarchical clustering techniques applied to NETtalk reveal that these different networks have similar internal representations of letter-to-sound correspondences within groups of processing units, which suggests that invariant internal representations may be found in assemblies of neurons intermediate in size between highly localized and completely distributed representations."
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst."
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2797623"
                        ],
                        "name": "H. Siegelmann",
                        "slug": "H.-Siegelmann",
                        "structuredName": {
                            "firstName": "Hava",
                            "lastName": "Siegelmann",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Siegelmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790264"
                        ],
                        "name": "Eduardo Sontag",
                        "slug": "Eduardo-Sontag",
                        "structuredName": {
                            "firstName": "Eduardo",
                            "lastName": "Sontag",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eduardo Sontag"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "10 It is well known that recurrent neural networks can simulate any finite state automata [52] as well as any multistack Turing machine in real time [53]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 44597102,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a49498e51840165d55b6badd4b52e34d17860bc0",
            "isKey": false,
            "numCitedBy": 834,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper deals with finite networks which consist of interconnections of synchronously evolving processors. Each processor updates its state by applying a \u201csigmoidal\u201d scalar nonlinearity to a linear combination of the previous states of all units. We prove that one may simulate all Turing Machines by rational nets. In particular, one can do this in linear time, and there is a net made up of about 1,000 processors which computes a universal partial-recursive function. Products (high order nets) are not required, contrary to what had been stated in the literature. Furthermore, we assert a similar theorem about non-deterministic Turing Machines. Consequences for undecidability and complexity issues about nets are discussed too."
            },
            "slug": "On-the-computational-power-of-neural-nets-Siegelmann-Sontag",
            "title": {
                "fragments": [],
                "text": "On the Computational Power of Neural Nets"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is proved that one may simulate all Turing Machines by rational nets in linear time, and there is a net made up of about 1,000 processors which computes a universal partial-recursive function."
            },
            "venue": {
                "fragments": [],
                "text": "J. Comput. Syst. Sci."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740471"
                        ],
                        "name": "C. Omlin",
                        "slug": "C.-Omlin",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Omlin",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Omlin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145157784"
                        ],
                        "name": "C. Lee Giles",
                        "slug": "C.-Lee-Giles",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Giles",
                            "middleNames": [
                                "Lee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lee Giles"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 89
                            }
                        ],
                        "text": "10It is well known that recurrent neural networks can simulate any finite state automata [52] as well as any multistack Turing machine in real time [53]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 218,
                                "start": 214
                            }
                        ],
                        "text": "Several prior knowledge injection algorithms have been introduced for recurrent networks, assuming that the available prior knowledge can be expressed in terms of state transition rules for a finite automaton (see [52] for a very detailed theoretical analysis)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 228941,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "677d17e00fcdad5baffc4fcd3925442f62fc9307",
            "isKey": false,
            "numCitedBy": 183,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "Recurrent neural networks that are <italic>trained</italic> to behave like deterministic finite-state automata (DFAs) can show deteriorating performance when tested on long strings. This deteriorating performance can be attributed to the instability of the internal representation of the learned DFA states. The use of a sigmoidel discriminant function together with the recurrent structure contribute to this instability. We prove that a simple algorithm can <italic>construct</italic> second-order recurrent neural networks with a sparse interconnection topology and sigmoidal discriminant function such that the internal DFA state representations are stable, that is, the constructed network correctly classifies strings of <italic>arbitrary length</italic>. The algorithm is based on encoding strengths of weights directly into the neural network. We derive a relationship between the weight strength and the number of DFA states for robust string classification. For a DFA with <italic>n</italic> state and <italic>m</italic>input alphabet symbols, the constructive algorithm generates a \u201cprogrammed\u201d neural network with <italic>O</italic>(<italic>n</italic>) neurons and <italic>O</italic>(<italic>mn</italic>) weights. We compare our algorithm to other methods proposed in the literature."
            },
            "slug": "Constructing-deterministic-finite-state-automata-in-Omlin-Giles",
            "title": {
                "fragments": [],
                "text": "Constructing deterministic finite-state automata in recurrent neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is proved that a simple algorithm can construct second-order recurrent neural networks with a sparse interconnection topology and sigmoidel discriminant function such that the internal DFA state representations are stable, that is, the constructed network correctly classifies strings of arbitrary length."
            },
            "venue": {
                "fragments": [],
                "text": "JACM"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2859277"
                        ],
                        "name": "T. Plate",
                        "slug": "T.-Plate",
                        "structuredName": {
                            "firstName": "Tony",
                            "lastName": "Plate",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Plate"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 170
                            }
                        ],
                        "text": "Concrete examples of distributed reduced descriptors are the recursive autoassociative memory (RAAM) by Pollack [12] and the holographic reduced representations by Plate [13]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 11
                            }
                        ],
                        "text": "[13] T. A. Plate, \u201cHolographic reduced representations,\u201dIEEE Trans."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2352281,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "564427596799f7967c91934966cd3c6bd31cb06d",
            "isKey": false,
            "numCitedBy": 542,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Associative memories are conventionally used to represent data with very simple structure: sets of pairs of vectors. This paper describes a method for representing more complex compositional structure in distributed representations. The method uses circular convolution to associate items, which are represented by vectors. Arbitrary variable bindings, short sequences of various lengths, simple frame-like structures, and reduced representations can be represented in a fixed width vector. These representations are items in their own right and can be used in constructing compositional structures. The noisy reconstructions extracted from convolution memories can be cleaned up by using a separate associative memory that has good reconstructive properties."
            },
            "slug": "Holographic-reduced-representations-Plate",
            "title": {
                "fragments": [],
                "text": "Holographic reduced representations"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper describes a method for representing more complex compositional structure in distributed representations that uses circular convolution to associate items, which are represented by vectors."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113861474"
                        ],
                        "name": "C. Bishop",
                        "slug": "C.-Bishop",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Bishop",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Bishop"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "many learning systems, such as Boltzmann machines [29], multilayered perceptrons [30], (input\u2013output) hidden Markov models [23], [24], (just to mention some of them) can be easily regarded as particular graphical models."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60563397,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b9b1b1654ce0eea729c4160bfedcbb3246460b1d",
            "isKey": false,
            "numCitedBy": 8595,
            "numCiting": 250,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nThis is the first comprehensive treatment of feed-forward neural networks from the perspective of statistical pattern recognition. After introducing the basic concepts, the book examines techniques for modelling probability density functions and the properties and merits of the multi-layer perceptron and radial basis function network models. Also covered are various forms of error functions, principal algorithms for error function minimalization, learning and generalization in neural networks, and Bayesian techniques and their applications. Designed as a text, with over 100 exercises, this fully up-to-date work will benefit anyone involved in the fields of neural computation and pattern recognition."
            },
            "slug": "Neural-networks-for-pattern-recognition-Bishop",
            "title": {
                "fragments": [],
                "text": "Neural networks for pattern recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "This is the first comprehensive treatment of feed-forward neural networks from the perspective of statistical pattern recognition, and is designed as a text, with over 100 exercises, to benefit anyone involved in the fields of neural computation and pattern recognition."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759124"
                        ],
                        "name": "C. Goller",
                        "slug": "C.-Goller",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Goller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Goller"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 71
                            }
                        ],
                        "text": "ture networks within the SETHEO system is discussed in great detail in [5], where a more sophisticated formulation of the learning goal is given."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 256,
                                "start": 253
                            }
                        ],
                        "text": "Several tasks that must be performed for learning a control heuristic, like nding a rating for the applicable rules according to the current context or selecting the next subgoal, can be regarded as problems of learning a classi cation of logical terms [5]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 35388031,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2b19ab7f0d9a5ef60be7045bc80539590b00605a",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Automated deduction has a long tradition in computer science and most of the symbolic AI systems perform some kind of logic-based deductive inference. The central problem in automated deduction is the explosive growth of search spaces with deduction length. Methods of guiding and controlling the search process are indispensable. I will present a connectionist approach for learning search-control heuristics from examples of successful deductions. It originated in the context of the relatively new and still emerging eld of hybrid (symbolic/connectionist) systems. The goal of this eld is the identiication of strengths and weaknesses in the symbolic and the connectionist paradigms, and based on such insights, the design of hybrid systems that can beneet from the strengths of both. I begin with brieey comparing search-control heuristics for automated deduction systems with other methods that improve the eeciency of search, such as search strategies, calculus reenements and extensions, and analogical reasoning. The most important characteristics of heuristics are that they are simple and useful in most cases, however, they are not guaranteed to help. Search-control heuristics can be viewed as approximations to unknown and almost never realizable perfect search-control strategies. Approximation on the other hand is an inherent characteristic of connectionist methods, especially of the most successful and widespread backpropagation (BP) networks. Search-control heuristics for automated deduction systems are traditionally represented by heuristic evaluation functions for states or inference steps in the search space. These heuristic evaluation functions compute ratings for symbolic structures (e.g. graphs, trees, terms, formulas) of arbitrary size. Representing and processing the symbolic structures traditionally used in AI, belong to the most important and challenging open questions in the connectionist community. Most of the previous connectionist approaches for learning heuris-tic evaluation functions use a xed, a priori deened set of features for symbolic structures SF71, SE90, SE91, SG93, Gol94, Fuc96]. The deenition and selection of features, however, introduces a very strong bias and severely limits the class of relations that can be expressed and learned. It can be stated that the selection of a good set of features is already more than half the way to nding a good evaluation function. Compared to the feature-selection and deenition problem, the particular learning method which is applied is only of minor importance. I present a new and very powerful approach for adaptive structure processing which automatically nds the features which are relevant for the task at hand: folding architecture networks (FA-networks) \u2026"
            },
            "slug": "A-connectionist-approach-for-learning-heuristics-Goller",
            "title": {
                "fragments": [],
                "text": "A connectionist approach for learning search-control heuristics for automated deduction systems"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A connectionist approach for learning search-control heuristics for automated deduction systems with other methods that improve the eeciency of search, such as search strategies, calculus reenements and extensions, and analogical reasoning."
            },
            "venue": {
                "fragments": [],
                "text": "DISKI"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2586725"
                        ],
                        "name": "M. Forcada",
                        "slug": "M.-Forcada",
                        "structuredName": {
                            "firstName": "Mikel",
                            "lastName": "Forcada",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Forcada"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 153
                            }
                        ],
                        "text": "In the particular case of temporal sequences, solutions to the problem of learning asynchronous transductions have been proposed with recurrent networks [59] and IOHMM\u2019s [60]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14347371,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "118923596b1ce92d5654469c757bb125c9f3fcb8",
            "isKey": false,
            "numCitedBy": 2,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent work has shown that recurrent neural networks can be trained to behave as nite-state automata from samples of input strings and their corresponding outputs. However, most of the work has focused on training simple networks to behave as the simplest class of deterministic machines, Mealy (or Moore) machines. The class of translations that can be performed by these machines are very limited. For example, input and output strings have the same length. However, deterministic state machines can perform more complex translation tasks, and it has been recently shown that they can be inferred from input{output pairs. In this paper we show how a slightly augmented architecture based on a second-order recurrent neural network may be trained to behave as an instance of the most powerful class of deterministic sequential translator."
            },
            "slug": "Beyond-Mealy-machines-:-Learning-translatorswith-Forcada",
            "title": {
                "fragments": [],
                "text": "Beyond Mealy machines : Learning translatorswith recurrent neural"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "This paper shows how a slightly augmented architecture based on a second-order recurrent neural network may be trained to behave as an instance of the most powerful class of deterministic sequential translator."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2812486"
                        ],
                        "name": "P. Simard",
                        "slug": "P.-Simard",
                        "structuredName": {
                            "firstName": "Patrice",
                            "lastName": "Simard",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Simard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688235"
                        ],
                        "name": "P. Frasconi",
                        "slug": "P.-Frasconi",
                        "structuredName": {
                            "firstName": "Paolo",
                            "lastName": "Frasconi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Frasconi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 96
                            }
                        ],
                        "text": "3For recurrent neural networks this is the well know problem of learning long-term dependencies [10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 206457500,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d0be39ee052d246ae99c082a565aba25b811be2d",
            "isKey": false,
            "numCitedBy": 6144,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "Recurrent neural networks can be used to map input sequences to output sequences, such as for recognition, production or prediction problems. However, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases. These results expose a trade-off between efficient learning by gradient descent and latching on information for long periods. Based on an understanding of this problem, alternatives to standard gradient descent are considered."
            },
            "slug": "Learning-long-term-dependencies-with-gradient-is-Bengio-Simard",
            "title": {
                "fragments": [],
                "text": "Learning long-term dependencies with gradient descent is difficult"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work shows why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases, and exposes a trade-off between efficient learning by gradient descent and latching on information for long periods."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49497622"
                        ],
                        "name": "K. Lari",
                        "slug": "K.-Lari",
                        "structuredName": {
                            "firstName": "Kaveh",
                            "lastName": "Lari",
                            "middleNames": [
                                "Sookhak"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Lari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145259603"
                        ],
                        "name": "S. Young",
                        "slug": "S.-Young",
                        "structuredName": {
                            "firstName": "Steve",
                            "lastName": "Young",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Young"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "It is interesting to note a formal resemblance between (8) and the recurrences in the inside\u2010outside algorithm for learning stochastic context free grammars (SCFG\u2019s) [ 38 ]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 53736294,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9ba08e0a53bfdcbe0b70a4761c3e2b62f150fc74",
            "isKey": false,
            "numCitedBy": 713,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Applications-of-stochastic-context-free-grammars-Lari-Young",
            "title": {
                "fragments": [],
                "text": "Applications of stochastic context-free grammars using the Inside-Outside algorithm"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 7
                            }
                        ],
                        "text": "Hinton [11] has introduced the concept of distributed reduced descriptors in order to allow neural networks to represent compositional structures."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7544770,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "71dd4d477ca17b4db3b270d25225822ff3a41fac",
            "isKey": false,
            "numCitedBy": 364,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Mapping-Part-Whole-Hierarchies-into-Connectionist-Hinton",
            "title": {
                "fragments": [],
                "text": "Mapping Part-Whole Hierarchies into Connectionist Networks"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144497046"
                        ],
                        "name": "N. Nilsson",
                        "slug": "N.-Nilsson",
                        "structuredName": {
                            "firstName": "Nils",
                            "lastName": "Nilsson",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Nilsson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "A well-known search algorithm guided by heuristics is A [4], that uses the dynamic programming principle for seeking an optimal-cost path."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 20370792,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "affcf19551b01c4c8009d061750700d91c2f79e9",
            "isKey": false,
            "numCitedBy": 3833,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "A classic introduction to artificial intelligence intended to bridge the gap between theory and practice, \"Principles of Artificial Intelligence\" describes fundamental AI ideas that underlie applications such as natural language processing, automatic programming, robotics, machine vision, automatic theorem proving, and intelligent data retrieval. Rather than focusing on the subject matter of the applications, the book is organized around general computational concepts involving the kinds of data structures used, the types of operations performed on the data structures, and the properties of the control strategies used. \"Principles of Artificial Intelligence\"evolved from the author's courses and seminars at Stanford University and University of Massachusetts, Amherst, and is suitable for text use in a senior or graduate AI course, or for individual study."
            },
            "slug": "Principles-of-Artificial-Intelligence-Nilsson",
            "title": {
                "fragments": [],
                "text": "Principles of Artificial Intelligence"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "This classic introduction to artificial intelligence describes fundamental AI ideas that underlie applications such as natural language processing, automatic programming, robotics, machine vision, automatic theorem proving, and intelligent data retrieval."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145430701"
                        ],
                        "name": "J. Pearl",
                        "slug": "J.-Pearl",
                        "structuredName": {
                            "firstName": "Judea",
                            "lastName": "Pearl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Pearl"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In the former case, inference can be performed directly on the Bayesian network using a local message passing algorithm often referred to as - propagation [27]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "parents, and parents of the children [27]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Belief or conditional independence networks became popular in artificial intelligence as a tool for reasoning in probabilistic expert systems [27]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 32583695,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "70ef29e6f0ce082bb8a47fd85b9bfb7cc0f20c93",
            "isKey": true,
            "numCitedBy": 18218,
            "numCiting": 230,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nProbabilistic Reasoning in Intelligent Systems is a complete andaccessible account of the theoretical foundations and computational methods that underlie plausible reasoning under uncertainty. The author provides a coherent explication of probability as a language for reasoning with partial belief and offers a unifying perspective on other AI approaches to uncertainty, such as the Dempster-Shafer formalism, truth maintenance systems, and nonmonotonic logic. The author distinguishes syntactic and semantic approaches to uncertainty\u0097and offers techniques, based on belief networks, that provide a mechanism for making semantics-based systems operational. Specifically, network-propagation techniques serve as a mechanism for combining the theoretical coherence of probability theory with modern demands of reasoning-systems technology: modular declarative inputs, conceptually meaningful inferences, and parallel distributed computation. Application areas include diagnosis, forecasting, image interpretation, multi-sensor fusion, decision support systems, plan recognition, planning, speech recognition\u0097in short, almost every task requiring that conclusions be drawn from uncertain clues and incomplete information. \nProbabilistic Reasoning in Intelligent Systems will be of special interest to scholars and researchers in AI, decision theory, statistics, logic, philosophy, cognitive psychology, and the management sciences. Professionals in the areas of knowledge-based systems, operations research, engineering, and statistics will find theoretical and computational tools of immediate practical use. The book can also be used as an excellent text for graduate-level courses in AI, operations research, or applied probability."
            },
            "slug": "Probabilistic-reasoning-in-intelligent-systems-of-Pearl",
            "title": {
                "fragments": [],
                "text": "Probabilistic reasoning in intelligent systems - networks of plausible inference"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The author provides a coherent explication of probability as a language for reasoning with partial belief and offers a unifying perspective on other AI approaches to uncertainty, such as the Dempster-Shafer formalism, truth maintenance systems, and nonmonotonic logic."
            },
            "venue": {
                "fragments": [],
                "text": "Morgan Kaufmann series in representation and reasoning"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758714"
                        ],
                        "name": "S. Fahlman",
                        "slug": "S.-Fahlman",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Fahlman",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Fahlman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 192,
                                "start": 188
                            }
                        ],
                        "text": "Another improvement in this direction has been the development of the cascade-correlation network for structure [48], [19] (a generalization of recurrent cascade-correlation for sequences [50]) which obtained better results with respect to the LRAAM-based networks on a subset of the classification problems reported in [48]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 15720720,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9e8cf03655d224b0994d0f9d4f5aa80bca07021a",
            "isKey": false,
            "numCitedBy": 197,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Recurrent Cascade-Correlation (RCC) is a recurrent version of the Cascade-Correlation learning architecture of Fahlman and Lebiere [Fahlman, 1990]. RCC can learn from examples to map a sequence of inputs into a desired sequence of outputs. New hidden units with recurrent connections are added to the network as needed during training. In effect, the network builds up a finite-state machine tailored specifically for the current problem. RCC retains the advantages of Cascade-Correlation: fast learning, good generalization, automatic construction of a near-minimal multi-layered network, and incremental training."
            },
            "slug": "The-Recurrent-Cascade-Correlation-Architecture-Fahlman",
            "title": {
                "fragments": [],
                "text": "The Recurrent Cascade-Correlation Architecture"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "Recurrent Cascade-Correlation is a recurrent version of the Cascade- Correlation learning architecture of Fahlman and Lebiere that can learn from examples to map a sequence of inputs into a desired sequence of outputs."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749125"
                        ],
                        "name": "Colin Giles",
                        "slug": "Colin-Giles",
                        "structuredName": {
                            "firstName": "Colin",
                            "lastName": "Giles",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Colin Giles"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152819463"
                        ],
                        "name": "T. Maxwell",
                        "slug": "T.-Maxwell",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Maxwell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Maxwell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 93
                            }
                        ],
                        "text": "High-order neural networks, proposed mainly by Giles and associates for both static networks [40] and recurrent networks [41], are very interesting models especially for dealing with symbolic tasks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15585692,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "54fae67c6386e4a351c0d38421a5136de2bb8556",
            "isKey": false,
            "numCitedBy": 701,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "High-order neural networks have been shown to have impressive computational, storage, and learning capabilities. This performance is because the order or structure of a high-order neural network can be tailored to the order or structure of a problem. Thus, a neural network designed for a particular class of problems becomes specialized but also very efficient in solving those problems. Furthermore, a priori knowledge, such as geometric invariances, can be encoded in high-order networks. Because this knowledge does not have to be learned, these networks are very efficient in solving problems that utilize this knowledge."
            },
            "slug": "Learning,-invariance,-and-generalization-in-neural-Giles-Maxwell",
            "title": {
                "fragments": [],
                "text": "Learning, invariance, and generalization in high-order neural networks."
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "High-order neural networks have been shown to have impressive computational, storage, and learning capabilities because the order or structure of a high- order neural network can be tailored to the order of a problem."
            },
            "venue": {
                "fragments": [],
                "text": "Applied optics"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2586725"
                        ],
                        "name": "M. Forcada",
                        "slug": "M.-Forcada",
                        "structuredName": {
                            "firstName": "Mikel",
                            "lastName": "Forcada",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Forcada"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1798578"
                        ],
                        "name": "Rafael C. Carrasco",
                        "slug": "Rafael-C.-Carrasco",
                        "structuredName": {
                            "firstName": "Rafael",
                            "lastName": "Carrasco",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rafael C. Carrasco"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Initial states in these models may be assumed to be fixed, or may be learned from data [33]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16946201,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f6cde498cfaf7b862da3fa358caee9749e65ed3d",
            "isKey": false,
            "numCitedBy": 51,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent work has shown that second-order recurrent neural networks (2ORNNs) may be used to infer regular languages. This paper presents a modified version of the real-time recurrent learning (RTRL) algorithm used to train 2ORNNs, that learns the initial state in addition to the weights. The results of this modification, which adds extra flexibility at a negligible cost in time complexity, suggest that it may be used to improve the learning of regular languages when the size of the network is small."
            },
            "slug": "Learning-the-Initial-State-of-a-Second-Order-Neural-Forcada-Carrasco",
            "title": {
                "fragments": [],
                "text": "Learning the Initial State of a Second-Order Recurrent Neural Network during Regular-Language Inference"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "A modified version of the real-time recurrent learning (RTRL) algorithm used to train 2ORNNs is presented, that learns the initial state in addition to the weights, suggesting that it may be used to improve the learning of regular languages when the size of the network is small."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749815"
                        ],
                        "name": "A. Sperduti",
                        "slug": "A.-Sperduti",
                        "structuredName": {
                            "firstName": "Alessandro",
                            "lastName": "Sperduti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sperduti"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "LRAAM\u2019s make it possible to carry out the synthesis of distributed reduced descriptors for fixed outdegree directed labeled graphs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 40
                            }
                        ],
                        "text": "More recently, the labeling RAAM model (LRAAM) [14]\u2013[16] has been proposed as an extension of RAAM\u2019s, while some advances on the LRAAM access by content capabilities has been discussed in [17]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 68
                            }
                        ],
                        "text": "A preliminary move toward this direction is reported in [48], where LRAAM-based networks were successfully used to perform classification of symbolic recursive structures encoding logical terms."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 249,
                                "start": 244
                            }
                        ],
                        "text": "Another improvement in this\ndirection has been the development of the cascade-correlation\nnetwork for structure [48], [19] (a generalization of recurrent\ncascade-correlation for sequences [50]) which obtained better results with respect to the LRAAM-based networks on a subset of the classification problems reported in [48]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 151
                            }
                        ],
                        "text": "In the field of natural language processing, very good results on the classification of distributed representations of syntactical trees devised by an LRAAM according to the typology of dialogue acts were obtained by Cadoret [18]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 40
                            }
                        ],
                        "text": "The experimental comparison between the LRAAM-based approach and the backpropagation through structure algorithm reported in [47] shows that the latter algorithm obtains slightly better results for all examples solvable by the LRAAM-based approach, but with smaller networks and training times."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 169
                            }
                        ],
                        "text": "Moreover, the backpropagation through structure algorithm was able to successfully learn classification tasks that could not be solved in a reasonable amount of time\nby LRAAM-based networks."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 47
                            }
                        ],
                        "text": "More recently, the labeling RAAM model (LRAAM) [14, 15, 16] has been proposed as an extension of RAAMs, while some advances on the LRAAM access by content capabilities has been discussed in [17]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5853754,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f03db7ef9cf309561eb02eb317b875deb8817c01",
            "isKey": false,
            "numCitedBy": 41,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we propose an extension to the RAAM by Pollack. This extension, the Labeling RAAM (LRAAM), can encode labeled graphs with cycles by representing pointers explicitly. Data encoded in an LRAAM can be accessed by pointer as well as by content. Direct access by content can be achieved by transforming the encoder network of the LRAAM into an analog Hopfield network with hidden units. Different access procedures can be defined depending on the access key. Sufficient conditions on the asymptotical stability of the associated Hopfield network are briefly introduced."
            },
            "slug": "Encoding-Labeled-Graphs-by-Labeling-RAAM-Sperduti",
            "title": {
                "fragments": [],
                "text": "Encoding Labeled Graphs by Labeling RAAM"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "The Labeling RAAM (LRAAM), an extension to the RAAM by Pollack, can encode labeled graphs with cycles by representing pointers explicitly by transforming the encoder network of the LRAAM into an analog Hopfield network with hidden units."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3252362"
                        ],
                        "name": "R. Schalkoff",
                        "slug": "R.-Schalkoff",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schalkoff",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schalkoff"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 93
                            }
                        ],
                        "text": "Here by a DOAG we mean a DAG with vertex set vert and edge set edg where for each vertex a total order on the edges leaving from is defined."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 33656944,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cea28c8b2d211110b3e48b4c545420282ea68f67",
            "isKey": false,
            "numCitedBy": 1052,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "STATISTICAL PATTERN RECOGNITION (StatPR). Supervised Learning (Training) Using Parametric and Nonparametric Approaches. Linear Discriminant Functions and the Discrete and Binary Feature Cases. Unsupervised Learning and Clustering. SYNTACTIC PATTERN RECOGNITION (SyntPR). Overview. Syntactic Recognition via Parsing and Other Grammars. Graphical Approaches to SyntPR. Learning via Grammatical Inference. NEURAL PATTERN RECOGNITION (NeurPR). Introduction to Neural Networks. Introduction to Neural Pattern Associators and Matrix Approaches. Feedforward Networks and Training by Backpropagation. Content Addressable Memory Approaches and Unsupervised Learning in NeurPR. Appendices. References. Permission Source Notes. Index."
            },
            "slug": "Pattern-recognition-statistical,-structural-and-Schalkoff",
            "title": {
                "fragments": [],
                "text": "Pattern recognition - statistical, structural and neural approaches"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "This chapter discusses supervised learning using Parametric and Nonparametric Approaches and unsupervised Learning in NeurPR, and discusses feedforward Networks and Training by Backpropagation."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144342820"
                        ],
                        "name": "E. M. Gold",
                        "slug": "E.-M.-Gold",
                        "structuredName": {
                            "firstName": "E.",
                            "lastName": "Gold",
                            "middleNames": [
                                "Mark"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. M. Gold"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 151
                            }
                        ],
                        "text": "However, the ability of representing finite automata is not sufficient to guarantee that a given regular grammar can be actually learned from examples [54]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8943792,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "9dfa951bec812bd7b8c905c587bca50b7883a10f",
            "isKey": false,
            "numCitedBy": 802,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Complexity-of-Automaton-Identification-from-Given-Gold",
            "title": {
                "fragments": [],
                "text": "Complexity of Automaton Identification from Given Data"
            },
            "venue": {
                "fragments": [],
                "text": "Inf. Control."
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796044"
                        ],
                        "name": "L. Saul",
                        "slug": "L.-Saul",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Saul",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Saul"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132120"
                        ],
                        "name": "T. Jaakkola",
                        "slug": "T.-Jaakkola",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Jaakkola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jaakkola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 193
                            }
                        ],
                        "text": "A recent interesting solution for approximate inference relies on mean eld theory from statistical physics and can be shown to be more and more accurate as the density of connections increases [37]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 7424318,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0a79433b5feacd9e8feeafa629dae5a85f362fef",
            "isKey": false,
            "numCitedBy": 438,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop a mean field theory for sigmoid belief networks based on ideas from statistical mechanics. Our mean field theory provides a tractable approximation to the true probability distribution in these networks; it also yields a lower bound on the likelihood of evidence. We demonstrate the utility of this framework on a benchmark problem in statistical pattern recognition-the classification of handwritten digits."
            },
            "slug": "Mean-Field-Theory-for-Sigmoid-Belief-Networks-Saul-Jaakkola",
            "title": {
                "fragments": [],
                "text": "Mean Field Theory for Sigmoid Belief Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "The utility of a mean field theory for sigmoid belief networks based on ideas from statistical mechanics is demonstrated on a benchmark problem in statistical pattern recognition-the classification of handwritten digits."
            },
            "venue": {
                "fragments": [],
                "text": "J. Artif. Intell. Res."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144312607"
                        ],
                        "name": "T. McCabe",
                        "slug": "T.-McCabe",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "McCabe",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. McCabe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9116234,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1e41ed1ac234cba0138329047e16a8a424389e77",
            "isKey": false,
            "numCitedBy": 5974,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a graph-theoretic complexity measure and illustrates how it can be used to manage and control program complexity. The paper first explains how the graph-theory concepts apply and gives an intuitive explanation of the graph concepts in programming terms. The control graphs of several actual Fortran programs are then presented to illustrate the correlation between intuitive complexity and the graph-theoretic complexity. Several properties of the graph-theoretic complexity are then proved which show, for example, that complexity is independent of physical size (adding or subtracting functional statements leaves complexity unchanged) and complexity depends only on the decision structure of a program."
            },
            "slug": "A-Complexity-Measure-McCabe",
            "title": {
                "fragments": [],
                "text": "A Complexity Measure"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "Several properties of the graph-theoretic complexity are proved which show, for example, that complexity is independent of physical size and complexity depends only on the decision structure of a program."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Software Engineering"
            },
            "year": 1976
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1750670"
                        ],
                        "name": "L. Wos",
                        "slug": "L.-Wos",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Wos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Wos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "73050891"
                        ],
                        "name": "Ross Overbeck",
                        "slug": "Ross-Overbeck",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Overbeck",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross Overbeck"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695428"
                        ],
                        "name": "E. Lusk",
                        "slug": "E.-Lusk",
                        "structuredName": {
                            "firstName": "Ewing",
                            "lastName": "Lusk",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Lusk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144547576"
                        ],
                        "name": "J. M. Boyle",
                        "slug": "J.-M.-Boyle",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Boyle",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. M. Boyle"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "For example, states in model-elimination provers and PROLOG-systems consist of logical expressions [44], which are suitable for a labeled DOAG representation, as explained in Example 1."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62658435,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dd08ffd6b7ce38b79be1e721f9f5135eefb8c13f",
            "isKey": false,
            "numCitedBy": 382,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This book explains what automated reasoning is and what it can do and then demonstrates how to use it to solve complex problems with applications in logic circuit design, circuit validation, real-time system design and expert systems. A diskette containing the automated reasoning program \"Otter\" is included. It is available for the first time to PCs and workstations. The book has input files, commentary, examples in \"Otter\" notation and a user's manual that enables readers to experiment with the material presented as well as with ideas of their own. Other features include: various challenge problems that allow readers to test, compare and evaluate new ideas and techniques; techniques for answering open questions and finding shorter proofs; methods for finding such proofs; examples and puzzles to aid in the understanding of parallel versions of an automated reasoning program; and problems."
            },
            "slug": "Automated-Reasoning:-Introduction-and-Applications-Wos-Overbeck",
            "title": {
                "fragments": [],
                "text": "Automated Reasoning: Introduction and Applications"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This book explains what automated reasoning is and what it can do and then demonstrates how to use it to solve complex problems with applications in logic circuit design, circuit validation, real-time system design and expert systems."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2911738"
                        ],
                        "name": "D. Angluin",
                        "slug": "D.-Angluin",
                        "structuredName": {
                            "firstName": "Dana",
                            "lastName": "Angluin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Angluin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152471016"
                        ],
                        "name": "Carl H. Smith",
                        "slug": "Carl-H.-Smith",
                        "structuredName": {
                            "firstName": "Carl H.",
                            "lastName": "Smith",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carl H. Smith"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3209224,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f6524a6b8e6091c0a11d665180a5ad94bbf1d3b4",
            "isKey": false,
            "numCitedBy": 900,
            "numCiting": 190,
            "paperAbstract": {
                "fragments": [],
                "text": "There has been a great deal of theoretical and experimental work in computer science on inductive inference systems, that is, systems that try to infer general rules from examples. However, a complete and applicable theory of such systems is still a distant goal. This survey highlights and explains the main ideas that have been developed in the study of inductive inference, with special emphasis on the relations between the general theory and the specific algorithms and implementations. 154 references."
            },
            "slug": "Inductive-Inference:-Theory-and-Methods-Angluin-Smith",
            "title": {
                "fragments": [],
                "text": "Inductive Inference: Theory and Methods"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This survey highlights and explains the main ideas that have been developed in the study of inductive inference, with special emphasis on the relations between the general theory and the specific algorithms and implementations."
            },
            "venue": {
                "fragments": [],
                "text": "CSUR"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144288586"
                        ],
                        "name": "A. Back",
                        "slug": "A.-Back",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Back",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Back"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733691"
                        ],
                        "name": "A. Tsoi",
                        "slug": "A.-Tsoi",
                        "structuredName": {
                            "firstName": "Ah",
                            "lastName": "Tsoi",
                            "middleNames": [
                                "Chung"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Tsoi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14019448,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "dd250fe242be3a42b02530ec1be393fc3160e7f9",
            "isKey": false,
            "numCitedBy": 14,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a unifying view of discrete-time operator models used in the context of finite word length linear signal processing. Comparisons are made between the recently presented gamma operator model, and the delta and rho operator models for performing nonlinear system identification and prediction using neural networks. A new model based on an adaptive bilinear transformation which generalizes all of the above models is presented."
            },
            "slug": "A-Comparison-of-Discrete-Time-Operator-Models-and-Back-Tsoi",
            "title": {
                "fragments": [],
                "text": "A Comparison of Discrete-Time Operator Models and for Nonlinear System Identification"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "A unifying view of discrete-time operator models used in the context of finite word length linear signal processing and a new model based on an adaptive bilinear transformation which generalizes all of the above models is presented."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1812533"
                        ],
                        "name": "R. Prather",
                        "slug": "R.-Prather",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Prather",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Prather"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 184,
                                "start": 181
                            }
                        ],
                        "text": "It is commonly accepted that most procedural languages can be expressed as a flowgraph using a number of basic elements, such as decision node, junction node, and begin a d end node[3]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14865722,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "58646bec06a0c1be87ef2073f33abb45da70707a",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the main goals of software engineering is to assess the quality of the software we develop, however this is to be measured. A software metric, assigning a number to each piece of program text, will generally attempt to address one or more quality attributes in its assessment. These might include software reliability, testability, maintainability, and the like, and, in general, a kind of complexity of the software in some specific and restricted sense. In this tutorial a new cubic flowgraph model is introduced and shown to be useful in exploring a whole range of software metric design and analysis techniques, specifically in reference to the subclass of software metrics that are said to be hierarchical. In particular, the cubic flowgraph model leads to the discovery of certain inequalities among existing metrics, serves to provide a new characterization of the important class of prime flowgraphs, and points the way to an. effective method for counting and enumerating the primes."
            },
            "slug": "Design-and-analysis-of-hierarchical-software-Prather",
            "title": {
                "fragments": [],
                "text": "Design and analysis of hierarchical software metrics"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "In this tutorial a new cubic flowgraph model is introduced and shown to be useful in exploring a whole range of software metric design and analysis techniques, specifically in reference to the subclass of software metrics that are said to be hierarchical."
            },
            "venue": {
                "fragments": [],
                "text": "CSUR"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712517"
                        ],
                        "name": "L. Rabiner",
                        "slug": "L.-Rabiner",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Rabiner",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Rabiner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 161
                            }
                        ],
                        "text": "The most signi cant examples are neural architectures such as recurrent neural networks (RNNs) [22] and probabilistic models such as hidden Markov models (HMMs) [23] or input-output HMMs (IOHMMs) [24]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 51
                            }
                        ],
                        "text": "1 Hidden Markov models Hidden Markov models (HMMs) [23] are a well known device for learning distributions of temporal sequences (i."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 144
                            }
                        ],
                        "text": "As a matter of fact, many learning systems, such as Boltzmann machines [29], multilayered perceptrons [30], (input-output) hidden Markov models [23], [24], (just to mention some of them) can be easily regarded as particular graphical models."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13618539,
            "fieldsOfStudy": [
                "Geology"
            ],
            "id": "8fe2ea0a67954f1380b3387e3262f1cdb9f9b3e5",
            "isKey": false,
            "numCitedBy": 24804,
            "numCiting": 98,
            "paperAbstract": {
                "fragments": [],
                "text": "The fabric comprises a novel type of netting which will have particular utility in screening out mosquitoes and like insects and pests. The fabric is defined of voids having depth as well as width and length. The fabric is usable as a material from which to form clothing for wear, or bed coverings, or sleeping bags, etc., besides use simply as a netting."
            },
            "slug": "A-Tutorial-on-Hidden-Markov-Models-and-Selected-Rabiner",
            "title": {
                "fragments": [],
                "text": "A Tutorial on Hidden Markov Models and Selected Applications"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The fabric comprises a novel type of netting which will have particular utility in screening out mosquitoes and like insects and pests."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715952"
                        ],
                        "name": "A. Aho",
                        "slug": "A.-Aho",
                        "structuredName": {
                            "firstName": "Alfred",
                            "lastName": "Aho",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Aho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699267"
                        ],
                        "name": "R. V. Book",
                        "slug": "R.-V.-Book",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Book",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. V. Book"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 58
                            }
                        ],
                        "text": "The standard\nfeature-based approach encodes each graphas a fixed-size vector of predefined features."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 118563655,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d67c28d726f467a866e4a816e07a9ee84b683f43",
            "isKey": false,
            "numCitedBy": 107,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Currents-In-The-Theory-Of-Computing-Aho-Book",
            "title": {
                "fragments": [],
                "text": "Currents In The Theory Of Computing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1973
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 44
                            }
                        ],
                        "text": "[24] Y. Bengio and P. Frasconi, \u201cInput\u2013output HMM\u2019s for sequence processing,\u201dIEEE Trans."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 50
                            }
                        ],
                        "text": "It can be noticed that even in the simple case of HMM\u2019s and IOHMM\u2019s, stationarity is a very common assumption."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 153
                            }
                        ],
                        "text": "Recently, numerous researchers have approached grammatical inference using adaptive models such as recurrent neural networks [22]10 or input-output HMMs [24]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 137
                            }
                        ],
                        "text": "Recently, numerous researchers have approached grammatical inference using adaptive models such as recurrent neural networks [22]10 or IOHMM\u2019s [24]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 56
                            }
                        ],
                        "text": "The main difference is that the recursive network for IOHMM\u2019s [shown in Fig."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 164
                            }
                        ],
                        "text": "In the particular case of temporal sequences, solutions to the problem of learning asynchronous transductions have been proposed with recurrent networks [59] and IOHMM\u2019s [60]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 59
                            }
                        ],
                        "text": "Example 4.1 Hidden Markov Models:The hidden Markov models (HMM\u2019s) [23] are a well-known device for learning\n7Extensions to include multiple state variables can be conceived but are not described in this paper."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "HMM\u2019s and IOHMM\u2019s correspond to the simplest form of HRM\u2019s, for dealing with the class of linear chains ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 157
                            }
                        ],
                        "text": "These tables can be recursively computed by the following inward\u2013outward equations (that generalize the well-known Baum\u2013Welch\u2019s forward\u2013backward propagation for HMM\u2019s):\n(8)\nwhere the parameters are transition probabilities and the parameters are emission probabilities."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 156
                            }
                        ],
                        "text": "The most significant examples are neural architectures such as recurrent neural networks (RNN\u2019s) [22] and probabilistic models such as hidden Markov models (HMM\u2019s) [23] or input\u2013output HMM\u2019s (IOHMM\u2019s) [24]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 15
                            }
                        ],
                        "text": "Intuitively, IOHMM\u2019s and recurrent neural networks are in the same relationship as HRM\u2019s and recursive neural networks described in Section IV-B."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 200,
                                "start": 196
                            }
                        ],
                        "text": "The most signi cant examples are neural architectures such as recurrent neural networks (RNNs) [22] and probabilistic models such as hidden Markov models (HMMs) [23] or input-output HMMs (IOHMMs) [24]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 27
                            }
                        ],
                        "text": "Input-output HMMs (IOHMMs) [24] are recent extension of HMMs for supervised learning on temporal domains."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 8
                            }
                        ],
                        "text": "8Often, HMM\u2019s are depicted in a different graphical form, known asstate transition diagram."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 19
                            }
                        ],
                        "text": "State variables in HMM\u2019s satisfy the Markov conditional independency graphically represented in Fig."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 102
                            }
                        ],
                        "text": "Therefore, it seems quite natural to expect that many theoretical analyzes for recurrent networks and HMM\u2019s can be generalized to the recursive models presented in this paper."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 46
                            }
                        ],
                        "text": "9(a) shows the recursive network for standard HMM\u2019s.8 Input\u2013output HMM\u2019s (IOHMM\u2019s) [24] are recent extension of HMM\u2019s for supervised learning on temporal domains."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 150
                            }
                        ],
                        "text": "As a matter of fact, many learning systems, such as Boltzmann machines [29], multilayered perceptrons [30], (input-output) hidden Markov models [23], [24], (just to mention some of them) can be easily regarded as particular graphical models."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 75
                            }
                        ],
                        "text": "Specializations of HRM\u2019s to the case of sequentially structured data (e.g., HMM\u2019s and IOHMM\u2019s) are typically trained using the EM algorithm."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 195
                            }
                        ],
                        "text": "The maximization step consists of updating the parameters using the expected sufficient statistics\n4) Inference in Binary Tree HRM\u2019s:The model we consider in this section is a generalization of IOHMM\u2019s for processing binary trees of categorical variables."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Input-output HMM's for sequence processing,"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on  Neural Networks,"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1406748860"
                        ],
                        "name": "james w.thatcher",
                        "slug": "james-w.thatcher",
                        "structuredName": {
                            "firstName": "james",
                            "lastName": "w.thatcher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "james w.thatcher"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "automata [34], that we briefly recall here."
                    },
                    "intents": []
                }
            ],
            "corpusId": 203669070,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4ab2d2ac2a88061138446af93d8d49d993ec5d91",
            "isKey": false,
            "numCitedBy": 233,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "tree-automata:-an-informal-survey-w.thatcher",
            "title": {
                "fragments": [],
                "text": "tree automata: an informal survey"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2492622"
                        ],
                        "name": "S. C. Kremer",
                        "slug": "S.-C.-Kremer",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Kremer",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. C. Kremer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2182045"
                        ],
                        "name": "R. Elio",
                        "slug": "R.-Elio",
                        "structuredName": {
                            "firstName": "Ren\u00e9e",
                            "lastName": "Elio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Elio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734963"
                        ],
                        "name": "M. Dawson",
                        "slug": "M.-Dawson",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Dawson",
                            "middleNames": [
                                "R.",
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Dawson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 124
                            }
                        ],
                        "text": "A detailed discussion of the language identification problem on connectionist grammatical inference systems can be found in [55]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 59769468,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "6f2adce1600d03bbcdaa70f7d1b8999bc52a341a",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-theory-of-grammatical-induction-in-the-paradigm-Kremer-Elio",
            "title": {
                "fragments": [],
                "text": "A theory of grammatical induction in the connectionist paradigm"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2235757"
                        ],
                        "name": "V. Cadoret",
                        "slug": "V.-Cadoret",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Cadoret",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Cadoret"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 8
                            }
                        ],
                        "text": "[18] V. Cadoret, \u201cEncoding syntactical trees with labeling recursive autoassociative memory,\u201d inProc."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 226,
                                "start": 222
                            }
                        ],
                        "text": "In the eld of natural language processing, very good results on the classi cation of distributed representations of syntactical trees devised by an LRAAM according to the typology of dialogue acts were obtained by Cadoret [18]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 224,
                                "start": 217
                            }
                        ],
                        "text": "In the field of natural language processing, very good results on the classification of distributed representations of syntactical trees devised by an LRAAM according to the typology of dialogue acts were obtained by Cadoret [18]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 39999249,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0f67caf90fe8f43845adc67f40d084bef25fbb31",
            "isKey": false,
            "numCitedBy": 6,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Encoding-Syntactical-Trees-with-Labelling-Recursive-Cadoret",
            "title": {
                "fragments": [],
                "text": "Encoding Syntactical Trees with Labelling Recursive Auto-Associative Memory"
            },
            "venue": {
                "fragments": [],
                "text": "ECAI"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 71
                            }
                        ],
                        "text": "As a matter of fact, many learning systems, such as Boltzmann machines [29], multilayered perceptrons [30], (input\u2013output) hidden Markov models [23], [24], (just to mention some of them) can be easily regarded as particular graphical models."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 45873552,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b8ab201acaa70cadece6e830c228e2f05f875d25",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Asymmetric-Parallel-Boltzmann-Machines-are-Belief-Neal",
            "title": {
                "fragments": [],
                "text": "Asymmetric Parallel Boltzmann Machines are Belief Networks"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2836317"
                        ],
                        "name": "O. Firschein",
                        "slug": "O.-Firschein",
                        "structuredName": {
                            "firstName": "Oscar",
                            "lastName": "Firschein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Firschein"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 96
                            }
                        ],
                        "text": "This was recognized early with the introduction of syntactic and structural pattern recognition [6]\u2013[9], that are based on the premise that the structureof an entity is very important for both classification and description."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6554647,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "86dbe878a56e5052a66c036996416a782b4da618",
            "isKey": false,
            "numCitedBy": 334,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Syntactic-pattern-recognition-and-applications-Firschein",
            "title": {
                "fragments": [],
                "text": "Syntactic pattern recognition and applications"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the IEEE"
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "corpusId": 236490549,
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Connection Science"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Elman, \\Finding structure in time"
            },
            "venue": {
                "fragments": [],
                "text": "Cognitive Science"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\Constructing deterministic nite-state automata in recurrent neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of the ACM"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Principe, \\The gamma model { A new neural net model for temporal processing"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\Using back-propagation for guiding the search of a theorem prover"
            },
            "venue": {
                "fragments": [],
                "text": "Int. Journal of Neural Network Research and Applications"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 125
                            }
                        ],
                        "text": "The experimental comparison between the LRAAM-based approach and the backpropagation through structure algorithm reported in [47] shows that the latter algorithm obtains slightly better results for all examples solvable by the LRAAM-based approach, but with smaller networks and training times."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 81
                            }
                        ],
                        "text": "9 The results obtained with this network and other standard neuralnetwork models [47] improved the search time of one order of magnitude, however, as is clear from the discussion in Section V-A and the examples of features given above, the encoding of structural information in a fixed-size vector is not able to capture all the relevant information gathered by"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 99
                            }
                        ],
                        "text": "A refinement of this work led to the definition of the backpropagation through structure algorithm [47]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning task-dependent distributed structure-representations by backpropagation through structure"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Int. Conf. Neural Networks  , 1996, pp. 347\u2013352."
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 18
                            }
                        ],
                        "text": "McCabe complexity [2]) have been developed which try to codify the above properties of a (portion of) program numerically."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Cabe, \\A software complexity measure,"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Software Engineering,  vol. 2,"
            },
            "year": 1976
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 160
                            }
                        ],
                        "text": "1 Tree automata Perhaps, the best known stationary transductions that operate on structures more complex that linear chains are those realized by tree automata [34], that we brie y recall here."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Tree automata: An informal survey,\" in Currents in the Theory of Computing (A"
            },
            "venue": {
                "fragments": [],
                "text": "Aho,  ed.),"
            },
            "year": 1973
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Input \u2013 output HMM \u2019 s for sequence processing"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans . Neural Networks"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 20
                            }
                        ],
                        "text": ", McCabe complexity [2]) have been developed which try to codify the above properties of a (portion of) program numerically."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A software complexity measure"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Software Eng., vol. 2, pp. 308\u2013320, 1976."
            },
            "year": 1976
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 248,
                                "start": 244
                            }
                        ],
                        "text": "Research concerning inference in Bayesian networks dates back to the 1980\u2019s, when the principal concern was the construction of probabilistic expert systems, and is now relatively mature, making available general and well-understood algorithms [35]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 247,
                                "start": 243
                            }
                        ],
                        "text": "In the latter case, the DAG must be first \u201ccompiled\u201d into a new structure, called junction tree, whose nodes contain clusters of variables; then inference relies on a local message propagation algorithm between the nodes of the junction trees [35]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian updating in recursive graphical models by local computations"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Statist. Quarterly, vol. 4, pp. 269\u2013282, 1990."
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 89
                            }
                        ],
                        "text": "Here by a DOAG we mean a DAG with vertex set vert and edge set edg where for each vertex a total order on the edges leaving from is defined."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Syntactic Pattern Recognition and Applications. Englewood Cliffs"
            },
            "venue": {
                "fragments": [],
                "text": "Syntactic Pattern Recognition and Applications. Englewood Cliffs"
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 81
                            }
                        ],
                        "text": "The results obtained with this network and other standard neural networks models [47] improved the search time of one order of magnitude, however, as is clear from the discussion in Section 5."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 125
                            }
                        ],
                        "text": "The experimental comparison between the LRAAM-based approach and the backpropagation through structure algorithm reported in [47] shows that the latter algorithm obtains slightly better results for all examples solvable by the LRAAM-based approach, but with smaller networks and training times."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 97
                            }
                        ],
                        "text": "A re nement of this work led to the de nition of the backpropagation through structure algorithm [47]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "uchler, \\Learning task-dependent distributed structure-representations by back-  propagation through structure,"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE International Conference on Neural Networks,"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 147
                            }
                        ],
                        "text": "However, the ability of representing nite automata is not su cient to guarantee that a given regular grammar can be actually learned from examples [54]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Complexity of automaton identi cation from given data,"
            },
            "venue": {
                "fragments": [],
                "text": "Information and Control,  vol"
            },
            "year": 1978
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 39
                            }
                        ],
                        "text": "general and well understood algorithms [35]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 245,
                                "start": 241
                            }
                        ],
                        "text": "In the latter case, the DAG must be rst \\compiled\" into a new structure, called junction tree, whose nodes contain clusters of variables; then inference relies on a local message propagation algorithm between the nodes of the junction trees [35]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Olosen, \\Bayesian updating in recursive graphical models  by local computations,"
            },
            "venue": {
                "fragments": [],
                "text": "Computational Statistical Quarterly,"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 65
                            }
                        ],
                        "text": "A detailed analysis on optimal convergence issues is proposed in [42]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Optimal learning of data structures"
            },
            "venue": {
                "fragments": [],
                "text": "inProc. Int. Joint Conf. Artificial Intell. , Nagoya, Japan, August 23\u201329, 1997, pp. 1066\u20131071."
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Soda, \\Representation of nite state automata in recurrent radial basis function networks"
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 96
                            }
                        ],
                        "text": "A belief network is an annotated graph in which nodes represent random variables in the universe of discourse, andmissing edges encode a set of conditional independence statements among these variables."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Labeling RAAM Connection Sci"
            },
            "venue": {
                "fragments": [],
                "text": "Labeling RAAM Connection Sci"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 168
                            }
                        ],
                        "text": "High-order recursive recurrent neural networks High-order neural networks, proposed mainly by Giles and associates for both static networks [40] and recurrent networks [41], are very interesting models especially for dealing with symbolic tasks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Experimental comparison of the e ect of order in recurrent neural  networks,\" Int"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Pattern Recognition and Arti cial Intelligence,"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Input \u2013 output HMM \u2019 s for sequence processing"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans . Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\Experimental comparison of the eeect of order in recurrent neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Special Issue on Applications of Neural Networks to Pattern Recognition (I. Guyon Ed"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Sperduti has contributed to the organization of several workshops on this subject and also served on the program committee of conferences on neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Sperduti has contributed to the organization of several workshops on this subject and also served on the program committee of conferences on neural networks"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 190,
                                "start": 186
                            }
                        ],
                        "text": "Another improvement in this direction has been the development of the Cascade-Correlation Network for structure [49, 19] (a generalization of recurrent Cascade-Correlation for sequences [50]) which obtained better results with respect to the LRAAM-based networks on a subset of the classi cation problems reported in [48]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The recurrent cascade-correlation architecture,\" in Advances in Neural Information  Processing Systems (R"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 42
                            }
                        ],
                        "text": "injected, the harder the learning becomes [1]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Recurrent neural networks and prior knowledge for sequence processing: A constrained nondeterministic approach,\u201dKnowledge-Based"
            },
            "venue": {
                "fragments": [],
                "text": "Syst.  ,"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 167
                            }
                        ],
                        "text": "This means that a fully developed theory on structures will automatically cover 10It is well known that recurrent neural networks can simulate any nite state automata [52] as well as any multi-stack Turing machine in real time [53]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Constructing deterministic nite-state automata in recurrent neural net-  works,"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of the ACM,"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 57
                            }
                        ],
                        "text": "A good review of discrete time operators can be found in [26]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A comparison of discrete-time operator models and for nonlinear system  identi cation,\" in Advances in Neural Information Processing Systems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 272,
                                "start": 268
                            }
                        ],
                        "text": "A rst attempt to learn an evaluation function in SETHEO was performed by training a back-propagation network on feature vectors representing prede ned features for the structures encountered during the search, like the proof-context and the applicable inference steps [46]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Using back-propagation for guiding the search of a theorem prover,\"  Int"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Neural Network Research and Applications,"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Omlin, \\Extraction, insertion and reenement of symbolic rules in dynamicallydriven recurrent neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Connection Science"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Recurrent neural networks for missing or asynchronous data,\" in Ad-  vances in Neural Information Processing Systems"
            },
            "venue": {
                "fragments": [],
                "text": "eds.),  vol"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 62
                            }
                        ],
                        "text": "\u2022 The vertex set of is the Cartesian product of the vertex sets of and For each denotes theth input variable at node denotes the\nth state variable at node and denotes theth output variable at node ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Graphical Models, no. 17 in Statistical Science Series"
            },
            "venue": {
                "fragments": [],
                "text": "Graphical Models, no. 17 in Statistical Science Series"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A survey of inductive inference : Theory and methods"
            },
            "venue": {
                "fragments": [],
                "text": "ACM Comput . Survey"
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Principles of Artiicial Intelligence. Palo Alto: Tioga"
            },
            "venue": {
                "fragments": [],
                "text": "Principles of Artiicial Intelligence. Palo Alto: Tioga"
            },
            "year": 1980
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An informal survey"
            },
            "venue": {
                "fragments": [],
                "text": "Currents in the Theory of Computing"
            },
            "year": 1973
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 154
                            }
                        ],
                        "text": "The state vector Xv is obtained using an additional layer of sigmoidal units on the top of radial basis function units, with weight matrix W 2 IRn p (see [39] for more details)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 66
                            }
                        ],
                        "text": "Recursive radial basis functions Recurrent radial basis functions [39] can be extended to the more general computation needed to process structures by relying on the following parametric representation, for i = 1; : : : ; p: Xr i;v = o X k=1 jjq 1 k Xv Ai;kjj2 + jjU v Bijj2! (11) where is often chosen as an exponential function."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Representation of nite state automata in recurrent  radial basis function networks,"
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning,"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 118
                            }
                        ],
                        "text": "An interesting solution consists of merging the training examples into an optimally compressed supergraph (as done in [36]), so that only one junction tree has to be built for the entire training set."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Hidden recursive models for probabilistic learning of struc-  tured information,\" preprint, Dipartimento di Sistemi e Informatica, Universit  a di Firenze, Firenze"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 187,
                                "start": 183
                            }
                        ],
                        "text": "3 Inference of tree grammars In classical grammatical inference, a learner is presented a set of labeled strings and is requested to infer a set of rules that de ne a formal language [51]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A survey of inductive inference: Theory and methods,"
            },
            "venue": {
                "fragments": [],
                "text": "ACM Comput.  Surv.,"
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 128
                            }
                        ],
                        "text": "neural networks (RNN\u2019s) [22] and probabilistic models such as hidden Markov models (HMM\u2019s) [23] or input\u2013output HMM\u2019s (IOHMM\u2019s) [24]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 31
                            }
                        ],
                        "text": "8 Input\u2013output HMM\u2019s (IOHMM\u2019s) [24] are recent extension of HMM\u2019s for supervised learning on temporal domains."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 150
                            }
                        ],
                        "text": "As a matter of fact, many learning systems, such as Boltzmann machines [29], multilayered perceptrons [30], (input\u2013output) hidden Markov models [23], [24], (just to mention some of them) can be easily regarded as particular graphical models."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Input\u2013output HMM\u2019s for sequence processing,\u201dIEEE"
            },
            "venue": {
                "fragments": [],
                "text": "Trans. Neural Networks ,"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Extracting re ned rules from knowledge-based neural networks,"
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning,"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Syntactical Pattern Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Syntactical Pattern Recognition"
            },
            "year": 1978
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 102
                            }
                        ],
                        "text": "As a matter of fact, many learning systems, such as Boltzmann machines [29], multilayered perceptrons [30], (input\u2013output) hidden Markov models [23], [24], (just to mention some of them) can be easily regarded as particular graphical models."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Networks for Pattern Recognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 273,
                                "start": 269
                            }
                        ],
                        "text": "A first attempt to learn an evaluation function in SETHEO was performed by training a backpropagation network on feature vectors representing predefined features for the structures encountered during the search, like the proofcontext and the applicable inference steps [46]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Using backpropagation for guiding the search of a theorem prover"
            },
            "venue": {
                "fragments": [],
                "text": " Int. J. Neural Network Res. Applicat.  , vol. 2, no. 1, pp. 3\u201316, 1991."
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\Complexity of automaton identiication from given data"
            },
            "venue": {
                "fragments": [],
                "text": "Information and Control"
            },
            "year": 1978
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 39
                            }
                        ],
                        "text": "More details on SETHEO can be found in [45]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Setheo: A highperformance theorem prover"
            },
            "venue": {
                "fragments": [],
                "text": " J. Automated Reasoning  , vol. 8, no. 2, pp. 183\u2013212, 1992."
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Plate, \\Holographic reduced representations"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Neural Networks"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 52
                            }
                        ],
                        "text": "In fact, experimental results have been reported in [36] showing that binary-tree HRM\u2019s are able to learn regular grammars on binary tree domains."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 118
                            }
                        ],
                        "text": "An interesting solution consists of merging the training examples into an optimally compressed supergraph (as done in [36]), so that only one junction tree has to be built for the entire training set."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Hidden recursive models for probabilistic learning of structured information"
            },
            "venue": {
                "fragments": [],
                "text": "preprint, Dipartimento di Sistemi e Informatica, Universit \u00e0 di Firenze, Firenze, Italy, 1997."
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 155
                            }
                        ],
                        "text": "In classical grammatical inference, a learner is presented a set of labeled strings and is requested to infer a set of rules that define a formal language [51]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A survey of inductive inference: Theory and methods,\u201dACM"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Survey ,"
            },
            "year": 1983
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 50,
            "methodology": 16,
            "result": 8
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 100,
        "totalPages": 10
    },
    "page_url": "https://www.semanticscholar.org/paper/A-general-framework-for-adaptive-processing-of-data-Frasconi-Gori/a5eb96540ef53b49eac2246d6b13635fe6e54451?sort=total-citations"
}