{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144429686"
                        ],
                        "name": "James W. Davis",
                        "slug": "James-W.-Davis",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Davis",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James W. Davis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688328"
                        ],
                        "name": "A. Bobick",
                        "slug": "A.-Bobick",
                        "structuredName": {
                            "firstName": "Aaron",
                            "lastName": "Bobick",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bobick"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 161
                            }
                        ],
                        "text": "Some other approaches either track multiple points on the object, or track a bounding box enclosing the complete object, which provides some shape information [2]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7448285,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3c706d5065bc9cc48e6cabed2619c5c801e155dd",
            "isKey": false,
            "numCitedBy": 420,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "A new view-based approach to the representation and recognition of action is presented. The basis of the representation is a temporal template | a static vector-image where the vector value at each point is a function of the motion properties at the corresponding spatial location in an image sequence. Using 18 aerobics exercises as a test domain, we explore the representational power of a simple, two component version of the templates: the rst value is a binary value indicating the presence of motion, and the second value is a function of the recency of motion in a sequence. We then develop a recognition method which matches these temporal templates against stored instances of views of known actions. The method automatically performs temporal segmentation, is invariant to linear changes in speed, and runs in real-time on a standard platform. We recently incorporated this technique into the KidsRoom: an interactive, narrative play-space for children."
            },
            "slug": "The-Representation-and-Recognition-of-Action-Using-Davis-Bobick",
            "title": {
                "fragments": [],
                "text": "The Representation and Recognition of Action Using Temporal Templates"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "A new view-based approach to the representation and recognition of action is presented, using a temporal template | a static vector-image where the vector value at each point is a function of the motion properties at the corresponding spatial location in an image sequence."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR 1997"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145206607"
                        ],
                        "name": "C. Rao",
                        "slug": "C.-Rao",
                        "structuredName": {
                            "firstName": "Cen",
                            "lastName": "Rao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Rao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1858702"
                        ],
                        "name": "A. Yilmaz",
                        "slug": "A.-Yilmaz",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Yilmaz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yilmaz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145103012"
                        ],
                        "name": "M. Shah",
                        "slug": "M.-Shah",
                        "structuredName": {
                            "firstName": "Mubarak",
                            "lastName": "Shah",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Shah"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 140
                            }
                        ],
                        "text": "Single\npoint tracking generates a motion trajectory, and there are several approaches employing motion trajectories for action recognition [8]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2769833,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1128c0cd1e504d555c57cb39cdd7b6be399eb5a7",
            "isKey": false,
            "numCitedBy": 516,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Analysis of human perception of motion shows that information for representing the motion is obtained from the dramatic changes in the speed and direction of the trajectory. In this paper, we present a computational representation of human action to capture these dramatic changes using spatio-temporal curvature of 2-D trajectory. This representation is compact, view-invariant, and is capable of explaining an action in terms of meaningful action units called dynamic instants and intervals. A dynamic instant is an instantaneous entity that occurs for only one frame, and represents an important change in the motion characteristics. An interval represents the time period between two dynamic instants during which the motion characteristics do not change. Starting without a model, we use this representation for recognition and incremental learning of human actions. The proposed method can discover instances of the same action performed by differentpeople from different view points. Experiments on 47 actions performed by 7 individuals in an environment with no constraints shows the robustness of the proposed method."
            },
            "slug": "View-Invariant-Representation-and-Recognition-of-Rao-Yilmaz",
            "title": {
                "fragments": [],
                "text": "View-Invariant Representation and Recognition of Actions"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "This paper presents a computational representation of human action to capture these dramatic changes using spatio-temporal curvature of 2-D trajectory that is compact, view-invariant, and capable of explaining an action in terms of meaningful action units called dynamic instants and intervals."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143991676"
                        ],
                        "name": "I. Laptev",
                        "slug": "I.-Laptev",
                        "structuredName": {
                            "firstName": "Ivan",
                            "lastName": "Laptev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Laptev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3205375"
                        ],
                        "name": "T. Lindeberg",
                        "slug": "T.-Lindeberg",
                        "structuredName": {
                            "firstName": "Tony",
                            "lastName": "Lindeberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Lindeberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 5
                            }
                        ],
                        "text": "In [6], Laptev and Lindeberg extended the 2D Harris detector to (x, y, t) and find temporal interest points."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2619278,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "f90d79809325d2b78e35a79ecb372407f81b3993",
            "isKey": false,
            "numCitedBy": 2381,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "Local image features or interest points provide compact and abstract representations of patterns in an image. We propose to extend the notion of spatial interest points into the spatio-temporal domain and show how the resulting features often reflect interesting events that can be used for a compact representation of video data as well as for its interpretation. To detect spatio-temporal events, we build on the idea of the Harris and Forstner interest point operators and detect local structures in space-time where the image values have significant local variations in both space and time. We then estimate the spatio-temporal extents of the detected events and compute their scale-invariant spatio-temporal descriptors. Using such descriptors, we classify events and construct video representation in terms of labeled space-time points. For the problem of human motion analysis, we illustrate how the proposed method allows for detection of walking people in scenes with occlusions and dynamic backgrounds."
            },
            "slug": "Space-time-interest-points-Laptev-Lindeberg",
            "title": {
                "fragments": [],
                "text": "Space-time interest points"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This work builds on the idea of the Harris and Forstner interest point operators and detects local structures in space-time where the image values have significant local variations in both space and time to detect spatio-temporal events."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Ninth IEEE International Conference on Computer Vision"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688328"
                        ],
                        "name": "A. Bobick",
                        "slug": "A.-Bobick",
                        "structuredName": {
                            "firstName": "Aaron",
                            "lastName": "Bobick",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bobick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144429686"
                        ],
                        "name": "James W. Davis",
                        "slug": "James-W.-Davis",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Davis",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James W. Davis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2006961,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "886431a362bfdbcc6dd518f844eb374950b9de86",
            "isKey": false,
            "numCitedBy": 2878,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "A view-based approach to the representation and recognition of human movement is presented. The basis of the representation is a temporal template-a static vector-image where the vector value at each point is a function of the motion properties at the corresponding spatial location in an image sequence. Using aerobics exercises as a test domain, we explore the representational power of a simple, two component version of the templates: The first value is a binary value indicating the presence of motion and the second value is a function of the recency of motion in a sequence. We then develop a recognition method matching temporal templates against stored instances of views of known actions. The method automatically performs temporal segmentation, is invariant to linear changes in speed, and runs in real-time on standard platforms."
            },
            "slug": "The-Recognition-of-Human-Movement-Using-Temporal-Bobick-Davis",
            "title": {
                "fragments": [],
                "text": "The Recognition of Human Movement Using Temporal Templates"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "A view-based approach to the representation and recognition of human movement is presented, and a recognition method matching temporal templates against stored instances of views of known actions is developed."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145379616"
                        ],
                        "name": "Douglas Ayers",
                        "slug": "Douglas-Ayers",
                        "structuredName": {
                            "firstName": "Douglas",
                            "lastName": "Ayers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Douglas Ayers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145103012"
                        ],
                        "name": "M. Shah",
                        "slug": "M.-Shah",
                        "structuredName": {
                            "firstName": "Mubarak",
                            "lastName": "Shah",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Shah"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 104
                            }
                        ],
                        "text": "Some popular approaches for action recognition include Hidden Markov Models [10], Finite State Machines [1], neural networks and Context Free Grammars."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10222347,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3ac8fe1ca09e994a1fdc244c66ecbf0a2b8d5148",
            "isKey": false,
            "numCitedBy": 174,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Monitoring-human-behavior-from-video-taken-in-an-Ayers-Shah",
            "title": {
                "fragments": [],
                "text": "Monitoring human behavior from video taken in an office environment"
            },
            "venue": {
                "fragments": [],
                "text": "Image Vis. Comput."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144611617"
                        ],
                        "name": "M. Irani",
                        "slug": "M.-Irani",
                        "structuredName": {
                            "firstName": "Michal",
                            "lastName": "Irani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Irani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5586199,
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "id": "81324332b73b1a82344aab3cce6a1eefbbb98a7d",
            "isKey": false,
            "numCitedBy": 159,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Shows that the set of all flow fields in a sequence of frames imaging a rigid scene resides in a low-dimensional linear subspace. Based on this observation, we develop a method for simultaneous estimation of optical flow across multiple frames, which uses these subspace constraints. The multi-frame subspace constraints are strong constraints, and they replace commonly used heuristic constraints, such as spatial or temporal smoothness. The subspace constraints are geometrically meaningful and are not violated at depth discontinuities or when the camera motion changes abruptly. Furthermore, we show that the subspace constraints on flow fields apply for a variety of imaging models, scene models and motion models. Hence, the presented approach for constrained multi-frame flow estimation is general. However, our approach does not require prior knowledge of the underlying world or camera model. Although linear subspace constraints have been used successfully in the past for recovering 3D information, it has been assumed that 2D correspondences are given. However, correspondence estimation is a fundamental problem in motion analysis. In this paper, we use multi-frame subspace constraints to constrain the 2D correspondence estimation process itself, and not for 3D recovery."
            },
            "slug": "Multi-frame-optical-flow-estimation-using-subspace-Irani",
            "title": {
                "fragments": [],
                "text": "Multi-frame optical flow estimation using subspace constraints"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper develops a method for simultaneous estimation of optical flow across multiple frames, which uses multi-frame subspace constraints to constrain the 2D correspondence estimation process itself, and not for 3D recovery."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Seventh IEEE International Conference on Computer Vision"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1858702"
                        ],
                        "name": "A. Yilmaz",
                        "slug": "A.-Yilmaz",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Yilmaz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yilmaz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2153900758"
                        ],
                        "name": "Xin Li",
                        "slug": "Xin-Li",
                        "structuredName": {
                            "firstName": "Xin",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xin Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145103012"
                        ],
                        "name": "M. Shah",
                        "slug": "M.-Shah",
                        "structuredName": {
                            "firstName": "Mubarak",
                            "lastName": "Shah",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Shah"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 790431,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2ea87451b066226de1b42b8e61f5a80c3c351bbf",
            "isKey": false,
            "numCitedBy": 581,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a tracking method which tracks the complete object regions, adapts to changing visual features, and handles occlusions. Tracking is achieved by evolving the contour from frame to frame by minimizing some energy functional evaluated in the contour vicinity defined by a band. Our approach has two major components related to the visual features and the object shape. Visual features (color, texture) are modeled by semiparametric models and are fused using independent opinion polling. Shape priors consist of shape level sets and are used to recover the missing object regions during occlusion. We demonstrate the performance of our method in real sequences with and without object occlusions."
            },
            "slug": "Contour-based-object-tracking-with-occlusion-in-Yilmaz-Li",
            "title": {
                "fragments": [],
                "text": "Contour-based object tracking with occlusion handling in video acquired using mobile cameras"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A tracking method which tracks the complete object regions, adapts to changing visual features, and handles occlusions, which has two major components related to the visual features and the object shape."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737754"
                        ],
                        "name": "J. Siskind",
                        "slug": "J.-Siskind",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Siskind",
                            "middleNames": [
                                "Mark"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Siskind"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731810"
                        ],
                        "name": "Q. Morris",
                        "slug": "Q.-Morris",
                        "structuredName": {
                            "firstName": "Quaid",
                            "lastName": "Morris",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Q. Morris"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 76
                            }
                        ],
                        "text": "Some popular approaches for action recognition include Hidden Markov Models [10], Finite State Machines [1], neural networks and Context Free Grammars."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2576270,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fdf6a55f332d46a78c017f2fe14e6be44637e64a",
            "isKey": false,
            "numCitedBy": 80,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a novel framework, based on maximum likelihood, for training models to recognise simple spatial-motion events, such as those described by the verbs pick up, put down, push, pull, drop, and throw, and classifying novel observations into previously trained classes. The model that we employ does not presuppose prior recognition or tracking of 3D object pose, shape, or identity. We describe our general framework for using maximum-likelihood techniques for visual event classification, the details of the generative model that we use to characterise observations as instances of event types, and the implemented computational techniques used to support training and classification for this generative model. We conclude by illustrating the operation of our implementation on a small example."
            },
            "slug": "A-Maximum-Likelihood-Approach-to-Visual-Event-Siskind-Morris",
            "title": {
                "fragments": [],
                "text": "A Maximum-Likelihood Approach to Visual Event Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "A novel framework, based on maximum likelihood, for training models to recognise simple spatial-motion events, such as those described by the verbs pick up, put down, push, pull, drop, and throw, and classifying novel observations into previously trained classes is presented."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51064498"
                        ],
                        "name": "Zhengyou Zhang",
                        "slug": "Zhengyou-Zhang",
                        "structuredName": {
                            "firstName": "Zhengyou",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhengyou Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3190498,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "95880bede25dc10b4ea922b26be82edfd43179bf",
            "isKey": false,
            "numCitedBy": 1023,
            "numCiting": 198,
            "paperAbstract": {
                "fragments": [],
                "text": "Two images of a single scene/object are related by the epipolar geometry, which can be described by a 3\u00d73 singular matrix called the essential matrix if images' internal parameters are known, or the fundamental matrix otherwise. It captures all geometric information contained in two images, and its determination is very important in many applications such as scene modeling and vehicle navigation. This paper gives an introduction to the epipolar geometry, and provides a complete review of the current techniques for estimating the fundamental matrix and its uncertainty. A well-founded measure is proposed to compare these techniques. Projective reconstruction is also reviewed. The software which we have developed for this review is available on the Internet."
            },
            "slug": "Determining-the-Epipolar-Geometry-and-its-A-Review-Zhang",
            "title": {
                "fragments": [],
                "text": "Determining the Epipolar Geometry and its Uncertainty: A Review"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A complete review of the current techniques for estimating the fundamental matrix and its uncertainty is provided, and a well-founded measure is proposed to compare these techniques."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2084269"
                        ],
                        "name": "H. Chui",
                        "slug": "H.-Chui",
                        "structuredName": {
                            "firstName": "Haili",
                            "lastName": "Chui",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Chui"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145257017"
                        ],
                        "name": "Anand Rangarajan",
                        "slug": "Anand-Rangarajan",
                        "structuredName": {
                            "firstName": "Anand",
                            "lastName": "Rangarajan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anand Rangarajan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7442835,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9511b4e7a3d64b9737ea25b5c268478e3d248209",
            "isKey": false,
            "numCitedBy": 1574,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-new-point-matching-algorithm-for-non-rigid-Chui-Rangarajan",
            "title": {
                "fragments": [],
                "text": "A new point matching algorithm for non-rigid registration"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Vis. Image Underst."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144938732"
                        ],
                        "name": "R. Jain",
                        "slug": "R.-Jain",
                        "structuredName": {
                            "firstName": "Ramesh",
                            "lastName": "Jain",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 19666061,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e3132c544e52ea158928a0f39b54ca335cba5517",
            "isKey": false,
            "numCitedBy": 94,
            "numCiting": 183,
            "paperAbstract": {
                "fragments": [],
                "text": "Computer vision researchers have been frustrated in their attempts to automatically derive depth information from conventional two-dimensional intensity images. Research on \"shape from texture\", \"shape from shading\", and \"shape from focus\" is still in a laboratory stage and had not seen much use in commercial machine vision systems. A range image or a depth map contains explicit information about the distance from the sensor to the object surfaces within the field of view in the scene. Information about \"surface geometry\" which is important for, say, three-dimensional object recognition is more easily extracted from \"2 1/2 D\" range images than from \"2D\" intensity images. As a result, both active sensors such as laser range finders and passive techniques such as multi-camera stereo vision are being increasingly utilized by vision researchers to solve a variety of problems. This book contains chapters written by distinguished computer vision researchers covering the following areas: Overview of 3D Vision Range Sensing Geometric Processing Object Recognition Navigation Inspection Multisensor Fusion A workshop report, written by the editors, also appears in the book. It summarizes the state of the art and proposes future research directions in range image sensing, processing, interpretation, and applications. The book also contains an extensive, up-to-date bibliography on the above topics. This book provides a unique perspective on the problem of three-dimensional sensing and processing; it is the only comprehensive collection of papers devoted to range images. Both academic researchers interested in research issues in 3D vision and industrial engineers in search of solutions to particular problems will find this a useful reference book."
            },
            "slug": "Analysis-and-Interpretation-of-Range-Images-Jain-Jain",
            "title": {
                "fragments": [],
                "text": "Analysis and Interpretation of Range Images"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This book provides a unique perspective on the problem of three-dimensional sensing and processing; it is the only comprehensive collection of papers devoted to range images."
            },
            "venue": {
                "fragments": [],
                "text": "Springer Series in Perception Engineering"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2084269"
                        ],
                        "name": "H. Chui",
                        "slug": "H.-Chui",
                        "structuredName": {
                            "firstName": "Haili",
                            "lastName": "Chui",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Chui"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145257017"
                        ],
                        "name": "Anand Rangarajan",
                        "slug": "Anand-Rangarajan",
                        "structuredName": {
                            "firstName": "Anand",
                            "lastName": "Rangarajan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anand Rangarajan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 1318268,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dabad3d9c7af2aec185737a1577d44c3b6164286",
            "isKey": false,
            "numCitedBy": 504,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new robust point matching algorithm (RPM) that can jointly estimate the correspondence and non-rigid transformations between two point-sets that may be of different sizes. The algorithm utilizes the soft assign for the correspondence and the thin-plate spline for the non-rigid mapping. Embedded within a deterministic annealing framework, the algorithm can automatically reject a fraction of the points as outliers. Experiments on both 2D synthetic point-sets with varying degrees of deformation, noise and outliers, and on real 3D sulcal point-sets (extracted from brain MRI) demonstrate the robustness of the algorithm."
            },
            "slug": "A-new-algorithm-for-non-rigid-point-matching-Chui-Rangarajan",
            "title": {
                "fragments": [],
                "text": "A new algorithm for non-rigid point matching"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "A new robust point matching algorithm (RPM) that can jointly estimate the correspondence and non-rigid transformations between two point-sets that may be of different sizes is presented."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings IEEE Conference on Computer Vision and Pattern Recognition. CVPR 2000 (Cat. No.PR00662)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "122003543"
                        ],
                        "name": "S. Niyogi",
                        "slug": "S.-Niyogi",
                        "structuredName": {
                            "firstName": "Sumanta",
                            "lastName": "Niyogi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Niyogi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2169042067"
                        ],
                        "name": "E. Adelson",
                        "slug": "E.-Adelson",
                        "structuredName": {
                            "firstName": "EH",
                            "lastName": "Adelson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Adelson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 77
                            }
                        ],
                        "text": "Generating a volume from a set of images has been previously considered in [11] for walking persons."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 766730,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "2a4744d550764de0170fe31bcec73e6562e1438a",
            "isKey": false,
            "numCitedBy": 160,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Human motions generate characteristic spatiotemporal patterns. We have developed a set of techniques for analyzing the patterns generated by people walking across the field of view. After change detection, the XYT pattern can be fit with a smooth spatiotemporal surface. This surface is approximately periodic, reflecting the periodicity of the gait. The surface can be expressed as a combination of a standard parameterized surface-the canonical walk-and a deviation surface that is specific to the individual walk.<<ETX>>"
            },
            "slug": "Analyzing-gait-with-spatiotemporal-surfaces-Niyogi-Adelson",
            "title": {
                "fragments": [],
                "text": "Analyzing gait with spatiotemporal surfaces"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "A set of techniques for analyzing the patterns generated by people walking across the field of view, including the XYT pattern, which can be fit with a smooth spatiotemporal surface reflecting the periodicity of the gait."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 1994 IEEE Workshop on Motion of Non-rigid and Articulated Objects"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1809809"
                        ],
                        "name": "L. Shapiro",
                        "slug": "L.-Shapiro",
                        "structuredName": {
                            "firstName": "Linda",
                            "lastName": "Shapiro",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Shapiro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710238"
                        ],
                        "name": "R. Haralick",
                        "slug": "R.-Haralick",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Haralick",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Haralick"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3334210,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9149f8a8fb2df6464a4b03a726439cb27e896462",
            "isKey": false,
            "numCitedBy": 647,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we formally define the structural description of an object and the concepts of exact and inexact matching of two structural descriptions. We discuss the problems associated with a brute-force backtracking tree search for inexact matching and develop several different algorithms to make the tree search more efficient. We develop the formula for the expected number of nodes in the tree for backtracking alone and with a forward checking algorithm. Finally, we present experimental results showing that forward checking is the most efficient of the algorithms tested."
            },
            "slug": "Structural-Descriptions-and-Inexact-Matching-Shapiro-Haralick",
            "title": {
                "fragments": [],
                "text": "Structural Descriptions and Inexact Matching"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "The structural description of an object and the concepts of exact and inexact matching of two structural descriptions are formally defined and the formula for the expected number of nodes in the tree for backtracking alone and with a forward checking algorithm is developed."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1750195"
                        ],
                        "name": "E. Trucco",
                        "slug": "E.-Trucco",
                        "structuredName": {
                            "firstName": "Emanuele",
                            "lastName": "Trucco",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Trucco"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61003302,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "2fdf3b848d2dded13caa0351376dae861a31f640",
            "isKey": false,
            "numCitedBy": 414,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Geometric Invariance in Computer Vision, edited by Joseph L. Mundy and Andrew Zisserman, the MIT Press, 1992, $70.95 in Europe."
            },
            "slug": "Geometric-Invariance-in-Computer-Vision-Trucco",
            "title": {
                "fragments": [],
                "text": "Geometric Invariance in Computer Vision"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Geometric Invariance in Computer Vision, edited by Joseph L. Mundy and Andrew Zisserman, the MIT Press, 1992, $70.95 in Europe."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462253"
                        ],
                        "name": "C. Schmid",
                        "slug": "C.-Schmid",
                        "structuredName": {
                            "firstName": "Cordelia",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schmid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715959"
                        ],
                        "name": "Stefano Soatto",
                        "slug": "Stefano-Soatto",
                        "structuredName": {
                            "firstName": "Stefano",
                            "lastName": "Soatto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stefano Soatto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145086151"
                        ],
                        "name": "Carlo Tomasi",
                        "slug": "Carlo-Tomasi",
                        "structuredName": {
                            "firstName": "Carlo",
                            "lastName": "Tomasi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carlo Tomasi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60289829,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ee2da8a472897d297c6ad25229ae2ba7cec18348",
            "isKey": false,
            "numCitedBy": 306,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Proceedings.-2005-IEEE-Computer-Society-Conference-Schmid-Soatto",
            "title": {
                "fragments": [],
                "text": "Proceedings. 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 84
                            }
                        ],
                        "text": "View invariance of maxima/minima of curvature in a trajectory has been addressed in [8]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 139
                            }
                        ],
                        "text": "Single point tracking generates a motion trajectory, and there are several approaches employing motion trajectories for action recognition [8]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "View invariant representation and recognition"
            },
            "venue": {
                "fragments": [],
                "text": "of actions. IJCV,"
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 119
                            }
                        ],
                        "text": "The g matrix is called the metric tensor of the surface and has the same role as the speed function for spatial curves [5]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 117
                            }
                        ],
                        "text": "Gaussian curvature, K , and the mean curvature, H , are two algebraic invariants derived from the Weingarten mapping [5]:"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Analysis and interpretation of range"
            },
            "venue": {
                "fragments": [],
                "text": "images. Springer-Verlag,"
            },
            "year": 1990
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 8
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 18,
        "totalPages": 2
    },
    "page_url": "https://www.semanticscholar.org/paper/Actions-sketch:-a-novel-action-representation-Yilmaz-Shah/487c37dce9b93d48f753ab2ec3fc997edb5639ce?sort=total-citations"
}