{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2585394"
                        ],
                        "name": "Richard Futrell",
                        "slug": "Richard-Futrell",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Futrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Richard Futrell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51445267"
                        ],
                        "name": "Ethan Gotlieb Wilcox",
                        "slug": "Ethan-Gotlieb-Wilcox",
                        "structuredName": {
                            "firstName": "Ethan",
                            "lastName": "Gotlieb Wilcox",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ethan Gotlieb Wilcox"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46710186"
                        ],
                        "name": "T. Morita",
                        "slug": "T.-Morita",
                        "structuredName": {
                            "firstName": "Takashi",
                            "lastName": "Morita",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Morita"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143643017"
                        ],
                        "name": "R. Levy",
                        "slug": "R.-Levy",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Levy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Levy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 105
                            }
                        ],
                        "text": "It is also increasingly used in computational linguistics (Linzen et al., 2016; Marvin and Linzen, 2018; Futrell et al., 2018; Wilcox et al., 2018, 2019)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 52165443,
            "fieldsOfStudy": [
                "Computer Science",
                "Linguistics"
            ],
            "id": "31f48073366d5fad7b1c9e60af315690475b52f6",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": "Recurrent neural networks (RNNs) are the state of the art in sequence modeling for natural language. However, it remains poorly understood what grammatical characteristics of natural language they implicitly learn and represent as a consequence of optimizing the language modeling objective. Here we deploy the methods of controlled psycholinguistic experimentation to shed light on to what extent RNN behavior reflects incremental syntactic state and grammatical dependency representations known to characterize human linguistic behavior. We broadly test two publicly available long short-term memory (LSTM) English sequence models, and learn and test a new Japanese LSTM. We demonstrate that these models represent and maintain incremental syntactic state, but that they do not always generalize in the same way as humans. Furthermore, none of our models learn the appropriate grammatical dependency configurations licensing reflexive pronouns or negative polarity items."
            },
            "slug": "RNNs-as-psycholinguistic-subjects:-Syntactic-state-Futrell-Wilcox",
            "title": {
                "fragments": [],
                "text": "RNNs as psycholinguistic subjects: Syntactic state and grammatical dependency"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "It is demonstrated that these models represent and maintain incremental syntactic state, but that they do not always generalize in the same way as humans."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46236380"
                        ],
                        "name": "Alex Warstadt",
                        "slug": "Alex-Warstadt",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Warstadt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Warstadt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3644767"
                        ],
                        "name": "Samuel R. Bowman",
                        "slug": "Samuel-R.-Bowman",
                        "structuredName": {
                            "firstName": "Samuel",
                            "lastName": "Bowman",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Samuel R. Bowman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 57825754,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1b553c34270543a36ca12784821a7817b36e66ad",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 69,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent pretrained sentence encoders achieve state of the art results on language understanding tasks, but does this mean they have implicit knowledge of syntactic structures? We introduce a grammatically annotated development set for the Corpus of Linguistic Acceptability (CoLA; Warstadt et al., 2018), which we use to investigate the grammatical knowledge of three pretrained encoders, including the popular OpenAI Transformer (Radford et al., 2018) and BERT (Devlin et al., 2018). We fine-tune these encoders to do acceptability classification over CoLA and compare the models' performance on the annotated analysis set. Some phenomena, e.g. modification by adjuncts, are easy to learn for all models, while others, e.g. long-distance movement, are learned effectively only by models with strong overall performance, and others still, e.g. morphological agreement, are hardly learned by any model."
            },
            "slug": "Grammatical-Analysis-of-Pretrained-Sentence-with-Warstadt-Bowman",
            "title": {
                "fragments": [],
                "text": "Grammatical Analysis of Pretrained Sentence Encoders with Acceptability Judgments"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A grammatically annotated development set for the Corpus of Linguistic Acceptability (CoLA; Warstadt et al., 2018) is introduced, which is used to investigate the grammatical knowledge of three pretrained encoders, including the popular OpenAI Transformer and BERT."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1800564"
                        ],
                        "name": "Jey Han Lau",
                        "slug": "Jey-Han-Lau",
                        "structuredName": {
                            "firstName": "Jey",
                            "lastName": "Lau",
                            "middleNames": [
                                "Han"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jey Han Lau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143615270"
                        ],
                        "name": "Alexander Clark",
                        "slug": "Alexander-Clark",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Clark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander Clark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2732560"
                        ],
                        "name": "Shalom Lappin",
                        "slug": "Shalom-Lappin",
                        "structuredName": {
                            "firstName": "Shalom",
                            "lastName": "Lappin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shalom Lappin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1056628,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7b36c7c5d6490d02ad28298a42980b43d4b1ded8",
            "isKey": false,
            "numCitedBy": 115,
            "numCiting": 81,
            "paperAbstract": {
                "fragments": [],
                "text": "The question of whether humans represent grammatical knowledge as a binary condition on membership in a set of well-formed sentences, or as a probabilistic property has been the subject of debate among linguists, psychologists, and cognitive scientists for many decades. Acceptability judgments present a serious problem for both classical binary and probabilistic theories of grammaticality. These judgements are gradient in nature, and so cannot be directly accommodated in a binary formal grammar. However, it is also not possible to simply reduce acceptability to probability. The acceptability of a sentence is not the same as the likelihood of its occurrence, which is, in part, determined by factors like sentence length and lexical frequency. In this paper, we present the results of a set of large-scale experiments using crowd-sourced acceptability judgments that demonstrate gradience to be a pervasive feature in acceptability judgments. We then show how one can predict acceptability judgments on the basis of probability by augmenting probabilistic language models with an acceptability measure. This is a function that normalizes probability values to eliminate the confounding factors of length and lexical frequency. We describe a sequence of modeling experiments with unsupervised language models drawn from state-of-the-art machine learning methods in natural language processing. Several of these models achieve very encouraging levels of accuracy in the acceptability prediction task, as measured by the correlation between the acceptability measure scores and mean human acceptability values. We consider the relevance of these results to the debate on the nature of grammatical competence, and we argue that they support the view that linguistic knowledge can be intrinsically probabilistic."
            },
            "slug": "Grammaticality,-Acceptability,-and-Probability:-A-Lau-Clark",
            "title": {
                "fragments": [],
                "text": "Grammaticality, Acceptability, and Probability: A Probabilistic View of Linguistic Knowledge"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is argued that the results of a set of large-scale experiments using crowd-sourced acceptability judgments that demonstrate gradience to be a pervasive feature inacceptability judgments support the view that linguistic knowledge can be intrinsically probabilistic."
            },
            "venue": {
                "fragments": [],
                "text": "Cogn. Sci."
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145919859"
                        ],
                        "name": "Joachim Wagner",
                        "slug": "Joachim-Wagner",
                        "structuredName": {
                            "firstName": "Joachim",
                            "lastName": "Wagner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joachim Wagner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144116340"
                        ],
                        "name": "Jennifer Foster",
                        "slug": "Jennifer-Foster",
                        "structuredName": {
                            "firstName": "Jennifer",
                            "lastName": "Foster",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jennifer Foster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7519068"
                        ],
                        "name": "Josef van Genabith",
                        "slug": "Josef-van-Genabith",
                        "structuredName": {
                            "firstName": "Josef",
                            "lastName": "van Genabith",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Josef van Genabith"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16336094,
            "fieldsOfStudy": [
                "Linguistics",
                "Computer Science"
            ],
            "id": "8cccc5bbd07f0647b0a3ac0928995945897bb18b",
            "isKey": false,
            "numCitedBy": 44,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "A classifier which is capable of distinguishing a syntactically well formed sentence from a syntactically ill formed one has the potential to be useful in an L2 language-learning context. In this article, we describe a classifier which classifies English sentences as either well formed or ill formed using information gleaned from three different natural language processing techniques. We describe the issues involved in acquiring data to train such a classifier and present experimental results for this classifier on a variety of ill formed sentences. We demonstrate that (a) the combination of information from a variety of linguistic sources is helpful, (b) the trade-off between accuracy on well formed sentences and accuracy on ill formed sentences can be fine tuned by training multiple classifiers in a voting scheme, and (c) the performance of the classifier is varied, with better performance on transcribed spoken sentences produced by less advanced language learners."
            },
            "slug": "Judging-Grammaticality:-Experiments-in-Sentence-Wagner-Foster",
            "title": {
                "fragments": [],
                "text": "Judging Grammaticality: Experiments in Sentence Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "It is demonstrated that the combination of information from a variety of linguistic sources is helpful, the trade-off between accuracy on well formed sentences and accuracy on ill formed sentences can be fine tuned by training multiple classifiers in a voting scheme."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145840115"
                        ],
                        "name": "S. Lawrence",
                        "slug": "S.-Lawrence",
                        "structuredName": {
                            "firstName": "Steve",
                            "lastName": "Lawrence",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lawrence"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145157784"
                        ],
                        "name": "C. Lee Giles",
                        "slug": "C.-Lee-Giles",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Giles",
                            "middleNames": [
                                "Lee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lee Giles"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37805323"
                        ],
                        "name": "Sandiway Fong",
                        "slug": "Sandiway-Fong",
                        "structuredName": {
                            "firstName": "Sandiway",
                            "lastName": "Fong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sandiway Fong"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6673581,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "304b814e51e3515ed7e4521cbfba5c8e36ff44d8",
            "isKey": false,
            "numCitedBy": 151,
            "numCiting": 214,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper examines the inductive inference of a complex grammar with neural networks and specifically, the task considered is that of training a network to classify natural language sentences as grammatical or ungrammatical, thereby exhibiting the same kind of discriminatory power provided by the Principles and Parameters linguistic framework, or Government-and-Binding theory. Neural networks are trained, without the division into learned vs. innate components assumed by Chomsky (1956), in an attempt to produce the same judgments as native speakers on sharply grammatical/ungrammatical data. How a recurrent neural network could possess linguistic capability and the properties of various common recurrent neural network architectures are discussed. The problem exhibits training behavior which is often not present with smaller grammars and training was initially difficult. However, after implementing several techniques aimed at improving the convergence of the gradient descent backpropagation-through-time training algorithm, significant learning was possible. It was found that certain architectures are better able to learn an appropriate grammar. The operation of the networks and their training is analyzed. Finally, the extraction of rules in the form of deterministic finite state automata is investigated."
            },
            "slug": "Natural-Language-Grammatical-Inference-with-Neural-Lawrence-Giles",
            "title": {
                "fragments": [],
                "text": "Natural Language Grammatical Inference with Recurrent Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It was found that certain architectures are better able to learn an appropriate grammar than others, and the extraction of rules in the form of deterministic finite state automata is investigated."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Knowl. Data Eng."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2618874"
                        ],
                        "name": "Michael Heilman",
                        "slug": "Michael-Heilman",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Heilman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Heilman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145557710"
                        ],
                        "name": "A. Cahill",
                        "slug": "A.-Cahill",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Cahill",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Cahill"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723404"
                        ],
                        "name": "Nitin Madnani",
                        "slug": "Nitin-Madnani",
                        "structuredName": {
                            "firstName": "Nitin",
                            "lastName": "Madnani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nitin Madnani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145931805"
                        ],
                        "name": "Melissa Lopez",
                        "slug": "Melissa-Lopez",
                        "structuredName": {
                            "firstName": "Melissa",
                            "lastName": "Lopez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Melissa Lopez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145271381"
                        ],
                        "name": "Matthew David Mulholland",
                        "slug": "Matthew-David-Mulholland",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Mulholland",
                            "middleNames": [
                                "David"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew David Mulholland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739099"
                        ],
                        "name": "J. Tetreault",
                        "slug": "J.-Tetreault",
                        "structuredName": {
                            "firstName": "Joel",
                            "lastName": "Tetreault",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tetreault"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8719319,
            "fieldsOfStudy": [
                "Computer Science",
                "Linguistics"
            ],
            "id": "9d5f5340667f3285a36f7e510d32dd54afb1dc37",
            "isKey": false,
            "numCitedBy": 65,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Automated methods for identifying whether sentences are grammatical have various potential applications (e.g., machine translation, automated essay scoring, computer-assisted language learning). In this work, we construct a statistical model of grammaticality using various linguistic features (e.g., misspelling counts, parser outputs, n-gram language model scores). We also present a new publicly available dataset of learner sentences judged for grammaticality on an ordinal scale. In evaluations, we compare our system to the one from Post (2011) and find that our approach yields state-of-the-art performance."
            },
            "slug": "Predicting-Grammaticality-on-an-Ordinal-Scale-Heilman-Cahill",
            "title": {
                "fragments": [],
                "text": "Predicting Grammaticality on an Ordinal Scale"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work constructs a statistical model of grammaticality using various linguistic features (e.g., misspelling counts, parser outputs, n-gram language model scores) and presents a new publicly available dataset of learner sentences judged for grammaticaly on an ordinal scale."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51445267"
                        ],
                        "name": "Ethan Gotlieb Wilcox",
                        "slug": "Ethan-Gotlieb-Wilcox",
                        "structuredName": {
                            "firstName": "Ethan",
                            "lastName": "Gotlieb Wilcox",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ethan Gotlieb Wilcox"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1483502658"
                        ],
                        "name": "Peng Qian",
                        "slug": "Peng-Qian",
                        "structuredName": {
                            "firstName": "Peng",
                            "lastName": "Qian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peng Qian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2585394"
                        ],
                        "name": "Richard Futrell",
                        "slug": "Richard-Futrell",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Futrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Richard Futrell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143668305"
                        ],
                        "name": "Miguel Ballesteros",
                        "slug": "Miguel-Ballesteros",
                        "structuredName": {
                            "firstName": "Miguel",
                            "lastName": "Ballesteros",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Miguel Ballesteros"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143643017"
                        ],
                        "name": "R. Levy",
                        "slug": "R.-Levy",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Levy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Levy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 127
                            }
                        ],
                        "text": "It is also increasingly used in computational linguistics (Linzen et al., 2016; Marvin and Linzen, 2018; Futrell et al., 2018; Wilcox et al., 2018, 2019)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 67855753,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "28a0ac83233d123b38917706e4ea4e34eee43482",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "State-of-the-art LSTM language models trained on large corpora learn sequential contingencies in impressive detail, and have been shown to acquire a number of non-local grammatical dependencies with some success. Here we investigate whether supervision with hierarchical structure enhances learning of a range of grammatical dependencies, a question that has previously been addressed only for subject-verb agreement. Using controlled experimental methods from psycholinguistics, we compare the performance of word-based LSTM models versus Recurrent Neural Network Grammars (RNNGs) (Dyer et al. 2016) which represent hierarchical syntactic structure and use neural control to deploy it in left-to-right processing, on two classes of non-local grammatical dependencies in English\u2014Negative Polarity licensing and Filler-Gap Dependencies\u2014tested in a range of configurations. Using the same training data for both models, we find that the RNNG outperforms the LSTM on both types of grammatical dependencies and even learns many of the Island Constraints on the filler-gap dependency. Structural supervision thus provides data efficiency advantages over purely string-based training of neural language models in acquiring human-like generalizations about non-local grammatical dependencies."
            },
            "slug": "Structural-Supervision-Improves-Learning-of-Wilcox-Qian",
            "title": {
                "fragments": [],
                "text": "Structural Supervision Improves Learning of Non-Local Grammatical Dependencies"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "It is found that the RNNG outperforms the LSTM on both types of grammatical dependencies and even learns many of the Island Constraints on the filler-gap dependency, which provides data efficiency advantages over purely string-based training of neural language models in acquiring human-like generalizations about non-local grammatical Dependencies."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2467508"
                        ],
                        "name": "Tal Linzen",
                        "slug": "Tal-Linzen",
                        "structuredName": {
                            "firstName": "Tal",
                            "lastName": "Linzen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tal Linzen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2202008"
                        ],
                        "name": "Emmanuel Dupoux",
                        "slug": "Emmanuel-Dupoux",
                        "structuredName": {
                            "firstName": "Emmanuel",
                            "lastName": "Dupoux",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Emmanuel Dupoux"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "79775260"
                        ],
                        "name": "Yoav Goldberg",
                        "slug": "Yoav-Goldberg",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Goldberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoav Goldberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 104
                            }
                        ],
                        "text": "This contributes to a growing effort to test ANNs\u2019 ability to make finegrained grammatical distinctions (Linzen et al., 2016; Adi et al., 2017; Conneau et al., 2018; Ettinger et al., 2018; Marvin and Linzen, 2018)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 116
                            }
                        ],
                        "text": "This approach is also taken in earlier computational work on this task (Lawrence et al., 2000; Wagner et al., 2009; Linzen et al., 2016)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 59
                            }
                        ],
                        "text": "It is also increasingly used in computational linguistics (Linzen et al., 2016; Marvin and Linzen, 2018; Futrell et al., 2018; Wilcox et al., 2018, 2019)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 134
                            }
                        ],
                        "text": "The poor performance of our models on contrasts involving agreement (Singular/Pl and Reflexive) is surprising in light of findings by (Linzen et al., 2016) that LSTMs can identify agreement errors easily even without access to sub-word information."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 47
                            }
                        ],
                        "text": "These exact experiments have been conducted by Linzen et al. (2016) and Marvin and Linzen (2018), re-\nspectively, although these works differ from our approach in that they do not evaluate domain general acceptability classifiers on these contrasts."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 26
                            }
                        ],
                        "text": "An alternative adopted by Linzen et al. (2016) and Marvin and Linzen (2018) is to evaluate whether language models\u2019 assign higher probability to the acceptable sentence in a minimal pair."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 135
                            }
                        ],
                        "text": "The poor performance of our models on contrasts involving agreement (Singular/Pl and Reflexive) is surprising in light of findings by (Linzen et al., 2016) that LSTMs can identify agreement\nerrors easily even without access to sub-word information."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 14091946,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3aa52436575cf6768a0a1a476601825f6a62e58f",
            "isKey": true,
            "numCitedBy": 634,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "The success of long short-term memory (LSTM) neural networks in language processing is typically attributed to their ability to capture long-distance statistical regularities. Linguistic regularities are often sensitive to syntactic structure; can such dependencies be captured by LSTMs, which do not have explicit structural representations? We begin addressing this question using number agreement in English subject-verb dependencies. We probe the architecture\u2019s grammatical competence both using training objectives with an explicit grammatical target (number prediction, grammaticality judgments) and using language models. In the strongly supervised settings, the LSTM achieved very high overall accuracy (less than 1% errors), but errors increased when sequential and structural information conflicted. The frequency of such errors rose sharply in the language-modeling setting. We conclude that LSTMs can capture a non-trivial amount of grammatical structure given targeted supervision, but stronger architectures may be required to further reduce errors; furthermore, the language modeling signal is insufficient for capturing syntax-sensitive dependencies, and should be supplemented with more direct supervision if such dependencies need to be captured."
            },
            "slug": "Assessing-the-Ability-of-LSTMs-to-Learn-Linzen-Dupoux",
            "title": {
                "fragments": [],
                "text": "Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is concluded that LSTMs can capture a non-trivial amount of grammatical structure given targeted supervision, but stronger architectures may be required to further reduce errors; furthermore, the language modeling signal is insufficient for capturing syntax-sensitive dependencies, and should be supplemented with more direct supervision if such dependencies need to be captured."
            },
            "venue": {
                "fragments": [],
                "text": "TACL"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2412497"
                        ],
                        "name": "Kyle Mahowald",
                        "slug": "Kyle-Mahowald",
                        "structuredName": {
                            "firstName": "Kyle",
                            "lastName": "Mahowald",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kyle Mahowald"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1480965881"
                        ],
                        "name": "Kyle Peter Jeremy Edward Graff",
                        "slug": "Kyle-Peter-Jeremy-Edward-Graff",
                        "structuredName": {
                            "firstName": "Kyle",
                            "lastName": "Graff",
                            "middleNames": [
                                "Peter",
                                "Jeremy",
                                "Edward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kyle Peter Jeremy Edward Graff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39170957"
                        ],
                        "name": "Kyle J. Hartman",
                        "slug": "Kyle-J.-Hartman",
                        "structuredName": {
                            "firstName": "Kyle",
                            "lastName": "Hartman",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kyle J. Hartman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052399928"
                        ],
                        "name": "Kyle Peter Jeremy Edward Gibson",
                        "slug": "Kyle-Peter-Jeremy-Edward-Gibson",
                        "structuredName": {
                            "firstName": "Kyle",
                            "lastName": "Gibson",
                            "middleNames": [
                                "Peter",
                                "Jeremy",
                                "Edward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kyle Peter Jeremy Edward Gibson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 152228641,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "c3411a5e30c14e60e82c8c56de7bd733eb1c0e4f",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract:While published linguistic judgments sometimes differ from the judgments found in large-scale formal experiments with naive participants, there is not a consensus as to how often these errors occur nor as to how often formal experiments should be used in syntax and semantics research. In this article, we first present the results of a large-scale replication of the Sprouse et al. 2013 study on 100 English contrasts randomly sampled from Linguistic Inquiry 2001\u20132010 and tested in both a forced-choice experiment and an acceptability rating experiment. Like Sprouse, Sch\u00fctze, and Almeida, we find that the effect sizes of published linguistic acceptability judgments are not uniformly large or consistent but rather form a continuum from very large effects to small or nonexistent effects. We then use this data as a prior in a Bayesian framework to propose a small n acceptability paradigm for linguistic acceptability judgments (SNAP Judgments). This proposal makes it easier and cheaper to obtain meaningful quantitative data in syntax and semantics research. Specifically, for a contrast of linguistic interest for which a researcher is confident that sentence A is better than sentence B, we recommend that the researcher should obtain judgments from at least five unique participants, using at least five unique sentences of each type. If all participants in the sample agree that sentence A is better than sentence B, then the researcher can be confident that the result of a full forced-choice experiment would likely be 75% or more agreement in favor of sentence A (with a mean of 93%). We test this proposal by sampling from the existing data and find that it gives reliable performance."
            },
            "slug": "SNAP-judgments:-A-small-N-acceptability-paradigm-Mahowald-Graff",
            "title": {
                "fragments": [],
                "text": "SNAP judgments: A small N acceptability paradigm (SNAP) for linguistic acceptability judgments"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2016
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 26
                            }
                        ],
                        "text": "This practice aligns with Chomsky\u2019s (1957) definition of grammaticality as the binary notion of membership in a set of wellformed strings. However, Lau et al. (2016) find that when speakers are presented with the option to use a gradient scale to report sentence acceptability, they predictably and systematically use the full scale, rather than clustering their judgments near the extremes as would be expected for a fundamentally binary phenomenon."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 0
                            }
                        ],
                        "text": "Chomsky\u2019s (1957) methodology merely allows linguists to claim that their theories are empirically adequate13."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 270,
                                "start": 240
                            }
                        ],
                        "text": "Acceptability judgments like these are the primary source of empirical data in much of theoretical linguistics, with the objective of a generative grammar being to generate all and only those sentences which native speakers find acceptable (Chomsky, 1957; Sch\u00fctze, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 107
                            }
                        ],
                        "text": "This has been the predominant methodology for research in generative linguistics over the last sixty years (Chomsky, 1957; Sch\u00fctze, 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 26
                            }
                        ],
                        "text": "This practice aligns with Chomsky\u2019s (1957) definition of grammaticality as the binary notion of membership in a set of wellformed strings."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 75
                            }
                        ],
                        "text": "This definition also includes generative grammars of the type described by Chomsky (1957) above."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 84
                            }
                        ],
                        "text": "Acceptability judgments are central to the formulation of generative linguistics in Chomsky\u2019s influential (1957) book Syntactic Structures."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Syntactic Structures"
            },
            "venue": {
                "fragments": [],
                "text": "Mouton."
            },
            "year": 1957
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2727584"
                        ],
                        "name": "Yossi Adi",
                        "slug": "Yossi-Adi",
                        "structuredName": {
                            "firstName": "Yossi",
                            "lastName": "Adi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yossi Adi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2098460"
                        ],
                        "name": "Einat Kermany",
                        "slug": "Einat-Kermany",
                        "structuredName": {
                            "firstName": "Einat",
                            "lastName": "Kermany",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Einat Kermany"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2083259"
                        ],
                        "name": "Yonatan Belinkov",
                        "slug": "Yonatan-Belinkov",
                        "structuredName": {
                            "firstName": "Yonatan",
                            "lastName": "Belinkov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yonatan Belinkov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3120346"
                        ],
                        "name": "Ofer Lavi",
                        "slug": "Ofer-Lavi",
                        "structuredName": {
                            "firstName": "Ofer",
                            "lastName": "Lavi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ofer Lavi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2089067"
                        ],
                        "name": "Yoav Goldberg",
                        "slug": "Yoav-Goldberg",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Goldberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoav Goldberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 125
                            }
                        ],
                        "text": "This contributes to a growing effort to test ANNs\u2019 ability to make finegrained grammatical distinctions (Linzen et al., 2016; Adi et al., 2017; Conneau et al., 2018; Ettinger et al., 2018; Marvin and Linzen, 2018)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 105
                            }
                        ],
                        "text": "This contributes to a growing effort to test ANNs\u2019 ability to make fine-grained grammatical distinctions (Linzen et al., 2016; Adi et al., 2017; Conneau et al., 2018; Ettinger et al., 2018; Marvin and Linzen, 2018)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6771196,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e44da7d8c71edcc6e575fa7faadd5e75785a7901",
            "isKey": false,
            "numCitedBy": 381,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": "There is a lot of research interest in encoding variable length sentences into fixed length vectors, in a way that preserves the sentence meanings. Two common methods include representations based on averaging word vectors, and representations based on the hidden states of recurrent neural networks such as LSTMs. The sentence vectors are used as features for subsequent machine learning tasks or for pre-training in the context of deep learning. However, not much is known about the properties that are encoded in these sentence representations and about the language information they capture. We propose a framework that facilitates better understanding of the encoded representations. We define prediction tasks around isolated aspects of sentence structure (namely sentence length, word content, and word order), and score representations by the ability to train a classifier to solve each prediction task when using the representation as input. We demonstrate the potential contribution of the approach by analyzing different sentence representation mechanisms. The analysis sheds light on the relative strengths of different sentence embedding methods with respect to these low level prediction tasks, and on the effect of the encoded vector's dimensionality on the resulting representations."
            },
            "slug": "Fine-grained-Analysis-of-Sentence-Embeddings-Using-Adi-Kermany",
            "title": {
                "fragments": [],
                "text": "Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This work proposes a framework that facilitates better understanding of the encoded representations of sentence vectors and demonstrates the potential contribution of the approach by analyzing different sentence representation mechanisms."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3450996"
                        ],
                        "name": "Ryan Kiros",
                        "slug": "Ryan-Kiros",
                        "structuredName": {
                            "firstName": "Ryan",
                            "lastName": "Kiros",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ryan Kiros"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1844940337"
                        ],
                        "name": "Yukun Zhu",
                        "slug": "Yukun-Zhu",
                        "structuredName": {
                            "firstName": "Yukun",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yukun Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804104"
                        ],
                        "name": "R. Zemel",
                        "slug": "R.-Zemel",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Zemel",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Zemel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2422559"
                        ],
                        "name": "R. Urtasun",
                        "slug": "R.-Urtasun",
                        "structuredName": {
                            "firstName": "Raquel",
                            "lastName": "Urtasun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Urtasun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37895334"
                        ],
                        "name": "S. Fidler",
                        "slug": "S.-Fidler",
                        "structuredName": {
                            "firstName": "Sanja",
                            "lastName": "Fidler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Fidler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 105
                            }
                        ],
                        "text": "In similar low-resource settings, transfer learning with sentence embeddings has proven to be effective (Kiros et al., 2015; Conneau et al., 2017)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9126867,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6e795c6e9916174ae12349f5dc3f516570c17ce8",
            "isKey": false,
            "numCitedBy": 1928,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe an approach for unsupervised learning of a generic, distributed sentence encoder. Using the continuity of text from books, we train an encoder-decoder model that tries to reconstruct the surrounding sentences of an encoded passage. Sentences that share semantic and syntactic properties are thus mapped to similar vector representations. We next introduce a simple vocabulary expansion method to encode words that were not seen as part of training, allowing us to expand our vocabulary to a million words. After training our model, we extract and evaluate our vectors with linear models on 8 tasks: semantic relatedness, paraphrase detection, image-sentence ranking, question-type classification and 4 benchmark sentiment and subjectivity datasets. The end result is an off-the-shelf encoder that can produce highly generic sentence representations that are robust and perform well in practice."
            },
            "slug": "Skip-Thought-Vectors-Kiros-Zhu",
            "title": {
                "fragments": [],
                "text": "Skip-Thought Vectors"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "The approach for unsupervised learning of a generic, distributed sentence encoder is described, using the continuity of text from books to train an encoder-decoder model that tries to reconstruct the surrounding sentences of an encoded passage."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39139825"
                        ],
                        "name": "Matthew E. Peters",
                        "slug": "Matthew-E.-Peters",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Peters",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew E. Peters"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50043859"
                        ],
                        "name": "Mark Neumann",
                        "slug": "Mark-Neumann",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Neumann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Neumann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2136562"
                        ],
                        "name": "Mohit Iyyer",
                        "slug": "Mohit-Iyyer",
                        "structuredName": {
                            "firstName": "Mohit",
                            "lastName": "Iyyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mohit Iyyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40642935"
                        ],
                        "name": "Matt Gardner",
                        "slug": "Matt-Gardner",
                        "structuredName": {
                            "firstName": "Matt",
                            "lastName": "Gardner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matt Gardner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143997772"
                        ],
                        "name": "Christopher Clark",
                        "slug": "Christopher-Clark",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Clark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher Clark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2544107"
                        ],
                        "name": "Kenton Lee",
                        "slug": "Kenton-Lee",
                        "structuredName": {
                            "firstName": "Kenton",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenton Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1982950"
                        ],
                        "name": "Luke Zettlemoyer",
                        "slug": "Luke-Zettlemoyer",
                        "structuredName": {
                            "firstName": "Luke",
                            "lastName": "Zettlemoyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luke Zettlemoyer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 61
                            }
                        ],
                        "text": "It also uses contexualized word embeddings inspired by ELMo (Peters et al., 2018)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "(ii) We train ELMo-style contextualized word embeddings, which, following ELMo (Peters et al., 2018), represent wi as a linear combination of the hidden states hji for each layer j in an LSTM LM, though we depart from the original paper by using only a forward LM."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 80
                            }
                        ],
                        "text": "(ii) We train ELMo-style contextualized word embeddings, which, following ELMo (Peters et al., 2018), represent wi as a linear combination of the hidden states hji for each layer j in an LSTM LM, though we depart from the original paper by using only a forward LM. (iii) We also use the pretrained\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 44
                            }
                        ],
                        "text": "This finding aligns with the conclusions of Peters et al. (2018)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 3626819,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3febb2bed8865945e7fddc99efd791887bb7e14f",
            "isKey": true,
            "numCitedBy": 7987,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals."
            },
            "slug": "Deep-Contextualized-Word-Representations-Peters-Neumann",
            "title": {
                "fragments": [],
                "text": "Deep Contextualized Word Representations"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A new type of deep contextualized word representation is introduced that models both complex characteristics of word use and how these uses vary across linguistic contexts, allowing downstream models to mix different types of semi-supervision signals."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51445267"
                        ],
                        "name": "Ethan Gotlieb Wilcox",
                        "slug": "Ethan-Gotlieb-Wilcox",
                        "structuredName": {
                            "firstName": "Ethan",
                            "lastName": "Gotlieb Wilcox",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ethan Gotlieb Wilcox"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143643017"
                        ],
                        "name": "R. Levy",
                        "slug": "R.-Levy",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Levy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Levy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46710186"
                        ],
                        "name": "T. Morita",
                        "slug": "T.-Morita",
                        "structuredName": {
                            "firstName": "Takashi",
                            "lastName": "Morita",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Morita"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2585394"
                        ],
                        "name": "Richard Futrell",
                        "slug": "Richard-Futrell",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Futrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Richard Futrell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 52156878,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "461297186f25a51d96b52b4f5f04b03e0bed476e",
            "isKey": false,
            "numCitedBy": 98,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "RNN language models have achieved state-of-the-art perplexity results and have proven useful in a suite of NLP tasks, but it is as yet unclear what syntactic generalizations they learn. Here we investigate whether state-of-the-art RNN language models represent long-distance filler\u2013gap dependencies and constraints on them. Examining RNN behavior on experimentally controlled sentences designed to expose filler\u2013gap dependencies, we show that RNNs can represent the relationship in multiple syntactic positions and over large spans of text. Furthermore, we show that RNNs learn a subset of the known restrictions on filler\u2013gap dependencies, known as island constraints: RNNs show evidence for wh-islands, adjunct islands, and complex NP islands. These studies demonstrates that state-of-the-art RNN models are able to learn and generalize about empty syntactic positions."
            },
            "slug": "What-do-RNN-Language-Models-Learn-about-Filler\u2013Gap-Wilcox-Levy",
            "title": {
                "fragments": [],
                "text": "What do RNN Language Models Learn about Filler\u2013Gap Dependencies?"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "These studies demonstrates that state-of-the-art RNN models are able to learn and generalize about empty syntactic positions, and shows that RNNs show evidence for wh-islands, adjunct islands, and complex NP islands."
            },
            "venue": {
                "fragments": [],
                "text": "BlackboxNLP@EMNLP"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37907837"
                        ],
                        "name": "Allyson Ettinger",
                        "slug": "Allyson-Ettinger",
                        "structuredName": {
                            "firstName": "Allyson",
                            "lastName": "Ettinger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Allyson Ettinger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143718836"
                        ],
                        "name": "Ahmed Elgohary",
                        "slug": "Ahmed-Elgohary",
                        "structuredName": {
                            "firstName": "Ahmed",
                            "lastName": "Elgohary",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ahmed Elgohary"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143843506"
                        ],
                        "name": "C. Phillips",
                        "slug": "C.-Phillips",
                        "structuredName": {
                            "firstName": "Colin",
                            "lastName": "Phillips",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Phillips"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680292"
                        ],
                        "name": "P. Resnik",
                        "slug": "P.-Resnik",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Resnik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Resnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 255,
                                "start": 234
                            }
                        ],
                        "text": "Examples of such tasks are to determine whether the sentence is in active or passive voice (Shi et al., 2016), whether the subject is singular or plural (Conneau et al., 2018), or whether a given token is under the scope of negation (Ettinger et al., 2018)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Using data generation techniques inspired by Ettinger et al. (2016), we build five auxiliary data sets (described below) using simple rewrite grammars which target specific grammatical contrasts."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": ", 2018), or whether a given token is under the scope of negation (Ettinger et al., 2018)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 186,
                                "start": 165
                            }
                        ],
                        "text": "This contributes to a growing effort to test ANNs\u2019 ability to make finegrained grammatical distinctions (Linzen et al., 2016; Adi et al., 2017; Conneau et al., 2018; Ettinger et al., 2018; Marvin and Linzen, 2018)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This contributes to a growing effort to test ANNs\u2019 ability to make fine-grained grammatical distinctions (Linzen et al., 2016; Adi et al., 2017; Conneau et al., 2018; Ettinger et al., 2018; Marvin and Linzen, 2018)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 49363457,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "843c6b0a35b02e2c3d74bb545e74bc655e16e992",
            "isKey": true,
            "numCitedBy": 61,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "An important component of achieving language understanding is mastering the composition of sentence meaning, but an immediate challenge to solving this problem is the opacity of sentence vector representations produced by current neural sentence composition models. We present a method to address this challenge, developing tasks that directly target compositional meaning information in sentence vector representations with a high degree of precision and control. To enable the creation of these controlled tasks, we introduce a specialized sentence generation system that produces large, annotated sentence sets meeting specified syntactic, semantic and lexical constraints. We describe the details of the method and generation system, and then present results of experiments applying our method to probe for compositional information in embeddings from a number of existing sentence composition models. We find that the method is able to extract useful information about the differing capacities of these models, and we discuss the implications of our results with respect to these systems\u2019 capturing of sentence information. We make available for public use the datasets used for these experiments, as well as the generation system."
            },
            "slug": "Assessing-Composition-in-Sentence-Vector-Ettinger-Elgohary",
            "title": {
                "fragments": [],
                "text": "Assessing Composition in Sentence Vector Representations"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work introduces a specialized sentence generation system that produces large, annotated sentence sets meeting specified syntactic, semantic and lexical constraints and finds that the method is able to extract useful information about the differing capacities of these models."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144748776"
                        ],
                        "name": "E. Gibson",
                        "slug": "E.-Gibson",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Gibson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Gibson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144733430"
                        ],
                        "name": "Evelina Fedorenko",
                        "slug": "Evelina-Fedorenko",
                        "structuredName": {
                            "firstName": "Evelina",
                            "lastName": "Fedorenko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Evelina Fedorenko"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 42721575,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "0727f17053cee3d8e4c9ab3e1f2af8c3b74a8cbf",
            "isKey": false,
            "numCitedBy": 106,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Weak-quantitative-standards-in-linguistics-research-Gibson-Fedorenko",
            "title": {
                "fragments": [],
                "text": "Weak quantitative standards in linguistics research"
            },
            "venue": {
                "fragments": [],
                "text": "Trends in Cognitive Sciences"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144906624"
                        ],
                        "name": "Alex Wang",
                        "slug": "Alex-Wang",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50286460"
                        ],
                        "name": "Amanpreet Singh",
                        "slug": "Amanpreet-Singh",
                        "structuredName": {
                            "firstName": "Amanpreet",
                            "lastName": "Singh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Amanpreet Singh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38614754"
                        ],
                        "name": "Julian Michael",
                        "slug": "Julian-Michael",
                        "structuredName": {
                            "firstName": "Julian",
                            "lastName": "Michael",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Julian Michael"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145783676"
                        ],
                        "name": "Felix Hill",
                        "slug": "Felix-Hill",
                        "structuredName": {
                            "firstName": "Felix",
                            "lastName": "Hill",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Felix Hill"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39455775"
                        ],
                        "name": "Omer Levy",
                        "slug": "Omer-Levy",
                        "structuredName": {
                            "firstName": "Omer",
                            "lastName": "Levy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Omer Levy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3644767"
                        ],
                        "name": "Samuel R. Bowman",
                        "slug": "Samuel-R.-Bowman",
                        "structuredName": {
                            "firstName": "Samuel",
                            "lastName": "Bowman",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Samuel R. Bowman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5034059,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "93b8da28d006415866bf48f9a6e06b5242129195",
            "isKey": false,
            "numCitedBy": 2635,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "Human ability to understand language is general, flexible, and robust. In contrast, most NLU models above the word level are designed for a specific task and struggle with out-of-domain data. If we aspire to develop models with understanding beyond the detection of superficial correspondences between inputs and outputs, then it is critical to develop a unified model that can execute a range of linguistic tasks across different domains. To facilitate research in this direction, we present the General Language Understanding Evaluation (GLUE, gluebenchmark.com): a benchmark of nine diverse NLU tasks, an auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models. For some benchmark tasks, training data is plentiful, but for others it is limited or does not match the genre of the test set. GLUE thus favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks. While none of the datasets in GLUE were created from scratch for the benchmark, four of them feature privately-held test data, which is used to ensure that the benchmark is used fairly. We evaluate baselines that use ELMo (Peters et al., 2018), a powerful transfer learning technique, as well as state-of-the-art sentence representation models. The best models still achieve fairly low absolute scores. Analysis with our diagnostic dataset yields similarly weak performance over all phenomena tested, with some exceptions."
            },
            "slug": "GLUE:-A-Multi-Task-Benchmark-and-Analysis-Platform-Wang-Singh",
            "title": {
                "fragments": [],
                "text": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A benchmark of nine diverse NLU tasks, an auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models, which favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks."
            },
            "venue": {
                "fragments": [],
                "text": "BlackboxNLP@EMNLP"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3086368"
                        ],
                        "name": "R. Berwick",
                        "slug": "R.-Berwick",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Berwick",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Berwick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3089790"
                        ],
                        "name": "P. Pietroski",
                        "slug": "P.-Pietroski",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Pietroski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Pietroski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3065909"
                        ],
                        "name": "Beracah Yankama",
                        "slug": "Beracah-Yankama",
                        "structuredName": {
                            "firstName": "Beracah",
                            "lastName": "Yankama",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Beracah Yankama"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "114531657"
                        ],
                        "name": "Noam Chomsky",
                        "slug": "Noam-Chomsky",
                        "structuredName": {
                            "firstName": "Noam",
                            "lastName": "Chomsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noam Chomsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2799578,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "ed48fcf08cbfd340568a50302223abf3a2ebf2c6",
            "isKey": false,
            "numCitedBy": 248,
            "numCiting": 92,
            "paperAbstract": {
                "fragments": [],
                "text": "A central goal of modern generative grammar has been to discover invariant properties of human languages that reflect \"the innate schematism of mind that is applied to the data of experience\" and that \"might reasonably be attributed to the organism itself as its contribution to the task of the acquisition of knowledge\" (Chomsky, 1971). Candidates for such invariances include the structure dependence of grammatical rules, and in particular, certain constraints on question formation. Various \"poverty of stimulus\" (POS) arguments suggest that these invariances reflect an innate human endowment, as opposed to common experience: Such experience warrants selection of the grammars acquired only if humans assume, a priori, that selectable grammars respect substantive constraints. Recently, several researchers have tried to rebut these POS arguments. In response, we illustrate why POS arguments remain an important source of support for appeal to a priori structure-dependent constraints on the grammars that humans naturally acquire."
            },
            "slug": "Poverty-of-the-Stimulus-Revisited-Berwick-Pietroski",
            "title": {
                "fragments": [],
                "text": "Poverty of the Stimulus Revisited"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "It is illustrated why POS arguments remain an important source of support for appeal to a priori structure-dependent constraints on the grammars that humans naturally acquire."
            },
            "venue": {
                "fragments": [],
                "text": "Cogn. Sci."
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2770201"
                        ],
                        "name": "G. Pullum",
                        "slug": "G.-Pullum",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Pullum",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Pullum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2917207"
                        ],
                        "name": "Barbara C. Scholz",
                        "slug": "Barbara-C.-Scholz",
                        "structuredName": {
                            "firstName": "Barbara",
                            "lastName": "Scholz",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Barbara C. Scholz"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 50
                            }
                        ],
                        "text": "While the APS has been subject to much criticism (Pullum and Scholz, 2002), it remains a foundation of much of contemporary linguistics."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 143735248,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "9dab34fd55d4df75e17a07e60bfd4820a1c63273",
            "isKey": false,
            "numCitedBy": 513,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract This article examines a type of argument for linguistic nativism that takes the following form: (i) a fact about some natural language is exhibited that allegedly could not be learned from experience without access to a certain kind of (positive) data; (ii) it is claimed that data of the type in question are not found in normal linguistic experience; hence (iii) it is concluded that people cannot be learning the language from mere exposure to language use. We analyze the components of this sort of argument carefully, and examine four exemplars, none of which hold up. We conclude that linguists have some additional work to do if they wish to sustain their claims about having provided support for linguistic nativism, and we offer some reasons for thinking that the relevant kind of future work on this issue is likely to further undermine the linguistic nativist position."
            },
            "slug": "Empirical-assessment-of-stimulus-poverty-arguments-Pullum-Scholz",
            "title": {
                "fragments": [],
                "text": "Empirical assessment of stimulus poverty arguments"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 26
                            }
                        ],
                        "text": "This practice aligns with Chomsky\u2019s (1957) definition of grammaticality as the binary notion of membership in a set of wellformed strings. However, Lau et al. (2016) find that when speakers are presented with the option to use a gradient scale to report sentence acceptability, they predictably and systematically use the full scale, rather than clustering their judgments near the extremes as would be expected for a fundamentally binary phenomenon."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 0
                            }
                        ],
                        "text": "Chomsky\u2019s (1957) methodology merely allows linguists to claim that their theories are empirically adequate13."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 270,
                                "start": 240
                            }
                        ],
                        "text": "Acceptability judgments like these are the primary source of empirical data in much of theoretical linguistics, with the objective of a generative grammar being to generate all and only those sentences which native speakers find acceptable (Chomsky, 1957; Sch\u00fctze, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 107
                            }
                        ],
                        "text": "This has been the predominant methodology for research in generative linguistics over the last sixty years (Chomsky, 1957; Sch\u00fctze, 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 26
                            }
                        ],
                        "text": "This practice aligns with Chomsky\u2019s (1957) definition of grammaticality as the binary notion of membership in a set of wellformed strings."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 75
                            }
                        ],
                        "text": "This definition also includes generative grammars of the type described by Chomsky (1957) above."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 84
                            }
                        ],
                        "text": "Acceptability judgments are central to the formulation of generative linguistics in Chomsky\u2019s influential (1957) book Syntactic Structures."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Syntactic Structures"
            },
            "venue": {
                "fragments": [],
                "text": "Mouton."
            },
            "year": 1957
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47087284"
                        ],
                        "name": "Jon Sprouse",
                        "slug": "Jon-Sprouse",
                        "structuredName": {
                            "firstName": "Jon",
                            "lastName": "Sprouse",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jon Sprouse"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144498637"
                        ],
                        "name": "Diogo Almeida",
                        "slug": "Diogo-Almeida",
                        "structuredName": {
                            "firstName": "Diogo",
                            "lastName": "Almeida",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Diogo Almeida"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 145129668,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "ad6d55453d81350c0fe419e4f5b94d3a0c86d313",
            "isKey": false,
            "numCitedBy": 136,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "There has been a consistent pattern of criticism of the reliability of acceptability judgment data in syntax for at least 50 years (e.g., Hill 1961), culminating in several high-profile criticisms within the past ten years (Edelman & Christiansen 2003, Ferreira 2005, Wasow & Arnold 2005, Gibson & Fedorenko 2010, in press). The fundamental claim of these critics is that traditional acceptability judgment collection methods, which tend to be relatively informal compared to methods from experimental psychology, lead to an intolerably high number of false positive results. In this paper we empirically assess this claim by formally testing all 469 (unique, US-English) data points from a popular syntax textbook (Adger 2003) using 440 na\u00efve participants, two judgment tasks (magnitude estimation and yes\u2013no), and three different types of statistical analyses (standard frequentist tests, linear mixed effects models, and Bayes factor analyses). The results suggest that the maximum discrepancy between traditional methods and formal experimental methods is 2%. This suggests that even under the (likely unwarranted) assumption that the discrepant results are all false positives that have found their way into the syntactic literature due to the shortcomings of traditional methods, the minimum replication rate of these 469 data points is 98%. We discuss the implications of these results for questions about the reliability of syntactic data, as well as the practical consequences of these results for the methodological options available to syntacticians."
            },
            "slug": "Assessing-the-reliability-of-textbook-data-in-Core-Sprouse-Almeida",
            "title": {
                "fragments": [],
                "text": "Assessing the reliability of textbook data in syntax: Adger's Core Syntax1"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Linguistics"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2467508"
                        ],
                        "name": "Tal Linzen",
                        "slug": "Tal-Linzen",
                        "structuredName": {
                            "firstName": "Tal",
                            "lastName": "Linzen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tal Linzen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50856622"
                        ],
                        "name": "Yohei Oseki",
                        "slug": "Yohei-Oseki",
                        "structuredName": {
                            "firstName": "Yohei",
                            "lastName": "Oseki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yohei Oseki"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 49309061,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "77cef64bc2e3395ae6f8a5fbc0caa5670f8fabb3",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 88,
            "paperAbstract": {
                "fragments": [],
                "text": "The reliability of acceptability judgments made by individual linguists has often been called into question. Recent large-scale replication studies conducted in response to this criticism have shown that the majority of published English acceptability judgments are robust. We make two observations about these replication studies. First, we raise the concern that English acceptability judgments may be more reliable than judgments in other languages. Second, we argue that it is unnecessary to replicate judgments that illustrate uncontroversial descriptive facts; rather, candidates for replication can emerge during formal or informal peer review. We present two experiments motivated by these arguments. Published Hebrew and Japanese acceptability contrasts considered questionable by the authors of the present paper were rated for acceptability by a large sample of naive participants. Approximately half of the contrasts did not replicate. We suggest that the reliability of acceptability judgments, especially in languages other than English, can be improved using a simple open review system, and that formal experiments are only necessary in controversial cases."
            },
            "slug": "The-reliability-of-acceptability-judgments-across-Linzen-Oseki",
            "title": {
                "fragments": [],
                "text": "The reliability of acceptability judgments across languages"
            },
            "venue": {
                "fragments": [],
                "text": "Glossa: a journal of general linguistics"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1979489"
                        ],
                        "name": "Kyunghyun Cho",
                        "slug": "Kyunghyun-Cho",
                        "structuredName": {
                            "firstName": "Kyunghyun",
                            "lastName": "Cho",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kyunghyun Cho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3158246"
                        ],
                        "name": "Bart van Merrienboer",
                        "slug": "Bart-van-Merrienboer",
                        "structuredName": {
                            "firstName": "Bart",
                            "lastName": "Merrienboer",
                            "middleNames": [
                                "van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bart van Merrienboer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1854385"
                        ],
                        "name": "\u00c7aglar G\u00fcl\u00e7ehre",
                        "slug": "\u00c7aglar-G\u00fcl\u00e7ehre",
                        "structuredName": {
                            "firstName": "\u00c7aglar",
                            "lastName": "G\u00fcl\u00e7ehre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u00c7aglar G\u00fcl\u00e7ehre"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3335364"
                        ],
                        "name": "Dzmitry Bahdanau",
                        "slug": "Dzmitry-Bahdanau",
                        "structuredName": {
                            "firstName": "Dzmitry",
                            "lastName": "Bahdanau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dzmitry Bahdanau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2076086"
                        ],
                        "name": "Fethi Bougares",
                        "slug": "Fethi-Bougares",
                        "structuredName": {
                            "firstName": "Fethi",
                            "lastName": "Bougares",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fethi Bougares"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144518416"
                        ],
                        "name": "Holger Schwenk",
                        "slug": "Holger-Schwenk",
                        "structuredName": {
                            "firstName": "Holger",
                            "lastName": "Schwenk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Holger Schwenk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 97
                            }
                        ],
                        "text": "These models are widely used to encode features of sentences in fixed-length sentence embeddings (Cho et al., 2014; Sutskever et al., 2014; Kiros et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5590763,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0b544dfe355a5070b60986319a3f51fb45d1348e",
            "isKey": false,
            "numCitedBy": 15051,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose a novel neural network model called RNN Encoder\u2010 Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder\u2010Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases."
            },
            "slug": "Learning-Phrase-Representations-using-RNN-for-Cho-Merrienboer",
            "title": {
                "fragments": [],
                "text": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Qualitatively, the proposed RNN Encoder\u2010Decoder model learns a semantically and syntactically meaningful representation of linguistic phrases."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701686"
                        ],
                        "name": "Ilya Sutskever",
                        "slug": "Ilya-Sutskever",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Sutskever",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilya Sutskever"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689108"
                        ],
                        "name": "Oriol Vinyals",
                        "slug": "Oriol-Vinyals",
                        "structuredName": {
                            "firstName": "Oriol",
                            "lastName": "Vinyals",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oriol Vinyals"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2827616"
                        ],
                        "name": "Quoc V. Le",
                        "slug": "Quoc-V.-Le",
                        "structuredName": {
                            "firstName": "Quoc",
                            "lastName": "Le",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Quoc V. Le"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "997) can discover some forms of structure in raw language data (LeCun et al., 2015). These models are widely used to encode features of sentences in \ufb01xed-length sentence embeddings (Cho et al., 2014; Sutskever et al., 2014; Kiros et al., 2015). Evaluating generalpurpose sentence embeddings is an active research area. Some approaches probe the contents of sentence embeddings using common natural language processing task"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7961699,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cea967b59209c6be22829699f05b8b1ac4dc092d",
            "isKey": false,
            "numCitedBy": 14881,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier."
            },
            "slug": "Sequence-to-Sequence-Learning-with-Neural-Networks-Sutskever-Vinyals",
            "title": {
                "fragments": [],
                "text": "Sequence to Sequence Learning with Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper presents a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure, and finds that reversing the order of the words in all source sentences improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2451423"
                        ],
                        "name": "A. Goldberg",
                        "slug": "A.-Goldberg",
                        "structuredName": {
                            "firstName": "Adele",
                            "lastName": "Goldberg",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Goldberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46418092"
                        ],
                        "name": "R. Jackendoff",
                        "slug": "R.-Jackendoff",
                        "structuredName": {
                            "firstName": "Ray",
                            "lastName": "Jackendoff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Jackendoff"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 16793207,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "dba80dda49ceed4e0c728e42486ae0fb28d6e659",
            "isKey": false,
            "numCitedBy": 518,
            "numCiting": 131,
            "paperAbstract": {
                "fragments": [],
                "text": "English resultative expressions have been a major focus of research on the syntax-semantics interface. We argue in this article that a family of related constructions is required to account for their distribution. We demonstrate that a number of generalizations follow from the semantics of the constructions we posit: the syntactic argument structure of the sentence is predicted by general principles of argument linking; and the aspectual structure of the sentence is determined by the aspectual structure of the constructional subevent, which is in turn predictable from general principles correlating event structure with change, extension, motion, and paths. Finally, the semantics and syntax of resultatives explain the possibilities for temporal relations between the two subevents. While these generalizations clearly exist, there is also a great deal of idiosyncrasy involved in resultatives. Many idiosyncratic instances and small subclasses of the construction must be learned and stored individually. This account serves to justify aspects of what we share in our overall vision of grammar, whatwe might call the CONSTRUCTIONAL view. To the extent that our treatment of the resultative can be stated only within the constructional view, it serves as evidence for this view as a whole."
            },
            "slug": "The-English-Resultative-as-a-Family-of-Goldberg-Jackendoff",
            "title": {
                "fragments": [],
                "text": "The English Resultative as a Family of Constructions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791502537"
                        ],
                        "name": "Jong-Bok Kim",
                        "slug": "Jong-Bok-Kim",
                        "structuredName": {
                            "firstName": "Jong-Bok",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jong-Bok Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2519145"
                        ],
                        "name": "P. Sells",
                        "slug": "P.-Sells",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Sells",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Sells"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": " he reads. Culicover and Jackendoff (1999) 1 I said that my father, he was tight as a hoot-owl. Ross (1967) 1 The jeweller inscribed the ring with the name. Levin (1993) 0 many evidence was provided. Kim and Sells (2008) 1 They can sing. Kim and Sells (2008) 1 The men would have been all working. Baltin (1982) 0 Who do you think that will question Seamus \ufb01rst? Carnie (2013) 0 Usually, any lion is majestic. Dayal (199"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 74
                            }
                        ],
                        "text": "By way of illustration, consider the three largest sources in\nthe corpus: Kim & Sells (2008) is a recent undergraduate syntax textbook, Levin (1993) is a comprehensive reference detailing the lexical properties of thousands of verbs, and Ross (1967) is an influential dissertation focusing on\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "(1999) 233 59.2 Comparatives Dayal (1998) 179 75.4 Modality Gazdar (1981) 110 65.5 Coordination Goldberg and Jackendoff (2004) 106 77.4 Resultative Kadmon and Landman (1993) 93 81.7 Negative Polarity Kim and Sells (2008) 1965 71.2 Syntax Textbook Levin (1993) 1459 69.0 Verb alternations Miller (2002) 426 84.5 Syntax Textbook Rappaport Hovav and Levin (2008) 151 69.5 Dative alternation Ross (1967) 1029 61.8 Islands Sa"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60992997,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "600d22608626a0f185209d164fe380b916fcb71e",
            "isKey": true,
            "numCitedBy": 37,
            "numCiting": 238,
            "paperAbstract": {
                "fragments": [],
                "text": "Focusing on the descriptive facts of English, this volume provides a systematic introduction to English syntax for students with no prior knowledge of English grammar or syntactic analysis. \"English Syntax\" aims to help students appreciate the various sentence patterns available in the language, understand insights into core data of its syntax, develop analytic abilities to further explore the patterns of English, and learn precise ways of formalizing syntactic analysis for a variety of English data and major constructions such as agreement, raising and control, the auxiliary system, passive, wh-questions, relative clauses, extrapolation, and clefts."
            },
            "slug": "English-Syntax:-An-Introduction-Kim-Sells",
            "title": {
                "fragments": [],
                "text": "English Syntax: An Introduction"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "\"English Syntax\" aims to help students appreciate the various sentence patterns available in the language, understand insights into core data of its syntax, develop analytic abilities to further explore the patterns of English, and learn precise ways of formalizing syntactic analysis for a variety of English data and major constructions."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143794227"
                        ],
                        "name": "Xing Shi",
                        "slug": "Xing-Shi",
                        "structuredName": {
                            "firstName": "Xing",
                            "lastName": "Shi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xing Shi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8350409"
                        ],
                        "name": "Inkit Padhi",
                        "slug": "Inkit-Padhi",
                        "structuredName": {
                            "firstName": "Inkit",
                            "lastName": "Padhi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Inkit Padhi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152971314"
                        ],
                        "name": "Kevin Knight",
                        "slug": "Kevin-Knight",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Knight",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin Knight"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Examples of such tasks are to determine whether the sentence is in active or passive voice (Shi et al., 2016), whether the subject is singular or plural (Conneau et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 92
                            }
                        ],
                        "text": "Examples of such tasks are to determine whether the sentence is in active or passive voice (Shi et al., 2016), whether the subject is singular or plural (Conneau et al., 2018), or whether a given token is under the scope of negation (Ettinger et al., 2018)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7197724,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d821ce08da6c0084d5eacbdf65e25556bc1b9bc3",
            "isKey": false,
            "numCitedBy": 270,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate whether a neural, encoderdecoder translation system learns syntactic information on the source side as a by-product of training. We propose two methods to detect whether the encoder has learned local and global source syntax. A fine-grained analysis of the syntactic structure learned by the encoder reveals which kinds of syntax are learned and which are missing."
            },
            "slug": "Does-String-Based-Neural-MT-Learn-Source-Syntax-Shi-Padhi",
            "title": {
                "fragments": [],
                "text": "Does String-Based Neural MT Learn Source Syntax?"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "This work investigates whether a neural, encoderdecoder translation system learns syntactic information on the source side as a by-product of training and proposes two methods to detect whether the encoder has learned local and global source syntax."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3190237"
                        ],
                        "name": "A. Carnie",
                        "slug": "A.-Carnie",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Carnie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Carnie"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "(1993) 0 many evidence was provided. Kim and Sells (2008) 1 They can sing. Kim and Sells (2008) 1 The men would have been all working. Baltin (1982) 0 Who do you think that will question Seamus \ufb01rst? Carnie (2013) 0 Usually, any lion is majestic. Dayal (1998) 1 The gardener planted roses in the garden. Miller (2002) 1 I wrote Blair a letter, but I tore it up before I sent it. Rappaport Hovav and Levin (2008) T"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "*The Bill\u2019s book has a red cover. N % Description Adger (2003) 948 71.9 Syntax Textbook Baltin (1982) 96 66.7 Movement Baltin and Collins (2001) 880 66.7 Handbook Bresnan (1973) 259 69.1 Comparatives Carnie (2013) 870 80.3 Syntax Textbook Culicover and Jackendoff (1999) 233 59.2 Comparatives Dayal (1998) 179 75.4 Modality Gazdar (1981) 110 65.5 Coordination Goldberg and Jackendoff (2004) 106 77.4 Resultative K"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 117514051,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "105a9af643954a65f17333ff5d06732759be0492",
            "isKey": true,
            "numCitedBy": 305,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface and Acknowledgments. Part 1: Preliminaries:. 1. Generative Grammar. 0. Preliminaries. 1. Syntax as a Cognitive Science. 2. Modeling Syntax. 3. Syntax as Science - the Scientific Method. An Example of the Scientific Method as Applied to Syntax. Sources of Data. 4. Where do the Rules Come From?. Learning vs. Acquisition. Innateness: Language as an Instinct. The Logical Problem of Language Acquisition. Other Arguments for UG. Explaining Language Variation. 5. Choosing among Theories about Syntax. 6. The Scientific Method and the Structure of this Textbook. 7 Summary. Ideas Introduced in this Chapter. Further Reading. General Problem Sets. Challenge Problem Sets. 2. Parts of Speech. 0. Words and Why They Matter to Syntax. 1. Determining Part of Speech. The Problem of Traditional Definitions. Distributional Criteria. 2. The Major Parts of Speech: N, V, Adj, and Adv. Nouns. Verbs. Adjectives. Adverbs. 3. Open vs. Closed Lexical vs. Functional. Open vs. Closed Parts of Speech. Lexical vs. Functional. Some Functional (Closed) Categories of English. Summary. 4. Subcategories and Features. Subcategories of Nouns. Subcategories of Verbs. 5. Summary. Ideas Introduced in this Chapter. Further Reading. General Problem Sets. Challenge Problem Sets. 3. Constituency, Trees, and Rules. 0. Introduction. 1. Rules and Trees. Noun Phrases (NPs). Adjective Phrases (AdjPs) and Adverb Phrases (AdvPs). Prepositional Phrases (PPs). Verb Phrases (VPs). Clauses. Summary. 2. How to Draw a Tree. Bottom-up Trees. The Top-down Method of Drawing Trees. Bracketed Diagrams. 3. Modification and Ambiguity. 4. Constituency Tests. 5. Summary and Conclusion. Appendix: How to do Foreign Language PSR Problems. A1. Doing problems with word-by-word glosses. A2. Doing problems without word-by-word glosses. Ideas Introduced in this Chapter. Further Reading. General Problem Sets. Challenge Problem Sets. 4. Structural Relations. 0. Introduction. 1. The Parts of a Tree. 2. Domination. Domination. Exhaustive Domination. Immediate Domination. 3. Precedence. 4. C-command. 5. Grammatical Relations. 6. Summary and Conclusions. Ideas Introduced in this Chapter. Further Reading. General Problem Sets. Challenge Problem Sets. 5. Binding Theory. 0. Introduction. 1. The Notions Coindex and Antecedent. 2. Binding. 3. Locality Conditions on the Binding of Anaphors. 4. The Distribution of Pronouns. 5. The Distribution of R-expressions. 6. Conclusion. Ideas Introduced in this Chapter. Further Reading. General Problem Sets. Challenge Problem Sets. Part 2: The Base:. 6. X-bar Theory. 0. Introduction. 1. Bar-level Projections. V-bar. Adj-bar and Adv-bar. P-bar. 2. Generalizing the Rules: The X-bar Schema. 3. Complements, Adjuncts, and Specifiers. Complements and Adjuncts in NPs. Complements and Adjuncts in VPs, AdjPs, AdvPs, and PPs. The Notion Specifier. 4. Some Definitional Housekeeping. 5. Parameters of Word Order. 6. Drawing Trees in X-bar Notation. Important Considerations in Tree Drawing. A Sample Tree. 7. X-bar Theory: A Summary. Ideas Introduced in this Chapter. Further Reading. General Problem Sets. Challenge Problem Sets. 7. Extending X-bar Theory to Functional Categories. 0. Introduction. 1. Determiner Phrases (DPs). 2. A Descriptive Tangent into Clause Types. 3. Complementizer Phrases (CPs). 4. Tense Phrases (TPs). 5. CP, TP, DP tree. Ideas Introduced in this Chapter. Further Reading. General Problem Sets. Challenge Problem Sets. 8. Constraining X-bar Theory: The Lexicon. 0. Introduction. 1. Some Basic Terminology. 2. Thematic Relations and Theta Roles. 3. The Lexicon. 4. Expletives and the Extended Projection Principle. 5. Summary. Ideas Introduced in this Chapter. Further Reading. General Problem Sets. Challenge Problem Sets. Part 3: Movement:. 9. Head-to-Head Movement. 0. Introduction. 1. Verb Movement (V -> T). French. Irish. 2. T Movement (T -> C). 3. Do-support. 4. Multiple Auxiliaries and Affix-hopping in English. Multiple Auxiliaries. Affix-hopping. 5. Summary. Appendix: Tests for Determining if a Language has V -> T or Affix Lowering. Ideas Introduced in this Chapter. Further Reading. General Problem Sets. Challenge Problem Sets. 10. DP Movement. 0. Introduction. 1. A Puzzle for the Theory of Theta Roles. 2. Passives. 3. Case. 4. Raising: Reprise. 5. Passives: Reprise. 6. Closing Up a Loose End. 7. Conclusion. Ideas Introduced in this Chapter. Further Reading. General Problem Sets. Challenge Problem Sets. 11. Wh-movement. 0. Introduction. 1. Movement in Wh-questions. 2. Islands. 3. The Minimal Link Condition. Wh-islands and the Minimal Link Condition. The MLC in DP Movement and Head Movement. 4. Echo Questions (Wh-in-situ) in English. 5. Conclusion. Ideas Introduced in this Chapter. Further Reading. General Problem Sets. Challenge Problem Sets. 12. A Unified Theory of Movement. 0. Introduction. 1. Move. 2. Explaining Cross-linguistic Differences. 3. Scope, Covert Movement, and The MLC. MLC Effects in Wh-in-situ Languages. English Quantifiers and Scope. 4. Conclusion. Ideas Introduced in this Chapter. Further Reading. General Problem Sets. Challenge Problem Sets. Part 4: Advanced Topics:. 13. Expanded VPs. 0. Introduction. 1. The Problem of Ditransitive Verbs. 2. Light Verbs. 3. Object Shift. 4. Ditransitives: Reprise. Ideas Introduced in this Chapter. Further Reading. General Problem Sets. Challenge Problem Sets. 14. Raising, Control, and Empty Categories. 0. Introduction. 1. Raising vs. Control. Two Kinds of Theta Grids for Main Predicates. Distinguishing Raising from Control. What is PRO?. 2. Two Kinds of Raising, Two Kinds of Control. Two Kinds of Raising. Two Kinds of Control. Summary of Predicate Types. 3. Control Theory. 4. Another Kind of Null Subject: \"Little\" pro. 5. Summary. Ideas Introduced in this Chapter. Further Reading. General Problem Sets. Challenge Problem Sets. 15. Advanced Topics in Binding Theory. 0. A Quick Review of Chapter 5 Binding Theory. 1. Levels of Representation. 2. The Definition of Binding Domain. A Miscellany of Domain Violations. Anaphors. Pronouns. Ideas Introduced in this Chapter. Further Reading. General Problem Sets. Challenge Problem Sets. Part 5: Alternatives:. 16. Lexical-Functional Grammar. 0. Alternative Theories. 1. C-structure. 2. Functions. 3. The Lexicon. 4. F-structure. Why F-structures?. 5. Assorted Phenomena. Head Mobility. Passives. Raising and Control. Wh-movement: Long Distance Dependencies. 6. Conclusion. Ideas Introduced in this Chapter. Further Reading. General Problem Set. Challenge Problem Sets. 17. Head-Driven Phrase Structure Grammar. 0. Introduction. 1. Features. 2. The Lexicon. 3. Rules, Features, and Trees. 4. Binding. 5. Long Distance Dependencies. Ideas Introduced in this Chapter. Further Reading. General Problem Set. Challenge Problem Sets. Conclusions and Directions for Further Study. References. Index."
            },
            "slug": "Syntax:-A-Generative-Introduction-Carnie",
            "title": {
                "fragments": [],
                "text": "Syntax: A Generative Introduction"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701400"
                        ],
                        "name": "G. Gazdar",
                        "slug": "G.-Gazdar",
                        "structuredName": {
                            "firstName": "Gerald",
                            "lastName": "Gazdar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Gazdar"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 123564073,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "006cce6ef401f84f39aeed27d5196acdb990c7da",
            "isKey": false,
            "numCitedBy": 499,
            "numCiting": 100,
            "paperAbstract": {
                "fragments": [],
                "text": "Consider eliminating the transformational component of a generative grammar. In particular, consider the elimination of all movement rules, whether bounded or unbounded, and all rules making reference to identity of indices. Suppose, in fact, that the permitted class of generative grammars constituted a subset of those phrase structure grammars capable only of generating context-free languages. Such a move would have two important metatheoretical consequences, one having to do with learnability, the other with processability. In the first place, we would be imposing a rather dramatic restriction on the class of grammars that the language acquisition device needs to consider as candidates for the language being learned. And in the second place, we would have the beginnings of an explanation for the obvious, but largely ignored, fact that humans process the utterances they hear very rapidly.1 Sentences of a context-free language are provably parsable in a time which is proportional to the cube of the length of the sentence or less (Younger (1967), Earley (1970)). But no such restrictive result holds for the recursive or recursively enumerable sets potentially generable by grammars which include a transformational component."
            },
            "slug": "Unbounded-Dependencies-and-Coordinate-Structure-Gazdar",
            "title": {
                "fragments": [],
                "text": "Unbounded Dependencies and Coordinate Structure"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "Consider eliminating the transformational component of a generative grammar, and the elimination of all movement rules, whether bounded or unbounded, and all rules making reference to identity of indices."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 26
                            }
                        ],
                        "text": "This practice aligns with Chomsky\u2019s (1957) definition of grammaticality as the binary notion of membership in a set of wellformed strings. However, Lau et al. (2016) find that when speakers are presented with the option to use a gradient scale to report sentence acceptability, they predictably and systematically use the full scale, rather than clustering their judgments near the extremes as would be expected for a fundamentally binary phenomenon."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 0
                            }
                        ],
                        "text": "Chomsky\u2019s (1957) methodology merely allows linguists to claim that their theories are empirically adequate13."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 270,
                                "start": 240
                            }
                        ],
                        "text": "Acceptability judgments like these are the primary source of empirical data in much of theoretical linguistics, with the objective of a generative grammar being to generate all and only those sentences which native speakers find acceptable (Chomsky, 1957; Sch\u00fctze, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 107
                            }
                        ],
                        "text": "This has been the predominant methodology for research in generative linguistics over the last sixty years (Chomsky, 1957; Sch\u00fctze, 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 26
                            }
                        ],
                        "text": "This practice aligns with Chomsky\u2019s (1957) definition of grammaticality as the binary notion of membership in a set of wellformed strings."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 75
                            }
                        ],
                        "text": "This definition also includes generative grammars of the type described by Chomsky (1957) above."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 84
                            }
                        ],
                        "text": "Acceptability judgments are central to the formulation of generative linguistics in Chomsky\u2019s influential (1957) book Syntactic Structures."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Syntactic Structures"
            },
            "venue": {
                "fragments": [],
                "text": "Mouton."
            },
            "year": 1957
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "114531657"
                        ],
                        "name": "Noam Chomsky",
                        "slug": "Noam-Chomsky",
                        "structuredName": {
                            "firstName": "Noam",
                            "lastName": "Chomsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noam Chomsky"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62151939,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "e33291ea8fe85f07b6cd722730235487dc98bdf4",
            "isKey": false,
            "numCitedBy": 7147,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "In his foundational book, The Minimalist Program, published in 1995, Noam Chomsky offered a significant contribution to the generative tradition in linguistics. This twentieth-anniversary edition reissues this classic work with a new preface by the author. In four essays, Chomsky attempts to situate linguistic theory in the broader cognitive sciences, with the essays formulating and progressively developing the minimalist approach to linguistic theory. Building on the theory of principles and parameters and, in particular, on principles of economy of derivation and representation, the minimalist framework takes Universal Grammar as providing a unique computational system, with derivations driven by morphological properties, to which the syntactic variation of languages is also restricted. Within this theoretical framework, linguistic expressions are generated by optimally efficient derivations that must satisfy the conditions that hold on interface levels, the only levels of linguistic representation. The interface levels provide instructions to two types of performance systems, articulatory-perceptual and conceptual-intentional. All syntactic conditions, then, express properties of these interface levels, reflecting the interpretive requirements of language and keeping to very restricted conceptual resources. In the preface to this edition, Chomsky emphasizes that the minimalist approach developed in the book and in subsequent work \"is a program, not a theory.\" With this book, Chomsky built on pursuits from the earliest days of generative grammar to formulate a new research program that had far-reaching implications for the field."
            },
            "slug": "The-Minimalist-Program-Chomsky",
            "title": {
                "fragments": [],
                "text": "The Minimalist Program"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145091160"
                        ],
                        "name": "B. Levin",
                        "slug": "B.-Levin",
                        "structuredName": {
                            "firstName": "Beth",
                            "lastName": "Levin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Levin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 128
                            }
                        ],
                        "text": "Used intransitively (6-b), it is the subject (the bubble) that undergoes a change of state and the cause need not be specified (Levin, 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 136
                            }
                        ],
                        "text": "By way of illustration, consider the three largest sources in\nthe corpus: Kim & Sells (2008) is a recent undergraduate syntax textbook, Levin (1993) is a comprehensive reference detailing the lexical properties of thousands of verbs, and Ross (1967) is an influential dissertation focusing on\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "the corpus: Kim & Sells (2008) is a recent undergraduate syntax textbook, Levin (1993) is a comprehensive reference detailing the lexical properties of thousands of verbs, and Ross (1967) is an influential dissertation focusing on wh-movement and extraction islands in English syntax."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 138
                            }
                        ],
                        "text": "This is due both to the high frequency of these constructions as well as the inclusion of several sources directly addressing this topic (Levin, 1993; Collins, 2005; Rappaport Hovav and Levin, 2008)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62585813,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "6cbc1eb25f4ab29a613418b3b0740e74141a0f17",
            "isKey": true,
            "numCitedBy": 3246,
            "numCiting": 246,
            "paperAbstract": {
                "fragments": [],
                "text": "In this rich reference work, Beth Levin classifies over 3,000 English verbs according to shared meaning and behavior. Levin starts with the hypothesis that a verb's meaning influences its syntactic behavior and develops it into a powerful tool for studying the English verb lexicon. She shows how identifying verbs with similar syntactic behavior provides an effective means of distinguishing semantically coherent verb classes, and isolates these classes by examining verb behavior with respect to a wide range of syntactic alternations that reflect verb meaning. The first part of the book sets out alternate ways in which verbs can express their arguments. The second presents classes of verbs that share a kernel of meaning and explores in detail the behavior of each class, drawing on the alternations in the first part. Levin's discussion of each class and alternation includes lists of relevant verbs, illustrative examples, comments on noteworthy properties, and bibliographic references. The result is an original, systematic picture of the organization of the verb inventory. Easy to use, \"English Verb Classes and Alternations\" sets the stage for further explorations of the interface between lexical semantics and syntax. It will prove indispensable for theoretical and computational linguists, psycholinguists, cognitive scientists, lexicographers, and teachers of English as a second language. Beth Levin is associate professor of linguistics at Northwestern University."
            },
            "slug": "English-Verb-Classes-and-Alternations:-A-Levin",
            "title": {
                "fragments": [],
                "text": "English Verb Classes and Alternations: A Preliminary Investigation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Beth Levin shows how identifying verbs with similar syntactic behavior provides an effective means of distinguishing semantically coherent verb classes, and isolates these classes by examining verb behavior with respect to a wide range of syntactic alternations that reflect verb meaning."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143845796"
                        ],
                        "name": "Jeffrey Pennington",
                        "slug": "Jeffrey-Pennington",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Pennington",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeffrey Pennington"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2166511"
                        ],
                        "name": "R. Socher",
                        "slug": "R.-Socher",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Socher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Socher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "each layer j in an LSTM LM, though we depart from the original paper by using only a forward LM. (iii) In a more exploratory experiment, we also use pretrained 300- dimensional (6B) GloVe embeddings (Pennington et al., 2014). Note that these embeddings are trained on orders of magnitude more words than human learners ever see, limiting the possible interpretations of a positive result in this setting. CBOW Baseline For a"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 150
                            }
                        ],
                        "text": "\u2026LM, though we depart from the original paper by using only a forward LM. (iii) We also use the pretrained 300-dimensional (6B) GloVe embeddings from Pennington et al. (2014).12\nReal/Fake Auxiliary Task We train sentence encoders on a real/fake task in which the objective is to distinguish real\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1957433,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f37e1b62a767a307c046404ca96bc140b3e68cb5",
            "isKey": false,
            "numCitedBy": 22536,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition."
            },
            "slug": "GloVe:-Global-Vectors-for-Word-Representation-Pennington-Socher",
            "title": {
                "fragments": [],
                "text": "GloVe: Global Vectors for Word Representation"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods and produces a vector space with meaningful substructure."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47087284"
                        ],
                        "name": "Jon Sprouse",
                        "slug": "Jon-Sprouse",
                        "structuredName": {
                            "firstName": "Jon",
                            "lastName": "Sprouse",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jon Sprouse"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3280967"
                        ],
                        "name": "Carson T. Sch\u00fctze",
                        "slug": "Carson-T.-Sch\u00fctze",
                        "structuredName": {
                            "firstName": "Carson",
                            "lastName": "Sch\u00fctze",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carson T. Sch\u00fctze"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144498637"
                        ],
                        "name": "Diogo Almeida",
                        "slug": "Diogo-Almeida",
                        "structuredName": {
                            "firstName": "Diogo",
                            "lastName": "Almeida",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Diogo Almeida"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 46411835,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "8115a3b69a6dbc74b18788bc6a754edc6256e856",
            "isKey": false,
            "numCitedBy": 179,
            "numCiting": 85,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-comparison-of-informal-and-formal-acceptability-a-Sprouse-Sch\u00fctze",
            "title": {
                "fragments": [],
                "text": "A comparison of informal and formal acceptability judgments using a random sample from Linguistic Inquiry 2001--2010"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2708017"
                        ],
                        "name": "J. Bresnan",
                        "slug": "J.-Bresnan",
                        "structuredName": {
                            "firstName": "Joan",
                            "lastName": "Bresnan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bresnan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 53492415,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "09c2837c58e2b5a2aa9454446e6b067751a5dced",
            "isKey": false,
            "numCitedBy": 507,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "The comparative clause construction in English is almost notorious for its syntactic complexity. Exhibiting a variety of grammatical processes-recursion, deletions, permutations, and suppletions-it is a fecund source of ambiguities and puzzles. I mention here four well-known problems of the comparative, to which 1 offer a solution in what follows. What accounts for the fact that in (A), (i) and (ii) can be read as (roughly) synonymous, while (iii) and (iv) cannot? (A) 1. I've never seen a man taller than my father. 11. I've never seen a taller man than my father. lll. I've never seen a man taller than my mother. IV. I've never seen a taller man than my mother. Why does (iv) depart from grammaticality in (B)? (B) 1. Jack eats caviar more than he eats mush. 11. Jack eats more caviar than he eats mush. lll. Jack eats caviar more than he sleeps. IV. *Jack eats more caviar than he sleeps. What explains the ungrammaticality of (Civ)?"
            },
            "slug": "Syntax-of-the-Comparative-Clause-Construction-in-Bresnan",
            "title": {
                "fragments": [],
                "text": "Syntax of the Comparative Clause Construction in English"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The comparative clause construction in English is almost notorious for its syntactic complexity, exhibiting a variety of grammatical processes-recursion, deletions, permutations, and suppletions-it is a fecund source of ambiguities and puzzles."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3280967"
                        ],
                        "name": "Carson T. Sch\u00fctze",
                        "slug": "Carson-T.-Sch\u00fctze",
                        "structuredName": {
                            "firstName": "Carson",
                            "lastName": "Sch\u00fctze",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carson T. Sch\u00fctze"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "mary source of empirical data in much of theoretical linguistics, and the goal of a generative grammar is to generate all and only those sentences which native speakers \ufb01nd acceptable (Chomsky, 1957; Schutze, 1996).\u00a8 Despite this centrality, there has been relatively little work on acceptability classi\ufb01cation in computational linguistics. The recent explosion of progress in deep learning inspires us to revisit "
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "imacy requirements of verbal arguments. 2.4 Concerns about Acceptability Judgments Binary vs. Gradient Judgments Though discrete binary acceptability judgments are standard in generative linguistics (Schutze, 1996), Lau et al. (2016)\u00a8 \ufb01nd that when speakers are presented with the option to use a gradient scale to report sentence acceptability, they predictably and systematically use the full scale, rather than "
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "rates are actually grammatical, i.e., acceptable to a native speaker. (p.13) This has been the predominant methodology for research in generative linguistics over the last sixty years (Chomsky, 1957; Schutze, 1996). Linguists\u00a8 generally provide example sentences annotated with binary acceptability judgments from themselves or several native speakers. 2.2 The Acceptability Classi\ufb01cation Task Following common pra"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 141069514,
            "fieldsOfStudy": [
                "Linguistics",
                "Psychology"
            ],
            "id": "9eb31f967e7503280834b5cbe8fbfa0f952d8f6d",
            "isKey": true,
            "numCitedBy": 595,
            "numCiting": 291,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface Acknowledgments 1: Introduction 2: Definitions and Historical Background 3: Judging Grammaticality: The Nature of Metalinguistic Performance 4: Subject-Related Factors in Grammaticality Judgments 5: Task-Related Factors in Grammaticality Judgments 6: Theoretical and Methodological Implications 7: Looking Back and Looking Ahead References Index"
            },
            "slug": "The-empirical-base-of-linguistics:-Grammaticality-Sch\u00fctze",
            "title": {
                "fragments": [],
                "text": "The empirical base of linguistics: Grammaticality judgments and linguistic methodology"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3025325"
                        ],
                        "name": "W. Kintsch",
                        "slug": "W.-Kintsch",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Kintsch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Kintsch"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 219322170,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "040a6d4542111c1b6161533aa0818c0b025505a0",
            "isKey": false,
            "numCitedBy": 562,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Predication-Kintsch",
            "title": {
                "fragments": [],
                "text": "Predication"
            },
            "venue": {
                "fragments": [],
                "text": "Cogn. Sci."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2393013"
                        ],
                        "name": "I. Sag",
                        "slug": "I.-Sag",
                        "structuredName": {
                            "firstName": "Ivan",
                            "lastName": "Sag",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Sag"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "150325684"
                        ],
                        "name": "Thomas Wasov",
                        "slug": "Thomas-Wasov",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Wasov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Wasov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": " et al. (2013) 651 70.4 Syntax Textbook In-Domain 9515 71.3 Chung et al. (1995) 148 66.9 Sluicing Collins (2005) 66 68.2 Passive Jackendoff (1971) 94 67.0 Gapping Sag (1997) 112 57.1 Relative clauses Sag et al. (2003) 460 70.9 Syntax Textbook Williams (1980) 169 76.3 Predication Out-of-Domain 1049 69.2 Total 10657 70.5 Table 2: The contents of CoLA by source. N is the total number of examples. % is the percent of "
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 207622006,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "bbea6de2ca7db01a1913ac9f656d21164a71ad43",
            "isKey": true,
            "numCitedBy": 881,
            "numCiting": 156,
            "paperAbstract": {
                "fragments": [],
                "text": "This second edition of \"Syntactic Theory: A Formal Introduction\" expands and improves upon a truly unique introductory syntax textbook. Like the first edition its focus is on the development of precisely formulated grammars whose empirical predictions can be directly tested. There is also considerable emphasis on the prediction and evaluation of grammatical hypotheses, as well as on integrating syntactic hypotheses with matters of semantic analysis. The book covers the core areas of English syntax from the last quarter century, including complementation, control, \"raising constructions\", passives, the auxiliary system, and the analysis of long distance dependency constructions. \"Syntactic Theory's\" step-by-step introduction to a consistent grammar in these core areas is complemented by extensive problem sets drawing from a variety of languages. The book's theoretical perspective is presented in the context of current models of language processing, and the practical value of the constraint-based, lexicalist grammatical architecture proposed has already been demonstrated in computer language processing applications. This thoroughly reworked second edition includes revised and extended problems sets, updated analyses, additional examples and more detailed exposition throughout."
            },
            "slug": "Syntactic-Theory:-A-Formal-Introduction-Sag-Wasov",
            "title": {
                "fragments": [],
                "text": "Syntactic Theory: A Formal Introduction"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "This second edition of \"Syntactic Theory: A Formal Introduction\" expands and improves upon a truly unique introductory syntax textbook, focusing on the development of precisely formulated grammars whose empirical predictions can be directly tested."
            },
            "venue": {
                "fragments": [],
                "text": "Computational Linguistics"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "114531657"
                        ],
                        "name": "Noam Chomsky",
                        "slug": "Noam-Chomsky",
                        "structuredName": {
                            "firstName": "Noam",
                            "lastName": "Chomsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noam Chomsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The influential argument from the poverty of the stimulus (APS) holds that the extent of human linguistic competence cannot be explained by purely domain general learning mechanisms and that humans must be born with a Universal Grammar which imparts specific knowledge of grammatical universals to the child and makes learning possible (Chomsky, 1965)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In his foundationalwork on generative syntax, Chomsky (1957) defines an empirically adequate grammar of a language L as one that generates all and only those strings of L which native speakers of L judge to be acceptable."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 293,
                                "start": 280
                            }
                        ],
                        "text": "\u2026(APS) holds that the extent of human linguistic competence cannot be explained by purely domain general learning mechanisms and that humans must be born with a Universal Grammar which imparts specific knowledge of grammatical universals to the child and makes learning possible (Chomsky, 1965)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12867884,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "16c762445f11fa2020994918dc4f93e76264df17",
            "isKey": true,
            "numCitedBy": 14181,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : Contents: Methodological preliminaries: Generative grammars as theories of linguistic competence; theory of performance; organization of a generative grammar; justification of grammars; formal and substantive grammars; descriptive and explanatory theories; evaluation procedures; linguistic theory and language learning; generative capacity and its linguistic relevance Categories and relations in syntactic theory: Scope of the base; aspects of deep structure; illustrative fragment of the base component; types of base rules Deep structures and grammatical transformations Residual problems: Boundaries of syntax and semantics; structure of the lexicon"
            },
            "slug": "\u0935\u093e\u0915\u094d\u092f\u0935\u093f\u0928\u094d\u092f\u093e\u0938-\u0915\u093e-\u0938\u0948\u0926\u094d\u0927\u093e\u0928\u094d\u0924\u093f\u0915-\u092a\u0915\u094d\u0937-=-Aspects-of-the-Chomsky",
            "title": {
                "fragments": [],
                "text": "Aspects of the Theory of Syntax"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "Methodological preliminaries of generative grammars as theories of linguistic competence; theory of performance; organization of a generative grammar; justification of grammar; descriptive and explanatory theories; evaluation procedures; linguistic theory and language learning."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1965
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 26
                            }
                        ],
                        "text": "This practice aligns with Chomsky\u2019s (1957) definition of grammaticality as the binary notion of membership in a set of wellformed strings. However, Lau et al. (2016) find that when speakers are presented with the option to use a gradient scale to report sentence acceptability, they predictably and systematically use the full scale, rather than clustering their judgments near the extremes as would be expected for a fundamentally binary phenomenon."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 0
                            }
                        ],
                        "text": "Chomsky\u2019s (1957) methodology merely allows linguists to claim that their theories are empirically adequate13."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 270,
                                "start": 240
                            }
                        ],
                        "text": "Acceptability judgments like these are the primary source of empirical data in much of theoretical linguistics, with the objective of a generative grammar being to generate all and only those sentences which native speakers find acceptable (Chomsky, 1957; Sch\u00fctze, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 107
                            }
                        ],
                        "text": "This has been the predominant methodology for research in generative linguistics over the last sixty years (Chomsky, 1957; Sch\u00fctze, 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 26
                            }
                        ],
                        "text": "This practice aligns with Chomsky\u2019s (1957) definition of grammaticality as the binary notion of membership in a set of wellformed strings."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 75
                            }
                        ],
                        "text": "This definition also includes generative grammars of the type described by Chomsky (1957) above."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 84
                            }
                        ],
                        "text": "Acceptability judgments are central to the formulation of generative linguistics in Chomsky\u2019s influential (1957) book Syntactic Structures."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Syntactic Structures"
            },
            "venue": {
                "fragments": [],
                "text": "Mouton."
            },
            "year": 1957
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48288365"
                        ],
                        "name": "Veneeta Dayal",
                        "slug": "Veneeta-Dayal",
                        "structuredName": {
                            "firstName": "Veneeta",
                            "lastName": "Dayal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Veneeta Dayal"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60654913,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "b9f9d7e8bf4a8fd786fee791ff4b11bbcce06259",
            "isKey": false,
            "numCitedBy": 191,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "The primary theoretical focus of this paper is on Free Choice uses of any, in particular on two phenomena that have remained largely unstudied. One involves the ability of any phrases to occur in affirmative episodic statements when aided by suitable noun modifiers. The other involves the difference between modals of necessity and possibility with respect to licensing of any. The central thesis advanced here is that FC any is a universal determiner whose domain of quantification is not a set of particular individuals but the set of possible individuals of the relevant kind. In a theory of genericity utilizing situations, an any phrase can be seen as having a universal quantifier binding the situation variable of the common noun. This inherent genericity is argued to be at the heart of the intuition that any statements support counterfactual inferences and do not involve existential commitments. A conflict in presuppositions is shown to account for the incompatibility of unmodified any phrases in affirmative episodic statements and the crucial role played by modification in ameliorating this clash is explicated. In the case of modals of necessity, the interaction between the universal force of any and the particular modal base is shown to be crucial. In view of these facts it is argued that FC any is not directly licensed by modal or generic operators as generally assumed but that its felicitous use is sensitive to the pragmatics of epistemic modality. Turning to its polarity sensitive uses, language internal as well as crosslinguistic evidence is presented to distinguish it from FC any in having the existential quantificational force typical of indefinites. The paper concludes by suggesting that the common tie between them is that they both occur in statements that apply to a class of entities, rather than to particular members of the class."
            },
            "slug": "Any-as-Inherently-Modal-Dayal",
            "title": {
                "fragments": [],
                "text": "Any as Inherently Modal"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is argued that FC any is not directly licensed by modal or generic operators as generally assumed but that its felicitous use is sensitive to the pragmatics of epistemic modality."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116154564"
                        ],
                        "name": "Jim Miller",
                        "slug": "Jim-Miller",
                        "structuredName": {
                            "firstName": "Jim",
                            "lastName": "Miller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jim Miller"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "n would have been all working. Baltin (1982) 0 Who do you think that will question Seamus \ufb01rst? Carnie (2013) 0 Usually, any lion is majestic. Dayal (1998) 1 The gardener planted roses in the garden. Miller (2002) 1 I wrote Blair a letter, but I tore it up before I sent it. Rappaport Hovav and Levin (2008) Table 3: CoLA random sample, drawn from the in-domain training set. 1: acceptable, 0: unacceptable. Human"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "ination Goldberg and Jackendoff (2004) 106 77.4 Resultative Kadmon and Landman (1993) 93 81.7 Negative Polarity Kim and Sells (2008) 1965 71.2 Syntax Textbook Levin (1993) 1459 69.0 Verb alternations Miller (2002) 426 84.5 Syntax Textbook Rappaport Hovav and Levin (2008) 151 69.5 Dative alternation Ross (1967) 1029 61.8 Islands Sag et al. (1985) 153 68.6 Coordination Sportiche et al. (2013) 651 70.4 Syntax Tex"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60923443,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "455fb26011f26003920a1af7e4c66657823435bc",
            "isKey": true,
            "numCitedBy": 26,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "1. Heads and modifiers 2. Constituent structure 3. Constructions 4. Word classes 5. The Lexicon 6. Clauses and sentences 7. Clauses main and subordinate 8. Clauses finite and non-finite 9. Grammatical functions 10. Syntactic linkage 11. Heads and modifiers revisited 12. Roles 13. Clauses, sentences and text 14. Grammar and semantics: aspect, tense, voice Discussion of the exercises Further reading"
            },
            "slug": "An-Introduction-to-English-Syntax-Miller",
            "title": {
                "fragments": [],
                "text": "An Introduction to English Syntax"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "87984582"
                        ],
                        "name": "Sandra Chung",
                        "slug": "Sandra-Chung",
                        "structuredName": {
                            "firstName": "Sandra",
                            "lastName": "Chung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sandra Chung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69848782"
                        ],
                        "name": "William A. Ladusaw",
                        "slug": "William-A.-Ladusaw",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Ladusaw",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "William A. Ladusaw"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144104770"
                        ],
                        "name": "James Mccloskey",
                        "slug": "James-Mccloskey",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Mccloskey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Mccloskey"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 170906801,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "02bb2386e9ef1692d172b3d11c47d7fd200d505d",
            "isKey": false,
            "numCitedBy": 477,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a novel analysis of Sluicing, an ellipsis construction first described by Ross (1969) and illustrated by the bracketed portion ofI want to do something, but I'm just not sure [what _]. Starting from the assumption that a sluice consists of a displaced Wh-constituent and an empty IP, we show how simple and general LF operations fill out the empty IP and thereby provide it with an interpretable Logical Form. The LF operations we appeal to rely on the influential theory of indefinites developed by Irene Heim and Hans Kamp, and are in harmony with certain aspects of Chomsky's Minimalist Program for linguistic theory. The analysis accounts directly for the familiar properties of Sluicing, as well as some facts which have not previously been observed."
            },
            "slug": "Sluicing-and-logical-form-Chung-Ladusaw",
            "title": {
                "fragments": [],
                "text": "Sluicing and logical form"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2120932"
                        ],
                        "name": "A. Clark",
                        "slug": "A.-Clark",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Clark",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Clark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2732560"
                        ],
                        "name": "Shalom Lappin",
                        "slug": "Shalom-Lappin",
                        "structuredName": {
                            "firstName": "Shalom",
                            "lastName": "Lappin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shalom Lappin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 59776435,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "9cd0b8535ea7d845158739907b068e0113c8a9ef",
            "isKey": false,
            "numCitedBy": 167,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface. 1 Introduction: Nativism in Linguistic Theory. 1.1 Historical Development. 1.2 The Rationalist-Empiricist Debate. 1.3 Nativism and Cognitive Modularity. 1.4 Connectionism, Nonmodularity, and Antinativism. 1.5 Adaptation and the Evolution of Natural Language. 1.6 Summary and Conclusions. 2 Clarifying the Argument from the Poverty of the Stimulus. 2.1 Formulating the APS. 2.2 Empiricist Learning versus Nativist Learning. 2.3 Our Version of the APS. 2.4 A Theory-Internal APS. 2.5 Evidence for the APS: Auxiliary Inversion as a Paradigm Case. 2.6 Debate on the PLD. 2.7 Learning Theory and Indispensable Data. 2.8 A Second Empirical Case: Anaphoric One. 2.9 Summary and Conclusions. 3 The Stimulus: Determining the Nature of Primary Linguistic Data. 3.1 Primary Linguistic Data. 3.2 Negative Evidence. 3.3 Semantic, Contextual, and Extralinguistic Evidence. 3.4 Prosodic Information. 3.5 Summary and Conclusions. 4 Learning in the Limit: The Gold Paradigm. 4.1 Formal Models of Language Acquisition. 4.2 Mathematical Models of Learnability. 4.3 The Gold Paradigm of Learnability. 4.4 Critique of the Positive-Evidence-Only APS in IIL. 4.5 Proper Positive Results. 4.6 Variants of the Gold Model. 4.7 Implications of Gold's Results for Linguistic Nativism. 4.8 Summary and Conclusions. 5 Probabilistic Learning Theory for Language Acquisition. 5.1 Chomsky's View of Statistical Learning. 5.2 Basic Assumptions of Statistical Learning Theory. 5.3 Learning Distributions. 5.4 Probabilistic Versions of the IIL Framework. 5.5 PAC Learning. 5.6 Consequences of PAC Learnability. 5.7 Problems with the Standard Model. 5.8 Summary and Conclusions. 6 A Formal Model of Indirect Negative Evidence. 6.1 Introduction. 6.2. From Low Probability to Ungrammaticality. 6.3 Modeling the DDA. 6.4 Applying the Functional Lower Bound. 6.5 Summary and Conclusions. 7 Computational Complexity and Efficient Learning. 7.1 Basic Concepts of Complexity 7.2 Efficient Learning. 7.3 Negative Results. 7.4 Interpreting Hardness Results. 7.5 Summary and Conclusions. 8 Positive Results in Efficient Learning. 8.1 Regular Languages. 8.2 Distributional Methods. 8.3 Distributional Learning of Context-Free Languages. 8.4 Lattice-Based Formalisms. 8.5 Arguments against Distributional Learning. 8.6 Summary and Conclusions. 9 Grammar Induction through Implemented Machine Learning. 9.1 Supervised Learning. 9.2Unsupervised Learning. 9.3 Summary and Conclusions. 10 Parameters in Linguistic Theory and Probabilistic Language Models. 10.1 Learnability of Parametric Models of Syntax. 10.2 UG Parameters and Language Variation. 10.3 Parameters in Probabilistic Language Models. 10.4 Inferring Constraints on Hypothesis Spaces with Hierarchical Bayesian Models. 10.5 Summary and Conclusions. 11 A Brief Look at Some Biological and Psychological Evidence. 11.1 Developmental Arguments. 11.2 Genetic Factors: Inherited Language Disorders. 11.3 Experimental Learning of Artificial Languages. 11.4 Summary and Conclusions. 12 Conclusion. 12.1 Summary. 12.2 Conclusions. References. Author Index. Subject Index."
            },
            "slug": "Linguistic-Nativism-and-the-Poverty-of-the-Stimulus-Clark-Lappin",
            "title": {
                "fragments": [],
                "text": "Linguistic Nativism and the Poverty of the Stimulus"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "This book discusses Nativism in Linguistic Theory with a focus on the development of the Positive-Evidence-Only APS in IIL and its applications to Language Acquisition."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2393013"
                        ],
                        "name": "I. Sag",
                        "slug": "I.-Sag",
                        "structuredName": {
                            "firstName": "Ivan",
                            "lastName": "Sag",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Sag"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7742946,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "1ee1e9f0d1dde068136be625dd3ddd45cd6a3b1e",
            "isKey": false,
            "numCitedBy": 484,
            "numCiting": 110,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper sketches a grammar of English relative clause constructions\n (including infinitival and reduced relatives) based on the notions of construction type and type constraints. Generalizations about dependency relations and clausal functions\n are factored into distinct dimensions contributing constraints to specific\n construction types in a multiple inheritance type hierarchy. The grammar presented here\n provides an account of extraction, pied piping and relative clause \u2018stacking\u2019 without appeal to transformational operations, transderivational competition, or invisible\n (\u2018empty\u2019) categories of any kind."
            },
            "slug": "English-relative-clause-constructions-Sag",
            "title": {
                "fragments": [],
                "text": "English relative clause constructions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2034844"
                        ],
                        "name": "P. Culicover",
                        "slug": "P.-Culicover",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Culicover",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Culicover"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1766112"
                        ],
                        "name": "R. Jackendoff",
                        "slug": "R.-Jackendoff",
                        "structuredName": {
                            "firstName": "Ray",
                            "lastName": "Jackendoff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Jackendoff"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "ource N % Topic Adger(2003) 948 71.9 Syntax Textbook Baltin(1982) 96 66.7 Movement Baltin and Collins(2001) 880 66.7 Handbook Bresnan(1973) 259 69.1 Comparatives Carnie(2013) 870 80.3 Syntax Textbook Culicover and Jackendoff (1999) 233 59.2 Comparatives Dayal(1998) 179 75.4 Modality Gazdar(1981) 110 65.5 Coordination Goldberg and Jackendoff (2004) 106 77.4 Resultative Kadmon and Landman (1993) 93 81.7 Negative Polarity Kim and "
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 57565558,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "58a62c988259d49869aae6354cb933eb5a900fc1",
            "isKey": true,
            "numCitedBy": 176,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "The English comparative correlative construction (e.g., The more you eat, the fatter you get) embeds like an ordinary CP, and each of its clauses displays an ordinary long-distance dependency. However, the connection between the two clauses is not ordinary: they are connected paratactically in syntax, but the first clause is interpreted as if it were a subordinate clause. The construction's mixture of the general and the idiosyncratic at all levels of detail challenges the distinction between core and periphery in grammar and the assumption that some level of underlying syntax directly mirrors semantic structure."
            },
            "slug": "The-View-from-the-Periphery:-The-English-Culicover-Jackendoff",
            "title": {
                "fragments": [],
                "text": "The View from the Periphery: The English Comparative Correlative"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "The English comparative correlative construction's mixture of the general and the idiosyncratic at all levels of detail challenges the distinction between core and periphery in grammar and the assumption that some level of underlying syntax directly mirrors semantic structure."
            },
            "venue": {
                "fragments": [],
                "text": "Linguistic Inquiry"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37907837"
                        ],
                        "name": "Allyson Ettinger",
                        "slug": "Allyson-Ettinger",
                        "structuredName": {
                            "firstName": "Allyson",
                            "lastName": "Ettinger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Allyson Ettinger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143718836"
                        ],
                        "name": "Ahmed Elgohary",
                        "slug": "Ahmed-Elgohary",
                        "structuredName": {
                            "firstName": "Ahmed",
                            "lastName": "Elgohary",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ahmed Elgohary"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680292"
                        ],
                        "name": "P. Resnik",
                        "slug": "P.-Resnik",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Resnik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Resnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2624014,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "53d43ccc593bf44e9aa52e3971df1b9dd396e30d",
            "isKey": false,
            "numCitedBy": 87,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a diagnostic method for probing specific information captured in vector representations of sentence meaning, via simple classification tasks with strategically constructed sentence sets. We identify some key types of semantic information that we might expect to be captured in sentence composition, and illustrate example classification tasks for targeting this information."
            },
            "slug": "Probing-for-semantic-evidence-of-composition-by-of-Ettinger-Elgohary",
            "title": {
                "fragments": [],
                "text": "Probing for semantic evidence of composition by means of simple classification tasks"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "A diagnostic method for probing specific information captured in vector representations of sentence meaning, via simple classification tasks with strategically constructed sentence sets, is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "RepEval@ACL"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726807"
                        ],
                        "name": "Diederik P. Kingma",
                        "slug": "Diederik-P.-Kingma",
                        "structuredName": {
                            "firstName": "Diederik",
                            "lastName": "Kingma",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Diederik P. Kingma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2503659"
                        ],
                        "name": "Jimmy Ba",
                        "slug": "Jimmy-Ba",
                        "structuredName": {
                            "firstName": "Jimmy",
                            "lastName": "Ba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jimmy Ba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "All neural network models are implemented in PyTorch and optimized using Adam (Kingma and Ba, 2014)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6628106,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
            "isKey": false,
            "numCitedBy": 90063,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm."
            },
            "slug": "Adam:-A-Method-for-Stochastic-Optimization-Kingma-Ba",
            "title": {
                "fragments": [],
                "text": "Adam: A Method for Stochastic Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "This work introduces Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments, and provides a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3213150"
                        ],
                        "name": "E. Loper",
                        "slug": "E.-Loper",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Loper",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Loper"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066194290"
                        ],
                        "name": "Steven Bird",
                        "slug": "Steven-Bird",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Bird",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steven Bird"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We lowercase and tokenize the BNCdata usingNLTK (Bird and Loper, 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 219306244,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bc2b394d707a186f64f5092870593ca52d3003fd",
            "isKey": false,
            "numCitedBy": 1092,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "The Natural Language Toolkit is a suite of program modules, data sets and tutorials supporting research and teaching in computational linguistics and natural language processing. NLTK is written in Python and distributed under the GPL open source license. Over the past year the toolkit has been rewritten, simplifying many linguistic data structures and taking advantage of recent enhancements in the Python language. This paper reports on the simplified toolkit and explains how it is used in teaching NLP."
            },
            "slug": "NLTK:-The-Natural-Language-Toolkit-Loper-Bird",
            "title": {
                "fragments": [],
                "text": "NLTK: The Natural Language Toolkit"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "The Natural Language Toolkit has been rewritten, simplifying many linguistic data structures and taking advantage of recent enhancements in the Python language."
            },
            "venue": {
                "fragments": [],
                "text": "ACL 2006"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 26
                            }
                        ],
                        "text": "This practice aligns with Chomsky\u2019s (1957) definition of grammaticality as the binary notion of membership in a set of wellformed strings. However, Lau et al. (2016) find that when speakers are presented with the option to use a gradient scale to report sentence acceptability, they predictably and systematically use the full scale, rather than clustering their judgments near the extremes as would be expected for a fundamentally binary phenomenon."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 0
                            }
                        ],
                        "text": "Chomsky\u2019s (1957) methodology merely allows linguists to claim that their theories are empirically adequate13."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 270,
                                "start": 240
                            }
                        ],
                        "text": "Acceptability judgments like these are the primary source of empirical data in much of theoretical linguistics, with the objective of a generative grammar being to generate all and only those sentences which native speakers find acceptable (Chomsky, 1957; Sch\u00fctze, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 107
                            }
                        ],
                        "text": "This has been the predominant methodology for research in generative linguistics over the last sixty years (Chomsky, 1957; Sch\u00fctze, 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 26
                            }
                        ],
                        "text": "This practice aligns with Chomsky\u2019s (1957) definition of grammaticality as the binary notion of membership in a set of wellformed strings."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 75
                            }
                        ],
                        "text": "This definition also includes generative grammars of the type described by Chomsky (1957) above."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 84
                            }
                        ],
                        "text": "Acceptability judgments are central to the formulation of generative linguistics in Chomsky\u2019s influential (1957) book Syntactic Structures."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Syntactic Structures"
            },
            "venue": {
                "fragments": [],
                "text": "Mouton."
            },
            "year": 1957
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49353854"
                        ],
                        "name": "Chris Collins",
                        "slug": "Chris-Collins",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris Collins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15973598,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "ab1b3405454f42027c2037024f1d88b5abb23104",
            "isKey": false,
            "numCitedBy": 471,
            "numCiting": 83,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract.\u2002 I propose a theory of the passive that combines aspects of the principles and parameters analysis (no specific rules, no downward movement) and Chomsky's (1957) Syntactic Structures analysis (the arguments in the passive are generated in the same positions as they are in the active)."
            },
            "slug": "A-smuggling-approach-to-the-passive-in-english-Collins",
            "title": {
                "fragments": [],
                "text": "A smuggling approach to the passive in english"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A theory of the passive that combines aspects of the principles and parameters analysis and Chomsky's Syntactic Structures analysis and the arguments in the passive are generated in the same positions as they are in the active."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3308557"
                        ],
                        "name": "S. Hochreiter",
                        "slug": "S.-Hochreiter",
                        "structuredName": {
                            "firstName": "Sepp",
                            "lastName": "Hochreiter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Hochreiter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "ection 6 are designed to meet these standards (see Section 6.2 for discussion). 3.2 Investigating the Black Box Recurrent neural network models like the Long Short-Term Memory (LSTM) networks we use (Hochreiter and Schmidhuber, 1997) can discover some forms of structure in raw language data (LeCun et al., 2015). These models are widely used to encode features of sentences in \ufb01xed-length sentence embeddings (Cho et al., 2014; Suts"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1915014,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "44d2abe2175df8153f465f6c39b68b76a0d40ab9",
            "isKey": true,
            "numCitedBy": 51694,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms."
            },
            "slug": "Long-Short-Term-Memory-Hochreiter-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Long Short-Term Memory"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A novel, efficient, gradient based method called long short-term memory (LSTM) is introduced, which can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "90061799"
                        ],
                        "name": "M. Baltin",
                        "slug": "M.-Baltin",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Baltin",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Baltin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49353854"
                        ],
                        "name": "Chris Collins",
                        "slug": "Chris-Collins",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris Collins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 60446503,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "1dd4246c1b577dcd530942007fd156b93ed8a969",
            "isKey": false,
            "numCitedBy": 631,
            "numCiting": 772,
            "paperAbstract": {
                "fragments": [],
                "text": "Contributors. Introduction. Part I: Derivation Versus Representation:. 1. Explaining Morphosyntactic Competition: Joan Bresnan (Stanford University). 2. Economy Conditions in Syntax: Chris Collins (Cornell University). 3. Derivation and Representation in Modern Transformational Syntax: Howard Lasnik (University of Connecticut). 4. Relativized Minimality Effects: Luigi Rizzi (Universite de Geneve). Part II: Movement:. 5. Head Movement: Ian Roberts (University of Stuttgart). 6. Object Shift and Scrambling: Hoskuldur Thrainsson (University of Iceland). 7. Wh--in--situ Languages: Akira Watanabe (University of Tokyo). 8. A--Movements: Mark Baltin (New York University). Part III: Argument Structure and Phrase Structure:. 9. Thematic Relations in Syntax: Jeffrey S. Gruber (independent scholar). 10. Predication: John Bowers (Cornell University). 11. Case: Hiroyuki Ura. 12. Phrase Structure: Naoki Fukui (University of California). 13. The Natures of Nonconfigurationality: Mark C. Baker (McGill University). 14. What VP Ellipsis Can Do, and What it Can't, but not Why: Kyle Johnson (University of Massachusetts at Amherst). Part IV: Functional Projections:. 15. Agreement Projections: Adriana Belletti (Universita di Siena). 16. Sentential Negation: Raffaella Zanuttini (Georgetown University). 17. The DP Hypothesis: Identifying Clausal Properties in the Nominal Domain: Judy B. Bernstein (Syracuse University). 18. The Structure of DPs: Some Principles, Parameters and Problems: Giuseppe Longobardi (University of Trieste). Part V: Interface With Interpretation:. 19. The Syntax of Scope: Anna Szabolcsi (New York University). 20. Deconstructing Binding: Eric Reuland and Martin Everaert (both Utrecht Institute of Linguistics). 21. Syntactic Reconstruction Effects: Andrew Barss (University of Arizona). Part VI: External Evaluation of Syntax:. 22. Syntactic Change: Anthony S. Kroch (University of Pennsylvania). 23. Setting Syntactic Parameters: Janet Dean Fodor (City University of New York). Bibliography. Index."
            },
            "slug": "The-handbook-of-contemporary-syntactic-theory-Baltin-Collins",
            "title": {
                "fragments": [],
                "text": "The handbook of contemporary syntactic theory"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This work explains Morphosyntactic Competition, the Structure of DPs, and the Natures of Nonconfigurationality in Modern Transformational Syntax, as well as investigating the role of rhetoric in the development of knowledge representation."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "21308992"
                        ],
                        "name": "Steven Bird",
                        "slug": "Steven-Bird",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Bird",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steven Bird"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1438450,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "01a660ec8aa995a88a00bfb41cb86c022047a9db",
            "isKey": false,
            "numCitedBy": 3447,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "NLTK, the Natural Language Toolkit, is a suite of open source program modules, tutorials and problem sets, providing ready-to-use computational linguistics courseware. NLTK covers symbolic and statistical natural language processing, and is interfaced to annotated corpora. Students augment and replace existing components, learn structured programming by example, and manipulate sophisticated models from the outset."
            },
            "slug": "NLTK:-The-Natural-Language-Toolkit-Bird",
            "title": {
                "fragments": [],
                "text": "NLTK: The Natural Language Toolkit"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "NLTK, the Natural Language Toolkit, is a suite of open source program modules, tutorials and problem sets, providing ready-to-use computational linguistics courseware that covers symbolic and statistical natural language processing."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153440022"
                        ],
                        "name": "Ian J. Goodfellow",
                        "slug": "Ian-J.-Goodfellow",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Goodfellow",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ian J. Goodfellow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760871"
                        ],
                        "name": "Aaron C. Courville",
                        "slug": "Aaron-C.-Courville",
                        "structuredName": {
                            "firstName": "Aaron",
                            "lastName": "Courville",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aaron C. Courville"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "ting the Black Box Recurrent neural network models like the Long Short-Term Memory (LSTM) networks we use (Hochreiter and Schmidhuber, 1997) can discover some forms of structure in raw language data (LeCun et al., 2015). These models are widely used to encode features of sentences in \ufb01xed-length sentence embeddings (Cho et al., 2014; Sutskever et al., 2014; Kiros et al., 2015). Evaluating generalpurpose sentence emb"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3074096,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a4cec122a08216fe8a3bc19b22e78fbaea096256",
            "isKey": false,
            "numCitedBy": 28186,
            "numCiting": 938,
            "paperAbstract": {
                "fragments": [],
                "text": "Machine-learning technology powers many aspects of modern society: from web searches to content filtering on social networks to recommendations on e-commerce websites, and it is increasingly present in consumer products such as cameras and smartphones. Machine-learning systems are used to identify objects in images, transcribe speech into text, match news items, posts or products with users\u2019 interests, and select relevant results of search. Increasingly, these applications make use of a class of techniques called deep learning. Conventional machine-learning techniques were limited in their ability to process natural data in their raw form. For decades, constructing a pattern-recognition or machine-learning system required careful engineering and considerable domain expertise to design a feature extractor that transformed the raw data (such as the pixel values of an image) into a suitable internal representation or feature vector from which the learning subsystem, often a classifier, could detect or classify patterns in the input. Representation learning is a set of methods that allows a machine to be fed with raw data and to automatically discover the representations needed for detection or classification. Deep-learning methods are representation-learning methods with multiple levels of representation, obtained by composing simple but non-linear modules that each transform the representation at one level (starting with the raw input) into a representation at a higher, slightly more abstract level. With the composition of enough such transformations, very complex functions can be learned. For classification tasks, higher layers of representation amplify aspects of the input that are important for discrimination and suppress irrelevant variations. An image, for example, comes in the form of an array of pixel values, and the learned features in the first layer of representation typically represent the presence or absence of edges at particular orientations and locations in the image. The second layer typically detects motifs by spotting particular arrangements of edges, regardless of small variations in the edge positions. The third layer may assemble motifs into larger combinations that correspond to parts of familiar objects, and subsequent layers would detect objects as combinations of these parts. The key aspect of deep learning is that these layers of features are not designed by human engineers: they are learned from data using a general-purpose learning procedure. Deep learning is making major advances in solving problems that have resisted the best attempts of the artificial intelligence community for many years. It has turned out to be very good at discovering intricate structures in high-dimensional data and is therefore applicable to many domains of science, business and government. In addition to beating records in image recognition and speech recognition, it has beaten other machine-learning techniques at predicting the activity of potential drug molecules, analysing particle accelerator data, reconstructing brain circuits, and predicting the effects of mutations in non-coding DNA on gene expression and disease. Perhaps more surprisingly, deep learning has produced extremely promising results for various tasks in natural language understanding, particularly topic classification, sentiment analysis, question answering and language translation. We think that deep learning will have many more successes in the near future because it requires very little engineering by hand, so it can easily take advantage of increases in the amount of available computation and data. New learning algorithms and architectures that are currently being developed for deep neural networks will only accelerate this progress."
            },
            "slug": "Deep-Learning-Goodfellow-Bengio",
            "title": {
                "fragments": [],
                "text": "Deep Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Deep learning is making major advances in solving problems that have resisted the best attempts of the artificial intelligence community for many years, and will have many more successes in the near future because it requires very little engineering by hand and can easily take advantage of increases in the amount of available computation and data."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4390470"
                        ],
                        "name": "David Adger",
                        "slug": "David-Adger",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Adger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Adger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "ergoes the middle voice alternation. Finally we add content to examples that are not complete sentences, replacing, for example *The Bill\u2019s book with *The Bill\u2019s book has a red cover. N % Description Adger (2003) 948 71.9 Syntax Textbook Baltin (1982) 96 66.7 Movement Baltin and Collins (2001) 880 66.7 Handbook Bresnan (1973) 259 69.1 Comparatives Carnie (2013) 870 80.3 Syntax Textbook Culicover and Jackendof"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "n the in-domain development set. Performance is highly variable, with MCC ranging from 0.487 for the ELMo-style real/fake model on the Ross (1967) data, to 0.023 for the real/fake model with GloVe on Adger (2003). 8 Fine-Grained Analysis Here, we run additional evaluations to probe whether our models are able to successfully learn grammatical generalizations. For these tests we generate \ufb01ve auxiliary datasets"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 61754681,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "cb7a32a853246266b8786001b78c1e5243221946",
            "isKey": true,
            "numCitedBy": 632,
            "numCiting": 124,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface i1 Core Concepts 11.1 What is a sentence? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11.1.1 Utterances, propositions and sentences . . . . . . . . . . . . . . . . . 11.1.2 Acceptability, grammaticality and stars . . . . . . . . . . . . . . . . . 21.1.3 Form and order . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51.2 Tacit Knowledge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61.3 Syntactic theories . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101.4 Back to Sentences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152 Morphosyntactic features 172.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172.2 Introducing Features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172.3 What are features? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 202.3.1 Feature Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 212.3.2 Interface Rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 242.4 Motivating Features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 252.4.1 Major Category Features . . . . . . . . . . . . . . . . . . . . . . . . . 262.4.2 More on the content of words . . . . . . . . . . . . . . . . . . . . . . 282.4.3 Some more minor features . . . . . . . . . . . . . . . . . . . . . . . . 332.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 413 Constituency and Theta Roles 483.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 483.2 Constituents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 483.3 Fundamental notions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 533.4 Determining the head . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 573.5 Predicting the head - \u03b8-roles and selectional features . . . . . . . . . . . . . 603.5.1 \u03b8 Roles and the \u03b8-Criterion . . . . . . . . . . . . . . . . . . . . . . . 613.5.2 Unassigned \u03b8-roles . . . . . . . . . . . . . . . . . . . . . . . . . . . . 643.5.3 Selectional Features and Lexical Representation . . . . . . . . . . . . 653.5.4 S-selectional Features . . . . . . . . . . . . . . . . . . . . . . . . . . . 683.6 Triggering Merge by feature checking . . . . . . . . . . . . . . . . . . . . . . 711"
            },
            "slug": "Core-Syntax:-A-Minimalist-Approach-Adger",
            "title": {
                "fragments": [],
                "text": "Core Syntax: A Minimalist Approach"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "Preface i1 Core Concepts 11.1.2 Acceptability, grammaticality and stars, and back to Sentences ."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "84651444"
                        ],
                        "name": "Matthias Schroder",
                        "slug": "Matthias-Schroder",
                        "structuredName": {
                            "firstName": "Matthias",
                            "lastName": "Schroder",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthias Schroder"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 0
                            }
                        ],
                        "text": "Wagner et al. (2009) distort real sentences by, for example, deleting words, inserting words, or altering verbal inflection."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 0
                            }
                        ],
                        "text": "Wagner et al. (2009) simply label a sentence unacceptable if it has gone through one of their automatic distortion procedures."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 63088794,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1a6d4f39c7b80b07f0872d7a4793b2f8b78a93cd",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "an introduction to syntactic analysis and theory is available in our digital library an online access to it is set as public so you can get it instantly. Our books collection hosts in multiple locations, allowing you to get the most less latency time to download any of our books like this one. Merely said, the an introduction to syntactic analysis and theory is universally compatible with any devices to read."
            },
            "slug": "An-Introduction-To-Syntactic-Analysis-And-Theory-Schroder",
            "title": {
                "fragments": [],
                "text": "An Introduction To Syntactic Analysis And Theory"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "An introduction to syntactic analysis and theory is available in the authors' digital library an online access to it is set as public so you can get it instantly."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2150934397"
                        ],
                        "name": "J. Ross",
                        "slug": "J.-Ross",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Ross",
                            "middleNames": [
                                "Robert"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ross"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 188
                            }
                        ],
                        "text": "\u2026sources in\nthe corpus: Kim & Sells (2008) is a recent undergraduate syntax textbook, Levin (1993) is a comprehensive reference detailing the lexical properties of thousands of verbs, and Ross (1967) is an influential dissertation focusing on wh-movement and extraction islands in English syntax."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We observe greater disagreement between human annotators and published judgments than Sprouse et al. (2013) do."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60624374,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "486af640c427afba9036799cbe2bc41774a4d6c2",
            "isKey": false,
            "numCitedBy": 3058,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Massachusetts Institute of Technology. Dept. of Modern Languages and Linguistics. Thesis. 1967. Ph.D."
            },
            "slug": "Constraints-on-variables-in-syntax-Ross",
            "title": {
                "fragments": [],
                "text": "Constraints on variables in syntax"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper is intended to provide a history of modern language pedagogical practices in the United States and its applications in the context of modern linguistics."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1967
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2307666"
                        ],
                        "name": "B. Matthews",
                        "slug": "B.-Matthews",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Matthews",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Matthews"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 44596673,
            "fieldsOfStudy": [
                "Chemistry"
            ],
            "id": "155345976aa505a10a45e9119f2853df4d7999d7",
            "isKey": false,
            "numCitedBy": 3830,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Comparison-of-the-predicted-and-observed-secondary-Matthews",
            "title": {
                "fragments": [],
                "text": "Comparison of the predicted and observed secondary structure of T4 phage lysozyme."
            },
            "venue": {
                "fragments": [],
                "text": "Biochimica et biophysica acta"
            },
            "year": 1975
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 26
                            }
                        ],
                        "text": "This practice aligns with Chomsky\u2019s (1957) definition of grammaticality as the binary notion of membership in a set of wellformed strings. However, Lau et al. (2016) find that when speakers are presented with the option to use a gradient scale to report sentence acceptability, they predictably and systematically use the full scale, rather than clustering their judgments near the extremes as would be expected for a fundamentally binary phenomenon."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 0
                            }
                        ],
                        "text": "Chomsky\u2019s (1957) methodology merely allows linguists to claim that their theories are empirically adequate13."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 270,
                                "start": 240
                            }
                        ],
                        "text": "Acceptability judgments like these are the primary source of empirical data in much of theoretical linguistics, with the objective of a generative grammar being to generate all and only those sentences which native speakers find acceptable (Chomsky, 1957; Sch\u00fctze, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 107
                            }
                        ],
                        "text": "This has been the predominant methodology for research in generative linguistics over the last sixty years (Chomsky, 1957; Sch\u00fctze, 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 26
                            }
                        ],
                        "text": "This practice aligns with Chomsky\u2019s (1957) definition of grammaticality as the binary notion of membership in a set of wellformed strings."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 75
                            }
                        ],
                        "text": "This definition also includes generative grammars of the type described by Chomsky (1957) above."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 84
                            }
                        ],
                        "text": "Acceptability judgments are central to the formulation of generative linguistics in Chomsky\u2019s influential (1957) book Syntactic Structures."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Syntactic Structures"
            },
            "venue": {
                "fragments": [],
                "text": "Mouton."
            },
            "year": 1957
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 26
                            }
                        ],
                        "text": "This practice aligns with Chomsky\u2019s (1957) definition of grammaticality as the binary notion of membership in a set of wellformed strings. However, Lau et al. (2016) find that when speakers are presented with the option to use a gradient scale to report sentence acceptability, they predictably and systematically use the full scale, rather than clustering their judgments near the extremes as would be expected for a fundamentally binary phenomenon."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 0
                            }
                        ],
                        "text": "Chomsky\u2019s (1957) methodology merely allows linguists to claim that their theories are empirically adequate13."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 270,
                                "start": 240
                            }
                        ],
                        "text": "Acceptability judgments like these are the primary source of empirical data in much of theoretical linguistics, with the objective of a generative grammar being to generate all and only those sentences which native speakers find acceptable (Chomsky, 1957; Sch\u00fctze, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 107
                            }
                        ],
                        "text": "This has been the predominant methodology for research in generative linguistics over the last sixty years (Chomsky, 1957; Sch\u00fctze, 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 26
                            }
                        ],
                        "text": "This practice aligns with Chomsky\u2019s (1957) definition of grammaticality as the binary notion of membership in a set of wellformed strings."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 75
                            }
                        ],
                        "text": "This definition also includes generative grammars of the type described by Chomsky (1957) above."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 84
                            }
                        ],
                        "text": "Acceptability judgments are central to the formulation of generative linguistics in Chomsky\u2019s influential (1957) book Syntactic Structures."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Syntactic Structures"
            },
            "venue": {
                "fragments": [],
                "text": "Mouton."
            },
            "year": 1957
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 222,
                                "start": 0
                            }
                        ],
                        "text": "Baltin (1982) 1 Would John hate that? Baltin (1982) 0 Who do you think that will question Seamus first? Carnie (2013) 0 Usually, any lion is majestic. Dayal (1998) 1 The gardener planted roses in the garden. Miller (2002) 1 I wrote Blair a letter, but I tore it up before I sent it."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 0
                            }
                        ],
                        "text": "Baltin (1982) 1 Would John hate that? Baltin (1982) 0 Who do you think that will question Seamus first? Carnie (2013) 0 Usually, any lion is majestic. Dayal (1998) 1 The gardener planted roses in the garden."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 316,
                                "start": 0
                            }
                        ],
                        "text": "Baltin (1982) 1 Would John hate that? Baltin (1982) 0 Who do you think that will question Seamus first? Carnie (2013) 0 Usually, any lion is majestic. Dayal (1998) 1 The gardener planted roses in the garden. Miller (2002) 1 I wrote Blair a letter, but I tore it up before I sent it. Rappaport Hovav and Levin (2008) 1 That\u2019s the kindest answer that I ever heard."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 0
                            }
                        ],
                        "text": "Baltin (1982) 1 Would John hate that? Baltin (1982) 0 Who do you think that will question Seamus first? Carnie (2013) 0 Usually, any lion is majestic."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A landing site theory of movement rules"
            },
            "venue": {
                "fragments": [],
                "text": "Linguistic Inquiry, 13(1):1\u201338."
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1766112"
                        ],
                        "name": "R. Jackendoff",
                        "slug": "R.-Jackendoff",
                        "structuredName": {
                            "firstName": "Ray",
                            "lastName": "Jackendoff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Jackendoff"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "67) 1029 61.8 Islands Sag et al. (1985) 153 68.6 Coordination Sportiche et al. (2013) 651 70.4 Syntax Textbook In-Domain 9515 71.3 Chung et al. (1995) 148 66.9 Sluicing Collins (2005) 66 68.2 Passive Jackendoff (1971) 94 67.0 Gapping Sag (1997) 112 57.1 Relative clauses Sag et al. (2003) 460 70.9 Syntax Textbook Williams (1980) 169 76.3 Predication Out-of-Domain 1049 69.2 Total 10657 70.5 Table 2: The contents of "
                    },
                    "intents": []
                }
            ],
            "corpusId": 118484235,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "98264b6e63ca2048291d454d97a5108619f73b58",
            "isKey": true,
            "numCitedBy": 203,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Gapping-and-related-rules-Jackendoff",
            "title": {
                "fragments": [],
                "text": "Gapping and related rules"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1970
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4878512"
                        ],
                        "name": "Dominique Sportiche",
                        "slug": "Dominique-Sportiche",
                        "structuredName": {
                            "firstName": "Dominique",
                            "lastName": "Sportiche",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dominique Sportiche"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34438764"
                        ],
                        "name": "H. Koopman",
                        "slug": "H.-Koopman",
                        "structuredName": {
                            "firstName": "Hilda",
                            "lastName": "Koopman",
                            "middleNames": [
                                "J"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Koopman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31951784"
                        ],
                        "name": "Edward P. Stabler",
                        "slug": "Edward-P.-Stabler",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Stabler",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Edward P. Stabler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": ") 1459 69.0 Verb alternations Miller (2002) 426 84.5 Syntax Textbook Rappaport Hovav and Levin (2008) 151 69.5 Dative alternation Ross (1967) 1029 61.8 Islands Sag et al. (1985) 153 68.6 Coordination Sportiche et al. (2013) 651 70.4 Syntax Textbook In-Domain 9515 71.3 Chung et al. (1995) 148 66.9 Sluicing Collins (2005) 66 68.2 Passive Jackendoff (1971) 94 67.0 Gapping Sag (1997) 112 57.1 Relative clauses Sag et al. (20"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 117004543,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "49a93d9ebc0b7fa490def247aa21086fa5be307b",
            "isKey": true,
            "numCitedBy": 41,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "An-Introduction-to-Syntactic-Analysis-and-Theory-Sportiche-Koopman",
            "title": {
                "fragments": [],
                "text": "An Introduction to Syntactic Analysis and Theory"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2013
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 61
                            }
                        ],
                        "text": "It also uses contexualized word embeddings inspired by ELMo (Peters et al., 2018)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 79
                            }
                        ],
                        "text": "(ii) We train ELMo-style contextualized word embeddings, which, following ELMo (Peters et al., 2018), represent wi as a linear combination of the hidden states hji for each layer j in an LSTM LM, though we depart from the original paper by using only a forward LM."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 80
                            }
                        ],
                        "text": "(ii) We train ELMo-style contextualized word embeddings, which, following ELMo (Peters et al., 2018), represent wi as a linear combination of the hidden states hji for each layer j in an LSTM LM, though we depart from the original paper by using only a forward LM. (iii) We also use the pretrained\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 44
                            }
                        ],
                        "text": "This finding aligns with the conclusions of Peters et al. (2018)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Deep contextualizedword representations"
            },
            "venue": {
                "fragments": [],
                "text": "InProceedingsof the 2018 Conference of the North American Chapter"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "90061799"
                        ],
                        "name": "M. Baltin",
                        "slug": "M.-Baltin",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Baltin",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Baltin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49353854"
                        ],
                        "name": "Chris Collins",
                        "slug": "Chris-Collins",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris Collins"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 124509876,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "37428457331fa780e5ae2995460cbbd37ab89f12",
            "isKey": false,
            "numCitedBy": 32,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Handbook-of-Contemporary-Syntactic-Theory-Baltin-Collins",
            "title": {
                "fragments": [],
                "text": "Handbook of Contemporary Syntactic Theory"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "corpusId": 9205666,
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Coordination and How to Distinguish Categories*"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 243283336,
            "fieldsOfStudy": [],
            "id": "1c03f985e4f3e24c10ea129dbc3effacc863fe86",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Any",
            "title": {
                "fragments": [],
                "text": "Any"
            },
            "venue": {
                "fragments": [],
                "text": "Definitions"
            },
            "year": 2020
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 74
                            }
                        ],
                        "text": "By way of illustration, consider the three largest sources in\nthe corpus: Kim & Sells (2008) is a recent undergraduate syntax textbook, Levin (1993) is a comprehensive reference detailing the lexical properties of thousands of verbs, and Ross (1967) is an influential dissertation focusing on\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": ". Jackendoff . 1971 . Gapping and related rules . Linguistic Inquiry , 2 ( 1 ) : 21 \u2013 35 . Nirit Kadmon and Fred Landman . 1993 . Any"
            },
            "venue": {
                "fragments": [],
                "text": "Linguistics and Philosophy"
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 47
                            }
                        ],
                        "text": "In his foundational work on generative syntax, Chomsky (1957) defines an empirically adequate grammar of a language L as one which generates all and only those strings of L which native speakers of L judge to be acceptable."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 141
                            }
                        ],
                        "text": "Acceptability judgments like these are the primary behavioral measure that generative linguists use to observe humans\u2019 grammatical knowledge (Chomsky, 1957; Sch\u00fctze, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Syntactic Structures. Mouton"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1957
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 105
                            }
                        ],
                        "text": "In similar low-resource settings, transfer learning with sentence embeddings has proven to be effective (Kiros et al., 2015; Conneau et al., 2017)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Antonio Torralba, and Sanja Fidler"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing Systems"
            },
            "year": 2015
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Gapping and related rules. Linguistic Inquiry"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1971
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Syntactic Theory: A Formal Introduction, 2nd ed"
            },
            "venue": {
                "fragments": [],
                "text": "CSLI Publications."
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 127
                            }
                        ],
                        "text": "It is also increasingly used in computational linguistics (Linzen et al., 2016; Marvin and Linzen, 2018; Futrell et al., 2018; Wilcox et al., 2018, 2019)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "GLUE : Amulti - task benchmark and analysis platform for natural language understand"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2018 EMNLP Workshop BlackboxNLP : Analyzing and Inter - pretingNeuralNetworks forNLP"
            },
            "year": 2018
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Constraints on Variables in Syntax . Ph . D . thesis , MIT . Ivan A . Sag . 1997 . English relative clause constructions"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Linguistics"
            },
            "year": 1967
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Syntactic Theory: A Formal Introduction, 2 edition"
            },
            "venue": {
                "fragments": [],
                "text": "CSLI Publications."
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 47
                            }
                        ],
                        "text": "In his foundational work on generative syntax, Chomsky (1957) defines an empirically adequate grammar of a language L as one which generates all and only those strings of L which native speakers of L judge to be acceptable."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 141
                            }
                        ],
                        "text": "Acceptability judgments like these are the primary behavioral measure that generative linguists use to observe humans\u2019 grammatical knowledge (Chomsky, 1957; Sch\u00fctze, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Syntactic Structures. Mouton. Noam Chomsky"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1957
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Any as inherently modal. Linguistics and Philosophy"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 28,
            "methodology": 16,
            "result": 3
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 78,
        "totalPages": 8
    },
    "page_url": "https://www.semanticscholar.org/paper/Neural-Network-Acceptability-Judgments-Warstadt-Singh/cb0f3ee1e98faf92429d601cdcd76c69c1e484eb?sort=total-citations"
}