{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49708132"
                        ],
                        "name": "M. Siddiqui",
                        "slug": "M.-Siddiqui",
                        "structuredName": {
                            "firstName": "Matheen",
                            "lastName": "Siddiqui",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Siddiqui"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3463966"
                        ],
                        "name": "G. Medioni",
                        "slug": "G.-Medioni",
                        "structuredName": {
                            "firstName": "G\u00e9rard",
                            "lastName": "Medioni",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Medioni"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 26
                            }
                        ],
                        "text": "The second intermediate representation is a list of joint hypotheses Jt, which \ncontains triples of (body part, 3D position, confidence) hypotheses, with say five hypothe\u00adses per body \npart."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 96
                            }
                        ],
                        "text": "Reprojecting the inferred parts into world space, we \nlocal\u00adize spatial modes of each part distribution and thus gener\u00adate (possibly several) confidence-weighted \nproposals for the 3D locations of each skeletal joint."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11627883,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cfb814a806d7b03ef606b5776a2d0849343bee14",
            "isKey": false,
            "numCitedBy": 103,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We estimate and track articulated human poses in sequences from a single view, real-time range sensor. We use a data driven MCMC approach to find an optimal pose based on a likelihood that compares synthesized depth images to the observed depth image. To speed up convergence of this search, we make use of bottom up detectors that generate candidate head, hand and forearm locations. Our Markov chain dynamics explore solutions about these parts and thus combine bottom up and top down processing. The current performance is 10 frames per second. We provide quantitative performance evaluation using hand annotated data. We demonstrate significant improvement over a baseline ICP approach. This algorithm is then adapted to estimate the specific shape parameters of subjects for use in tracking. In particular, limb dimensions are included in the human pose parametrization and are automatically estimated for each subject in short training sequences. Tracking performance is quantitatively evaluated using these person specific trained models."
            },
            "slug": "Human-pose-estimation-from-a-single-view-point,-Siddiqui-Medioni",
            "title": {
                "fragments": [],
                "text": "Human pose estimation from a single view point, real-time range sensor"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "This work uses a data driven MCMC approach to find an optimal pose based on a likelihood that compares synthesized depth images to the observed depth image, and makes use of bottom up detectors that generate candidate head, hand and forearm locations to speed up convergence."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Workshops"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1795065"
                        ],
                        "name": "C. Plagemann",
                        "slug": "C.-Plagemann",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Plagemann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Plagemann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1981419"
                        ],
                        "name": "Varun Ganapathi",
                        "slug": "Varun-Ganapathi",
                        "structuredName": {
                            "firstName": "Varun",
                            "lastName": "Ganapathi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Varun Ganapathi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736370"
                        ],
                        "name": "D. Koller",
                        "slug": "D.-Koller",
                        "structuredName": {
                            "firstName": "Daphne",
                            "lastName": "Koller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Koller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144867807"
                        ],
                        "name": "S. Thrun",
                        "slug": "S.-Thrun",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Thrun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Thrun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16897742,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6d7c5db7c9cc523115f80087743b643246a6c4d9",
            "isKey": false,
            "numCitedBy": 333,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "We deal with the problem of detecting and identifying body parts in depth images at video frame rates. Our solution involves a novel interest point detector for mesh and range data that is particularly well suited for analyzing human shape. The interest points, which are based on identifying geodesic extrema on the surface mesh, coincide with salient points of the body, which can be classified as, e.g., hand, foot or head using local shape descriptors. Our approach also provides a natural way of estimating a 3D orientation vector for a given interest point. This can be used to normalize the local shape descriptors to simplify the classification problem as well as to directly estimate the orientation of body parts in space. Experiments involving ground truth labels acquired via an active motion capture system show that our interest points in conjunction with a boosted patch classifier are significantly better in detecting body parts in depth images than state-of-the-art sliding-window based detectors."
            },
            "slug": "Real-time-identification-and-localization-of-body-Plagemann-Ganapathi",
            "title": {
                "fragments": [],
                "text": "Real-time identification and localization of body parts from depth images"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Experiments show that the interest points in conjunction with a boosted patch classifier are significantly better in detecting body parts in depth images than state-of-the-art sliding-window based detectors."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE International Conference on Robotics and Automation"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1769383"
                        ],
                        "name": "Lubomir D. Bourdev",
                        "slug": "Lubomir-D.-Bourdev",
                        "structuredName": {
                            "firstName": "Lubomir",
                            "lastName": "Bourdev",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lubomir D. Bourdev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 64
                            }
                        ],
                        "text": "Further, results on silhouette images suggest more general applicability of our approach."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9320620,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "55b29a2505149d06d8c1d616cd30edca40cb029c",
            "isKey": false,
            "numCitedBy": 1048,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the classic problems of detection, segmentation and pose estimation of people in images with a novel definition of a part, a poselet. We postulate two criteria (1) It should be easy to find a poselet given an input image (2) it should be easy to localize the 3D configuration of the person conditioned on the detection of a poselet. To permit this we have built a new dataset, H3D, of annotations of humans in 2D photographs with 3D joint information, inferred using anthropometric constraints. This enables us to implement a data-driven search procedure for finding poselets that are tightly clustered in both 3D joint configuration space as well as 2D image appearance. The algorithm discovers poselets that correspond to frontal and profile faces, pedestrians, head and shoulder views, among others. Each poselet provides examples for training a linear SVM classifier which can then be run over the image in a multiscale scanning mode. The outputs of these poselet detectors can be thought of as an intermediate layer of nodes, on top of which one can run a second layer of classification or regression. We show how this permits detection and localization of torsos or keypoints such as left shoulder, nose, etc. Experimental results show that we obtain state of the art performance on people detection in the PASCAL VOC 2007 challenge, among other datasets. We are making publicly available both the H3D dataset as well as the poselet parameters for use by other researchers."
            },
            "slug": "Poselets:-Body-part-detectors-trained-using-3D-pose-Bourdev-Malik",
            "title": {
                "fragments": [],
                "text": "Poselets: Body part detectors trained using 3D human pose annotations"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "A new dataset, H3D, is built of annotations of humans in 2D photographs with 3D joint information, inferred using anthropometric constraints, to address the classic problems of detection, segmentation and pose estimation of people in images with a novel definition of a part, a poselet."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE 12th International Conference on Computer Vision"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1963230"
                        ],
                        "name": "Huazhong Ning",
                        "slug": "Huazhong-Ning",
                        "structuredName": {
                            "firstName": "Huazhong",
                            "lastName": "Ning",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huazhong Ning"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143836295"
                        ],
                        "name": "W. Xu",
                        "slug": "W.-Xu",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144768792"
                        ],
                        "name": "Yihong Gong",
                        "slug": "Yihong-Gong",
                        "structuredName": {
                            "firstName": "Yihong",
                            "lastName": "Gong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yihong Gong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153652752"
                        ],
                        "name": "Thomas S. Huang",
                        "slug": "Thomas-S.-Huang",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Huang",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas S. Huang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1496310,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fa988a20e88bf658a5ceb836f9ea786aa5a7b0ff",
            "isKey": false,
            "numCitedBy": 85,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper addresses the problem of recovering 3D human pose from a single monocular image, using a discriminative bag-of-words approach. In previous work, the visual words are learned by unsupervised clustering algorithms. They capture the most common patterns and are good features for coarse-grain recognition tasks like object classification. But for those tasks which deal with subtle differences such as pose estimation, such representation may lack the needed discriminative power. In this paper, we propose to jointly learn the visual words and the pose regressors in a supervised manner. More specifically, we learn an individual distance metric for each visual word to optimize the pose estimation performance. The learned metrics rescale the visual words to suppress unimportant dimensions such as those corresponding to background. Another contribution is that we design an appearance and position context (APC) local descriptor that achieves both selectivity and invariance while requiring no background subtraction. We test our approach on both a quasi-synthetic dataset and a real dataset (HumanEva) to verify its effectiveness. Our approach also achieves fast computational speed thanks to the integral histograms used in APC descriptor extraction and fast inference of pose regressors."
            },
            "slug": "Discriminative-learning-of-visual-words-for-3D-pose-Ning-Xu",
            "title": {
                "fragments": [],
                "text": "Discriminative learning of visual words for 3D human pose estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper proposes to jointly learn the visual words and the pose regressors in a supervised manner to optimize the pose estimation performance, and designs an appearance and position context (APC) local descriptor that achieves both selectivity and invariance while requiring no background subtraction."
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3321919"
                        ],
                        "name": "Gr\u00e9gory Rogez",
                        "slug": "Gr\u00e9gory-Rogez",
                        "structuredName": {
                            "firstName": "Gr\u00e9gory",
                            "lastName": "Rogez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gr\u00e9gory Rogez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1856452"
                        ],
                        "name": "Jonathan Rihan",
                        "slug": "Jonathan-Rihan",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Rihan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Rihan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145686644"
                        ],
                        "name": "S. Ramalingam",
                        "slug": "S.-Ramalingam",
                        "structuredName": {
                            "firstName": "Srikumar",
                            "lastName": "Ramalingam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ramalingam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398367150"
                        ],
                        "name": "C. Orrite-Uru\u00f1uela",
                        "slug": "C.-Orrite-Uru\u00f1uela",
                        "structuredName": {
                            "firstName": "Carlos",
                            "lastName": "Orrite-Uru\u00f1uela",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Orrite-Uru\u00f1uela"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143635540"
                        ],
                        "name": "Philip H. S. Torr",
                        "slug": "Philip-H.-S.-Torr",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Torr",
                            "middleNames": [
                                "H.",
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philip H. S. Torr"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "Even without exploiting temporal or \nkine\u00admatic constraints, the 3D joint proposals are both accu\u00adrate and stable."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13351972,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4ab12d3270214cc1c327260ac0b26be544923bd8",
            "isKey": false,
            "numCitedBy": 178,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper addresses human pose recognition from video sequences by formulating it as a classification problem. Unlike much previous work we do not make any assumptions on the availability of clean segmentation. The first step of this work consists in a novel method of aligning the training images using 3D Mocap data. Next we define classes by discretizing a 2D manifold whose two dimensions are camera viewpoint and actions. Our main contribution is a pose detection algorithm based on random forests. A bottom-up approach is followed to build a decision tree by recursively clustering and merging the classes at each level. For each node of the decision tree we build a list of potentially discriminative features using the alignment of training images; in this paper we consider Histograms of Orientated Gradient (HOG). We finally grow an ensemble of trees by randomly sampling one of the selected HOG blocks at each node. Our proposed approach gives promising results with both fixed and moving cameras."
            },
            "slug": "Randomized-trees-for-human-pose-detection-Rogez-Rihan",
            "title": {
                "fragments": [],
                "text": "Randomized trees for human pose detection"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "This paper addresses human pose recognition from video sequences by formulating it as a classification problem and considers Histograms of Orientated Gradient (HOG), a pose detection algorithm based on random forests."
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2254686"
                        ],
                        "name": "Youding Zhu",
                        "slug": "Youding-Zhu",
                        "structuredName": {
                            "firstName": "Youding",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Youding Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35047644"
                        ],
                        "name": "K. Fujimura",
                        "slug": "K.-Fujimura",
                        "structuredName": {
                            "firstName": "Kikuo",
                            "lastName": "Fujimura",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Fujimura"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 21
                            }
                        ],
                        "text": "The second intermediate representation is a list of joint hypotheses Jt, which \ncontains triples of (body part, 3D position, confidence) hypotheses, with say five hypothe\u00adses per body \npart."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 68
                            }
                        ],
                        "text": "An object recognition \nalgorithm is trained to recognize these parts, so that at run time, a single input depth image is segmented \ninto a dense probabilistic body parts labeling."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 44429455,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0a02b604fa2508765eda92be4b9d50a6b778921a",
            "isKey": false,
            "numCitedBy": 64,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "A new 2-step method is presented for human upper-body pose estimation from depth sequences, in which coarse human part labeling takes place first, followed by more precise joint position estimation as the second phase. In the first step, a number of constraints are extracted from notable image features such as the head and torso. The problem of pose estimation is cast as that of label assignment with these constraints. Major parts of the human upper body are labeled by this process. The second step estimates joint positions optimally based on kinematic constraints using dense correspondences between depth profile and human model parts. The proposed framework is shown to overcome some issues of existing approaches for human pose tracking using similar types of data streams. Performance comparison with motion capture data is presented to demonstrate the accuracy of our approach."
            },
            "slug": "Constrained-Optimization-for-Human-Pose-Estimation-Zhu-Fujimura",
            "title": {
                "fragments": [],
                "text": "Constrained Optimization for Human Pose Estimation from Depth Sequences"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A new 2-step method is presented for human upper-body pose estimation from depth sequences, in which coarse human part labeling takes place first, followed by more precise joint position estimation as the second phase."
            },
            "venue": {
                "fragments": [],
                "text": "ACCV"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145984136"
                        ],
                        "name": "A. Agarwal",
                        "slug": "A.-Agarwal",
                        "structuredName": {
                            "firstName": "Ankur",
                            "lastName": "Agarwal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Agarwal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756114"
                        ],
                        "name": "B. Triggs",
                        "slug": "B.-Triggs",
                        "structuredName": {
                            "firstName": "Bill",
                            "lastName": "Triggs",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Triggs"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 302682,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3845d9e62540b8e2406343f801e026b562299ae0",
            "isKey": false,
            "numCitedBy": 464,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a learning based method for recovering 3D human body pose from single images and monocular image sequences. Our approach requires neither an explicit body model nor prior labelling of body pans in the image. Instead, it recovers pose by direct nonlinear regression against shape descriptor vectors extracted automatically from image silhouettes. For robustness against local silhouette segmentation errors, silhouette shape is encoded by histogram-of-shape-contexts descriptors. For the main regression, we evaluate both regularized least squares and relevance vector machine (RVM) regressors over both linear and kernel bases. The RVM's provide much sparser regressors without compromising performance, and kernel bases give a small but worthwhile improvement in performance. For realism and good generalization with respect to viewpoints, we train the regressors on images resynthesized from real human motion capture data, and test it both quantitatively on similar independent test data, and qualitatively on a real image sequence. Mean angular errors of 6-7 degrees are obtained - a factor of 3 better than the current state of the art for the much simpler upper body problem."
            },
            "slug": "3D-human-pose-from-silhouettes-by-relevance-vector-Agarwal-Triggs",
            "title": {
                "fragments": [],
                "text": "3D human pose from silhouettes by relevance vector regression"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "This work describes a learning based method for recovering 3D human body pose by direct nonlinear regression against shape descriptor vectors extracted automatically from image silhouettes, and results are a factor of 3 better than the current state of the art for the much simpler upper body problem."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1981419"
                        ],
                        "name": "Varun Ganapathi",
                        "slug": "Varun-Ganapathi",
                        "structuredName": {
                            "firstName": "Varun",
                            "lastName": "Ganapathi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Varun Ganapathi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1795065"
                        ],
                        "name": "C. Plagemann",
                        "slug": "C.-Plagemann",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Plagemann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Plagemann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736370"
                        ],
                        "name": "D. Koller",
                        "slug": "D.-Koller",
                        "structuredName": {
                            "firstName": "Daphne",
                            "lastName": "Koller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Koller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144867807"
                        ],
                        "name": "S. Thrun",
                        "slug": "S.-Thrun",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Thrun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Thrun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15596138,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cc20ef4c38405279150dde33865dacfc74275aa9",
            "isKey": false,
            "numCitedBy": 408,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Markerless tracking of human pose is a hard yet relevant problem. In this paper, we derive an efficient filtering algorithm for tracking human pose using a stream of monocular depth images. The key idea is to combine an accurate generative model \u2014 which is achievable in this setting using programmable graphics hardware \u2014 with a discriminative model that provides data-driven evidence about body part locations. In each filter iteration, we apply a form of local model-based search that exploits the nature of the kinematic chain. As fast movements and occlusion can disrupt the local search, we utilize a set of discriminatively trained patch classifiers to detect body parts. We describe a novel algorithm for propagating this noisy evidence about body part locations up the kinematic chain using the un-scented transform. The resulting distribution of body configurations allows us to reinitialize the model-based search. We provide extensive experimental results on 28 real-world sequences using automatic ground-truth annotations from a commercial motion capture system."
            },
            "slug": "Real-time-motion-capture-using-a-single-camera-Ganapathi-Plagemann",
            "title": {
                "fragments": [],
                "text": "Real time motion capture using a single time-of-flight camera"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "This paper derives an efficient filtering algorithm for tracking human pose using a stream of monocular depth images and describes a novel algorithm for propagating noisy evidence about body part locations up the kinematic chain using the un-scented transform."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "88836465"
                        ],
                        "name": "R. Okada",
                        "slug": "R.-Okada",
                        "structuredName": {
                            "firstName": "Ryuzo",
                            "lastName": "Okada",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Okada"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715959"
                        ],
                        "name": "Stefano Soatto",
                        "slug": "Stefano-Soatto",
                        "structuredName": {
                            "firstName": "Stefano",
                            "lastName": "Soatto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stefano Soatto"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17223595,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b7b23814948afc5525975ed44f3dd247100e6722",
            "isKey": false,
            "numCitedBy": 90,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the problem of estimating human body pose from a single image with cluttered background. We train multiple local linear regressors for estimating the 3D pose from a feature vector of gradient orientation histograms. Each linear regressor is capable of selecting relevant components of the feature vector depending on pose by training it on a pose cluster which is a subset of the training samples with similar pose. For discriminating the pose clusters, we use kernel Support Vector Machines (SVM) with pose-dependent feature selection. We achieve feature selection for kernel SVMs by estimating scale parameters of RBF kernel through minimization of the radius/margin bound, which is an upper bound of the expected generalization error, with efficient gradient descent. Human detection is also possible with these SVMs. Quantitative experiments show the effectiveness of pose-dependent feature selection to both human detection and pose estimation."
            },
            "slug": "Relevant-Feature-Selection-for-Human-Pose-and-in-Okada-Soatto",
            "title": {
                "fragments": [],
                "text": "Relevant Feature Selection for Human Pose Estimation and Localization in Cluttered Images"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "This work trains multiple local linear regressors for estimating the 3D pose from a feature vector of gradient orientation histograms and achieves feature selection for kernel SVMs by estimating scale parameters of RBF kernel through minimization of the radius/margin bound."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10771328"
                        ],
                        "name": "Greg Mori",
                        "slug": "Greg-Mori",
                        "structuredName": {
                            "firstName": "Greg",
                            "lastName": "Mori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Greg Mori"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "For training data, we generate realistic synthetic depth images of humans of many shapes and \nsizes in highly varied poses sampled from a large motion capture database."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10716734,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9cc1b4ca121fef59517f24863b113bce3e5acd1a",
            "isKey": false,
            "numCitedBy": 429,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem we consider in this paper is to take a single two-dimensional image containing a human body, locate the joint positions, and use these to estimate the body configuration and pose in three-dimensional space. The basic approach is to store a number of exemplar 2D views of the human body in a variety of different configurations and viewpoints with respect to the camera. On each of these stored views, the locations of the body joints (left elbow, right knee, etc.) are manually marked and labelled for future use. The test shape is then matched to each stored view, using the technique of shape context matching in conjunction with a kinematic chain-based deformation model. Assuming that there is a stored view sufficiently similar in configuration and pose, the correspondence process will succeed. The locations of the body joints are then transferred from the exemplar view to the test shape. Given the joint locations, the 3D body configuration and pose are then estimated. We can apply this technique to video by treating each frame independently - tracking just becomes repeated recognition! We present results on a variety of datasets."
            },
            "slug": "Estimating-Human-Body-Configurations-Using-Shape-Mori-Malik",
            "title": {
                "fragments": [],
                "text": "Estimating Human Body Configurations Using Shape Context Matching"
            },
            "tldr": {
                "abstractSimilarityScore": 87,
                "text": "The problem is to take a single two-dimensional image containing a human body, locate the joint positions, and use these to estimate the body configuration and pose in three-dimensional space."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054165706"
                        ],
                        "name": "S. Ioffe",
                        "slug": "S.-Ioffe",
                        "structuredName": {
                            "firstName": "Sergey",
                            "lastName": "Ioffe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ioffe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144016256"
                        ],
                        "name": "D. Forsyth",
                        "slug": "D.-Forsyth",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Forsyth",
                            "middleNames": [
                                "Alexander"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Forsyth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 37
                            }
                        ],
                        "text": "For training data, we generate realistic synthetic depth images of humans of many shapes and \nsizes in highly varied poses sampled from a large motion capture database."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14097182,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "31635ba161c6cea677f87a88d9874e5506819207",
            "isKey": false,
            "numCitedBy": 257,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "Finding people in pictures presents a particularly difficult object recognition problem. We show how to find people by finding candidate body segments, and then constructing assemblies of segments that are consistent with the constraints on the appearance of a person that result from kinematic properties. Since a reasonable model of a person requires at least nine segments, it is not possible to inspect every group, due to the huge combinatorial complexity.We propose two approaches to this problem. In one, the search can be pruned by using projected versions of a classifier that accepts groups corresponding to people. We describe an efficient projection algorithm for one popular classifier, and demonstrate that our approach can be used to determine whether images of real scenes contain people.The second approach employs a probabilistic framework, so that we can draw samples of assemblies, with probabilities proportional to their likelihood, which allows to draw human-like assemblies more often than the non-person ones. The main performance problem is in segmentation of images, but the overall results of both approaches on real images of people are encouraging."
            },
            "slug": "Probabilistic-Methods-for-Finding-People-Ioffe-Forsyth",
            "title": {
                "fragments": [],
                "text": "Probabilistic Methods for Finding People"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "This work shows how to find people by finding candidate body segments, and then constructing assemblies of segments that are consistent with the constraints on the appearance of a person that result from kinematic properties, using an efficient projection algorithm for one popular classifier."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1838674"
                        ],
                        "name": "Dragomir Anguelov",
                        "slug": "Dragomir-Anguelov",
                        "structuredName": {
                            "firstName": "Dragomir",
                            "lastName": "Anguelov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dragomir Anguelov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685978"
                        ],
                        "name": "B. Taskar",
                        "slug": "B.-Taskar",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Taskar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Taskar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2482074"
                        ],
                        "name": "Vassil Chatalbashev",
                        "slug": "Vassil-Chatalbashev",
                        "structuredName": {
                            "firstName": "Vassil",
                            "lastName": "Chatalbashev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vassil Chatalbashev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736370"
                        ],
                        "name": "D. Koller",
                        "slug": "D.-Koller",
                        "structuredName": {
                            "firstName": "Daphne",
                            "lastName": "Koller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Koller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35195064"
                        ],
                        "name": "D. Gupta",
                        "slug": "D.-Gupta",
                        "structuredName": {
                            "firstName": "Dinkar",
                            "lastName": "Gupta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Gupta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728179"
                        ],
                        "name": "G. Heitz",
                        "slug": "G.-Heitz",
                        "structuredName": {
                            "firstName": "Geremy",
                            "lastName": "Heitz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Heitz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 79
                            }
                        ],
                        "text": "Starting with a 3D surface model of a \ngeneric human body, the surface is divided into 31 distinct body parts (Section 3.1)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8396595,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "55a5e1a4e0068a4f2a8a8bdfbd777c249110ccfe",
            "isKey": true,
            "numCitedBy": 419,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the problem of segmenting 3D scan data into objects or object classes. Our segmentation framework is based on a subclass of Markov random fields (MRFs) which support efficient graph-cut inference. The MRF models incorporate a large set of diverse features and enforce the preference that adjacent scan points have the same classification label. We use a recently proposed maximum-margin framework to discriminatively train the model from a set of labeled scans; as a result we automatically learn the relative importance of the features for the segmentation task. Performing graph-cut inference in the trained MRF can then be used to segment new scenes very efficiently. We test our approach on three large-scale datasets produced by different kinds of 3D sensors, showing its applicability to both outdoor and indoor environments containing diverse objects."
            },
            "slug": "Discriminative-learning-of-Markov-random-fields-for-Anguelov-Taskar",
            "title": {
                "fragments": [],
                "text": "Discriminative learning of Markov random fields for segmentation of 3D scan data"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "This work addresses the problem of segmenting 3D scan data into objects or object classes by using a recently proposed maximum-margin framework to discriminatively train the model from a set of labeled scans and automatically learn the relative importance of the features for the segmentation task."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689738"
                        ],
                        "name": "V. Lepetit",
                        "slug": "V.-Lepetit",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Lepetit",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Lepetit"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2969681"
                        ],
                        "name": "Pascal Lagger",
                        "slug": "Pascal-Lagger",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Lagger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pascal Lagger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717736"
                        ],
                        "name": "P. Fua",
                        "slug": "P.-Fua",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Fua",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Fua"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11600779,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d0faf378ca4d6227b47bb36254a57f7ceeb0c566",
            "isKey": false,
            "numCitedBy": 514,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "In earlier work, we proposed treating wide baseline matching of feature points as a classification problem, in which each class corresponds to the set of all possible views of such a point. We used a K-mean plus Nearest Neighbor classifier to validate our approach, mostly because it was simple to implement. It has proved effective but still too slow for real-time use. In this paper, we advocate instead the use of randomized trees as the classification technique. It is both fast enough for real-time performance and more robust. It also gives us a principled way not only to match keypoints but to select during a training phase those that are the most recognizable ones. This results in a real-time system able to detect and position in 3D planar, non-planar, and even deformable objects. It is robust to illuminations changes, scale changes and occlusions."
            },
            "slug": "Randomized-trees-for-real-time-keypoint-recognition-Lepetit-Lagger",
            "title": {
                "fragments": [],
                "text": "Randomized trees for real-time keypoint recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper advocates the use of randomized trees as the classification technique, which is both fast enough for real-time performance and more robust, and gives a principled way not only to match keypoints but to select during a training phase those that are the most recognizable ones."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143774737"
                        ],
                        "name": "J. Shotton",
                        "slug": "J.-Shotton",
                        "structuredName": {
                            "firstName": "Jamie",
                            "lastName": "Shotton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shotton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33652486"
                        ],
                        "name": "J. Winn",
                        "slug": "J.-Winn",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Winn",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Winn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756036"
                        ],
                        "name": "C. Rother",
                        "slug": "C.-Rother",
                        "structuredName": {
                            "firstName": "Carsten",
                            "lastName": "Rother",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Rother"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716777"
                        ],
                        "name": "A. Criminisi",
                        "slug": "A.-Criminisi",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Criminisi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Criminisi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6075144,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fb444dc25bab36a8e273ed654d49e3841905e5af",
            "isKey": false,
            "numCitedBy": 1349,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a new approach to learning a discriminative model of object classes, incorporating appearance, shape and context information efficiently. The learned model is used for automatic visual recognition and semantic segmentation of photographs. Our discriminative model exploits novel features, based on textons, which jointly model shape and texture. Unary classification and feature selection is achieved using shared boosting to give an efficient classifier which can be applied to a large number of classes. Accurate image segmentation is achieved by incorporating these classifiers in a conditional random field. Efficient training of the model on very large datasets is achieved by exploiting both random feature selection and piecewise training methods. \n \nHigh classification and segmentation accuracy are demonstrated on three different databases: i) our own 21-object class database of photographs of real objects viewed under general lighting conditions, poses and viewpoints, ii) the 7-class Corel subset and iii) the 7-class Sowerby database used in [1]. The proposed algorithm gives competitive results both for highly textured (e.g. grass, trees), highly structured (e.g. cars, faces, bikes, aeroplanes) and articulated objects (e.g. body, cow)."
            },
            "slug": "TextonBoost:-Joint-Appearance,-Shape-and-Context-Shotton-Winn",
            "title": {
                "fragments": [],
                "text": "TextonBoost: Joint Appearance, Shape and Context Modeling for Multi-class Object Recognition and Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "A new approach to learning a discriminative model of object classes, incorporating appearance, shape and context information efficiently, is proposed, which is used for automatic visual recognition and semantic segmentation of photographs."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2422559"
                        ],
                        "name": "R. Urtasun",
                        "slug": "R.-Urtasun",
                        "structuredName": {
                            "firstName": "Raquel",
                            "lastName": "Urtasun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Urtasun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753210"
                        ],
                        "name": "Trevor Darrell",
                        "slug": "Trevor-Darrell",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Darrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Darrell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9981819,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ef9b6db5e02f1db18069cf8f42cd39c16446a740",
            "isKey": false,
            "numCitedBy": 234,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Discriminative approaches to human pose inference involve mapping visual observations to articulated body configurations. Current probabilistic approaches to learn this mapping have been limited in their ability to handle domains with a large number of activities that require very large training sets. We propose an online probabilistic regression scheme for efficient inference of complex, high- dimensional, and multimodal mappings. Our technique is based on a local mixture of Gaussian processes, where locality is defined based on both appearance and pose, and where the mapping hyperparameters can vary across local neighborhoods to better adapt to specific regions in the pose space. The mixture components are defined online in very small neighborhoods, so learning and inference is extremely efficient. When the mapping is one-to-one, we derive a bound on the approximation error of local regression (vs. global regression) for monotonically decreasing co- variance functions. Our method can determine when training examples are redundant given the rest of the database, and use this criteria for pruning. We report results on synthetic (Poser) and real (Humaneva) pose databases, obtaining fast and accurate pose estimates using training set sizes up to 105."
            },
            "slug": "Sparse-probabilistic-regression-for-human-pose-Urtasun-Darrell",
            "title": {
                "fragments": [],
                "text": "Sparse probabilistic regression for activity-independent human pose inference"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "An online probabilistic regression scheme for efficient inference of complex, high- dimensional, and multimodal mappings, based on a local mixture of Gaussian processes, where locality is defined based on both appearance and pose, and where the mapping hyperparameters can vary across local neighborhoods to better adapt to specific regions in the pose space."
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1754666"
                        ],
                        "name": "R. Poppe",
                        "slug": "R.-Poppe",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Poppe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Poppe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 75
                            }
                        ],
                        "text": "In particular, the use of randomized \ndecision forests allows recognition from a 20-class lexicon to be performed in real time.20 The adaptation \nto the pose estimation problem is rela\u00adtively straightforward."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9073796,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ea481ceaf3ad8bef871a9efdddb27c345e0c3b4e",
            "isKey": false,
            "numCitedBy": 900,
            "numCiting": 161,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Vision-based-human-motion-analysis:-An-overview-Poppe",
            "title": {
                "fragments": [],
                "text": "Vision-based human motion analysis: An overview"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Vis. Image Underst."
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144398147"
                        ],
                        "name": "L. Sigal",
                        "slug": "L.-Sigal",
                        "structuredName": {
                            "firstName": "Leonid",
                            "lastName": "Sigal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Sigal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32830733"
                        ],
                        "name": "S. Bhatia",
                        "slug": "S.-Bhatia",
                        "structuredName": {
                            "firstName": "Sidharth",
                            "lastName": "Bhatia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Bhatia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145920814"
                        ],
                        "name": "S. Roth",
                        "slug": "S.-Roth",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Roth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105795"
                        ],
                        "name": "Michael J. Black",
                        "slug": "Michael-J.-Black",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Black",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Black"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2090818"
                        ],
                        "name": "M. Isard",
                        "slug": "M.-Isard",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Isard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Isard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[39] use eigen-appearance template detectors for head, upper arms and lower legs proposals."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "per-frame initialization and recovery is designed to complement any appropriate tracking algorithm [7, 39, 16, 42, 13] that might further incorporate temporal and kinematic coherence."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14806670,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dc39b61661bc57c8239cd2678a09248c8d98e88f",
            "isKey": false,
            "numCitedBy": 389,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "We pose the problem of 3D human tracking as one of inference in a graphical model. Unlike traditional kinematic tree representations, our model of the body is a collection of loosely-connected limbs. Conditional probabilities relating the 3D pose of connected limbs are learned from motion-captured training data. Similarly, we learn probabilistic models for the temporal evolution of each limb (forward and backward in time). Human pose and motion estimation is then solved with non-parametric belief propagation using a variation of particle filtering that can be applied over a general loopy graph. The loose-limbed model and decentralized graph structure facilitate the use of low-level visual cues. We adopt simple limb and head detectors to provide \"bottom-up\" information that is incorporated into the inference process at every time-step; these detectors permit automatic initialization and aid recovery from transient tracking failures. We illustrate the method by automatically tracking a walking person in video imagery using four calibrated cameras. Our experimental apparatus includes a marker-based motion capture system aligned with the coordinate frame of the calibrated cameras with which we quantitatively evaluate the accuracy of our 3D person tracker."
            },
            "slug": "Tracking-loose-limbed-people-Sigal-Bhatia",
            "title": {
                "fragments": [],
                "text": "Tracking loose-limbed people"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "The problem of 3D human tracking as one of inference in a graphical model that is a collection of loosely-connected limbs and non-parametric belief propagation using a variation of particle filtering that can be applied over a general loopy graph is posed."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33652486"
                        ],
                        "name": "J. Winn",
                        "slug": "J.-Winn",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Winn",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Winn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143774737"
                        ],
                        "name": "J. Shotton",
                        "slug": "J.-Shotton",
                        "structuredName": {
                            "firstName": "Jamie",
                            "lastName": "Shotton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shotton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11200969,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f9e6771a0eeaf95a9400e4fc94911d258983ffe9",
            "isKey": false,
            "numCitedBy": 312,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper addresses the problem of detecting and segmenting partially occluded objects of a known category. We first define a part labelling which densely covers the object. Our Layout Consistent Random Field (LayoutCRF) model then imposes asymmetric local spatial constraints on these labels to ensure the consistent layout of parts whilst allowing for object deformation. Arbitrary occlusions of the object are handled by avoiding the assumption that the whole object is visible. The resulting system is both efficient to train and to apply to novel images, due to a novel annealed layout-consistent expansion move algorithm paired with a randomised decision tree classifier. We apply our technique to images of cars and faces and demonstrate state-of-the-art detection and segmentation performance even in the presence of partial occlusion."
            },
            "slug": "The-Layout-Consistent-Random-Field-for-Recognizing-Winn-Shotton",
            "title": {
                "fragments": [],
                "text": "The Layout Consistent Random Field for Recognizing and Segmenting Partially Occluded Objects"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "This paper addresses the problem of detecting and segmenting partially occluded objects of a known category by defining a part labelling which densely covers the object and imposing asymmetric local spatial constraints on these labels to ensure the consistent layout of parts whilst allowing for object deformation."
            },
            "venue": {
                "fragments": [],
                "text": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144035504"
                        ],
                        "name": "Z. Tu",
                        "slug": "Z.-Tu",
                        "structuredName": {
                            "firstName": "Zhuowen",
                            "lastName": "Tu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Tu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 153
                            }
                        ],
                        "text": "It works frame by \nframe across dramatically differing body shapes and sizes, and the learned discriminative approach naturally \nhandles self-occlusions and poses cropped by the image frame."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 52805831,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "969be710d87f31ea323fafa0041b21a6b4cebefc",
            "isKey": false,
            "numCitedBy": 272,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "The notion of using context information for solving high-level vision problems has been increasingly realized in the field. However, how to learn an effective and efficient context model, together with the image appearance, remains mostly unknown. The current literature using Markov random fields (MRFs) and conditional random fields (CRFs) often involves specific algorithm design, in which the modeling and computing stages are studied in isolation. In this paper, we propose an auto-context algorithm. Given a set of training images and their corresponding label maps, we first learn a classifier on local image patches. The discriminative probability (or classification confidence) maps by the learned classifier are then used as context information, in addition to the original image patches, to train a new classifier. The algorithm then iterates to approach the ground truth. Auto-context learns an integrated low-level and context model, and is very general and easy to implement. Under nearly the identical parameter setting in the training, we apply the algorithm on three challenging vision applications: object segmentation, human body configuration, and scene region labeling. It typically takes about 30 ~ 70 seconds to run the algorithm in testing. Moreover, the scope of the proposed algorithm goes beyond high-level vision. It has the potential to be used for a wide variety of problems of multi-variate labeling."
            },
            "slug": "Auto-context-and-its-application-to-high-level-Tu",
            "title": {
                "fragments": [],
                "text": "Auto-context and its application to high-level vision tasks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "An auto-context algorithm that learns an integrated low-level and context model, and is very general and easy to implement, and has the potential to be used for a wide variety of problems of multi-variate labeling."
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2428034"
                        ],
                        "name": "C. Bregler",
                        "slug": "C.-Bregler",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Bregler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Bregler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 37
                            }
                        ],
                        "text": "This greatly enhances the system s ability to recover from tracking errors, which are \ninimical to almost all existing solutions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 162
                            }
                        ],
                        "text": "Evaluating each pixel separately avoids a combinatorial search over the different body joints, although \nwithin a single part there are of course still dramatic differences in the contextual appearance (see \nFigure 2)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2751624,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8f6a3dea66b539d75c30fb24ecefe627bbb0c3a9",
            "isKey": false,
            "numCitedBy": 882,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper demonstrates a new visual motion estimation technique that is able to recover high degree-of-freedom articulated human body configurations in complex video sequences. We introduce the use of a novel mathematical technique, the product of exponential maps and twist motions, and its integration into a differential motion estimation. This results in solving simple linear systems, and enables us to recover robustly the kinematic degrees-of-freedom in noise and complex self occluded configurations. We demonstrate this on several image sequences of people doing articulated full body movements, and visualize the results in re-animating an artificial 3D human model. We are also able to recover and re-animate the famous movements of Eadweard Muybridge's motion studies from the last century. To the best of our knowledge, this is the first computer vision based system that is able to process such challenging footage and recover complex motions with such high accuracy."
            },
            "slug": "Tracking-people-with-twists-and-exponential-maps-Bregler-Malik",
            "title": {
                "fragments": [],
                "text": "Tracking people with twists and exponential maps"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "This paper demonstrates a new visual motion estimation technique that is able to recover high degree-of-freedom articulated human body configurations in complex video sequences, and is the first computer vision based system able to process such challenging footage and recover complex motions with such high accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 1998 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.98CB36231)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276554"
                        ],
                        "name": "R. Fergus",
                        "slug": "R.-Fergus",
                        "structuredName": {
                            "firstName": "Rob",
                            "lastName": "Fergus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fergus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5745749,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "62837ab473124ea43cb8d7c6a4b4ee0f6f14e8c5",
            "isKey": false,
            "numCitedBy": 2487,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method to learn and recognize object class models from unlabeled and unsegmented cluttered scenes in a scale invariant manner. Objects are modeled as flexible constellations of parts. A probabilistic representation is used for all aspects of the object: shape, appearance, occlusion and relative scale. An entropy-based feature detector is used to select regions and their scale within the image. In learning the parameters of the scale-invariant object model are estimated. This is done using expectation-maximization in a maximum-likelihood setting. In recognition, this model is used in a Bayesian manner to classify images. The flexible nature of the model is demonstrated by excellent results over a range of datasets including geometrically constrained classes (e.g. faces, cars) and flexible objects (such as animals)."
            },
            "slug": "Object-class-recognition-by-unsupervised-learning-Fergus-Perona",
            "title": {
                "fragments": [],
                "text": "Object class recognition by unsupervised scale-invariant learning"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The flexible nature of the model is demonstrated by excellent results over a range of datasets including geometrically constrained classes (e.g. faces, cars) and flexible objects (such as animals)."
            },
            "venue": {
                "fragments": [],
                "text": "2003 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2003. Proceedings."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2490189"
                        ],
                        "name": "Gregory Shakhnarovich",
                        "slug": "Gregory-Shakhnarovich",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Shakhnarovich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gregory Shakhnarovich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731948"
                        ],
                        "name": "Paul A. Viola",
                        "slug": "Paul-A.-Viola",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Viola",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paul A. Viola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753210"
                        ],
                        "name": "Trevor Darrell",
                        "slug": "Trevor-Darrell",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Darrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Darrell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2051403,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3e1556aea42601df3f457ad43dfb059498931a33",
            "isKey": false,
            "numCitedBy": 906,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "Example-based methods are effective for parameter estimation problems when the underlying system is simple or the dimensionality of the input is low. For complex and high-dimensional problems such as pose estimation, the number of required examples and the computational complexity rapidly become prohibitively high. We introduce a new algorithm that learns a set of hashing functions that efficiently index examples relevant to a particular estimation task. Our algorithm extends locality-sensitive hashing, a recently developed method to find approximate neighbors in time sublinear in the number of examples. This method depends critically on the choice of hash functions that are optimally relevant to a particular estimation problem. Experiments demonstrate that the resulting algorithm, which we call parameter-sensitive hashing, can rapidly and accurately estimate the articulated pose of human figures from a large database of example images."
            },
            "slug": "Fast-pose-estimation-with-parameter-sensitive-Shakhnarovich-Viola",
            "title": {
                "fragments": [],
                "text": "Fast pose estimation with parameter-sensitive hashing"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A new algorithm is introduced that learns a set of hashing functions that efficiently index examples relevant to a particular estimation task, and can rapidly and accurately estimate the articulated pose of human figures from a large database of example images."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Ninth IEEE International Conference on Computer Vision"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770537"
                        ],
                        "name": "D. Ramanan",
                        "slug": "D.-Ramanan",
                        "structuredName": {
                            "firstName": "Deva",
                            "lastName": "Ramanan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ramanan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144016256"
                        ],
                        "name": "D. Forsyth",
                        "slug": "D.-Forsyth",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Forsyth",
                            "middleNames": [
                                "Alexander"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Forsyth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15039233,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "768b9d18ebfc5ad2de18ab613d7baa0500239de8",
            "isKey": false,
            "numCitedBy": 324,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a tracker that can track moving people in long sequences without manual initialization. Moving people are modeled with the assumption that, while configuration can vary quite substantially from frame to frame, appearance does not. This leads to an algorithm that firstly builds a model of the appearance of the body of each individual by clustering candidate body segments, and then uses this model to find all individuals in each frame. Unusually, the tracker does not rely on a model of human dynamics to identify possible instances of people; such models are unreliable, because human motion is fast and large accelerations are common. We show our tracking algorithm can be interpreted as a loopy inference procedure on an underlying Bayes net. Experiments on video of real scenes demonstrate that this tracker can (a) count distinct individuals; (b) identify and track them; (c) recover when it loses track, for example, if individuals are occluded or briefly leave the view; (d) identify the configuration of the body largely correctly; and (e) is not dependent on particular models of human motion."
            },
            "slug": "Finding-and-tracking-people-from-the-bottom-up-Ramanan-Forsyth",
            "title": {
                "fragments": [],
                "text": "Finding and tracking people from the bottom up"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "A tracker that can track moving people in long sequences without manual initialization is described and it is shown the tracking algorithm can be interpreted as a loopy inference procedure on an underlying Bayes net."
            },
            "venue": {
                "fragments": [],
                "text": "2003 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2003. Proceedings."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704879"
                        ],
                        "name": "H. Kjellstr\u00f6m",
                        "slug": "H.-Kjellstr\u00f6m",
                        "structuredName": {
                            "firstName": "Hedvig",
                            "lastName": "Kjellstr\u00f6m",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Kjellstr\u00f6m"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105795"
                        ],
                        "name": "Michael J. Black",
                        "slug": "Michael-J.-Black",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Black",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Black"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144398147"
                        ],
                        "name": "L. Sigal",
                        "slug": "L.-Sigal",
                        "structuredName": {
                            "firstName": "Leonid",
                            "lastName": "Sigal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Sigal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6017383,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7776d0a6bf3cdf9b3cd18b13d32c6babed84614b",
            "isKey": false,
            "numCitedBy": 376,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper addresses the problem of probabilistically modeling 3D human motion for synthesis and tracking. Given the high dimensional nature of human motion, learning an explicit probabilistic model from available training data is currently impractical. Instead we exploit methods from texture synthesis that treat images as representing an implicit empirical distribution. These methods replace the problem of representing the probability of a texture pattern with that of searching the training data for similar instances of that pattern. We extend this idea to temporal data representing 3D human motion with a large database of example motions. To make the method useful in practice, we must address the problem of efficient search in a large training set; efficiency is particularly important for tracking. Towards that end, we learn a low dimensional linear model of human motion that is used to structure the example motion database into a binary tree. An approximate probabilistic tree search method exploits the coefficients of this low-dimensional representation and runs in sub-linear time. This probabilistic tree search returns a particular sample human motion with probability approximating the true distribution of human motions in the database. This sampling method is suitable for use with particle filtering techniques and is applied to articulated 3D tracking of humans within a Bayesian framework. Successful tracking results are presented, along with examples of synthesizing human motion using the model."
            },
            "slug": "Implicit-Probabilistic-Models-of-Human-Motion-for-Kjellstr\u00f6m-Black",
            "title": {
                "fragments": [],
                "text": "Implicit Probabilistic Models of Human Motion for Synthesis and Tracking"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A low dimensional linear model of human motion is learned that is used to structure the example motion database into a binary tree and an approximate probabilistic tree search method exploits the coefficients of this low-dimensional representation and runs in sub-linear time."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48397238"
                        ],
                        "name": "Robert Y. Wang",
                        "slug": "Robert-Y.-Wang",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Wang",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robert Y. Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145492783"
                        ],
                        "name": "Jovan Popovi\u0107",
                        "slug": "Jovan-Popovi\u0107",
                        "structuredName": {
                            "firstName": "Jovan",
                            "lastName": "Popovi\u0107",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jovan Popovi\u0107"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13945193,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f35a45cd6aa6cf4f342073243d2be132adb571da",
            "isKey": false,
            "numCitedBy": 332,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "Articulated hand-tracking systems have been widely used in virtual reality but are rarely deployed in consumer applications due to their price and complexity. In this paper, we propose an easy-to-use and inexpensive system that facilitates 3-D articulated user-input using the hands. Our approach uses a single camera to track a hand wearing an ordinary cloth glove that is imprinted with a custom pattern. The pattern is designed to simplify the pose estimation problem, allowing us to employ a nearest-neighbor approach to track hands at interactive rates. We describe several proof-of-concept applications enabled by our system that we hope will provide a foundation for new interactions in modeling, animation control and augmented reality."
            },
            "slug": "Real-time-hand-tracking-with-a-color-glove-Wang-Popovi\u0107",
            "title": {
                "fragments": [],
                "text": "Real-time hand-tracking with a color glove"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This paper proposes an easy-to-use and inexpensive system that facilitates 3-D articulated user-input using the hands that uses a single camera to track a hand wearing an ordinary cloth glove that is imprinted with a custom pattern."
            },
            "venue": {
                "fragments": [],
                "text": "SIGGRAPH '09"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700569"
                        ],
                        "name": "T. Moeslund",
                        "slug": "T.-Moeslund",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Moeslund",
                            "middleNames": [
                                "Baltzer"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Moeslund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144046599"
                        ],
                        "name": "A. Hilton",
                        "slug": "A.-Hilton",
                        "structuredName": {
                            "firstName": "Adrian",
                            "lastName": "Hilton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hilton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48378203"
                        ],
                        "name": "V. Kr\u00fcger",
                        "slug": "V.-Kr\u00fcger",
                        "structuredName": {
                            "firstName": "Volker",
                            "lastName": "Kr\u00fcger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Kr\u00fcger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9815253,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c4e16d64b77d133b7382440a08091d64008dd923",
            "isKey": false,
            "numCitedBy": 2737,
            "numCiting": 520,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-survey-of-advances-in-vision-based-human-motion-Moeslund-Hilton",
            "title": {
                "fragments": [],
                "text": "A survey of advances in vision-based human motion capture and analysis"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Vis. Image Underst."
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685089"
                        ],
                        "name": "Pedro F. Felzenszwalb",
                        "slug": "Pedro-F.-Felzenszwalb",
                        "structuredName": {
                            "firstName": "Pedro",
                            "lastName": "Felzenszwalb",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pedro F. Felzenszwalb"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713089"
                        ],
                        "name": "D. Huttenlocher",
                        "slug": "D.-Huttenlocher",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Huttenlocher",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Huttenlocher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 28
                            }
                        ],
                        "text": "Felzenszwalb & Huttenlocher [11] apply pictorial structures to estimate pose efficiently."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2277383,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cd9ab441df8b24f473a3635370c69620b00c1e60",
            "isKey": false,
            "numCitedBy": 2423,
            "numCiting": 90,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present a computationally efficient framework for part-based modeling and recognition of objects. Our work is motivated by the pictorial structure models introduced by Fischler and Elschlager. The basic idea is to represent an object by a collection of parts arranged in a deformable configuration. The appearance of each part is modeled separately, and the deformable configuration is represented by spring-like connections between pairs of parts. These models allow for qualitative descriptions of visual appearance, and are suitable for generic recognition problems. We address the problem of using pictorial structure models to find instances of an object in an image as well as the problem of learning an object model from training examples, presenting efficient algorithms in both cases. We demonstrate the techniques by learning models that represent faces and human bodies and using the resulting models to locate the corresponding objects in novel images."
            },
            "slug": "Pictorial-Structures-for-Object-Recognition-Felzenszwalb-Huttenlocher",
            "title": {
                "fragments": [],
                "text": "Pictorial Structures for Object Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "A computationally efficient framework for part-based modeling and recognition of objects, motivated by the pictorial structure models introduced by Fischler and Elschlager, that allows for qualitative descriptions of visual appearance and is suitable for generic recognition problems."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3223701"
                        ],
                        "name": "Daniel Grest",
                        "slug": "Daniel-Grest",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Grest",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Grest"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2402029"
                        ],
                        "name": "J. Woetzel",
                        "slug": "J.-Woetzel",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Woetzel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Woetzel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144839904"
                        ],
                        "name": "R. Koch",
                        "slug": "R.-Koch",
                        "structuredName": {
                            "firstName": "Reinhard",
                            "lastName": "Koch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Koch"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 11
                            }
                        ],
                        "text": "The second intermediate representation is a list of joint hypotheses Jt, which \ncontains triples of (body part, 3D position, confidence) hypotheses, with say five hypothe\u00adses per body \npart."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 46
                            }
                        ],
                        "text": "This greatly enhances the system s ability to recover from tracking errors, which are \ninimical to almost all existing solutions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 152
                            }
                        ],
                        "text": "In particular, the use of randomized \ndecision forests allows recognition from a 20-class lexicon to be performed in real time.20 The adaptation \nto the pose estimation problem is rela\u00adtively straightforward."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17067636,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dfd57230dbe26b8b1d4699f1bd7e7417a1344634",
            "isKey": false,
            "numCitedBy": 137,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper focuses on real-time markerless motion capture. The body pose of a person is estimated from depth images using an Iterative Closest Point algorithm. We present a very efficient approach, that estimates up to 28 degrees of freedom from 1000 data points with 4Hz. This is achieved by nonlinear optimization techniques using an analytically derived Jacobian and highly optimized correspondence search."
            },
            "slug": "Nonlinear-Body-Pose-Estimation-from-Depth-Images-Grest-Woetzel",
            "title": {
                "fragments": [],
                "text": "Nonlinear Body Pose Estimation from Depth Images"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "The body pose of a person is estimated from depth images using an Iterative Closest Point algorithm using an analytically derived Jacobian and highly optimized correspondence search to estimate up to 28 degrees of freedom from 1000 data points with 4Hz."
            },
            "venue": {
                "fragments": [],
                "text": "DAGM-Symposium"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784015"
                        ],
                        "name": "R. Navaratnam",
                        "slug": "R.-Navaratnam",
                        "structuredName": {
                            "firstName": "Ramanan",
                            "lastName": "Navaratnam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Navaratnam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47139824"
                        ],
                        "name": "A. Fitzgibbon",
                        "slug": "A.-Fitzgibbon",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Fitzgibbon",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Fitzgibbon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745672"
                        ],
                        "name": "R. Cipolla",
                        "slug": "R.-Cipolla",
                        "structuredName": {
                            "firstName": "Roberto",
                            "lastName": "Cipolla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Cipolla"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2365737,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3c536b7a77cdc07db357c9f5d58d0df706723723",
            "isKey": false,
            "numCitedBy": 104,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Many computer vision tasks may be expressed as the problem of learning a mapping between image space and a parameter space. For example, in human body pose estimation, recent research has directly modelled the mapping from image features (z) to joint angles (thetas). Fitting such models requires training data in the form of labelled (z, thetas) pairs, from which are learned the conditional densities p(thetas\\z). Inference is then simple: given test image features z, the conditional p(thetas\\z) is immediately computed. However large amounts of training data are required to fit the models, particularly in the case where the spaces are high dimensional. We show how the use of unlabelled data-samples from the marginal distributions p(z) and p(thetas)-may be used to improve fitting. This is valuable because it is often significantly easier to obtain unlabelled than labelled samples. We use a Gaussian process latent variable model to learn the mapping from a shared latent low-dimensional manifold to the feature and parameter spaces. This extends existing approaches to (a) use unlabelled data, and (b) represent one-to-many mappings. Experiments on synthetic and real problems demonstrate how the use of unlabelled data improves over existing techniques. In our comparisons, we include existing approaches that are explicitly semi-supervised as well as those which implicitly make use of unlabelled examples."
            },
            "slug": "The-Joint-Manifold-Model-for-Semi-supervised-Navaratnam-Fitzgibbon",
            "title": {
                "fragments": [],
                "text": "The Joint Manifold Model for Semi-supervised Multi-valued Regression"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A Gaussian process latent variable model is used to learn the mapping from a shared latent low-dimensional manifold to the feature and parameter spaces and Experiments on synthetic and real problems demonstrate how the use of unlabelled data improves over existing techniques."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE 11th International Conference on Computer Vision"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2808670"
                        ],
                        "name": "E. Kalogerakis",
                        "slug": "E.-Kalogerakis",
                        "structuredName": {
                            "firstName": "Evangelos",
                            "lastName": "Kalogerakis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Kalogerakis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747779"
                        ],
                        "name": "Aaron Hertzmann",
                        "slug": "Aaron-Hertzmann",
                        "structuredName": {
                            "firstName": "Aaron",
                            "lastName": "Hertzmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aaron Hertzmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109146968"
                        ],
                        "name": "Karan Singh",
                        "slug": "Karan-Singh",
                        "structuredName": {
                            "firstName": "Karan",
                            "lastName": "Singh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karan Singh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 5
                            }
                        ],
                        "text": "A combination of these joint locations comprises \nthe output pose q."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8852928,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "14c93d2be6a786ecee8783eea608b704f273ce98",
            "isKey": false,
            "numCitedBy": 296,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a data-driven approach to simultaneous segmentation and labeling of parts in 3D meshes. An objective function is formulated as a Conditional Random Field model, with terms assessing the consistency of faces with labels, and terms between labels of neighboring faces. The objective function is learned from a collection of labeled training meshes. The algorithm uses hundreds of geometric and contextual label features and learns different types of segmentations for different tasks, without requiring manual parameter tuning. Our algorithm achieves a significant improvement in results over the state-of-the-art when evaluated on the Princeton Segmentation Benchmark, often producing segmentations and labelings comparable to those produced by humans."
            },
            "slug": "Learning-3D-mesh-segmentation-and-labeling-Kalogerakis-Hertzmann",
            "title": {
                "fragments": [],
                "text": "Learning 3D mesh segmentation and labeling"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "This paper presents a data-driven approach to simultaneous segmentation and labeling of parts in 3D meshes, formulated as a Conditional Random Field model, with terms assessing the consistency of faces with labels, and terms between labels of neighboring faces."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144854796"
                        ],
                        "name": "D. Gavrila",
                        "slug": "D.-Gavrila",
                        "structuredName": {
                            "firstName": "Dariu",
                            "lastName": "Gavrila",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Gavrila"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 28
                            }
                        ],
                        "text": "While hierarchical matching [14] is faster, one would still need a massive exemplar set to achieve comparable accuracy."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 79
                            }
                        ],
                        "text": "As an example of a realizable system, the second variant uses chamfer matching [14] to compare the test image to the training exemplars."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17435119,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "79036203c360174b694314adab553aa00a6aeff3",
            "isKey": false,
            "numCitedBy": 516,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a prototype system for pedestrian detection on-board a moving vehicle. The system uses a generic two-step approach for efficient object detection. In the first step, contour features are used in a hierarchical template matching approach to efficiently \"lock\" onto candidate solutions. Shape matching is based on Distance Transforms. By capturing the objects shape variability by means of a template hierarchy and using a combined coarse-to-fine approach in shape and parameter space, this method achieves very large speed-ups compared to a brute-force method. We have measured gains of several orders of magnitude. The second step utilizes the richer set of intensity features in a pattern classification approach to verify the candidate solutions (i.e. using Radial Basis Functions). We present experimental results on pedestrian detection off-line and on-board our Urban Traffic Assistant vehicle and discuss the challenges that lie ahead."
            },
            "slug": "Pedestrian-Detection-from-a-Moving-Vehicle-Gavrila",
            "title": {
                "fragments": [],
                "text": "Pedestrian Detection from a Moving Vehicle"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "This paper presents a prototype system for pedestrian detection on-board a moving vehicle that uses a generic two-step approach for efficient object detection using a hierarchical template matching approach and achieves very large speed-ups compared to a brute-force method."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1795834"
                        ],
                        "name": "S. Knoop",
                        "slug": "S.-Knoop",
                        "structuredName": {
                            "firstName": "Steffen",
                            "lastName": "Knoop",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Knoop"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790090"
                        ],
                        "name": "S. Vacek",
                        "slug": "S.-Vacek",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Vacek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Vacek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053462341"
                        ],
                        "name": "Klaus Steinbach",
                        "slug": "Klaus-Steinbach",
                        "structuredName": {
                            "firstName": "Klaus",
                            "lastName": "Steinbach",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Klaus Steinbach"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144427136"
                        ],
                        "name": "R. Dillmann",
                        "slug": "R.-Dillmann",
                        "structuredName": {
                            "firstName": "R\u00fcdiger",
                            "lastName": "Dillmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Dillmann"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14333669,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "461e97d7e5b666582aa77c3eaf65d3e371e0189e",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present a new approach for fusion of different measurements and sensors for 3D model based tracking. The underlying model of the tracked body is defined geometrically with generalized cylinders, which can hierarchically be connected by different kinds of joints. This results in an articulated body model with constrained kinematic degrees of freedom. The fusion approach incorporates this model knowledge together with the measurements, and tracks the target body iteratively with an extended iterative closest point approach. The resulting tracking system named VooDoo is used to track humans in a human-robot interaction (HRI) context. We only rely on sensors on board the robot, i.e. a color camera, a 3D time-of-flight camera and a laser range finder. The system runs in realtime (~ 20 Hz) and is able to robustly track a human in the vicinity of the robot. The pose and trajectory of the human interaction partner can then be used for haptic interaction like hand-overs, and for activity and gesture recognition"
            },
            "slug": "Sensor-fusion-for-model-based-3D-tracking-Knoop-Vacek",
            "title": {
                "fragments": [],
                "text": "Sensor fusion for model based 3D tracking"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "A new approach for fusion of different measurements and sensors for 3D model based tracking that tracks the target body iteratively with an extended iterative closest point approach and is used to track humans in a human-robot interaction (HRI) context."
            },
            "venue": {
                "fragments": [],
                "text": "2006 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50172592"
                        ],
                        "name": "Serge J. Belongie",
                        "slug": "Serge-J.-Belongie",
                        "structuredName": {
                            "firstName": "Serge",
                            "lastName": "Belongie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Serge J. Belongie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2121009"
                        ],
                        "name": "J. Puzicha",
                        "slug": "J.-Puzicha",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Puzicha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Puzicha"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 129468,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "faf8444bad76e8aa727c8b2df42fefe7b8242957",
            "isKey": false,
            "numCitedBy": 5812,
            "numCiting": 94,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents my work on computing shape models that are computationally fast and invariant basic transformations like translation, scaling and rotation. In this paper, I propose shape detection using a feature called shape context. Shape context describes all boundary points of a shape with respect to any single boundary point. Thus it is descriptive of the shape of the object. Object recognition can be achieved by matching this feature with a priori knowledge of the shape context of the boundary points of the object. Experimental results are promising on handwritten digits, trademark images."
            },
            "slug": "Shape-matching-and-object-recognition-using-shape-Belongie-Malik",
            "title": {
                "fragments": [],
                "text": "Shape matching and object recognition using shape contexts"
            },
            "tldr": {
                "abstractSimilarityScore": 87,
                "text": "This paper presents work on computing shape models that are computationally fast and invariant basic transformations like translation, scaling and rotation, and proposes shape detection using a feature called shape context, which is descriptive of the shape of the object."
            },
            "venue": {
                "fragments": [],
                "text": "2010 3rd International Conference on Computer Science and Information Technology"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102459643"
                        ],
                        "name": "Suhas G. Salve",
                        "slug": "Suhas-G.-Salve",
                        "structuredName": {
                            "firstName": "Suhas",
                            "lastName": "Salve",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Suhas G. Salve"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9182439"
                        ],
                        "name": "K. Jondhale",
                        "slug": "K.-Jondhale",
                        "structuredName": {
                            "firstName": "Kalpana",
                            "lastName": "Jondhale",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Jondhale"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 206,
                                "start": 203
                            }
                        ],
                        "text": "Given a larger computational budget, one could employ potentially more powerful features based on, for example, depth integrals over regions, curvature, or local descriptors, for example, shape contexts.(3)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 122832387,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d4451e048254d18ea5b2862ee209a75d26601606",
            "isKey": false,
            "numCitedBy": 242,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents my work on computing shape models that are computationally fast and invariant basic transformations like translation, scaling and rotation. In this paper, I propose shape detection using a feature called shape context. Shape context describes all boundary points of a shape with respect to any single boundary point. Thus it is descriptive of the shape of the object. Object recognition can be achieved by matching this feature with a priori knowledge of the shape context of the boundary points of the object. Experimental results are promising on handwritten digits, trademark images."
            },
            "slug": "Shape-matching-and-object-recognition-using-shape-Salve-Jondhale",
            "title": {
                "fragments": [],
                "text": "Shape matching and object recognition using shape contexts"
            },
            "tldr": {
                "abstractSimilarityScore": 87,
                "text": "This paper presents work on computing shape models that are computationally fast and invariant basic transformations like translation, scaling and rotation, and proposes shape detection using a feature called shape context, which is descriptive of the shape of the object."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4801654"
                        ],
                        "name": "Y. Amit",
                        "slug": "Y.-Amit",
                        "structuredName": {
                            "firstName": "Yali",
                            "lastName": "Amit",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Amit"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707642"
                        ],
                        "name": "D. Geman",
                        "slug": "D.-Geman",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Geman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Geman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12470146,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "de5e95325e139fd0a46df1dd28aabecd0273b772",
            "isKey": false,
            "numCitedBy": 1152,
            "numCiting": 78,
            "paperAbstract": {
                "fragments": [],
                "text": "We explore a new approach to shape recognition based on a virtually infinite family of binary features (queries) of the image data, designed to accommodate prior information about shape invariance and regularity. Each query corresponds to a spatial arrangement of several local topographic codes (or tags), which are in themselves too primitive and common to be informative about shape. All the discriminating power derives from relative angles and distances among the tags. The important attributes of the queries are a natural partial ordering corresponding to increasing structure and complexity; semi-invariance, meaning that most shapes of a given class will answer the same way to two queries that are successive in the ordering; and stability, since the queries are not based on distinguished points and substructures. No classifier based on the full feature set can be evaluated, and it is impossible to determine a priori which arrangements are informative. Our approach is to select informative features and build tree classifiers at the same time by inductive learning. In effect, each tree provides an approximation to the full posterior where the features chosen depend on the branch that is traversed. Due to the number and nature of the queries, standard decision tree construction based on a fixed-length feature vector is not feasible. Instead we entertain only a small random sample of queries at each node, constrain their complexity to increase with tree depth, and grow multiple trees. The terminal nodes are labeled by estimates of the corresponding posterior distribution over shape classes. An image is classified by sending it down every tree and aggregating the resulting distributions. The method is applied to classifying handwritten digits and synthetic linear and nonlinear deformations of three hundred symbols. State-of-the-art error rates are achieved on the National Institute of Standards and Technology database of digits. The principal goal of the experiments on symbols is to analyze invariance, generalization error and related issues, and a comparison with artificial neural networks methods is presented in this context. Figure 1: LATEX Symbol"
            },
            "slug": "Shape-Quantization-and-Recognition-with-Randomized-Amit-Geman",
            "title": {
                "fragments": [],
                "text": "Shape Quantization and Recognition with Randomized Trees"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "A new approach to shape recognition based on a virtually infinite family of binary features (queries) of the image data, designed to accommodate prior information about shape invariance and regularity, and a comparison with artificial neural networks methods is presented."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1795834"
                        ],
                        "name": "S. Knoop",
                        "slug": "S.-Knoop",
                        "structuredName": {
                            "firstName": "Steffen",
                            "lastName": "Knoop",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Knoop"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790090"
                        ],
                        "name": "S. Vacek",
                        "slug": "S.-Vacek",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Vacek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Vacek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144427136"
                        ],
                        "name": "R. Dillmann",
                        "slug": "R.-Dillmann",
                        "structuredName": {
                            "firstName": "R\u00fcdiger",
                            "lastName": "Dillmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Dillmann"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 16
                            }
                        ],
                        "text": "The second intermediate representation is a list of joint hypotheses Jt, which \ncontains triples of (body part, 3D position, confidence) hypotheses, with say five hypothe\u00adses per body \npart."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 157
                            }
                        ],
                        "text": "In particular, the use of randomized \ndecision forests allows recognition from a 20-class lexicon to be performed in real time.20 The adaptation \nto the pose estimation problem is rela\u00adtively straightforward."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9119530,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c8feb997e64ed900c7cb4f4b691e312e067b52fa",
            "isKey": false,
            "numCitedBy": 154,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a tracking system called VooDoo for 3D tracking of human body movements based on a 3D body model and the iterative closest point (ICP) algorithm. The proposed approach is able to incorporate raw data from different input sensors, as well as results from feature trackers in 2D or 3D. All input data is processed within the same model fitting step by modeling all input measurements in 3D model space. The system has been implemented and runs in realtime at appr. 10-14 Hz. Experiments with complex human movements exhibit the characteristics and advantages of the proposed approach"
            },
            "slug": "Sensor-fusion-for-3D-human-body-tracking-with-an-3D-Knoop-Vacek",
            "title": {
                "fragments": [],
                "text": "Sensor fusion for 3D human body tracking with an articulated 3D body model"
            },
            "tldr": {
                "abstractSimilarityScore": 87,
                "text": "This paper proposes a tracking system for 3D tracking of human body movements based on a 3D body model and the iterative closest point (ICP) algorithm that is able to incorporate raw data from different input sensors, as well as results from feature trackers in 2D or 3D."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 2006 IEEE International Conference on Robotics and Automation, 2006. ICRA 2006."
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685020"
                        ],
                        "name": "D. Comaniciu",
                        "slug": "D.-Comaniciu",
                        "structuredName": {
                            "firstName": "Dorin",
                            "lastName": "Comaniciu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Comaniciu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145776090"
                        ],
                        "name": "P. Meer",
                        "slug": "P.-Meer",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Meer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Meer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 691081,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "74f4ecc3e4e5b91fbb54330b285ed5214afe2001",
            "isKey": false,
            "numCitedBy": 11480,
            "numCiting": 122,
            "paperAbstract": {
                "fragments": [],
                "text": "A general non-parametric technique is proposed for the analysis of a complex multimodal feature space and to delineate arbitrarily shaped clusters in it. The basic computational module of the technique is an old pattern recognition procedure: the mean shift. For discrete data, we prove the convergence of a recursive mean shift procedure to the nearest stationary point of the underlying density function and, thus, its utility in detecting the modes of the density. The relation of the mean shift procedure to the Nadaraya-Watson estimator from kernel regression and the robust M-estimators; of location is also established. Algorithms for two low-level vision tasks discontinuity-preserving smoothing and image segmentation - are described as applications. In these algorithms, the only user-set parameter is the resolution of the analysis, and either gray-level or color images are accepted as input. Extensive experimental results illustrate their excellent performance."
            },
            "slug": "Mean-Shift:-A-Robust-Approach-Toward-Feature-Space-Comaniciu-Meer",
            "title": {
                "fragments": [],
                "text": "Mean Shift: A Robust Approach Toward Feature Space Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is proved the convergence of a recursive mean shift procedure to the nearest stationary point of the underlying density function and, thus, its utility in detecting the modes of the density."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143774737"
                        ],
                        "name": "J. Shotton",
                        "slug": "J.-Shotton",
                        "structuredName": {
                            "firstName": "Jamie",
                            "lastName": "Shotton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shotton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143945334"
                        ],
                        "name": "Matthew Johnson",
                        "slug": "Matthew-Johnson",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Johnson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew Johnson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745672"
                        ],
                        "name": "R. Cipolla",
                        "slug": "R.-Cipolla",
                        "structuredName": {
                            "firstName": "Roberto",
                            "lastName": "Cipolla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Cipolla"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9952478,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d136d77dcdfb34381d8f581f3866d10293a519fd",
            "isKey": false,
            "numCitedBy": 1004,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose semantic texton forests, efficient and powerful new low-level features. These are ensembles of decision trees that act directly on image pixels, and therefore do not need the expensive computation of filter-bank responses or local descriptors. They are extremely fast to both train and test, especially compared with k-means clustering and nearest-neighbor assignment of feature descriptors. The nodes in the trees provide (i) an implicit hierarchical clustering into semantic textons, and (ii) an explicit local classification estimate. Our second contribution, the bag of semantic textons, combines a histogram of semantic textons over an image region with a region prior category distribution. The bag of semantic textons is computed over the whole image for categorization, and over local rectangular regions for segmentation. Including both histogram and region prior allows our segmentation algorithm to exploit both textural and semantic context. Our third contribution is an image-level prior for segmentation that emphasizes those categories that the automatic categorization believes to be present. We evaluate on two datasets including the very challenging VOC 2007 segmentation dataset. Our results significantly advance the state-of-the-art in segmentation accuracy, and furthermore, our use of efficient decision forests gives at least a five-fold increase in execution speed."
            },
            "slug": "Semantic-texton-forests-for-image-categorization-Shotton-Johnson",
            "title": {
                "fragments": [],
                "text": "Semantic texton forests for image categorization and segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "The proposed semantic texton forests are ensembles of decision trees that act directly on image pixels, and therefore do not need the expensive computation of filter-bank responses or local descriptors, and give at least a five-fold increase in execution speed."
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3128253"
                        ],
                        "name": "F. Moosmann",
                        "slug": "F.-Moosmann",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Moosmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Moosmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756114"
                        ],
                        "name": "B. Triggs",
                        "slug": "B.-Triggs",
                        "structuredName": {
                            "firstName": "Bill",
                            "lastName": "Triggs",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Triggs"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "82117876"
                        ],
                        "name": "F. Jurie",
                        "slug": "F.-Jurie",
                        "structuredName": {
                            "firstName": "Fr\u00e9d\u00e9ric",
                            "lastName": "Jurie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jurie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11904287,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c2d3746a1f755928b5011932285d686eb5a9127b",
            "isKey": false,
            "numCitedBy": 569,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Some of the most effective recent methods for content-based image classification work by extracting dense or sparse local image descriptors, quantizing them according to a coding rule such as k-means vector quantization, accumulating histograms of the resulting \"visual word\" codes over the image, and classifying these with a conventional classifier such as an SVM. Large numbers of descriptors and large codebooks are needed for good results and this becomes slow using k-means. We introduce Extremely Randomized Clustering Forests - ensembles of randomly created clustering trees - and show that these provide more accurate results, much faster training and testing and good resistance to background clutter in several state-of-the-art image classification tasks."
            },
            "slug": "Fast-Discriminative-Visual-Codebooks-using-Forests-Moosmann-Triggs",
            "title": {
                "fragments": [],
                "text": "Fast Discriminative Visual Codebooks using Randomized Clustering Forests"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work introduces Extremely Randomized Clustering Forests - ensembles of randomly created clustering trees - and shows that these provide more accurate results, much faster training and testing and good resistance to background clutter in several state-of-the-art image classification tasks."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2072495054"
                        ],
                        "name": "B. A. Shepherd",
                        "slug": "B.-A.-Shepherd",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Shepherd",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. A. Shepherd"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10470250,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "98e45102842acb101d66dc53dc898877d34a54b8",
            "isKey": false,
            "numCitedBy": 64,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper investigates the applicability to a shape-recognition problem of a concept learning algorithm which generates decision rules from examples. A comprehensive analysis of this algorithm applied to an industrial vision problem is described. This problem has no obvious 'best' solution and much effort has been devoted to performing a realistic appraisal of the algorithm by making a detailed set of comparisons with the performances of appropriate alternative classifiers. Results presented show the algorithm to be comparable in performance with the alternative classifiers but superior in terms of both the cost of making a classification and also the intelligibility of the solution."
            },
            "slug": "An-Appraisal-of-a-Decision-Tree-Approach-to-Image-Shepherd",
            "title": {
                "fragments": [],
                "text": "An Appraisal of a Decision Tree Approach to Image Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "This paper investigates the applicability to a shape-recognition problem of a concept learning algorithm which generates decision rules from examples and shows the algorithm to be comparable in performance with the alternative classifiers but superior in terms of both the cost of making a classification and also the intelligibility of the solution."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2423230"
                        ],
                        "name": "L. Breiman",
                        "slug": "L.-Breiman",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Breiman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Breiman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 89141,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "13d4c2f76a7c1a4d0a71204e1d5d263a3f5a7986",
            "isKey": false,
            "numCitedBy": 65197,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund & R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148\u2013156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression."
            },
            "slug": "Random-Forests-Breiman",
            "title": {
                "fragments": [],
                "text": "Random Forests"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the forest, and are also applicable to regression."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34824003"
                        ],
                        "name": "T. Sharp",
                        "slug": "T.-Sharp",
                        "structuredName": {
                            "firstName": "Toby",
                            "lastName": "Sharp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sharp"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13324291,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5fd035907cf17542631feb891babe3235e56c198",
            "isKey": false,
            "numCitedBy": 212,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a method for implementing the evaluation and training of decision trees and forests entirely on a GPU, and show how this method can be used in the context of object recognition."
            },
            "slug": "Implementing-Decision-Trees-and-Forests-on-a-GPU-Sharp",
            "title": {
                "fragments": [],
                "text": "Implementing Decision Trees and Forests on a GPU"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "A method for implementing the evaluation and training of decision trees and forests entirely on a GPU is described and how this method can be used in the context of object recognition is shown."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341779"
                        ],
                        "name": "J. R. Quinlan",
                        "slug": "J.-R.-Quinlan",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Quinlan",
                            "middleNames": [
                                "Ross"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. R. Quinlan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13252401,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bcee7c85d237b79491a773ef51e746bbbcf48e35",
            "isKey": false,
            "numCitedBy": 13488,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "The technology for building knowledge-based systems by inductive inference from examples has been demonstrated successfully in several practical applications. This paper summarizes an approach to synthesizing decision trees that has been used in a variety of systems, and it describes one such system, ID3, in detail. Results from recent studies show ways in which the methodology can be modified to deal with information that is noisy and/or incomplete. A reported shortcoming of the basic algorithm is discussed and two means of overcoming it are compared. The paper concludes with illustrations of current research directions."
            },
            "slug": "Induction-of-Decision-Trees-Quinlan",
            "title": {
                "fragments": [],
                "text": "Induction of Decision Trees"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper summarizes an approach to synthesizing decision trees that has been used in a variety of systems, and it describes one such system, ID3, in detail, which is described in detail."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144306657"
                        ],
                        "name": "T. Gonzalez",
                        "slug": "T.-Gonzalez",
                        "structuredName": {
                            "firstName": "Teofilo",
                            "lastName": "Gonzalez",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Gonzalez"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 205092276,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1cdd3c62172b7598cd090e349d38e9644734edfd",
            "isKey": false,
            "numCitedBy": 1547,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Clustering-to-Minimize-the-Maximum-Intercluster-Gonzalez",
            "title": {
                "fragments": [],
                "text": "Clustering to Minimize the Maximum Intercluster Distance"
            },
            "venue": {
                "fragments": [],
                "text": "Theor. Comput. Sci."
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 61
                            }
                        ],
                        "text": "We also evaluate on the real depth data from Ganapathi et al.(8) The results suggest that effects seen on synthetic data are mirrored in the real data, and further that our synthetic test set is by far the \u2018hardest\u2019 due to the extreme variability in pose and body shape."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 16
                            }
                        ],
                        "text": "Comparison with Ganapathi et al.8 Ganapathi \net al. pro\u00advided their test data and results for direct comparison."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 32
                            }
                        ],
                        "text": "Comparison with Ganapathi et al.(8) Ganapathi et al."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 45
                            }
                        ],
                        "text": "We also evaluate on the real depth data from Ganapathi et al.8 The results \nsuggest that effects seen on syn\u00adthetic data are mirrored in the real data, and further that our synthetic \ntest set is by far the hardest due to the extreme variability in pose and body shape."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 157
                            }
                        ],
                        "text": "We show both qualitative and quantitative results on several challenging datasets and compare with both nearest-neighbor approaches and the state of the art.(8) We provide further results in the supplementary material."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Real time motion"
            },
            "venue": {
                "fragments": [],
                "text": "In Proceedings of CVPR"
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Gong, y., Huang, t.s. discriminative learning of visual words for 3d human pose estimation"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of CVPR"
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "t.s. discriminative learning of visual words for 3d human pose estimation"
            },
            "venue": {
                "fragments": [],
                "text": "In Proceedings of CVPR"
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "CMu Mocap database"
            },
            "venue": {
                "fragments": [],
                "text": "CMu Mocap database"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Ankle R. Ankle L. Foot R. Foot Mean AP Head Neck L. Shoulder R. Shoulder L. Elbow R. Elbow L. Wrist R. Wrist L. Hand R. Hand L. Knee R. Knee L. Ankle R. Ankle L. Foot References"
            },
            "venue": {
                "fragments": [],
                "text": "Ankle R. Ankle L. Foot R. Foot Mean AP Head Neck L. Shoulder R. Shoulder L. Elbow R. Elbow L. Wrist R. Wrist L. Hand R. Hand L. Knee R. Knee L. Ankle R. Ankle L. Foot References"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "$Logis+c$Regression$ Logis+c$regression:$ @BULLET Minimize$log$loss$ @BULLET Define$$ $ $ where$x j $predefined$ features$(linear$classifier)$ @BULLET Jointly$op+mize$over$all$ weights$w 0"
            },
            "venue": {
                "fragments": [],
                "text": "[Viola&&&Jones] Boos0ng&vs.&Logis0c&Regression 76 Boos+ng$vs"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "@BULLET Problem:&find&faces&in&photograph&or&movie& @BULLET Weak'classifiers:'detect&light/dark&rectangle&in&image @BULLET Many&clever&tricks&to&make&extremely&fast&and&accurate 75"
            },
            "venue": {
                "fragments": [],
                "text": "@BULLET Problem:&find&faces&in&photograph&or&movie& @BULLET Weak'classifiers:'detect&light/dark&rectangle&in&image @BULLET Many&clever&tricks&to&make&extremely&fast&and&accurate 75"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 41
                            }
                        ],
                        "text": "This greatly enhances the system s ability to recover from tracking errors, which are \ninimical to almost all existing solutions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Tracking looselimbed people"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. CVPR"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 53
                            }
                        ],
                        "text": "It works frame by \nframe across dramatically differing body shapes and sizes, and the learned discriminative approach naturally \nhandles self-occlusions and poses cropped by the image frame."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Local probabilistic regression for activityindependent human pose inference"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. CVPR"
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "@BULLET Training&Data& ' 5000&faces& @BULLET All&frontal& ' 300&million&non'faces& @BULLET 9500&non"
            },
            "venue": {
                "fragments": [],
                "text": "@BULLET Training&Data& ' 5000&faces& @BULLET All&frontal& ' 300&million&non'faces& @BULLET 9500&non"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Viola&&&Jones] Application to face detection [Viola and Jones"
            },
            "venue": {
                "fragments": [],
                "text": "Viola&&&Jones] Application to face detection [Viola and Jones"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "darrell, t. local probabilistic regression for activity-independent human pose inference"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of CVPR"
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "thrun, s. Real time motion Jamie Shotton"
            },
            "venue": {
                "fragments": [],
                "text": "Andrew Blake, and Mat Cook ({jamiesho, tsharp, awf, ablake, and a-macook}@microsoft.com), Microsoft Research"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "4. breiman, l. Random forests"
            },
            "venue": {
                "fragments": [],
                "text": "Mach. Learn"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "{akipman and markfi}@microsoft.com), Xbox Incubation"
            },
            "venue": {
                "fragments": [],
                "text": "{akipman and markfi}@microsoft.com), Xbox Incubation"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Isard, M. tracking loose-limbed people"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of CVPR"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "C4.5 decision trees"
            },
            "venue": {
                "fragments": [],
                "text": "C4.5 decision trees"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Kinect for Xbox 360"
            },
            "venue": {
                "fragments": [],
                "text": "Kinect for Xbox 360"
            }
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 16,
            "methodology": 7,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 62,
        "totalPages": 7
    },
    "page_url": "https://www.semanticscholar.org/paper/Real-time-human-pose-recognition-in-parts-from-Shotton-Sharp/2915510a39448503ee873f9693cd3808ca74bd81?sort=total-citations"
}