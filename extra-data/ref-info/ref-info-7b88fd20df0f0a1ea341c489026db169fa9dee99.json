{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2415843"
                        ],
                        "name": "S. Gortler",
                        "slug": "S.-Gortler",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Gortler",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Gortler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2026212"
                        ],
                        "name": "R. Grzeszczuk",
                        "slug": "R.-Grzeszczuk",
                        "structuredName": {
                            "firstName": "Radek",
                            "lastName": "Grzeszczuk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Grzeszczuk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717841"
                        ],
                        "name": "R. Szeliski",
                        "slug": "R.-Szeliski",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Szeliski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Szeliski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1400248273"
                        ],
                        "name": "Michael F. Cohen",
                        "slug": "Michael-F.-Cohen",
                        "structuredName": {
                            "firstName": "Michael F.",
                            "lastName": "Cohen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael F. Cohen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 4
                            }
                        ],
                        "text": "The Lumigraph."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 201,
                                "start": 182
                            }
                        ],
                        "text": "#169; 2005 ACM 0730-0301/05/0700-0577 $5.00 The most general image-based rendering approaches, such \nas Quicktime VR [Chen 1995], Light.elds [Levoy and Hanrahan 1996], and Lumigraph [Gortler et al. 1996] \nall require a huge num\u00adber of photographs as well as special equipment."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 138
                            }
                        ],
                        "text": "The most general image-based rendering approaches, such as Quicktime VR [Chen 1995], Lightfields [Levoy and Hanrahan 1996], and Lumigraph [Gortler et al. 1996] all require a huge number of photographs as well as special equipment."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2036193,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2a737fbad8dd29730313c89ae1123efeab48786d",
            "isKey": true,
            "numCitedBy": 2700,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper discusses a new method for capturing the complete appearanceof both synthetic and real world objects and scenes, representing this information, and then using this representation to render images of the object from new camera positions. Unlike the shape capture process traditionally used in computer vision and the rendering process traditionally used in computer graphics, our approach does not rely on geometric representations. Instead we sample and reconstruct a 4D function, which we call a Lumigraph. The Lumigraph is a subsetof the complete plenoptic function that describes the flow of light at all positions in all directions. With the Lumigraph, new images of the object can be generated very quickly, independent of the geometric or illumination complexity of the scene or object. The paper discusses a complete working system including the capture of samples, the construction of the Lumigraph, and the subsequent rendering of images from this new representation."
            },
            "slug": "The-lumigraph-Gortler-Grzeszczuk",
            "title": {
                "fragments": [],
                "text": "The lumigraph"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "A new method for capturing the complete appearance of both synthetic and real world objects and scenes, representing this information, and then using this representation to render images of the object from new camera positions."
            },
            "venue": {
                "fragments": [],
                "text": "SIGGRAPH"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742208"
                        ],
                        "name": "M. Pollefeys",
                        "slug": "M.-Pollefeys",
                        "structuredName": {
                            "firstName": "Marc",
                            "lastName": "Pollefeys",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pollefeys"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681236"
                        ],
                        "name": "L. Gool",
                        "slug": "L.-Gool",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Gool",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gool"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2989123"
                        ],
                        "name": "M. Vergauwen",
                        "slug": "M.-Vergauwen",
                        "structuredName": {
                            "firstName": "Maarten",
                            "lastName": "Vergauwen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Vergauwen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2450841"
                        ],
                        "name": "F. Verbiest",
                        "slug": "F.-Verbiest",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Verbiest",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Verbiest"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804147"
                        ],
                        "name": "K. Cornelis",
                        "slug": "K.-Cornelis",
                        "structuredName": {
                            "firstName": "Kurt",
                            "lastName": "Cornelis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Cornelis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2072921"
                        ],
                        "name": "Jan Tops",
                        "slug": "Jan-Tops",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Tops",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jan Tops"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144839904"
                        ],
                        "name": "R. Koch",
                        "slug": "R.-Koch",
                        "structuredName": {
                            "firstName": "Reinhard",
                            "lastName": "Koch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Koch"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 120
                            }
                        ],
                        "text": "Automatic methodsexistto reconstruct \ncertain typesof scenes from multiple images or video sequences (e.g. [Nist\u00b4er 2001; Pollefeys et al. \n2004]), but, to the best of our knowledge, no one has yet attempted automatic single-view modeling."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 0
                            }
                        ],
                        "text": "[Nist\u00e9r 2001; Pollefeys et al. 2004]), but, to the best of our knowledge, no one has yet attempted automatic single-view modeling."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1219093,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "82ebff86c9f862522d5a78ecac3717243daddd43",
            "isKey": false,
            "numCitedBy": 1036,
            "numCiting": 109,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper a complete system to build visual models from camera images is presented. The system can deal with uncalibrated image sequences acquired with a hand-held camera. Based on tracked or matched features the relations between multiple views are computed. From this both the structure of the scene and the motion of the camera are retrieved. The ambiguity on the reconstruction is restricted from projective to metric through self-calibration. A flexible multi-view stereo matching scheme is used to obtain a dense estimation of the surface geometry. From the computed data different types of visual models are constructed. Besides the traditional geometry- and image-based approaches, a combined approach with view-dependent geometry and texture is presented. As an application fusion of real and virtual scenes is also shown."
            },
            "slug": "Visual-Modeling-with-a-Hand-Held-Camera-Pollefeys-Gool",
            "title": {
                "fragments": [],
                "text": "Visual Modeling with a Hand-Held Camera"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "A complete system to build visual models from camera images is presented and a combined approach with view-dependent geometry and texture is presented, as an application fusion of real and virtual scenes is also shown."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152827088"
                        ],
                        "name": "Li Zhang",
                        "slug": "Li-Zhang",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403813014"
                        ],
                        "name": "Guillaume Dugas-Phocion",
                        "slug": "Guillaume-Dugas-Phocion",
                        "structuredName": {
                            "firstName": "Guillaume",
                            "lastName": "Dugas-Phocion",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guillaume Dugas-Phocion"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144041018"
                        ],
                        "name": "Jean-Sebastien Samson",
                        "slug": "Jean-Sebastien-Samson",
                        "structuredName": {
                            "firstName": "Jean-Sebastien",
                            "lastName": "Samson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jean-Sebastien Samson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679223"
                        ],
                        "name": "S. Seitz",
                        "slug": "S.-Seitz",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Seitz",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Seitz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 0
                            }
                        ],
                        "text": "[Zhang et al. 2001] models free-form scenes by letting the user place constraints, such as normal directions, anywhere on the image plane and then optimizing for the best 3D model to fit these constraints."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 345139,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0193a8ca0dc5c34cb81cccb8070666d6275738c7",
            "isKey": false,
            "numCitedBy": 150,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a novel approach for reconstructing free-form, texture-mapped, 3D scene models from a single painting or photograph. Given a sparse set of user-specified constraints on the local shape of the scene, a smooth 3D surface that satisfies the constraints is generated This problem is formulated as a constrained variational optimization problem. In contrast to previous work in single view reconstruction, our technique enables high quality reconstructions of free-form curved surfaces with arbitrary reflectance properties. A key feature of the approach is a novel hierarchical transformation technique for accelerating convergence on a non-uniform, piecewise continuous grid. The technique is interactive and updates the model in real time as constraints are added, allowing fast reconstruction of photorealistic scene models. The approach is shown to yield high quality results on a large variety of images."
            },
            "slug": "Single-view-modeling-of-free-form-scenes-Zhang-Dugas-Phocion",
            "title": {
                "fragments": [],
                "text": "Single view modeling of free-form scenes"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The technique is interactive and updates the model in real time as constraints are added, allowing fast reconstruction of photorealistic scene models, and is shown to yield high quality results on a large variety of images."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40166275"
                        ],
                        "name": "D. Liebowitz",
                        "slug": "D.-Liebowitz",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Liebowitz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Liebowitz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716777"
                        ],
                        "name": "A. Criminisi",
                        "slug": "A.-Criminisi",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Criminisi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Criminisi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 47
                            }
                        ],
                        "text": "Figure 6: Original image taken from results of [Liebowitz et al. 1999] and two novel views from the 3D model generated by our system."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 0
                            }
                        ],
                        "text": "[Liebowitz et al. 1999; Criminisi et al. 2000] offer the most accurate (but also the most labor-intensive) approach, recovering a metric reconstruction of an architectural scene by using projective geometry constraints [Hartley and Zisserman 2004] to compute 3D locations of user-specified points given their projected distances from the ground plane."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 1
                            }
                        ],
                        "text": "[Liebowitz et al. 1999; \nCriminisi et al. 2000] offer the most accurate (but also the most labor-intensive) approach, re\u00adcovering \na metric reconstruction of an architectural scene by using projective geometry constraints [Hartley and \nZisserman 2004] to compute3Dlocationsof user-speci.ed\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 84
                            }
                        ],
                        "text": "If the labeled ground appears above Figure 6: Original image taken from results of \n[Liebowitz et al. 1999] and two novel views from the 3D model generated by our system."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14667994,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "e8686936c7572c6b3d683720d68acafe676f58c5",
            "isKey": true,
            "numCitedBy": 326,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": "We present methods for creating 3D graphical models of scenes from a limited numbers of images, i.e. one or two, in situations where no scene co\u2010ordinate measurements are available. The methods employ constraints available from geometric relationships that are common in architectural scenes \u2013 such as parallelism and orthogonality \u2013 together with constraints available from the camera. In particular, by using the circular points of a plane simple, linear algorithms are given for computing plane rectification, plane orientation and camera calibration from a single image. Examples of image based 3D modelling are given for both single images and image pairs."
            },
            "slug": "Creating-Architectural-Models-from-Images-Liebowitz-Criminisi",
            "title": {
                "fragments": [],
                "text": "Creating Architectural Models from Images"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "Methods for creating 3D graphical models of scenes from a limited numbers of images, i.e. one or two, in situations where no scene co\u2010ordinate measurements are available are presented."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Graph. Forum"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3083483"
                        ],
                        "name": "D. Nist\u00e9r",
                        "slug": "D.-Nist\u00e9r",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Nist\u00e9r",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Nist\u00e9r"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 107
                            }
                        ],
                        "text": "Automatic methodsexistto reconstruct \ncertain typesof scenes from multiple images or video sequences (e.g. [Nist\u00b4er 2001; Pollefeys et al. \n2004]), but, to the best of our knowledge, no one has yet attempted automatic single-view modeling."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 62222198,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b7fa589eec4658cf7f5beecb4b34273ad41c46f1",
            "isKey": false,
            "numCitedBy": 127,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This thesis describes a system that completely automaticallybuilds a three-dimensional model of a scene given a sequence ofimages of the scene. The system also estimates the internalparameters of the camera and the poses from where the originalimages were taken. Results that have been produced from realworld sequences acquired with a handheld video camera arepresented.The main contribution of the thesis is in building acomplete system and applying it to full-scale real worldproblems, thereby facing the practical difficulties of far fromideal imagery. Contributions are also made to several systemcomponents, most notably in dealing with variable amounts ofmotion between frames, auto-calibration and densereconstruction from a large number of images. Thesecontributions are presented as appended papers to enable theexperienced reader to easily study the novelty of the thesis.The main text gives a detailed coherent account of thetheoretical foundation for the system and its components.There are several motivations for constructing systems ofthe proposed type. One motivation is to make it possible forany amateur photographer to produce graphical models of theworld with the use of a computer. The viewer of the materialcan then navigate through the model and view it from any point.Another application is the insertion of synthetic objects intoan existing video sequence. This task is frequently carried outin movie making but is then performed with a great deal ofexpensive manual work. A quite futuristic but highlyinteresting application is augmented reality where theuser\u0092s view of the world is augmented by the insertion ofsynthetic objects."
            },
            "slug": "Automatic-Dense-Reconstruction-from-Uncalibrated-Nist\u00e9r",
            "title": {
                "fragments": [],
                "text": "Automatic Dense Reconstruction from Uncalibrated Video Sequences"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "This thesis describes a system that completely automatically builds a three-dimensional model of a scene given a sequence of images of the scene, and estimates the internalparameters of the camera and the poses from where the original images were taken."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2158146169"
                        ],
                        "name": "H. Kang",
                        "slug": "H.-Kang",
                        "structuredName": {
                            "firstName": "Hyung",
                            "lastName": "Kang",
                            "middleNames": [
                                "Woo"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Kang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1491450937"
                        ],
                        "name": "Soon Hyung Pyo",
                        "slug": "Soon-Hyung-Pyo",
                        "structuredName": {
                            "firstName": "Soon",
                            "lastName": "Pyo",
                            "middleNames": [
                                "Hyung"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Soon Hyung Pyo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794380"
                        ],
                        "name": "K. Anjyo",
                        "slug": "K.-Anjyo",
                        "structuredName": {
                            "firstName": "Ken-ichi",
                            "lastName": "Anjyo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Anjyo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "11467825"
                        ],
                        "name": "Sung-yong Shin",
                        "slug": "Sung-yong-Shin",
                        "structuredName": {
                            "firstName": "Sung-yong",
                            "lastName": "Shin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sung-yong Shin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 145
                            }
                        ],
                        "text": "This is a severe limitation (that would affect most of the images in this paper, including Figure 1(left)) which has been partially addressed by [Kang et al. 2001] and [Oh et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 146
                            }
                        ],
                        "text": "This is a severe limita\u00adtion (that would affect most of \nthe images in this paper, including Figure 1(left)) which has been partially addressed by [Kang et al. \n2001]and[Ohetal."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1868566,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8c814dc53b5b7072ee3e4a8ffc101a76ca4c59bc",
            "isKey": false,
            "numCitedBy": 64,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Tour into the picture (TIP) proposed by Horry et al.13 is a method for generating a sequence of walk\u2010through images from a single reference picture (or image). By navigating a 3D scene model constructed from the picture, TIP produces convincing 3D effects. Assuming that the picture has one vanishing point, they proposed the scene modeling scheme called spidery mesh. However, this scheme has to go through major modification when the picture contains multiple vanishing points or does not have any well\u2010defined vanishing point. Moreover, the spidery mesh is hard to generalize for other types of images such as panoramic images. In this paper, we propose a new scheme for TIP which is based on a single vanishing line instead of a vanishing point. Based on projective geometry, our scheme is simple and yet general enough to address the problems faced with the previous method. We also show that our scheme can be naturally extended to a panoramic image."
            },
            "slug": "Tour-Into-the-Picture-using-a-Vanishing-Line-and-to-Kang-Pyo",
            "title": {
                "fragments": [],
                "text": "Tour Into the Picture using a Vanishing Line and its Extension to Panoramic Images"
            },
            "tldr": {
                "abstractSimilarityScore": 36,
                "text": "A new scheme for TIP which is based on a single vanishing line instead of a vanishing point is proposed, which is simple and yet general enough to address the problems faced with the previous method."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Graph. Forum"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739995"
                        ],
                        "name": "Y. Horry",
                        "slug": "Y.-Horry",
                        "structuredName": {
                            "firstName": "Youichi",
                            "lastName": "Horry",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Horry"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794380"
                        ],
                        "name": "K. Anjyo",
                        "slug": "K.-Anjyo",
                        "structuredName": {
                            "firstName": "Ken-ichi",
                            "lastName": "Anjyo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Anjyo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144050755"
                        ],
                        "name": "K. Arai",
                        "slug": "K.-Arai",
                        "structuredName": {
                            "firstName": "Kiyoshi",
                            "lastName": "Arai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Arai"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 23
                            }
                        ],
                        "text": "Tour into the \nPicture [Horry et al. 1997], the main inspiration for this work, models a scene as an axis-aligned box, \na sort of theater stage, with .oor, ceiling, backdrop, and two side planes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 22
                            }
                        ],
                        "text": "Tour into the Picture [Horry et al. 1997], the main inspiration for this work, models a scene as an axis-aligned box, a sort of theater stage, with floor, ceiling, backdrop, and two side planes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6914801,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "d30c530bfebfa6e6d31ccb2bd503ebb17845aab6",
            "isKey": false,
            "numCitedBy": 452,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "A new method called TIP (Tour Into the Picture) is presented for easily making animations from one 2D picture or photograph of a scene. In TIP, animation is created from the viewpoint of a camera which can be three-dimensionally \"walked or flownthrough\" the 2D picture or photograph. To make such animation, conventional computer vision techniques cannot be applied in the 3D modeling process for the scene, using only a single 2D image. Instead a spidery mesh is employed in our method to obtain a simple scene model from the 2D image of the scene using a graphical user interface. Animation is thus easily generated without the need of multiple 2D images. Unlike existing methods, our method is not intended to construct a precise 3D scene model. The scene model is rather simple, and not fully 3D-structured. The modeling process starts by specifying the vanishing point in the 2D image. The background in the scene model then consists of at most five rectangles, whereas hierarchical polygons are used as a model for each foreground object. Furthermore a virtual camera is moved around the 3D scene model, with the viewing angle being freely controlled. This process is easily and effectively performed using the spidery mesh interface. We have obtained a wide variety of animated scenes which demonstrate the efficiency of TIP. CR"
            },
            "slug": "Tour-into-the-picture:-using-a-spidery-mesh-to-make-Horry-Anjyo",
            "title": {
                "fragments": [],
                "text": "Tour into the picture: using a spidery mesh interface to make animation from a single image"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "A new method called TIP (Tour Into the Picture) is presented for easily making animations from one 2D picture or photograph of a scene using a graphical user interface, which is not intended to construct a precise 3D scene model."
            },
            "venue": {
                "fragments": [],
                "text": "SIGGRAPH"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40240299"
                        ],
                        "name": "R. Ziegler",
                        "slug": "R.-Ziegler",
                        "structuredName": {
                            "firstName": "Remo",
                            "lastName": "Ziegler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Ziegler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752521"
                        ],
                        "name": "W. Matusik",
                        "slug": "W.-Matusik",
                        "structuredName": {
                            "firstName": "Wojciech",
                            "lastName": "Matusik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Matusik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143758236"
                        ],
                        "name": "H. Pfister",
                        "slug": "H.-Pfister",
                        "structuredName": {
                            "firstName": "Hanspeter",
                            "lastName": "Pfister",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Pfister"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145170879"
                        ],
                        "name": "L. McMillan",
                        "slug": "L.-McMillan",
                        "structuredName": {
                            "firstName": "Leonard",
                            "lastName": "McMillan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. McMillan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 0
                            }
                        ],
                        "text": "[Ziegler et al. 2003] finds the maximum-volume 3D model consistent with multiple manually-labeled images."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 1
                            }
                        ],
                        "text": "[Ziegler et al. \n2003] .nds the maximum-volume3D model consistent with multiple manually-labeled images."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3064473,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6c6c7ee35a5b49a9df847342885cc3de714d95bc",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present a novel algorithm for reconstructing 3D scenes from a set of images. The user defines a set of polygonal regions with corresponding labels in each image using familiar 2D photo-editing tools. Our reconstruction algorithm computes the 3D model with maximum volume that is consistent with the set of regions in the input images. The algorithm is fast, uses only 2D intersection operations, and directly computes a polygonal model. We implemented a user-assisted system for 3D scene reconstruction and show results on scenes that are difficult or impossible to reconstruct with other methods."
            },
            "slug": "3D-Reconstruction-Using-Labeled-Image-Regions-Ziegler-Matusik",
            "title": {
                "fragments": [],
                "text": "3D Reconstruction Using Labeled Image Regions"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A user-assisted system for 3D scene reconstruction is implemented and results on scenes that are difficult or impossible to reconstruct with other methods are shown."
            },
            "venue": {
                "fragments": [],
                "text": "Symposium on Geometry Processing"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1778676"
                        ],
                        "name": "P. Debevec",
                        "slug": "P.-Debevec",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Debevec",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Debevec"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31589308"
                        ],
                        "name": "C. J. Taylor",
                        "slug": "C.-J.-Taylor",
                        "structuredName": {
                            "firstName": "Camillo",
                            "lastName": "Taylor",
                            "middleNames": [
                                "Jose"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. J. Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 39
                            }
                        ],
                        "text": "Popular urban modeling systems \nsuch as Fac\u00b8ade [Debevec et al. 1996], Photo-Builder [Cipolla et al. 1999] and REALVIZ ImageModeler greatly \nreduce the number of images required and use no special equipment (although cameras must still be calibrated), \nbut at the expense of considerable user interactionanda speci.c domainof applicability."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 46
                            }
                        ],
                        "text": "Popular urban modeling systems such as Fa\u00e7ade [Debevec et al. 1996], PhotoBuilder [Cipolla et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 47
                            }
                        ],
                        "text": "Popular urban modeling systems \nsuch as Fac\u00b8ade [Debevec et al. 1996], Photo-Builder [Cipolla et al. 1999] and REALVIZ ImageModeler greatly \nreduce the number of images required and use no special equipment (although cameras must still be calibrated), \nbut at the expense of considerable user\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2609415,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "37b0da92da8796835383f85c65cc81a386052a99",
            "isKey": false,
            "numCitedBy": 1992,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new approach for modeling and rendering existing architectural scenes from a sparse set of still photographs. Our modeling approach, which combines both geometry-based and imagebased techniques, has two components. The first component is a photogrammetricmodeling method which facilitates the recovery of the basic geometry of the photographed scene. Our photogrammetric modeling approach is effective, convenient, and robust because it exploits the constraints that are characteristic of architectural scenes. The second component is a model-based stereo algorithm, which recovers how the real scene deviates from the basic model. By making use of the model, our stereo technique robustly recovers accurate depth from widely-spaced image pairs. Consequently, our approach can model large architectural environments with far fewer photographs than current image-based modeling approaches. For producing renderings, we present view-dependent texture mapping, a method of compositing multiple views of a scene that better simulates geometric detail on basic models. Our approach can be used to recover models for use in either geometry-based or image-based rendering systems. We present results that demonstrate our approach\u2019s ability to create realistic renderings of architectural scenes from viewpoints far from the original photographs. CR Descriptors: I.2.10 [Artificial Intelligence]: Vision and Scene Understanding Modeling and recovery of physical attributes; I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism Color, shading, shadowing, and texture I.4.8 [Image Processing]: Scene Analysis Stereo; J.6 [Computer-Aided Engineering]: Computer-aided design (CAD)."
            },
            "slug": "Modeling-and-rendering-architecture-from-a-hybrid-Debevec-Taylor",
            "title": {
                "fragments": [],
                "text": "Modeling and rendering architecture from photographs: a hybrid geometry- and image-based approach"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "This work presents a new approach for modeling and rendering existing architectural scenes from a sparse set of still photographs, which combines both geometry-based and imagebased techniques, and presents view-dependent texture mapping, a method of compositing multiple views of a scene that better simulates geometric detail on basic models."
            },
            "venue": {
                "fragments": [],
                "text": "SIGGRAPH"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745672"
                        ],
                        "name": "R. Cipolla",
                        "slug": "R.-Cipolla",
                        "structuredName": {
                            "firstName": "Roberto",
                            "lastName": "Cipolla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Cipolla"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34574494"
                        ],
                        "name": "D. Robertson",
                        "slug": "D.-Robertson",
                        "structuredName": {
                            "firstName": "Duncan",
                            "lastName": "Robertson",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Robertson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719388"
                        ],
                        "name": "Edmond Boyer",
                        "slug": "Edmond-Boyer",
                        "structuredName": {
                            "firstName": "Edmond",
                            "lastName": "Boyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Edmond Boyer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 20
                            }
                        ],
                        "text": "1996], PhotoBuilder [Cipolla et al. 1999] and REALVIZ ImageModeler greatly reduce the number of images required and use no special equipment (although cameras must still be calibrated), but at the expense of considerable user interaction and a specific domain of applicability."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 69
                            }
                        ],
                        "text": "Popular urban modeling systems \nsuch as Fac\u00b8ade [Debevec et al. 1996], Photo-Builder [Cipolla et al. 1999] and REALVIZ ImageModeler greatly \nreduce the number of images required and use no special equipment (although cameras must still be calibrated), \nbut at the expense of considerable user interactionanda speci.c domainof applicability."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 84
                            }
                        ],
                        "text": "Popular urban modeling systems \nsuch as Fac\u00b8ade [Debevec et al. 1996], Photo-Builder [Cipolla et al. 1999] and REALVIZ ImageModeler greatly \nreduce the number of images required and use no special equipment (although cameras must still be calibrated), \nbut at the expense of considerable user\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7598127,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c84b6aff4b5fc6af1429b62f99f62cb66d65c83e",
            "isKey": false,
            "numCitedBy": 70,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the problem of recovering 3D models from uncalibrated images of architectural scenes. We propose a simple, geometrically intuitive method which exploits the strong rigidity constraints of parallelism and orthogonality present in indoor and outdoor architectural scenes. We show how these simple constraints can be used to calibrate the cameras and to recover the projection matrices for each viewpoint. The projection matrices are used to recover partial 3D models of the scene and these can be used to visualise new viewpoints. Our approach does not need any a priori information about the cameras being used. A working system called PhotoBuilder had been designed and implemented to allow a user to interactively build a VRML model of a building from uncalibrated images from arbitrary viewpoints."
            },
            "slug": "PhotoBuilder-3D-models-of-architectural-scenes-from-Cipolla-Robertson",
            "title": {
                "fragments": [],
                "text": "PhotoBuilder-3D models of architectural scenes from uncalibrated images"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "A simple, geometrically intuitive method is proposed which exploits the strong rigidity constraints of parallelism and orthogonality present in indoor and outdoor architectural scenes to recover 3D models from uncalibrated images of architectural scenes."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings IEEE International Conference on Multimedia Computing and Systems"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1801789"
                        ],
                        "name": "M. Levoy",
                        "slug": "M.-Levoy",
                        "structuredName": {
                            "firstName": "Marc",
                            "lastName": "Levoy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Levoy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144872229"
                        ],
                        "name": "P. Hanrahan",
                        "slug": "P.-Hanrahan",
                        "structuredName": {
                            "firstName": "Pat",
                            "lastName": "Hanrahan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hanrahan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 141
                            }
                        ],
                        "text": "#169; 2005 ACM 0730-0301/05/0700-0577 $5.00 The most general image-based rendering approaches, such \nas Quicktime VR [Chen 1995], Light.elds [Levoy and Hanrahan 1996], and Lumigraph [Gortler et al. 1996] \nall require a huge num\u00adber of photographs as well as special equipment."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 97
                            }
                        ],
                        "text": "The most general image-based rendering approaches, such as Quicktime VR [Chen 1995], Lightfields [Levoy and Hanrahan 1996], and Lumigraph [Gortler et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1363510,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bad341086b9f4fd66f4102c10f11f433f76a621f",
            "isKey": true,
            "numCitedBy": 3992,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "A number of techniques have been proposed for flying through scenes by redisplaying previously rendered or digitized views. Techniques have also been proposed for interpolating between views by warping input images, using depth information or correspondences between multiple images. In this paper, we describe a simple and robust method for generating new views from arbitrary camera positions without depth information or feature matching, simply by combining and resampling the available images. The key to this technique lies in interpreting the input images as 2D slices of a 4D function the light field. This function completely characterizes the flow of light through unobstructed space in a static scene with fixed illumination. We describe a sampled representation for light fields that allows for both efficient creation and display of inward and outward looking views. We hav e created light fields from large arrays of both rendered and digitized images. The latter are acquired using a video camera mounted on a computer-controlled gantry. Once a light field has been created, new views may be constructed in real time by extracting slices in appropriate directions. Since the success of the method depends on having a high sample rate, we describe a compression system that is able to compress the light fields we have generated by more than a factor of 100:1 with very little loss of fidelity. We also address the issues of antialiasing during creation, and resampling during slice extraction. CR Categories: I.3.2 [Computer Graphics]: Picture/Image Generation \u2014 Digitizing and scanning, Viewing algorithms; I.4.2 [Computer Graphics]: Compression \u2014 Approximate methods Additional keywords: image-based rendering, light field, holographic stereogram, vector quantization, epipolar analysis"
            },
            "slug": "Light-field-rendering-Levoy-Hanrahan",
            "title": {
                "fragments": [],
                "text": "Light field rendering"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper describes a sampled representation for light fields that allows for both efficient creation and display of inward and outward looking views, and describes a compression system that is able to compress the light fields generated by more than a factor of 100:1 with very little loss of fidelity."
            },
            "venue": {
                "fragments": [],
                "text": "SIGGRAPH"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1410019001"
                        ],
                        "name": "Shenchang Eric Chen",
                        "slug": "Shenchang-Eric-Chen",
                        "structuredName": {
                            "firstName": "Shenchang Eric",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shenchang Eric Chen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 117
                            }
                        ],
                        "text": "#169; 2005 ACM 0730-0301/05/0700-0577 $5.00 The most general image-based rendering approaches, such \nas Quicktime VR [Chen 1995], Light.elds [Levoy and Hanrahan 1996], and Lumigraph [Gortler et al. 1996] \nall require a huge num\u00adber of photographs as well as special equipment."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 72
                            }
                        ],
                        "text": "The most general image-based rendering approaches, such as Quicktime VR [Chen 1995], Lightfields [Levoy and Hanrahan 1996], and Lumigraph [Gortler et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6213603,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fd15bde71933b991f994f15aa02f866ec59aafce",
            "isKey": true,
            "numCitedBy": 1508,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Traditionally, virtual reality systems use 3D computer graphics to model and render virtual environments in real-time. This approach usually requires laborious modeling and expensive special purpose rendering hardware. The rendering quality and scene complexity are often limited because of the real-time constraint. This paper presents a new approach which uses 360-degree cylindrical panoramic images to compose a virtual environment. The panoramic image is digitally warped on-the-fly to simulate camera panning and zooming. The panoramic images can be created with computer rendering, specialized panoramic cameras or by \"stitching\" together overlapping photographs taken with a regular camera. Walking in a space is currently accomplished by \"hopping\" to different panoramic points. The image-based approach has been used in the commercial product QuickTime VR, a virtual reality extension to Apple Computer's QuickTime digital multimedia framework. The paper describes the architecture, the file format, the authoring process and the interactive players of the VR system. In addition to panoramic viewing, the system includes viewing of an object from different directions and hit-testing through orientation-independent hot spots. CR"
            },
            "slug": "QuickTime-VR:-an-image-based-approach-to-virtual-Chen",
            "title": {
                "fragments": [],
                "text": "QuickTime VR: an image-based approach to virtual environment navigation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper presents a new approach which uses 360-degree cylindrical panoramic images to compose a virtual environment which includes viewing of an object from different directions and hit-testing through orientation-independent hot spots."
            },
            "venue": {
                "fragments": [],
                "text": "SIGGRAPH"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716777"
                        ],
                        "name": "A. Criminisi",
                        "slug": "A.-Criminisi",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Criminisi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Criminisi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145950884"
                        ],
                        "name": "I. Reid",
                        "slug": "I.-Reid",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Reid",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Reid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 0
                            }
                        ],
                        "text": "[Liebowitz et al. 1999; Criminisi et al. 2000] offer the most accurate (but also the most labor-intensive) approach, recovering a metric reconstruction of an architectural scene by using projective geometry constraints [Hartley and Zisserman 2004] to compute 3D locations of user-specified points given their projected distances from the ground plane."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 24
                            }
                        ],
                        "text": "[Liebowitz et al. 1999; \nCriminisi et al. 2000] offer the most accurate (but also the most labor-intensive) approach, re\u00adcovering \na metric reconstruction of an architectural scene by using projective geometry constraints [Hartley and \nZisserman 2004] to compute3Dlocationsof user-speci.ed\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2499410,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9e96483e264f5a063cadfa232fa325a56d146a0b",
            "isKey": false,
            "numCitedBy": 750,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe how 3D affine measurements may be computed from a single perspective view of a scene given only minimal geometric information determined from the image. This minimal information is typically the vanishing line of a reference plane, and a vanishing point for a direction not parallel to the plane. It is shown that affine scene structure may then be determined from the image, without knowledge of the camera's internal calibration (e.g. focal length), nor of the explicit relation between camera and world (pose).In particular, we show how to (i) compute the distance between planes parallel to the reference plane (up to a common scale factor); (ii) compute area and length ratios on any plane parallel to the reference plane; (iii) determine the camera's location. Simple geometric derivations are given for these results. We also develop an algebraic representation which unifies the three types of measurement and, amongst other advantages, permits a first order error propagation analysis to be performed, associating an uncertainty with each measurement.We demonstrate the technique for a variety of applications, including height measurements in forensic images and 3D graphical modelling from single images."
            },
            "slug": "Single-View-Metrology-Criminisi-Reid",
            "title": {
                "fragments": [],
                "text": "Single View Metrology"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "An algebraic representation is developed which unifies the three types of measurement and permits a first order error propagation analysis to be performed, associating an uncertainty with each measurement."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743020"
                        ],
                        "name": "J. Kosecka",
                        "slug": "J.-Kosecka",
                        "structuredName": {
                            "firstName": "Jana",
                            "lastName": "Kosecka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kosecka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Wei Zhang",
                        "slug": "Wei-Zhang",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 71
                            }
                        ],
                        "text": "Our system finds long, straight edges in the image using the method of [Kosecka and Zhang 2002]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1413778,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d47bca050a09e69be810fd7c677ed5aefe66c797",
            "isKey": false,
            "numCitedBy": 369,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we describe a flexible approach for determining the relative orientation of the camera with respect to the scene. The main premise of the approach is the fact that in man-made environments, the majority of lines is aligned with the principal orthogonal directions of the world coordinate frame. We exploit this observation towards efficient detection and estimation of vanishing points, which provide strong constraints on camera parameters and relative orientation of the camera with respect to the scene.By combining efficient image processing techniques in the line detection and initialization stage we demonstrate that simultaneous grouping and estimation of vanishing directions can be achieved in the absence of internal parameters of the camera. Constraints between vanishing points are then used for partial calibration and relative rotation estimation. The algorithm has been tested in a variety of indoors and outdoors scenes and its efficiency and automation makes it amenable for implementation on robotic platforms."
            },
            "slug": "Video-Compass-Kosecka-Zhang",
            "title": {
                "fragments": [],
                "text": "Video Compass"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "A flexible approach for determining the relative orientation of the camera with respect to the scene based on the fact that in man-made environments, the majority of lines is aligned with the principal orthogonal directions of the world coordinate frame is described."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145163573"
                        ],
                        "name": "A. Singhal",
                        "slug": "A.-Singhal",
                        "structuredName": {
                            "firstName": "Amit",
                            "lastName": "Singhal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Singhal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33642939"
                        ],
                        "name": "Jiebo Luo",
                        "slug": "Jiebo-Luo",
                        "structuredName": {
                            "firstName": "Jiebo",
                            "lastName": "Luo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiebo Luo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34763422"
                        ],
                        "name": "W. Zhu",
                        "slug": "W.-Zhu",
                        "structuredName": {
                            "firstName": "Weiyu",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Zhu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14356209,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "01c2f706c107c9853a86d1a2d6cd8e6f82b31db1",
            "isKey": false,
            "numCitedBy": 191,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Scene content understanding facilitates a large number of applications, ranging from content-based image retrieval to other multimedia applications. Material detection refers to the problem of identifying key semantic material types (such as sky, grass, foliage, water, and snow in images). In this paper, we present a holistic approach to determining scene content, based on a set of individual material detection algorithms, as well as probabilistic spatial context models. A major limitation of individual material detectors is the significant number of misclassifications that occur because of the similarities in color and texture characteristics of various material types. We have developed a spatial context-aware material detection system that reduces misclassification by constraining the beliefs to conform to the probabilistic spatial context models. Experimental results show that the accuracy of materials detection is improved by 13% using the spatial context models over the individual material detectors themselves."
            },
            "slug": "Probabilistic-spatial-context-models-for-scene-Singhal-Luo",
            "title": {
                "fragments": [],
                "text": "Probabilistic spatial context models for scene content understanding"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A spatial context-aware material detection system that reduces misclassification by constraining the beliefs to conform to the probabilistic spatial context models is developed."
            },
            "venue": {
                "fragments": [],
                "text": "2003 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2003. Proceedings."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2841773"
                        ],
                        "name": "S. Konishi",
                        "slug": "S.-Konishi",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Konishi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Konishi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145081362"
                        ],
                        "name": "A. Yuille",
                        "slug": "A.-Yuille",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Yuille",
                            "middleNames": [
                                "Loddon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yuille"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 203,
                                "start": 133
                            }
                        ],
                        "text": "However, unlike most scene recognition approaches which aim to model semantic classes, such as cars, vegetation, roads, or buildings [Everingham et al. 1999; Konishi and Yuille 2000; Singhal et al. 2003], our goal is to model geometric classes that depend on the orientation of a physical object with relation to the scene."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14777835,
            "fieldsOfStudy": [
                "Environmental Science"
            ],
            "id": "9083e6709b52cba61b8a6c9d2628c489b5e2c1db",
            "isKey": false,
            "numCitedBy": 97,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper investigates the use of colour and texture cues for segmentation of images within two specified domains. The first is the Sowerby dataset, which contains one hundred colour photographs of country roads in England that have been interactively segmented and classified into six classes-edge, vegetation, air, road, building, and other. The second domain is a set of thirty five-images, taken in San Francisco, which have been interactively segmented into similar classes. In each domain we learn the joint probability distributions of filter responses, based on colour and texture, for each class. These distributions are then used for classification. We restrict ourselves to a limited number of filters in order to ensure that the learnt filter responses do not overfit the training data (our region classes are chosen so as to ensure that there is enough data to avoid over fitting). We do performance analysis on the two datasets by evaluating the false positive and false negative error rates for the classification. This shows that the learnt models achieve high accuracy in classifying individual pixels into those classes for which the filter responses are approximately spatially homogeneous (i.e. road, vegetation, and air but not edge and building). A more sensitive performance measure, the Chernoff information, is calculated in order to quantify how well the cues for edge and building are doing. This demonstrates that statistical knowledge of the domain is a powerful tool for segmentation."
            },
            "slug": "Statistical-cues-for-domain-specific-image-with-Konishi-Yuille",
            "title": {
                "fragments": [],
                "text": "Statistical cues for domain specific image segmentation with performance analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "Performance analysis on the Sowerby dataset shows that the learnt models achieve high accuracy in classifying individual pixels into those classes for which the filter responses are approximately spatially homogeneous."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings IEEE Conference on Computer Vision and Pattern Recognition. CVPR 2000 (Cat. No.PR00662)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111190090"
                        ],
                        "name": "Yin Li",
                        "slug": "Yin-Li",
                        "structuredName": {
                            "firstName": "Yin",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yin Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152147500"
                        ],
                        "name": "Jian Sun",
                        "slug": "Jian-Sun",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2088295"
                        ],
                        "name": "Chi-Keung Tang",
                        "slug": "Chi-Keung-Tang",
                        "structuredName": {
                            "firstName": "Chi-Keung",
                            "lastName": "Tang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chi-Keung Tang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144154486"
                        ],
                        "name": "H. Shum",
                        "slug": "H.-Shum",
                        "structuredName": {
                            "firstName": "Harry",
                            "lastName": "Shum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Shum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 93
                            }
                        ],
                        "text": "Future work could include the following improvements: 1) use segmentation techniques such as [Li et al. 2004] to improve labeling accuracy near region boundaries (our initial attempts at this have not been successful) or to segment out foreground objects; 2) estimate the orientation of vertical regions from the image data, allowing a more robust polyline fit; and 3) an extension of the system to indoor scenes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 146
                            }
                        ],
                        "text": "Superpixels correspond to small, nearly-uniform regions in the image and have been found useful by other computer vision and graphics researchers [Tao et al. 2001; Ren and Malik 2003; Li et al. 2004]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1904479,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4f51d84009a1ec30dffa5ab8a1c3214f669086cf",
            "isKey": false,
            "numCitedBy": 650,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present Lazy Snapping, an interactive image cutout tool. Lazy Snapping separates coarse and fine scale processing, making object specification and detailed adjustment easy. Moreover, Lazy Snapping provides instant visual feedback, snapping the cutout contour to the true object boundary efficiently despite the presence of ambiguous or low contrast edges. Instant feedback is made possible by a novel image segmentation algorithm which combines graph cut with pre-computed over-segmentation. A set of intuitive user interface (UI) tools is designed and implemented to provide flexible control and editing for the users. Usability studies indicate that Lazy Snapping provides a better user experience and produces better segmentation results than the state-of-the-art interactive image cutout tool, Magnetic Lasso in Adobe Photoshop."
            },
            "slug": "Lazy-snapping-Li-Sun",
            "title": {
                "fragments": [],
                "text": "Lazy snapping"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Usability studies indicate that Lazy Snapping provides a better user experience and produces better segmentation results than the state-of-the-art interactive image cutout tool, Magnetic Lasso in Adobe Photoshop."
            },
            "venue": {
                "fragments": [],
                "text": "ACM Trans. Graph."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2114833718"
                        ],
                        "name": "Xiaofeng Ren",
                        "slug": "Xiaofeng-Ren",
                        "structuredName": {
                            "firstName": "Xiaofeng",
                            "lastName": "Ren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaofeng Ren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 146
                            }
                        ],
                        "text": "Superpixels correspond to small, nearly-uniform regions in the image and have been found useful by other computer vision and graphics researchers [Tao et al. 2001; Ren and Malik 2003; Li et al. 2004]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 101
                            }
                        ],
                        "text": "The constellation likelihoods are based on the features xkand \ngraphics researchers [Tao et al. 2001; Ren and Malik 2003; Li computed over the constellation s spatial \nsupport."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13571735,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9a9049a50dfe94fa4473880a9b60c99333ade685",
            "isKey": false,
            "numCitedBy": 1644,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a two-class classification model for grouping. Human segmented natural images are used as positive examples. Negative examples of grouping are constructed by randomly matching human segmentations and images. In a preprocessing stage an image is over-segmented into super-pixels. We define a variety of features derived from the classical Gestalt cues, including contour, texture, brightness and good continuation. Information-theoretic analysis is applied to evaluate the power of these grouping cues. We train a linear classifier to combine these features. To demonstrate the power of the classification model, a simple algorithm is used to randomly search for good segmentations. Results are shown on a wide range of images."
            },
            "slug": "Learning-a-classification-model-for-segmentation-Ren-Malik",
            "title": {
                "fragments": [],
                "text": "Learning a classification model for segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "A two-class classification model for grouping is proposed that defines a variety of features derived from the classical Gestalt cues, including contour, texture, brightness and good continuation, and trains a linear classifier to combine these features."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Ninth IEEE International Conference on Computer Vision"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144891282"
                        ],
                        "name": "David R. Martin",
                        "slug": "David-R.-Martin",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Martin",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David R. Martin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143800213"
                        ],
                        "name": "Charless C. Fowlkes",
                        "slug": "Charless-C.-Fowlkes",
                        "structuredName": {
                            "firstName": "Charless",
                            "lastName": "Fowlkes",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charless C. Fowlkes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2082299938"
                        ],
                        "name": "D. Tal",
                        "slug": "D.-Tal",
                        "structuredName": {
                            "firstName": "Doron",
                            "lastName": "Tal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Tal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 64193,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9a1ed876196ec9733acb1daa6d65e35ff0414291",
            "isKey": false,
            "numCitedBy": 6038,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a database containing 'ground truth' segmentations produced by humans for images of a wide variety of natural scenes. We define an error measure which quantifies the consistency between segmentations of differing granularities and find that different human segmentations of the same image are highly consistent. Use of this dataset is demonstrated in two applications: (1) evaluating the performance of segmentation algorithms and (2) measuring probability distributions associated with Gestalt grouping factors as well as statistics of image region properties."
            },
            "slug": "A-database-of-human-segmented-natural-images-and-to-Martin-Fowlkes",
            "title": {
                "fragments": [],
                "text": "A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "A database containing 'ground truth' segmentations produced by humans for images of a wide variety of natural scenes is presented and an error measure is defined which quantifies the consistency between segmentations of differing granularities."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35106875"
                        ],
                        "name": "R. Duda",
                        "slug": "R.-Duda",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Duda",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Duda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3108177"
                        ],
                        "name": "P. Hart",
                        "slug": "P.-Hart",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Hart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2586918"
                        ],
                        "name": "D. Stork",
                        "slug": "D.-Stork",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Stork",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Stork"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 92
                            }
                        ],
                        "text": "Each likelihood function Pm in the weak learner is obtained using kernel density estimation [Duda et al. 2000] over the mth weighted distribution."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 361680,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "e75fceff79fefa063d00ebc56a20c7df5485cf2b",
            "isKey": false,
            "numCitedBy": 4146,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Pattern-classification,-2nd-Edition-Duda-Hart",
            "title": {
                "fragments": [],
                "text": "Pattern classification, 2nd Edition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "17173992"
                        ],
                        "name": "B. Wrobel",
                        "slug": "B.-Wrobel",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Wrobel",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Wrobel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 206,
                                "start": 178
                            }
                        ],
                        "text": "2000] offer the most accurate (but also the most labor-intensive) approach, recovering a metric reconstruction of an architectural scene by using projective geometry constraints [Hartley and Zisserman 2004] to compute 3D locations of user-specified points given their projected distances from the ground plane."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 106
                            }
                        ],
                        "text": "Knowledge of the vanishing line of a plane completely specifies its 3D orientation relative to the viewer [Hartley and Zisserman 2004], but such information cannot easily be extracted from outdoor, relatively unstructured images."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 105
                            }
                        ],
                        "text": "Knowledge of the vanishing line of a plane completely \nspec\u00adi.es its3D orientation relative to the viewer [Hartley and Zisser\u00adman 2004], but such information \ncannot easily be extracted from outdoor, relatively unstructured images."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 193,
                                "start": 167
                            }
                        ],
                        "text": "\u2026the most accurate (but also the most labor-intensive) approach, re\u00adcovering \na metric reconstruction of an architectural scene by using projective geometry constraints [Hartley and \nZisserman 2004] to compute3Dlocationsof user-speci.ed pointsgiven their projected distances from the \nground plane."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 44793400,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "339093c7ed71919ce59a7e78979a77abd25bad0c",
            "isKey": true,
            "numCitedBy": 16324,
            "numCiting": 222,
            "paperAbstract": {
                "fragments": [],
                "text": "Downloading the book in this website lists can give you more advantages. It will show you the best book collections and completed collections. So many books can be found in this website. So, this is not only this multiple view geometry in computer vision. However, this book is referred to read because it is an inspiring book to give you more chance to get experiences and also thoughts. This is simple, read the soft file of the book and you get it."
            },
            "slug": "Multiple-View-Geometry-in-Computer-Vision-Wrobel",
            "title": {
                "fragments": [],
                "text": "Multiple View Geometry in Computer Vision"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "This book is referred to read because it is an inspiring book to give you more chance to get experiences and also thoughts and it will show the best book collections and completed collections."
            },
            "venue": {
                "fragments": [],
                "text": "K\u00fcnstliche Intell."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35106875"
                        ],
                        "name": "R. Duda",
                        "slug": "R.-Duda",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Duda",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Duda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3108177"
                        ],
                        "name": "P. Hart",
                        "slug": "P.-Hart",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Hart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hart"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 114
                            }
                        ],
                        "text": "Foreachregion,we.tasetofline segments to the region s \nboundary with the labeled ground using the Hough transform [Duda and Hart 1972]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 122
                            }
                        ],
                        "text": "For each region, we fit a set of line segments to the region\u2019s boundary with the labeled ground using the Hough transform [Duda and Hart 1972]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1105637,
            "fieldsOfStudy": [
                "Geology"
            ],
            "id": "2415fd60305739543105118739f7118493257af3",
            "isKey": false,
            "numCitedBy": 6426,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Hough has proposed an interesting and computationally efficient procedure for detecting lines in pictures. This paper points out that the use of angle-radius rather than slope-intercept parameters simplifies the computation further. It also shows how the method can be used for more general curve fitting, and gives alternative interpretations that explain the source of its efficiency."
            },
            "slug": "Use-of-the-Hough-transformation-to-detect-lines-and-Duda-Hart",
            "title": {
                "fragments": [],
                "text": "Use of the Hough transformation to detect lines and curves in pictures"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is pointed out that the use of angle-radius rather than slope-intercept parameters simplifies the computation further, and how the method can be used for more general curve fitting."
            },
            "venue": {
                "fragments": [],
                "text": "CACM"
            },
            "year": 1972
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143707112"
                        ],
                        "name": "M. Collins",
                        "slug": "M.-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 241,
                                "start": 222
                            }
                        ],
                        "text": "From this data, to share \nits label, maximizing the average pairwise log-likelihoods we estimate the pairwise likelihood function \nusing the logistic re\u00adwith other superpixels in the constellation: gression form of Adaboost [Collins \net al. 2002]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "Adaboost combines an ensemble of estimators to im\u00adprove \naccuracyover anysingle density estimate."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 80
                            }
                        ],
                        "text": "The likelihood functions are \nestimated using the logistic regression version of Ad\u00adaboost[Collinsetal.2002]withweak learners basedon \neight-node decision trees [Quinlan 1993; Friedman et al. 2000]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 108
                            }
                        ],
                        "text": "From this data, we estimate the pairwise likelihood function using the logistic regression form of Adaboost [Collins et al. 2002]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 15
                            }
                        ],
                        "text": "We used twenty Adaboost iterations for the \nlearning of the pairwise likelihood and geometric labeling functions."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 89
                            }
                        ],
                        "text": "The likelihood functions are estimated using the logistic regression version of Adaboost [Collins et al. 2002] with weak learners based on eight-node decision trees [Quinlan 1993; Friedman et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 207651918,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b54c9359e8858842d1b1b744ac5ca573b8031dcc",
            "isKey": true,
            "numCitedBy": 662,
            "numCiting": 103,
            "paperAbstract": {
                "fragments": [],
                "text": "We give a unified account of boosting and logistic regression in which each learning problem is cast in terms of optimization of Bregman distances. The striking similarity of the two problems in this framework allows us to design and analyze algorithms for both simultaneously, and to easily adapt algorithms designed for one problem to the other. For both problems, we give new algorithms and explain their potential advantages over existing methods. These algorithms are iterative and can be divided into two types based on whether the parameters are updated sequentially (one at a time) or in parallel (all at once). We also describe a parameterized family of algorithms that includes both a sequential- and a parallel-update algorithm as special cases, thus showing how the sequential and parallel approaches can themselves be unified. For all of the algorithms, we give convergence proofs using a general formalization of the auxiliary-function proof technique. As one of our sequential-update algorithms is equivalent to AdaBoost, this provides the first general proof of convergence for AdaBoost. We show that all of our algorithms generalize easily to the multiclass case, and we contrast the new algorithms with the iterative scaling algorithm. We conclude with a few experimental results with synthetic data that highlight the behavior of the old and newly proposed algorithms in different settings."
            },
            "slug": "Logistic-Regression,-AdaBoost-and-Bregman-Distances-Collins-Schapire",
            "title": {
                "fragments": [],
                "text": "Logistic Regression, AdaBoost and Bregman Distances"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "A unified account of boosting and logistic regression in which each learning problem is cast in terms of optimization of Bregman distances, and a parameterized family of algorithms that includes both a sequential- and a parallel-update algorithm as special cases are described, thus showing how the sequential and parallel approaches can themselves be unified."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341779"
                        ],
                        "name": "J. R. Quinlan",
                        "slug": "J.-R.-Quinlan",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Quinlan",
                            "middleNames": [
                                "Ross"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. R. Quinlan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 159
                            }
                        ],
                        "text": "The likelihood functions are \nestimated using the logistic regression version of Ad\u00adaboost[Collinsetal.2002]withweak learners basedon \neight-node decision trees [Quinlan 1993; Friedman et al. 2000]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5262555,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "807c1f19047f96083e13614f7ce20f2ac98c239a",
            "isKey": false,
            "numCitedBy": 21897,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nClassifier systems play a major role in machine learning and knowledge-based systems, and Ross Quinlan's work on ID3 and C4.5 is widely acknowledged to have made some of the most significant contributions to their development. This book is a complete guide to the C4.5 system as implemented in C for the UNIX environment. It contains a comprehensive guide to the system's use , the source code (about 8,800 lines), and implementation notes. The source code and sample datasets are also available on a 3.5-inch floppy diskette for a Sun workstation. \n \nC4.5 starts with large sets of cases belonging to known classes. The cases, described by any mixture of nominal and numeric properties, are scrutinized for patterns that allow the classes to be reliably discriminated. These patterns are then expressed as models, in the form of decision trees or sets of if-then rules, that can be used to classify new cases, with emphasis on making the models understandable as well as accurate. The system has been applied successfully to tasks involving tens of thousands of cases described by hundreds of properties. The book starts from simple core learning methods and shows how they can be elaborated and extended to deal with typical problems such as missing data and over hitting. Advantages and disadvantages of the C4.5 approach are discussed and illustrated with several case studies. \n \nThis book and software should be of interest to developers of classification-based intelligent systems and to students in machine learning and expert systems courses."
            },
            "slug": "C4.5:-Programs-for-Machine-Learning-Quinlan",
            "title": {
                "fragments": [],
                "text": "C4.5: Programs for Machine Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A complete guide to the C4.5 system as implemented in C for the UNIX environment, which starts from simple core learning methods and shows how they can be elaborated and extended to deal with typical problems such as missing data and over hitting."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35106875"
                        ],
                        "name": "R. Duda",
                        "slug": "R.-Duda",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Duda",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Duda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3108177"
                        ],
                        "name": "P. Hart",
                        "slug": "P.-Hart",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Hart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2586918"
                        ],
                        "name": "D. Stork",
                        "slug": "D.-Stork",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Stork",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Stork"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 92
                            }
                        ],
                        "text": "Each likelihood function Pm in the weak learner is obtained using kernel density estimation [Duda et al. 2000] over the mth weighted distribution."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 361680,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "e75fceff79fefa063d00ebc56a20c7df5485cf2b",
            "isKey": false,
            "numCitedBy": 4146,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Pattern-classification,-2nd-Edition-Duda-Hart",
            "title": {
                "fragments": [],
                "text": "Pattern classification, 2nd Edition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685089"
                        ],
                        "name": "Pedro F. Felzenszwalb",
                        "slug": "Pedro-F.-Felzenszwalb",
                        "structuredName": {
                            "firstName": "Pedro",
                            "lastName": "Felzenszwalb",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pedro F. Felzenszwalb"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713089"
                        ],
                        "name": "D. Huttenlocher",
                        "slug": "D.-Huttenlocher",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Huttenlocher",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Huttenlocher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 59
                            }
                        ],
                        "text": "Our implementation uses the over-segmentation technique of [Felzenszwalb and Huttenlocher 2004]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 59
                            }
                        ],
                        "text": "Our implementation \nuses the over-segmentation tech\u00adniqueof [Felzenszwalb and Huttenlocher2004]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 207663697,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aeeffe327e6c93e9010c7b1e401caa9113723851",
            "isKey": false,
            "numCitedBy": 3751,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper addresses the problem of segmenting an image into regions. We define a predicate for measuring the evidence for a boundary between two regions using a graph-based representation of the image. We then develop an efficient segmentation algorithm based on this predicate, and show that although this algorithm makes greedy decisions it produces segmentations that satisfy global properties. We apply the algorithm to image segmentation using two different kinds of local neighborhoods in constructing the graph, and illustrate the results with both real and synthetic images. The algorithm runs in time nearly linear in the number of graph edges and is also fast in practice. An important characteristic of the method is its ability to preserve detail in low-variability image regions while ignoring detail in high-variability regions."
            },
            "slug": "Efficient-Graph-Based-Image-Segmentation-Felzenszwalb-Huttenlocher",
            "title": {
                "fragments": [],
                "text": "Efficient Graph-Based Image Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "An efficient segmentation algorithm is developed based on a predicate for measuring the evidence for a boundary between two regions using a graph-based representation of the image and it is shown that although this algorithm makes greedy decisions it produces segmentations that satisfy global properties."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1767791"
                        ],
                        "name": "B. Oh",
                        "slug": "B.-Oh",
                        "structuredName": {
                            "firstName": "Byong",
                            "lastName": "Oh",
                            "middleNames": [
                                "Mok"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Oh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108828899"
                        ],
                        "name": "Max Chen",
                        "slug": "Max-Chen",
                        "structuredName": {
                            "firstName": "Max",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Max Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143607090"
                        ],
                        "name": "J. Dorsey",
                        "slug": "J.-Dorsey",
                        "structuredName": {
                            "firstName": "Julie",
                            "lastName": "Dorsey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Dorsey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145403226"
                        ],
                        "name": "F. Durand",
                        "slug": "F.-Durand",
                        "structuredName": {
                            "firstName": "Fr\u00e9do",
                            "lastName": "Durand",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Durand"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 10
                            }
                        ],
                        "text": "2001] and [Oh et al. 2001], but at the cost of a less intuitive interface."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 52810271,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "30fbf8f722a41e19ff611290f6aab6dd8ce829c3",
            "isKey": false,
            "numCitedBy": 397,
            "numCiting": 206,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an image-based modeling and editing system that takes a single photo as input. We represent a scene as a layered collection of depth images, where each pixel encodes both color and depth. Starting from an input image, we employ a suite of user-assisted techniques, based on a painting metaphor, to assign depths and extract layers. We introduce two specific editing operations. The first, a \u201cclone brushing tool,\u201d permits the distortion-free copying of parts of a picture, by using a parameterization optimization technique. The second, a \u201ctexture-illuminance decoupling filter,\u201d discounts the effect of illumination on uniformly textured areas, by decoupling large- and small-scale features via bilateral filtering. Our system enables editing from different viewpoints, extracting and grouping of image-based objects, and modifying the shape, color, and illumination of these objects."
            },
            "slug": "Image-based-modeling-and-photo-editing-Oh-Chen",
            "title": {
                "fragments": [],
                "text": "Image-based modeling and photo editing"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "An image-based modeling and editing system that takes a single photo as input and employs a suite of user-assisted techniques, based on a painting metaphor, to assign depths and extract layers, enabling editing from different viewpoints and modifying the shape, color, and illumination of these objects."
            },
            "venue": {
                "fragments": [],
                "text": "SIGGRAPH"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056361"
                        ],
                        "name": "J. Friedman",
                        "slug": "J.-Friedman",
                        "structuredName": {
                            "firstName": "Jerome",
                            "lastName": "Friedman",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Friedman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784682"
                        ],
                        "name": "T. Hastie",
                        "slug": "T.-Hastie",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Hastie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Hastie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761784"
                        ],
                        "name": "R. Tibshirani",
                        "slug": "R.-Tibshirani",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Tibshirani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Tibshirani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703537"
                        ],
                        "name": "Y. Freund",
                        "slug": "Y.-Freund",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Freund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 60
                            }
                        ],
                        "text": "2002] with weak learners based on eight-node decision trees [Quinlan 1993; Friedman et al. 2000]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 193,
                                "start": 173
                            }
                        ],
                        "text": "The likelihood functions are \nestimated using the logistic regression version of Ad\u00adaboost[Collinsetal.2002]withweak learners basedon \neight-node decision trees [Quinlan 1993; Friedman et al. 2000]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2354909,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "277c2139eb4e11455a0b16759b7249c3b95b479e",
            "isKey": false,
            "numCitedBy": 1349,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "The main and important contribution of this paper is in establishing a connection between boosting, a newcomer to the statistics scene, and additive models. One of the main properties of boosting that has made it interesting to statisticians and others is its relative (but not complete) immunity to overrtting. As pointed out by the authors, the current paper does not address this issue. Leo Breiman 1] tried to explain this behaviour in terms of bias and variance. In our paper with Bartlett and Lee 4], we gave an explanation in terms of the \\margins\" of the training examples and the VC-dimension of the base class. Breiman, as well as the current paper, point out that our bounds are very rough and yield bounds that are not useful in practice. While this is clearly true at this time, it is also true that the analysis given by Breiman and by this paper yield no provable bounds whatsoever. It is completely unclear whether this analysis can be used to predict the performance of classiication rules outside of the training sample. At the root of this argument about boosting is a much more fundamental argument about the type of prior assumptions that one should make when embarking on the task of inducing a classiication rule from data. The assumption that seems to underlie the use of maximum likelihood in the 1"
            },
            "slug": "Discussion-of-the-Paper-\\additive-Logistic-a-View-Friedman-Hastie",
            "title": {
                "fragments": [],
                "text": "Discussion of the Paper \\additive Logistic Regression: a Statistical View of Boosting\" By"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "This paper establishes a connection between boosting, a newcomer to the statistics scene, and additive models and investigates the assumption that seems to underlie the use of maximum likelihood in the additive models."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Video compass Light field rendering"
            },
            "venue": {
                "fragments": [],
                "text": "Computer Vision and Pattern Recognition"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "European Conf. on Computer Vision"
            },
            "venue": {
                "fragments": [],
                "text": "European Conf. on Computer Vision"
            },
            "year": 2002
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 12,
            "methodology": 17
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 30,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Automatic-photo-pop-up-Hoiem-Efros/7b88fd20df0f0a1ea341c489026db169fa9dee99?sort=total-citations"
}