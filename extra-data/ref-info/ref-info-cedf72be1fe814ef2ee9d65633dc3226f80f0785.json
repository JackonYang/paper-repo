{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2907696"
                        ],
                        "name": "J. Shim",
                        "slug": "J.-Shim",
                        "structuredName": {
                            "firstName": "Jae",
                            "lastName": "Shim",
                            "middleNames": [
                                "Chang"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145253787"
                        ],
                        "name": "C. Dorai",
                        "slug": "C.-Dorai",
                        "structuredName": {
                            "firstName": "Chitra",
                            "lastName": "Dorai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Dorai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70029967"
                        ],
                        "name": "R. Bolle",
                        "slug": "R.-Bolle",
                        "structuredName": {
                            "firstName": "Ruud",
                            "lastName": "Bolle",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Bolle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Shim et al. [ 21 ] used the homogeneity of intensity of text regions in images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "applications including page segmentation [17, 18], address block location [19], license plate location [9, 20], and content- based image/video indexing [5,  21 ]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Shim et al. [ 21 ] 1998 Gray level difference between pairs of pixels Caption text, localization"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12062439,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1eb854ce0539b6fd18dbba942f52a80a735f5c8e",
            "isKey": true,
            "numCitedBy": 142,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Efficient content-based retrieval of image and video databases is an important application due to rapid proliferation of digital video data on the Internet and corporate intranets. Text either embedded or superimposed within video frames is very useful for describing the contents of the frames, as it enables both keyword and free-text based search, automatic video logging, and video cataloging. We have developed a scheme for automatically extracting text from digital images and videos for content annotation and retrieval. We present our approach to robust text extraction from video frames, which can handle complex image backgrounds, deal with different font sizes, font styles, and font appearances such as normal and inverse video. Our algorithm results in segmented characters that can be directly processed by an OCR system to produce ASCII text. Results from our experiments with over 5000 frames obtained from twelve MPEG video streams demonstrate the good performance of our system in terms of text identification accuracy and computational efficiency."
            },
            "slug": "Automatic-text-extraction-from-video-for-annotation-Shim-Dorai",
            "title": {
                "fragments": [],
                "text": "Automatic text extraction from video for content-based annotation and retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work has developed a scheme for automatically extracting text from digital images and videos for content annotation and retrieval that results in segmented characters that can be directly processed by an OCR system to produce ASCII text."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. Fourteenth International Conference on Pattern Recognition (Cat. No.98EX170)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2856656"
                        ],
                        "name": "Hae-Kwang Kim",
                        "slug": "Hae-Kwang-Kim",
                        "structuredName": {
                            "firstName": "Hae-Kwang",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hae-Kwang Kim"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 4
                            }
                        ],
                        "text": "Kim [1] selected a frame from shots detected by a scene-change detection method as a candidate containing text."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 123
                            }
                        ],
                        "text": "\u2212 Video content analysis: Extracted text regions or the output of character recognition can be useful in genre recognition [1, 79]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 95
                            }
                        ],
                        "text": "Image content can be divided into two main categories: perceptual content and semantic content [1]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 156
                            }
                        ],
                        "text": "A CC-based method could segment a character into multiple CCs, especially in the cases of polychrome text strings and low-resolution and noisy video images [1]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 4
                            }
                        ],
                        "text": "Kim [1] segments an image using color clustering in a color histogram in the RGB space."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8081258,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f455a0f0e6d402648da33930fb39ef5587aab0e3",
            "isKey": true,
            "numCitedBy": 116,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract An efficient automatic text detection and location method for video documents is proposed and its application for the content-based retrieval of video is presented and discussed. Target frames are selected at fixed time intervals from shots detected by a scene-change detection method. For each selected frame, segmentation by color clustering is performed around color peaks using a color histogram. For each color plane, text-lines are detected using heuristics, and the temporal and spatial position and the text-image of each text-line are stored in a database. Experimental results for text detection in video images and the performance of the method are reported for various video documents. A user interface for text-image based browsing is designed for direct content-based access to video documents, and other applications are discussed."
            },
            "slug": "Efficient-Automatic-Text-Location-Method-and-and-of-Kim",
            "title": {
                "fragments": [],
                "text": "Efficient Automatic Text Location Method and Content-Based Indexing and Structuring of Video Database"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "An efficient automatic text detection and location method for video documents is proposed and its application for the content-based retrieval of video is presented and discussed."
            },
            "venue": {
                "fragments": [],
                "text": "J. Vis. Commun. Image Represent."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144602022"
                        ],
                        "name": "K. Kim",
                        "slug": "K.-Kim",
                        "structuredName": {
                            "firstName": "Kwang",
                            "lastName": "Kim",
                            "middleNames": [
                                "In"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "121267347"
                        ],
                        "name": "K. Jung",
                        "slug": "K.-Jung",
                        "structuredName": {
                            "firstName": "Keechul",
                            "lastName": "Jung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Jung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115290599"
                        ],
                        "name": "Se Hyun Park",
                        "slug": "Se-Hyun-Park",
                        "structuredName": {
                            "firstName": "Se",
                            "lastName": "Park",
                            "middleNames": [
                                "Hyun"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Se Hyun Park"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70261685"
                        ],
                        "name": "Hang-Joon Kim",
                        "slug": "Hang-Joon-Kim",
                        "structuredName": {
                            "firstName": "Hang-Joon",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hang-Joon Kim"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[52] used support vector machines (SVMs) for analyzing the textural properties of text in images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 28448043,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "198a090d5a34dbbbc3fc7b3d74b26e0938665606",
            "isKey": false,
            "numCitedBy": 93,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "Textual data within video frames are very useful for describing the contents of the video frames, as they enable both keyword and free-text-based searching. In this paper, we pose the problem of text location in digital video as an example of supervised texture classification and use a support vector machine (SVM) as the texture classifier. Unlike other text detection methods, we do not incorporate any explicit texture feature extraction scheme. Instead, the gray-level values of the raw pixels are directly fed to the classifier. This is based on the observation that a SVM has the capability of learning in a high-dimensional space and of incorporating a feature extraction scheme in its own architecture. In comparison with a neural network-based text detection method, the SVM classifier illustrates the excellence of the proposed method."
            },
            "slug": "Support-vector-machine-based-text-detection-in-Kim-Jung",
            "title": {
                "fragments": [],
                "text": "Support vector machine-based text detection in digital video"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper poses the problem of text location in digital video as an example of supervised texture classification and uses a support vector machine (SVM) as the texture classifier, which illustrates the excellence of the proposed method."
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055230387"
                        ],
                        "name": "K. Jeong",
                        "slug": "K.-Jeong",
                        "structuredName": {
                            "firstName": "Ki-Young",
                            "lastName": "Jeong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Jeong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "121267347"
                        ],
                        "name": "K. Jung",
                        "slug": "K.-Jung",
                        "structuredName": {
                            "firstName": "Keechul",
                            "lastName": "Jung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Jung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715155"
                        ],
                        "name": "Eun Yi Kim",
                        "slug": "Eun-Yi-Kim",
                        "structuredName": {
                            "firstName": "Eun",
                            "lastName": "Kim",
                            "middleNames": [
                                "Yi"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eun Yi Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70261685"
                        ],
                        "name": "Hang-Joon Kim",
                        "slug": "Hang-Joon-Kim",
                        "structuredName": {
                            "firstName": "Hang-Joon",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hang-Joon Kim"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[50] also use a similar approach for TIE in complex color images, where a neural network is employed to train a set of texture discrimination masks that minimize the classification error for the two texture classes: text regions and non-text regions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9424151,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "de034f320270091bcf7e80400ad5bf5340b3a46a",
            "isKey": false,
            "numCitedBy": 46,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "The retrieval of video clips from multimedia databases has been increasingly spotlighted. Texts in videos include useful information for automatic annotation or indexing. Text location is the first step for recognizing the textual information. This paper proposes a neural network-based text location method for news video indexing. Text can be characterized by texture, location, alignment, and font size. The proposed method classifies text pixels and non-text pixels using a network that operates as a set of texture discrimination filters. We find and locate text regions using histogram analysis after removing errors in the classification results. Experimental results show that the proposed method is effective at locating texts."
            },
            "slug": "Neural-network-based-text-location-for-news-video-Jeong-Jung",
            "title": {
                "fragments": [],
                "text": "Neural network-based text location for news video indexing"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A neural network-based text location method for news video indexing using a network that operates as a set of texture discrimination filters and results show that the proposed method is effective at locating texts."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348)"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144739319"
                        ],
                        "name": "R. Lienhart",
                        "slug": "R.-Lienhart",
                        "structuredName": {
                            "firstName": "Rainer",
                            "lastName": "Lienhart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lienhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1750165"
                        ],
                        "name": "W. Effelsberg",
                        "slug": "W.-Effelsberg",
                        "structuredName": {
                            "firstName": "Wolfgang",
                            "lastName": "Effelsberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Effelsberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 0
                            }
                        ],
                        "text": "[30, 37] regard text regions as CCs with the same or similar color and size, and apply motion analysis to enhance the text extraction results for a video sequence."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 9
                            }
                        ],
                        "text": "Lienhart [30, 37] described a block-matching algorithm, which is an international standard for video compression such as H."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1306072,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "53ecdc232d2e640f890c831944761fe5604af033",
            "isKey": false,
            "numCitedBy": 202,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract. Efficient indexing and retrieval of digital video is an important function of video databases. One powerful index for retrieval is the text appearing in them. It enables content-based browsing. We present our new methods for automatic segmentation of text in digital videos. The algorithms we propose make use of typical characteristics of text in videos in order to enable and enhance segmentation performance. The unique features of our approach are the tracking of characters and words over their complete duration of occurrence in a video and the integration of the multiple bitmaps of a character over time into a single bitmap. The output of the text segmentation step is then directly passed to a standard OCR software package in order to translate the segmented text into ASCII. Also, a straightforward indexing and retrieval scheme is introduced. It is used in the experiments to demonstrate that the proposed text segmentation algorithms together with existing text recognition algorithms are suitable for indexing and retrieval of relevant video sequences in and from a video database. Our experimental results are very encouraging and suggest that these algorithms can be used in video retrieval applications as well as to recognize higher level semantics in videos."
            },
            "slug": "Automatic-text-segmentation-and-text-recognition-Lienhart-Effelsberg",
            "title": {
                "fragments": [],
                "text": "Automatic text segmentation and text recognition for video indexing"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The proposed text segmentation algorithms together with existing text recognition algorithms are suitable for indexing and retrieval of relevant video sequences in and from a video database."
            },
            "venue": {
                "fragments": [],
                "text": "Multimedia Systems"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115196978"
                        ],
                        "name": "Huiping Li",
                        "slug": "Huiping-Li",
                        "structuredName": {
                            "firstName": "Huiping",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huiping Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48471936"
                        ],
                        "name": "D. Doermann",
                        "slug": "D.-Doermann",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Doermann",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Doermann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3272081"
                        ],
                        "name": "O. Kia",
                        "slug": "O.-Kia",
                        "structuredName": {
                            "firstName": "Omid",
                            "lastName": "Kia",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Kia"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[51] 2000 Wavelet-based feature extraction and neural network for Scene text (slanted), localization, enhancement,"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "[51,53] for localizing and tracking text in video, where a small window (typically 16\u00d7 16) is applied to scan an image."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[51] classiAed pixels at regular intervals and interpolated the pixels located between the classiAed pixels."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "[51,53] presented several approaches for text enhancement."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 12
                            }
                        ],
                        "text": "Li\u2019s method [51] has no explicit feature extraction stage, unlike other approaches such as wavelets, FFT, and Gabor-based feature extraction."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15485643,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f8f5c282dc11937d29183b955dc3e4fbb677571b",
            "isKey": true,
            "numCitedBy": 652,
            "numCiting": 74,
            "paperAbstract": {
                "fragments": [],
                "text": "Text that appears in a scene or is graphically added to video can provide an important supplemental source of index information as well as clues for decoding the video's structure and for classification. In this work, we present algorithms for detecting and tracking text in digital video. Our system implements a scale-space feature extractor that feeds an artificial neural processor to detect text blocks. Our text tracking scheme consists of two modules: a sum of squared difference (SSD)-based module to find the initial position and a contour-based module to refine the position. Experiments conducted with a variety of video sources show that our scheme can detect and track text robustly."
            },
            "slug": "Automatic-text-detection-and-tracking-in-digital-Li-Doermann",
            "title": {
                "fragments": [],
                "text": "Automatic text detection and tracking in digital video"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work presents algorithms for detecting and tracking text in digital video that implements a scale-space feature extractor that feeds an artificial neural processor to detect text blocks."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Image Process."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1841097"
                        ],
                        "name": "B. Chun",
                        "slug": "B.-Chun",
                        "structuredName": {
                            "firstName": "Byung",
                            "lastName": "Chun",
                            "middleNames": [
                                "Tae"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Chun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3108050"
                        ],
                        "name": "Younglae Bae",
                        "slug": "Younglae-Bae",
                        "structuredName": {
                            "firstName": "Younglae",
                            "lastName": "Bae",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Younglae Bae"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111100677"
                        ],
                        "name": "Tai-Yun Kim",
                        "slug": "Tai-Yun-Kim",
                        "structuredName": {
                            "firstName": "Tai-Yun",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tai-Yun Kim"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[54] 1999 Filtering using neural network after FFT Caption text, localization"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 21029051,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "12d3ab5be7d015e027eb226d6d1ba1389e6445ae",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "Popular methods for extracting a text region in video images are in general based on analysis of a whole image such as merge and split method, and comparison of two frames. Thus, they take long computing time due to the use of a whole image. Therefore, this paper suggests the faster method of extracting a text region without processing a whole image. The proposed method uses line sampling methods, FFT and neural networks in order to extract texts in real time. In general, text areas are found in the higher frequency domain, thus, can be characterized using FFT. The candidate text areas can be thus found by applying the higher frequency characteristics to neural network. Therefore, the final text area is extracted by verifying the candidate areas. Experimental results show a perfect candidate extraction rate and about 92% text extraction rate. The strength of the proposed algorithm is its simplicity, real-time processing by not processing the entire image, and fast skipping of the images that do not contain a text."
            },
            "slug": "Automatic-text-extraction-in-digital-videos-using-Chun-Bae",
            "title": {
                "fragments": [],
                "text": "Automatic text extraction in digital videos using FFT and neural network"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The strength of the proposed algorithm is its simplicity, real-time processing by not processing the entire image, and fast skipping of the images that do not contain a text."
            },
            "venue": {
                "fragments": [],
                "text": "FUZZ-IEEE'99. 1999 IEEE International Fuzzy Systems. Conference Proceedings (Cat. No.99CH36315)"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3115459"
                        ],
                        "name": "Young-Kyu Lim",
                        "slug": "Young-Kyu-Lim",
                        "structuredName": {
                            "firstName": "Young-Kyu",
                            "lastName": "Lim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Young-Kyu Lim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2520496"
                        ],
                        "name": "Song-Ha Choi",
                        "slug": "Song-Ha-Choi",
                        "structuredName": {
                            "firstName": "Song-Ha",
                            "lastName": "Choi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Song-Ha Choi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50112753"
                        ],
                        "name": "Seong-Whan Lee",
                        "slug": "Seong-Whan-Lee",
                        "structuredName": {
                            "firstName": "Seong-Whan",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Seong-Whan Lee"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[33] made a simple assumption that text usually has a higher intensity than the background."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 39670541,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "12ea3d31c0dfb93433bb3db2c45101c21e6af8d1",
            "isKey": false,
            "numCitedBy": 61,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Video text extraction is a core technique for multimedia applications such as news-on-demand (NOD) and digital libraries, and research about video text extraction have been conducted vigorously. In this paper, we propose an efficient method for extracting texts in MPEG compressed videos for content-based indexing. The proposed method makes the best use of 2-level DCT coefficients and macroblock type information in MPEG compressed video, and this method can be organized into three stages to increase overall performance; text frame detection, text region extraction, and character extraction. The main advantage of the proposed method is that it can avoid the overhead of decompressing video into individual frames in the pixel domain. We evaluated this method using various types of news video data."
            },
            "slug": "Text-extraction-in-MPEG-compressed-video-for-Lim-Choi",
            "title": {
                "fragments": [],
                "text": "Text extraction in MPEG compressed video for content-based indexing"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "The main advantage of the proposed method is that it can avoid the overhead of decompressing video into individual frames in the pixel domain and make the best use of 2-level DCT coefficients and macroblock type information in MPEG compressed video."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 15th International Conference on Pattern Recognition. ICPR-2000"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721328"
                        ],
                        "name": "Sameer Kiran Antani",
                        "slug": "Sameer-Kiran-Antani",
                        "structuredName": {
                            "firstName": "Sameer",
                            "lastName": "Antani",
                            "middleNames": [
                                "Kiran"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sameer Kiran Antani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3110392"
                        ],
                        "name": "R. Kasturi",
                        "slug": "R.-Kasturi",
                        "structuredName": {
                            "firstName": "Rangachar",
                            "lastName": "Kasturi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kasturi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144938740"
                        ],
                        "name": "R. Jain",
                        "slug": "R.-Jain",
                        "structuredName": {
                            "firstName": "Ramesh",
                            "lastName": "Jain",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Jain"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9828531,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "950d56ca43b56e78da79f4ed07d5448ed80475b6",
            "isKey": false,
            "numCitedBy": 372,
            "numCiting": 194,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-survey-on-the-use-of-pattern-recognition-methods-Antani-Kasturi",
            "title": {
                "fragments": [],
                "text": "A survey on the use of pattern recognition methods for abstraction, indexing and retrieval of images and video"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2124754"
                        ],
                        "name": "U. Gargi",
                        "slug": "U.-Gargi",
                        "structuredName": {
                            "firstName": "Ullas",
                            "lastName": "Gargi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Gargi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721328"
                        ],
                        "name": "Sameer Kiran Antani",
                        "slug": "Sameer-Kiran-Antani",
                        "structuredName": {
                            "firstName": "Sameer",
                            "lastName": "Antani",
                            "middleNames": [
                                "Kiran"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sameer Kiran Antani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3110392"
                        ],
                        "name": "R. Kasturi",
                        "slug": "R.-Kasturi",
                        "structuredName": {
                            "firstName": "Rangachar",
                            "lastName": "Kasturi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kasturi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 75
                            }
                        ],
                        "text": "[22] used a formal evaluation method for TIE, and selected five algorithms [29, 57, 58, 59, 71] as promising."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 0
                            }
                        ],
                        "text": "[29, 32] performed text detection using the assumption that the number of intracoded blocks in P- and B- frames of an MPEG compressed video increases, when a text caption appears."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 92
                            }
                        ],
                        "text": "Table 1 shows a list of properties that have been utilized in recently published algorithms [25-30]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 30086203,
            "fieldsOfStudy": [
                "Computer Science",
                "Art"
            ],
            "id": "7e3ed958f25974dab09b445fbd6f297852b761fc",
            "isKey": false,
            "numCitedBy": 70,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Like shot changes, the presence of text in digital video is an important event that can be used to index digital video and provide extremely useful semantic information about the scene content. The special characteristics of digital video compared to document images both require and allow new robust approaches to recognition of text in video. We discuss the characteristics and special challenges of text in video and present a strategy of detecting, localizing, and segmenting text from video data for the text indexing problem. Preliminary results from our approach are presented."
            },
            "slug": "Indexing-text-events-in-digital-video-databases-Gargi-Antani",
            "title": {
                "fragments": [],
                "text": "Indexing text events in digital video databases"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The characteristics and special challenges of text in video are discussed and a strategy of detecting, localizing, and segmenting text from video data for the text indexing problem is presented."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. Fourteenth International Conference on Pattern Recognition (Cat. No.98EX170)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48764203"
                        ],
                        "name": "Victor Wu",
                        "slug": "Victor-Wu",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Victor Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758550"
                        ],
                        "name": "R. Manmatha",
                        "slug": "R.-Manmatha",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Manmatha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Manmatha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31338632"
                        ],
                        "name": "E. Riseman",
                        "slug": "E.-Riseman",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Riseman",
                            "middleNames": [
                                "M."
                            ],
                            "suffix": ""
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Riseman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "[45,46] segment an input image using a multi-scale texture segmentation scheme."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 208945,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3420ab835c1af02071364b1f4e0f69abf733d88c",
            "isKey": false,
            "numCitedBy": 263,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "There are many applications in which the automatic detection and recognition of text embedded in images is useful. These applications include digad libraries, multimedia systems, and Geographical Information Systems. When machine generated text is prdnted against clean backgrounds, it can be converted to a computer readable form (ASCII) using current Optical Character Recognition (OCR) technology. However, text is often printed against shaded or textured backgrounds or is embedded in images. Examples include maps, advertisements, photographs, videos and stock certificates. Current document segmentation and recognition technologies cannot handle these situafons well. In this paper, a four-step system which automaticnlly detects and extracts text in images i& proposed. First, a texture segmentation scheme is used to focus attention on regions where text may occur. Second, strokes are extracted from the segmented text regions. Using reasonable heuristics on text strings such as height similarity, spacing and alignment, the extracted strokes are then processed to form rectangular boxes surrounding the corresponding ttzt strings. To detect text over a wide range of font sizes, the above steps are first applied to a pyramid of images generated from the input image, and then the boxes formed at each resolution level of the pyramid are fused at the image in the original resolution level. Third, text is extracted by cleaning up the background and binarizing the detected ted strings. Finally, better text bounding boxes are generated by srsiny the binarized text as strokes. Text is then cleaned and binarized from these new boxes, and can then be passed through a commercial OCR engine for recognition if the text is of an OCR-recognizable font. The system is stable, robust, and works well on imayes (with or without structured layouts) from a wide van\u2019ety of sources, including digitized video frames, photographs, *This material is based on work supported in part by the National Science Foundation, Library of Congress and Department of Commerce under cooperative agreement number EEC9209623, in part by the United States Patent and mademark Office and Defense Advanced Research Projects Agency/IT0 under ARPA order number D468, issued by ESC/AXS contract number F19628-96-C-0235, in part by the National Science Foundation under grant number IF&9619117 and in part by NSF Multimedia CDA-9502639. Any opinions, findings and conclusions or recommendations expressed in this material are the author(s) and do not necessarily reflect those of the sponsors. Prrmission to make digital/hard copies ofall or part oflhis material for personal or clrrssroom use is granted without fee provided that the copies are not made or distributed for profit or commercial advantage, the copyright notice, the title ofthe publication and its date appear, and notice is given that copyright is by permission of the ACM, Inc. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires specific permission and/or fe DL 97 Philadelphia PA, USA Copyright 1997 AChi 0-89791~868-1197/7..$3.50 newspapers, advertisements, stock certifimtes, and personal checks. All parameters remain the same for-all the experiments."
            },
            "slug": "Finding-text-in-images-Wu-Manmatha",
            "title": {
                "fragments": [],
                "text": "Finding text in images"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A four-step system which automaticnlly detects and extracts text in images is proposed and works well on imayes (with or without structured layouts) from a wide range of sources, including digitized video frames, photographs, and personal checks."
            },
            "venue": {
                "fragments": [],
                "text": "DL '97"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784196"
                        ],
                        "name": "Datong Chen",
                        "slug": "Datong-Chen",
                        "structuredName": {
                            "firstName": "Datong",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Datong Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733733"
                        ],
                        "name": "H. Bourlard",
                        "slug": "H.-Bourlard",
                        "structuredName": {
                            "firstName": "Herv\u00e9",
                            "lastName": "Bourlard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bourlard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710257"
                        ],
                        "name": "J. Thiran",
                        "slug": "J.-Thiran",
                        "structuredName": {
                            "firstName": "Jean-Philippe",
                            "lastName": "Thiran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Thiran"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5170600,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e8cb23672f5d94a75a7ed9cc7c870be398bc0259",
            "isKey": false,
            "numCitedBy": 151,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper presents a fast and robust algorithm to identify text in image or video frames with complex backgrounds and compression effects. The algorithm first extracts the candidate text line on the basis of edge analysis, baseline location and heuristic constraints. Support Vector Machine (SVM) is then used to identify text line from the candidates in edge-based distance map feature space. Experiments based on a large amount of images and video frames from different sources showed the advantages of this algorithm compared to conventional methods in both identification quality and computation time."
            },
            "slug": "Text-identification-in-complex-background-using-SVM-Chen-Bourlard",
            "title": {
                "fragments": [],
                "text": "Text identification in complex background using SVM"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "A fast and robust algorithm to identify text in image or video frames with complex backgrounds and compression effects with advantages compared to conventional methods in both identification quality and computation time is presented."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110062608"
                        ],
                        "name": "Toshio Sato",
                        "slug": "Toshio-Sato",
                        "structuredName": {
                            "firstName": "Toshio",
                            "lastName": "Sato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Toshio Sato"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733113"
                        ],
                        "name": "T. Kanade",
                        "slug": "T.-Kanade",
                        "structuredName": {
                            "firstName": "Takeo",
                            "lastName": "Kanade",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanade"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2816639"
                        ],
                        "name": "Ellen K. Hughes",
                        "slug": "Ellen-K.-Hughes",
                        "structuredName": {
                            "firstName": "Ellen",
                            "lastName": "Hughes",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ellen K. Hughes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116645471"
                        ],
                        "name": "Michael A. Smith",
                        "slug": "Michael-A.-Smith",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Smith",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael A. Smith"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Fig. 11. Examples of Sato et al. [ 11 ]\u2019s approach: (a) original image (204\u00d714 pixels), (b) binary image, (c) character"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "also attracted some recent interest [ 7-16 ]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The main difference between the method by Sato et al. [ 11 ] and other edge-based methods is their use of recognition-"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Sato et al. [ 11 ] 1998 Smith and Kanade\u2019s localization method and recognition-based character extraction Recognition"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "\u2212 Text-based image indexing: This involves automatic text-based video structuring methods using caption data [ 11 , 78]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "pixels), (e) integration of four character-extraction filters, (f) binary image, (g) result of character segmentation (courtesy of Sato et al. [ 11 ])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Sato et al. [ 11 ] used a linear interpolation technique to magnify small text at a higher resolution for commercial OCR"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 43395565,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "67c4ed0ef1c978defe1c44868029790aaad21752",
            "isKey": true,
            "numCitedBy": 275,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Video OCR is a technique that can greatly help to locate topics of interest in a large digital news video archive via the automatic extraction and reading of captions and annotations. News captions generally provide vital search information about the video being presented, the names of people and places or descriptions of objects. In this paper, two difficult problems of character recognition for videos are addressed: low resolution characters and extremely complex backgrounds. We apply an interpolation filter, multi-frame integration and a combination of four filters to solve these problems. Segmenting characters is done by a recognition-based segmentation method and intermediate character recognition results are used to improve the segmentation. The overall recognition results are good enough for use in news indexing. Performing video OCR on news video and combining its results with other video understanding techniques will improve the overall understanding of the news video content."
            },
            "slug": "Video-OCR-for-digital-news-archive-Sato-Kanade",
            "title": {
                "fragments": [],
                "text": "Video OCR for digital news archive"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper applies an interpolation filter, multi-frame integration and a combination of four filters to solve the problems of character recognition for videos: low resolution characters and extremely complex backgrounds."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 1998 IEEE International Workshop on Content-Based Access of Image and Video Database"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31459400"
                        ],
                        "name": "W. Mao",
                        "slug": "W.-Mao",
                        "structuredName": {
                            "firstName": "Weng",
                            "lastName": "Mao",
                            "middleNames": [
                                "Zu"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Mao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145288211"
                        ],
                        "name": "K. Chung",
                        "slug": "K.-Chung",
                        "structuredName": {
                            "firstName": "Korris",
                            "lastName": "Chung",
                            "middleNames": [
                                "Fu-Lai"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Chung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2595159"
                        ],
                        "name": "Kenneth K. M. Lam",
                        "slug": "Kenneth-K.-M.-Lam",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Lam",
                            "middleNames": [
                                "K.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenneth K. M. Lam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144250750"
                        ],
                        "name": "W. Siu",
                        "slug": "W.-Siu",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Siu",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Siu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[82] propose a texture-based text localization method using Wavelet transform."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5893059,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4c2098f5478256b50f8e1917ae3b60d5b69bb969",
            "isKey": false,
            "numCitedBy": 67,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a multiscale texture-based method using local energy analysis for hybrid Chinese/English text detection in images and video frames. Local energy analysis has been shown to work well in text detection, where remarkable local energy variations of pixels correspond to text region or boundary of other objects and lower local energy variations of pixels correspond to background or the interior of non-text objects. Local energy variation is calculated in a local region based on the wavelet transform coefficients of images. Hybrid Chinese/English text in images and video frames can be detected whether it is aligned horizontally or vertically. The font size of text to be detected may vary in a wide range of values. The proposed method has been tested on 321 frame images obtained from local TV programs and a tested dataset with low missed rate and false alarm rate."
            },
            "slug": "Hybrid-Chinese/English-text-detection-in-images-and-Mao-Chung",
            "title": {
                "fragments": [],
                "text": "Hybrid Chinese/English text detection in images and video frames"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "A multiscale texture-based method using local energy analysis for hybrid Chinese/English text detection in images and video frames and a tested dataset with low missed rate and false alarm rate is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "Object recognition supported by user interaction for service robots"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2821130"
                        ],
                        "name": "David J. Crandall",
                        "slug": "David-J.-Crandall",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Crandall",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David J. Crandall"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3110392"
                        ],
                        "name": "R. Kasturi",
                        "slug": "R.-Kasturi",
                        "structuredName": {
                            "firstName": "Rangachar",
                            "lastName": "Kasturi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kasturi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "utilized in recently published algorithms [ 25-30 ]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10575879,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5f39ae0cd7c874758500bae542f87709ec03038f",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Automatic content-based video indexing is an important research problem. One approach is to extract text appearing in video as an indication of a scene's semantic content. Most work so far has focused only on detecting the spatial extent of text instances in individual video frames. But text occurring in video usually persists for several seconds. This constitutes a text event that should be entered only once in the video index. Therefore it is necessary to determine the temporal extent of text events by combining the results of text detection on individual frames, over time. This is a nontrivial problem because a text event may move, rotate, grow, shrink, or otherwise change throughout its lifetime. Such text effects are common in television programs and commercials to attract viewer attention, but have so far been ignored in the literature. We present a method for detecting and tracking moving, changing caption text events in MPEG-1 compressed video."
            },
            "slug": "Robust-detection-of-stylized-text-events-in-digital-Crandall-Kasturi",
            "title": {
                "fragments": [],
                "text": "Robust detection of stylized text events in digital video"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work presents a method for detecting and tracking moving, changing caption text events in MPEG-1 compressed video, which has so far been ignored in the literature."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of Sixth International Conference on Document Analysis and Recognition"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715634"
                        ],
                        "name": "Ming-Hsuan Yang",
                        "slug": "Ming-Hsuan-Yang",
                        "structuredName": {
                            "firstName": "Ming-Hsuan",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming-Hsuan Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1765887"
                        ],
                        "name": "D. Kriegman",
                        "slug": "D.-Kriegman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Kriegman",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Kriegman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145237406"
                        ],
                        "name": "N. Ahuja",
                        "slug": "N.-Ahuja",
                        "structuredName": {
                            "firstName": "Narendra",
                            "lastName": "Ahuja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Ahuja"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9045232,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ebb34b75982f628f9ce5995821fff81fd967dc2d",
            "isKey": false,
            "numCitedBy": 3968,
            "numCiting": 251,
            "paperAbstract": {
                "fragments": [],
                "text": "Images containing faces are essential to intelligent vision-based human-computer interaction, and research efforts in face processing include face recognition, face tracking, pose estimation and expression recognition. However, many reported methods assume that the faces in an image or an image sequence have been identified and localized. To build fully automated systems that analyze the information contained in face images, robust and efficient face detection algorithms are required. Given a single image, the goal of face detection is to identify all image regions which contain a face, regardless of its 3D position, orientation and lighting conditions. Such a problem is challenging because faces are non-rigid and have a high degree of variability in size, shape, color and texture. Numerous techniques have been developed to detect faces in a single image, and the purpose of this paper is to categorize and evaluate these algorithms. We also discuss relevant issues such as data collection, evaluation metrics and benchmarking. After analyzing these algorithms and identifying their limitations, we conclude with several promising directions for future research."
            },
            "slug": "Detecting-Faces-in-Images:-A-Survey-Yang-Kriegman",
            "title": {
                "fragments": [],
                "text": "Detecting Faces in Images: A Survey"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2124754"
                        ],
                        "name": "U. Gargi",
                        "slug": "U.-Gargi",
                        "structuredName": {
                            "firstName": "Ullas",
                            "lastName": "Gargi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Gargi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2821130"
                        ],
                        "name": "David J. Crandall",
                        "slug": "David-J.-Crandall",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Crandall",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David J. Crandall"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721328"
                        ],
                        "name": "Sameer Kiran Antani",
                        "slug": "Sameer-Kiran-Antani",
                        "structuredName": {
                            "firstName": "Sameer",
                            "lastName": "Antani",
                            "middleNames": [
                                "Kiran"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sameer Kiran Antani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2372810"
                        ],
                        "name": "T. Gandhi",
                        "slug": "T.-Gandhi",
                        "structuredName": {
                            "firstName": "Tarak",
                            "lastName": "Gandhi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Gandhi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "115338247"
                        ],
                        "name": "Ryan Keener",
                        "slug": "Ryan-Keener",
                        "structuredName": {
                            "firstName": "Ryan",
                            "lastName": "Keener",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ryan Keener"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3110392"
                        ],
                        "name": "R. Kasturi",
                        "slug": "R.-Kasturi",
                        "structuredName": {
                            "firstName": "Rangachar",
                            "lastName": "Kasturi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kasturi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[29,32] performed text detection using the assumption that the number of intracoded blocks in P- and B-frames of an MPEG compressed video increases, when a text caption appears."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 57425516,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1d784187382befa5cb50b62f10ddb87feb487102",
            "isKey": false,
            "numCitedBy": 42,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Video indexing is an important problem that has occupied recent research efforts. The text appearing in video can provide semantic information about the scene content. Detecting and recognizing text events can provide indices into the video for content based querying. We describe a system for detecting, tracking, and extracting artificial and scene text in MPEG-1 video. Preliminary results are presented."
            },
            "slug": "A-system-for-automatic-text-detection-in-video-Gargi-Crandall",
            "title": {
                "fragments": [],
                "text": "A system for automatic text detection in video"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A system for detecting, tracking, and extracting artificial and scene text in MPEG-1 video to provide indices into the video for content based querying is described."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Fifth International Conference on Document Analysis and Recognition. ICDAR '99 (Cat. No.PR00318)"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472153"
                        ],
                        "name": "S. Messelodi",
                        "slug": "S.-Messelodi",
                        "structuredName": {
                            "firstName": "Stefano",
                            "lastName": "Messelodi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Messelodi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2389886"
                        ],
                        "name": "C. M. Modena",
                        "slug": "C.-M.-Modena",
                        "structuredName": {
                            "firstName": "Carla",
                            "lastName": "Modena",
                            "middleNames": [
                                "Maria"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. M. Modena"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 30
                            }
                        ],
                        "text": "Messelodi and Modena\u2019s method [39] consists of three sequential stages: (i) extraction of elementary objects, (ii) Altering of objects, and (iii) text line selections."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 83
                            }
                        ],
                        "text": "They developed a binarization algorithm similar to Messelodi and Modena\u2019s approach [39]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 7
                            }
                        ],
                        "text": "Modena [39] divisive hierarchical clustering procedure localization"
                    },
                    "intents": []
                }
            ],
            "corpusId": 13597946,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "55f6886a7a70c800aa3c899d16e784a611dc9ba2",
            "isKey": true,
            "numCitedBy": 105,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Automatic-identification-and-skew-estimation-of-in-Messelodi-Modena",
            "title": {
                "fragments": [],
                "text": "Automatic identification and skew estimation of text lines in real scene images"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144739319"
                        ],
                        "name": "R. Lienhart",
                        "slug": "R.-Lienhart",
                        "structuredName": {
                            "firstName": "Rainer",
                            "lastName": "Lienhart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lienhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2083891040"
                        ],
                        "name": "F. Stuber",
                        "slug": "F.-Stuber",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Stuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Stuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14147742,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "778a307aa0cf8b2ed273b9089cb9aa8210f49f24",
            "isKey": false,
            "numCitedBy": 205,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We have developed algorithms for automatic character segmentation in motion pictures which extract automatically and reliably the text in pre-title sequences, credit titles, and closing sequences with title and credits. The algorithms we propose make use of typical characteristics of text in videos in order to enhance segmentation and, consequently, recognition performance. As a result, we get segmented characters from video pictures. These can be parsed by any OCR software. The recognition results of multiple instances of the same character throughout subsequent frames are combined to enhance recognition result and to compute the final output. We have tested our segmentation algorithms in a series of experiments with video clips recorded from television and achieved good segmentation results."
            },
            "slug": "Automatic-text-recognition-in-digital-videos-Lienhart-Stuber",
            "title": {
                "fragments": [],
                "text": "Automatic text recognition in digital videos"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "Algorithms for automatic character segmentation in motion pictures which extract automatically and reliably the text in pre-title sequences, credit titles, and closing sequences with title and credits are developed."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2565323"
                        ],
                        "name": "C. Strouthopoulos",
                        "slug": "C.-Strouthopoulos",
                        "structuredName": {
                            "firstName": "Charalambos",
                            "lastName": "Strouthopoulos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Strouthopoulos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144368634"
                        ],
                        "name": "N. Papamarkos",
                        "slug": "N.-Papamarkos",
                        "structuredName": {
                            "firstName": "Nikos",
                            "lastName": "Papamarkos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Papamarkos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2938539"
                        ],
                        "name": "A. Atsalakis",
                        "slug": "A.-Atsalakis",
                        "structuredName": {
                            "firstName": "Antonios",
                            "lastName": "Atsalakis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Atsalakis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 7
                            }
                        ],
                        "text": "A large number of techniques have been proposed to address this problem, and the purpose of this paper is to classify and review these algorithms, discuss benchmark data and performance evaluation, and to point out promising directions for future research."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15419767,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "89140621fa5771868e5a3e170769eb6089214aa4",
            "isKey": false,
            "numCitedBy": 66,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Text-extraction-in-complex-color-documents-Strouthopoulos-Papamarkos",
            "title": {
                "fragments": [],
                "text": "Text extraction in complex color documents"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115196978"
                        ],
                        "name": "Huiping Li",
                        "slug": "Huiping-Li",
                        "structuredName": {
                            "firstName": "Huiping",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huiping Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3272081"
                        ],
                        "name": "O. Kia",
                        "slug": "O.-Kia",
                        "structuredName": {
                            "firstName": "Omid",
                            "lastName": "Kia",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Kia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48471936"
                        ],
                        "name": "D. Doermann",
                        "slug": "D.-Doermann",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Doermann",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Doermann"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[64] presented a text tracking approach that is suitable for several circumstances, including scrolling, captions, text printed on an athlete\u2019s jersey, etc."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 16
                            }
                        ],
                        "text": "Li and Doermann [64] also used a multiple frame-based text image enhancement technique, where consecutive text blocks are registered using a pure translational motion model."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 206406471,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9612a282060e74a6fe64e34cfc0643f13f6672e1",
            "isKey": false,
            "numCitedBy": 42,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "One difficulty with using text from digital video for indexing and retrieval is that video images are often in low resolution and poor quality, and as a result, the text can not be recognized adequately by most commercial OCR software. Text image enhancement is necessary to achieve reasonable OCR accuracy. Our enhancement consists of two main procedures, resolution enhancement based on Shannon interpolation and text separation from complex image background. Experiments show our enhancement approach improves OCR accuracy considerably."
            },
            "slug": "Text-enhancement-in-digital-video-Li-Kia",
            "title": {
                "fragments": [],
                "text": "Text enhancement in digital video"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Experiments show the enhancement approach improves OCR accuracy considerably, and consists of two main procedures, resolution enhancement based on Shannon interpolation and text separation from complex image background."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784196"
                        ],
                        "name": "Datong Chen",
                        "slug": "Datong-Chen",
                        "structuredName": {
                            "firstName": "Datong",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Datong Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719610"
                        ],
                        "name": "J. Odobez",
                        "slug": "J.-Odobez",
                        "structuredName": {
                            "firstName": "Jean-Marc",
                            "lastName": "Odobez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Odobez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733733"
                        ],
                        "name": "H. Bourlard",
                        "slug": "H.-Bourlard",
                        "structuredName": {
                            "firstName": "Herv\u00e9",
                            "lastName": "Bourlard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bourlard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17843499,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3715698b8d5ca979ba2b60fe49b8f8720e482a1b",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we propose a method to segment and recognize text embedded in video and images. We modelize the gray level distribution in the text images as mixture of gaussians, and then assign each pixel to one of the gaussian layer. The assignment is based on prior of the contextual information, which is modeled by a Markov random field (MRF) with online estimated coefficients. Each layer is then processed through a connected component analysis module and forwarded to the OCR system as one segmentation hypothesis. By varying the number of gaussians, multiple hypotheses are provided to an OCR system and the final result is selected from the set of outputs, leading to an improvement of the system's performances."
            },
            "slug": "Text-segmentation-and-recognition-in-complex-based-Chen-Odobez",
            "title": {
                "fragments": [],
                "text": "Text segmentation and recognition in complex background based on Markov random field"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "By varying the number of gaussians, multiple hypotheses are provided to an OCR system and the final result is selected from the set of outputs, leading to an improvement of the system's performances."
            },
            "venue": {
                "fragments": [],
                "text": "Object recognition supported by user interaction for service robots"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "121267347"
                        ],
                        "name": "K. Jung",
                        "slug": "K.-Jung",
                        "structuredName": {
                            "firstName": "Keechul",
                            "lastName": "Jung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Jung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144602022"
                        ],
                        "name": "K. Kim",
                        "slug": "K.-Kim",
                        "structuredName": {
                            "firstName": "Kwang",
                            "lastName": "Kim",
                            "middleNames": [
                                "In"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2146355817"
                        ],
                        "name": "Junghyun Han",
                        "slug": "Junghyun-Han",
                        "structuredName": {
                            "firstName": "Junghyun",
                            "lastName": "Han",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Junghyun Han"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[55] adopt a mean shift algorithm as a mechanism for automatically selecting regions of interest (ROIs), thereby avoiding a time-consuming texture analysis of the entire image."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 62748767,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8e1f46e27a61ee566ae803c072e1886a52fb2d8f",
            "isKey": false,
            "numCitedBy": 14,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a hybrid approach of a texture-based method and a connected component-based one for extracting texts in real scene images. For detecting texts having a lot of variations in size, shape, etc, we use a multiple-continuously adaptive mean shift algorithm on the text probability image produced by a multi-layer perceptron. It is assumed that the scene text lies on planar rectangular surfaces with homogeneous background colors. We correct perspective distortion using warping parameters calculated after segmentation of an input image. We can detect and reconstruct text images accurately and efficiently."
            },
            "slug": "Text-extraction-in-real-scene-images-on-planar-Jung-Kim",
            "title": {
                "fragments": [],
                "text": "Text extraction in real scene images on planar planes"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "A hybrid approach of a texture-based method and a connected component-based one for extracting texts in real scene images using a multiple-continuously adaptive mean shift algorithm on the text probability image produced by a multi-layer perceptron."
            },
            "venue": {
                "fragments": [],
                "text": "Object recognition supported by user interaction for service robots"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39319377"
                        ],
                        "name": "Yu Zhong",
                        "slug": "Yu-Zhong",
                        "structuredName": {
                            "firstName": "Yu",
                            "lastName": "Zhong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yu Zhong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 33127988,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c47011df8c78713c4eb802677ce7b85f6f791586",
            "isKey": false,
            "numCitedBy": 88,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Object-localization-using-color,-texture-and-shape-Zhong-Jain",
            "title": {
                "fragments": [],
                "text": "Object localization using color, texture and shape"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115196978"
                        ],
                        "name": "Huiping Li",
                        "slug": "Huiping-Li",
                        "structuredName": {
                            "firstName": "Huiping",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huiping Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48471936"
                        ],
                        "name": "D. Doermann",
                        "slug": "D.-Doermann",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Doermann",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Doermann"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[51,53] for localizing and tracking text in video, where a small window (typically 16\u00d7 16) is applied to scan an image."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This method was subsequently enhanced for skewed text [53] as follows."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 38586525,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3224ce96a5d8174a546dcfba0f2a202bed3ded16",
            "isKey": false,
            "numCitedBy": 49,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present a video text detection system based on automated neural network training. Compared with previous work which detects only graphical text with fixed parameters, our system (1) provides a training mechanism so the parameters of the system can be adapted to changing environments, (2) can detect both graphical text and scene text located in complex backgrounds, (3) can detect text in any orientation and (4) can perform multilingual text detection. Experiments show the effectiveness of our system in various text detection tasks."
            },
            "slug": "A-video-text-detection-system-based-on-automated-Li-Doermann",
            "title": {
                "fragments": [],
                "text": "A video text detection system based on automated training"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "A video text detection system based on automated neural network training that can detect both graphical text and scene text located in complex backgrounds, can detect text in any orientation, and can perform multilingual text detection."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 15th International Conference on Pattern Recognition. ICPR-2000"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2461436"
                        ],
                        "name": "Jiangying Zhou",
                        "slug": "Jiangying-Zhou",
                        "structuredName": {
                            "firstName": "Jiangying",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiangying Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1828940"
                        ],
                        "name": "D. Lopresti",
                        "slug": "D.-Lopresti",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Lopresti",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lopresti"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[66\u201368] use their own OCR algorithm based on a surface Atting classiAer and an n-tuple classiAer."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 23073168,
            "fieldsOfStudy": [
                "Computer Science",
                "Art"
            ],
            "id": "3ef36b0186bb73619d08d02661616ce7df218ecd",
            "isKey": false,
            "numCitedBy": 83,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors examine the problem of locating and extracting text from images on the World Wide Web. They describe a text detection algorithm which is based on color clustering and connected component analysis. The algorithm first quantizes the color space of the input image into a number of color classes using a parameter-free clustering procedure. It then identifies text-like connected components in each color class based on their shapes. Finally, a post-processing procedure aligns text-like components into text lines. Experimental results suggest this approach is promising despite the challenging nature of the input data."
            },
            "slug": "Extracting-text-from-WWW-images-Zhou-Lopresti",
            "title": {
                "fragments": [],
                "text": "Extracting text from WWW images"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A text detection algorithm which is based on color clustering and connected component analysis is described, which suggests this approach is promising despite the challenging nature of the input data."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Fourth International Conference on Document Analysis and Recognition"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "146876323"
                        ],
                        "name": "Hui Cheng",
                        "slug": "Hui-Cheng",
                        "structuredName": {
                            "firstName": "Hui",
                            "lastName": "Cheng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hui Cheng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745655"
                        ],
                        "name": "C. Bouman",
                        "slug": "C.-Bouman",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Bouman",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Bouman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741931"
                        ],
                        "name": "J. Allebach",
                        "slug": "J.-Allebach",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Allebach",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Allebach"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[77] apply adaptive dithering after segmenting a document into several different classes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14624665,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "727838dba3adb89c9a03f3d841f3b056e4e825f7",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose a new approach to document segmentation which exploits both local texture characteristics and image structure to segment scanned documents into regions such as text, background, headings and images. Our method is based on the use of a multiscale Bayesian framework. This framework is chosen because it allows accurate modeling of both the image characteristics and contextual structure of each region. The parameters which describe the characteristics of typical images are extracted from a database of training images which are produced by scanning typical documents and hand segmenting them into the desired components. This training procedure is based on the expectation maximization (EM) algorithm and results in approximate maximum likelihood (ML) estimates of the model parameters for region textures and contextual structure at various resolutions. Once the training procedure is performed, scanned documents may be segmented using a fine-to-coarse-to-fine procedure that is computationally efficient."
            },
            "slug": "Multiscale-Document-Segmentation-Cheng-Bouman",
            "title": {
                "fragments": [],
                "text": "Multiscale Document Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "A new approach to document segmentation is proposed which exploits both local texture characteristics and image structure to segment scanned documents into regions such as text, background, headings and images through the use of a multiscale Bayesian framework."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693680"
                        ],
                        "name": "N. Chaddha",
                        "slug": "N.-Chaddha",
                        "structuredName": {
                            "firstName": "Navin",
                            "lastName": "Chaddha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Chaddha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153216710"
                        ],
                        "name": "R. Sharma",
                        "slug": "R.-Sharma",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Sharma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sharma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2075308385"
                        ],
                        "name": "A. Agrawal",
                        "slug": "A.-Agrawal",
                        "structuredName": {
                            "firstName": "Avneesh",
                            "lastName": "Agrawal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Agrawal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2041740460"
                        ],
                        "name": "A. Gupta",
                        "slug": "A.-Gupta",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Gupta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gupta"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "for TIE, and selected five algorithms [29,  57 , 58, 59, 71] as promising."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 58358347,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9754dba62f5a329c9dd64b7d091c7900f489f5f0",
            "isKey": false,
            "numCitedBy": 50,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "Block based algorithms have found widespread use in image and video compression. However, popular algorithms such as JPEG, which are very effective in compressing continuous tone images, do not perform well with mixed-mode images which have a substantial text component. With a growing number of applications where such images occur, e.g., color facsimile, digital libraries and educational videos, there are advantages in being able to classify each block as being text or continuous tone. With such a classification, different compression parameters or even algorithms may be employed for the two kinds of data to obtain high compression with minimal loss in visual quality. In this paper we analyze and compare four methods for block classification in mixed mode images, namely variance, absolute-deviation, edge, and DCT based methods. Our evaluation of each scheme is based on the accuracy of segmentation, robustness across different types of images and sensitivity to the threshold used for segmentation. Our results show that DCT based segmentation offers the best accuracy and robustness. Another advantage of DCT is that it is compatible with standards like JPEG, MPEG and H.261.<<ETX>>"
            },
            "slug": "Text-segmentation-in-mixed-mode-images-Chaddha-Sharma",
            "title": {
                "fragments": [],
                "text": "Text segmentation in mixed-mode images"
            },
            "tldr": {
                "abstractSimilarityScore": 36,
                "text": "This paper analyzes and compares four methods for block classification in mixed mode images, namely variance, absolute-deviation, edge, and DCT based methods, and shows that DCTbased segmentation offers the best accuracy and robustness."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 1994 28th Asilomar Conference on Signals, Systems and Computers"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144638781"
                        ],
                        "name": "A. Smeulders",
                        "slug": "A.-Smeulders",
                        "structuredName": {
                            "firstName": "Arnold",
                            "lastName": "Smeulders",
                            "middleNames": [
                                "W.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Smeulders"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717056"
                        ],
                        "name": "M. Worring",
                        "slug": "M.-Worring",
                        "structuredName": {
                            "firstName": "Marcel",
                            "lastName": "Worring",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Worring"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747647"
                        ],
                        "name": "S. Santini",
                        "slug": "S.-Santini",
                        "structuredName": {
                            "firstName": "Simone",
                            "lastName": "Santini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Santini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1722619"
                        ],
                        "name": "Amarnath Gupta",
                        "slug": "Amarnath-Gupta",
                        "structuredName": {
                            "firstName": "Amarnath",
                            "lastName": "Gupta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Amarnath Gupta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144938740"
                        ],
                        "name": "R. Jain",
                        "slug": "R.-Jain",
                        "structuredName": {
                            "firstName": "Ramesh",
                            "lastName": "Jain",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Jain"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 74
                            }
                        ],
                        "text": "A number of studies on the use of relatively low-level perceptual content [2-6] for image and video indexing have already been reported."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2827898,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0b7c4096ed697696a5f4fc8f3a6a750dc0cdecfe",
            "isKey": false,
            "numCitedBy": 6725,
            "numCiting": 410,
            "paperAbstract": {
                "fragments": [],
                "text": "Presents a review of 200 references in content-based image retrieval. The paper starts with discussing the working conditions of content-based retrieval: patterns of use, types of pictures, the role of semantics, and the sensory gap. Subsequent sections discuss computational steps for image retrieval systems. Step one of the review is image processing for retrieval sorted by color, texture, and local geometry. Features for retrieval are discussed next, sorted by: accumulative and global features, salient points, object and shape features, signs, and structural combinations thereof. Similarity of pictures and objects in pictures is reviewed for each of the feature types, in close connection to the types and means of feedback the user of the systems is capable of giving by interaction. We briefly discuss aspects of system engineering: databases, system architecture, and evaluation. In the concluding section, we present our view on: the driving force of the field, the heritage from computer vision, the influence on computer vision, the role of similarity and of interaction, the need for databases, the problem of evaluation, and the role of the semantic gap."
            },
            "slug": "Content-Based-Image-Retrieval-at-the-End-of-the-Smeulders-Worring",
            "title": {
                "fragments": [],
                "text": "Content-Based Image Retrieval at the End of the Early Years"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "The working conditions of content-based retrieval: patterns of use, types of pictures, the role of semantics, and the sensory gap are discussed, as well as aspects of system engineering: databases, system architecture, and evaluation."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2461436"
                        ],
                        "name": "Jiangying Zhou",
                        "slug": "Jiangying-Zhou",
                        "structuredName": {
                            "firstName": "Jiangying",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiangying Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1828940"
                        ],
                        "name": "D. Lopresti",
                        "slug": "D.-Lopresti",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Lopresti",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lopresti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3198175"
                        ],
                        "name": "T. Tasdizen",
                        "slug": "T.-Tasdizen",
                        "structuredName": {
                            "firstName": "Tolga",
                            "lastName": "Tasdizen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Tasdizen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 10184381,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0bad076c5f89ac43026a2b5fca648d488f54f45e",
            "isKey": false,
            "numCitedBy": 35,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we consider the problem of locating and extracting text from WWW images. A previous algorithm based on color clustering and connected components analysis works well as long as the color of each character is relatively uniform and the typography is fairly simple. It breaks down quickly, however, when these assumptions are violated. In this paper, we describe more robust techniques for dealing with this challenging problem. We present an improved color clustering algorithm that measures similarity based on both RGB and spatial proximity. Layout analysis is also incorporated to handle more complex typography. THese changes significantly enhance the performance of our text detection procedure."
            },
            "slug": "Finding-text-in-color-images-Zhou-Lopresti",
            "title": {
                "fragments": [],
                "text": "Finding text in color images"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "An improved color clustering algorithm that measures similarity based on both RGB and spatial proximity is presented, and layout analysis is also incorporated to handle more complex typography."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38486814"
                        ],
                        "name": "Bongkee Sin",
                        "slug": "Bongkee-Sin",
                        "structuredName": {
                            "firstName": "Bongkee",
                            "lastName": "Sin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bongkee Sin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109631931"
                        ],
                        "name": "Seon-Kyu Kim",
                        "slug": "Seon-Kyu-Kim",
                        "structuredName": {
                            "firstName": "Seon-Kyu",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Seon-Kyu Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2668078"
                        ],
                        "name": "Beom-Joon Cho",
                        "slug": "Beom-Joon-Cho",
                        "structuredName": {
                            "firstName": "Beom-Joon",
                            "lastName": "Cho",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Beom-Joon Cho"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[81] use frequency features such as the number of edge pixels in horizontal and vertical directions and Fourier spectrum to detect text regions in real scene images."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 39283552,
            "fieldsOfStudy": [
                "Geology"
            ],
            "id": "a5baf0480b140a17339547e24fc397aaf33937d3",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a (language-independent) method of locating rectangular text regions in natural scene images. The method consists of two steps that can be applied in succession or independently: the frequency of edge pixels across vertical and horizontal scan lines, and the fundamental frequency in the Fourier domain. The frequency feature of text images is highly intuitive, and this is the focus of the research. The detection of rectangles using a Hough transform is also addressed. Texts that are meaningful to many viewers usually appear in rectangles of colours of high contrast to the background. Hence it is natural to assume that the detection of rectangles may be helpful for locating desired texts correctly in natural outdoor scene images."
            },
            "slug": "Locating-characters-in-scene-images-using-frequency-Sin-Kim",
            "title": {
                "fragments": [],
                "text": "Locating characters in scene images using frequency features"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "This paper presents a (language-independent) method of locating rectangular text regions in natural scene images using the frequency of edge pixels across vertical and horizontal scan lines and the fundamental frequency in the Fourier domain."
            },
            "venue": {
                "fragments": [],
                "text": "Object recognition supported by user interaction for service robots"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2461436"
                        ],
                        "name": "Jiangying Zhou",
                        "slug": "Jiangying-Zhou",
                        "structuredName": {
                            "firstName": "Jiangying",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiangying Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1828940"
                        ],
                        "name": "D. Lopresti",
                        "slug": "D.-Lopresti",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Lopresti",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lopresti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113455824"
                        ],
                        "name": "Zhibin Lei",
                        "slug": "Zhibin-Lei",
                        "structuredName": {
                            "firstName": "Zhibin",
                            "lastName": "Lei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhibin Lei"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 29236195,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b5f719c98abe9e3da61cb39ca1b836aeb09a4624",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "A significant amount of text now present in World Wide Web documents is embedded in image data, and a large portion of it does not appear elsewhere at all. To make this information available, we need to develop techniques for recovering textual information from in-line Web images. In this paper, we describe two methods for Web image OCR. Recognizing text extracted from in-line Web images is difficult because characters in these images are often rendered at a low spatial resolution. Such images are typically considered to be 'low quality' by traditional OCR technologies. Our proposed methods utilize the information contained in the color bits to compensate for the loss of information due to low sampling resolution. The first method uses a polynomial surface fitting technique for object recognition. The second method is based on the traditional n-tuple technique. We collected a small set of character samples from Web documents and tested the two algorithms. Preliminary experimental results show that our n-tuple method works quite well. However, the surface fitting method performs rather poorly due to the coarseness and small number of color shades used in the text."
            },
            "slug": "OCR-for-World-Wide-Web-images-Zhou-Lopresti",
            "title": {
                "fragments": [],
                "text": "OCR for World Wide Web images"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Two methods for Web image OCR based on the traditional n-tuple technique, which utilize the information contained in the color bits to compensate for the loss of information due to low sampling resolution."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108487560"
                        ],
                        "name": "J. C. Lee",
                        "slug": "J.-C.-Lee",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lee",
                            "middleNames": [
                                "Chung-Mong"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. C. Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2257741"
                        ],
                        "name": "A. Kankanhalli",
                        "slug": "A.-Kankanhalli",
                        "structuredName": {
                            "firstName": "Atreyi",
                            "lastName": "Kankanhalli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kankanhalli"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 339,
                                "start": 328
                            }
                        ],
                        "text": "These two approaches\nTable 2 A brief survey of TIE\nAuthor Year Approach Features\nOhya et al. [34] 1994 Adaptive thresholding and relaxation operations Color, scene text (train, signboard, skew and curved), localization and recognition Lee and 1995 Coarse search using edge information, followed by Scene text (cargo container), Kankanhalli [35] connected component (CC) generation localization and recognition Smith and Kanade [7] 1995 3\u00d7 3 Alter seeking vertical edges Caption text, localization Zhong et al. [36] 1995 CC-based method after color reduction, local spatial Scene text (CD covers), localization variance-based method, and hybrid method Yeo and Liu [56] 1996 Localization based on large inter-frame di2erence in Caption text, localization MPEG compressed image Shim et al. [21] 1998 Gray-level di2erence between pairs of pixels Caption text, localization Jain and Yu 1998 CC-based method after multi-valued color image Color (book cover, Web image, video [38] decomposition frame), localization Sato et al. [11] 1998 Smith and Kanade\u2019s localization method and Recognition recognition-based character extraction Chun et al. [54] 1999 Filtering using neural network after FFT Caption text, localization Antani et al. [28] 1999 Multiple algorithms in functional parallelism Scene text, recognition Messelodi and 1999 CC generation, followed by text line selection using Scene images (book covers, slanted) Modena [39] divisive hierarchical clustering procedure localization Wu et al. [45] 1999 Localization based on multi-scale texture segmentation Video and scene images (newspaper, advertisement) recognition Hasan and Karam [42] 2000 Morphological approach Scene text, localization Li et al. [51] 2000 Wavelet-based feature extraction and neural network for Scene text (slanted), localization, enhancement, texture analysis and tracking Lim et al. [33] 2000 Text detection and localization using DCT coeCcient Caption text, MPEG compressed and macroblock type information video, localization Zhong et al. [27] 2000 Texture analysis in DCT compressed domain Caption text, JPEG and I-frames of MPEG, localization Jung [49] 2001 Gabor Alter-like multi-layer perceptron for texture Color, caption text, localization analysis Chen et al. [43] 2001 Text detection in edge-enhanced image Caption text, localization and recognition Strouthopoulos 2002 Page layout analysis after adaptive color reduction Color document image, localization et al. [16]\nwork in a bottom-up fashion; by identifying sub-structures, such as CCs or edges, and then merging these sub-structures to mark bounding boxes for text."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 0
                            }
                        ],
                        "text": "Lee and Kankanhalli [34] apply a CC-based method for cargo container veriAcation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 251,
                                "start": 247
                            }
                        ],
                        "text": "[34] 1994 Adaptive thresholding and relaxation operations Color, scene text (train, signboard, skew and curved), localization and recognition Lee and 1995 Coarse search using edge information, followed by Scene text (cargo container), Kankanhalli [35] connected component (CC) generation localization and recognition Smith and Kanade [7] 1995 3\u00d7 3 Alter seeking vertical edges Caption text, localization Zhong et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 20
                            }
                        ],
                        "text": "Lee and Kankanhalli [35] applied a CC-based method to the detection and recognition of text on cargo containers, which can have uneven lighting conditions and characters with di2erent sizes and shapes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11365167,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "07a0d7a8938fa04cad3a1afc28c3dcaf1724bde8",
            "isKey": true,
            "numCitedBy": 74,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "We have developed a generalized alphanumeric character extraction algorithm that can efficiently and accurately locate and extract characters from complex scene images. A scene image may be complex due to the following reasons: (1) the characters are embedded in an image with other objects, such as structural bars, company logos and smears; (2) the characters may be painted or printed in any color including white, and the background color may differ only slightly from that of the characters; (3) the font, size and format of the characters may be different; and (4) the lighting may be uneven. The main contribution of this research is that it permits the quick and accurate extraction of characters in a complex scene. A coarse search technique is used to locate potential characters, and then a fine grouping technique is used to extract characters accurately. Several additional techniques in the postprocessing phase eliminate spurious as well as overlapping characters. Experimental results of segmenting characters written on cargo container surfaces show that the system is feasible under real-life constraints. The program has been installed as part of a vision system which verifies container codes on vehicles passing through the Port of Singapore."
            },
            "slug": "Automatic-Extraction-of-Characters-in-Complex-Scene-Lee-Kankanhalli",
            "title": {
                "fragments": [],
                "text": "Automatic Extraction of Characters in Complex Scene Images"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "A generalized alphanumeric character extraction algorithm that can efficiently and accurately locate and extract characters from complex scene images is developed that permits the quick and accurate extraction of characters in a complex scene."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Pattern Recognit. Artif. Intell."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143863244"
                        ],
                        "name": "Xiansheng Hua",
                        "slug": "Xiansheng-Hua",
                        "structuredName": {
                            "firstName": "Xiansheng",
                            "lastName": "Hua",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiansheng Hua"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120640002"
                        ],
                        "name": "Wenyin Liu",
                        "slug": "Wenyin-Liu",
                        "structuredName": {
                            "firstName": "Wenyin",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wenyin Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108698841"
                        ],
                        "name": "HongJiang Zhang",
                        "slug": "HongJiang-Zhang",
                        "structuredName": {
                            "firstName": "HongJiang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "HongJiang Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[72] proposed a performance evaluation protocol for text localization algorithms based on Liu and Dori\u2019s method [73] of text segmentation from engineering drawings."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1989026,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "844ceedec94d46b15198de140646e0f2ae953eb0",
            "isKey": false,
            "numCitedBy": 47,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose an objective, comprehensive and difficulty-independent performance evaluation protocol for video text detection algorithms. The protocol includes a positive set and a negative set of indices at textbox level, which evaluate the detection quality in terms of both location accuracy and fragmentation of the detected textboxes. In the protocol, we assign a detection difficulty (DD) level to each ground truth textbox. The performance indices can then be normalized with respect to the textbox DD level and are therefore independent of the ground truth difficulty. We also assign a detection importance (DI) level to each ground truth textbox. The overall detection rate is the DI-weighted average of the detection qualities of all ground truth textboxes, which makes the detection rate more accurate to reveal the real performance. The automatic performance evaluation scheme has been applied on a text detection approach to determine the best parameters that can yield the best detection results."
            },
            "slug": "Automatic-performance-evaluation-for-video-text-Hua-Liu",
            "title": {
                "fragments": [],
                "text": "Automatic performance evaluation for video text detection"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "An objective, comprehensive and difficulty-independent performance evaluation protocol for video text detection algorithms that includes a positive set and a negative set of indices at textbox level, which evaluate the detection quality in terms of both location accuracy and fragmentation of the detected textboxes."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of Sixth International Conference on Document Analysis and Recognition"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38595399"
                        ],
                        "name": "Rein-Lien Hsu",
                        "slug": "Rein-Lien-Hsu",
                        "structuredName": {
                            "firstName": "Rein-Lien",
                            "lastName": "Hsu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rein-Lien Hsu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "92329551"
                        ],
                        "name": "M. Abdel-Mottaleb",
                        "slug": "M.-Abdel-Mottaleb",
                        "structuredName": {
                            "firstName": "Mohamed",
                            "lastName": "Abdel-Mottaleb",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Abdel-Mottaleb"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 2
                            }
                        ],
                        "text": "A large number of techniques have been proposed to address this problem, and the purpose of this paper is to classify and review these algorithms, discuss benchmark data and performance evaluation, and to point out promising directions for future research."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15398205,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a741ea5e483061f2ad1767290770ef35e74fcab4",
            "isKey": false,
            "numCitedBy": 2121,
            "numCiting": 78,
            "paperAbstract": {
                "fragments": [],
                "text": "Human face detection is often the first step in applications such as video surveillance, human computer interface, face recognition, and image database management. We propose a face detection algorithm for color images in the presence of varying lighting conditions as well as complex backgrounds. Our method detects skin regions over the entire image, and then generates face candidates based on the spatial arrangement of these skin patches. The algorithm constructs eye, mouth, and boundary maps for verifying each face candidate. Experimental results demonstrate successful detection over a wide variety of facial variations in color, position, scale, rotation, pose, and expression from several photo collections."
            },
            "slug": "Face-detection-in-color-images-Hsu-Abdel-Mottaleb",
            "title": {
                "fragments": [],
                "text": "Face detection in color images"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A face detection algorithm for color images in the presence of varying lighting conditions as well as complex backgrounds is proposed, and then generates face candidates based on the spatial arrangement of these skin patches."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 2001 International Conference on Image Processing (Cat. No.01CH37205)"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2372810"
                        ],
                        "name": "T. Gandhi",
                        "slug": "T.-Gandhi",
                        "structuredName": {
                            "firstName": "Tarak",
                            "lastName": "Gandhi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Gandhi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3110392"
                        ],
                        "name": "R. Kasturi",
                        "slug": "R.-Kasturi",
                        "structuredName": {
                            "firstName": "Rangachar",
                            "lastName": "Kasturi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kasturi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721328"
                        ],
                        "name": "Sameer Kiran Antani",
                        "slug": "Sameer-Kiran-Antani",
                        "structuredName": {
                            "firstName": "Sameer",
                            "lastName": "Antani",
                            "middleNames": [
                                "Kiran"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sameer Kiran Antani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[60] use a planar motion model for scene text localization."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5447028,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "db1a2b260892554c47d0205e4535da21e5d3de63",
            "isKey": false,
            "numCitedBy": 25,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper explores an approach for extracting scene text from a sequence of images with relative motion between the camera and the scene. It is assumed that the scene text lies on planar surfaces, whereas the other features are likely to be at random depths or undergoing independent motion. The motion model parameters of these planar surfaces are estimated using gradient based methods, and multiple motion segmentation. The equations of the planar surfaces, as well as the camera motion parameters are extracted by combining the motion models of multiple planar surfaces. This approach is expected to improve the reliability and robustness of the estimates, which are used to perform perspective correction on the individual surfaces. Perspective correction can lead to improvement in OCR performance. This work can be useful for detecting road signs and bill-boards from a moving vehicle."
            },
            "slug": "Application-of-planar-motion-segmentation-for-scene-Gandhi-Kasturi",
            "title": {
                "fragments": [],
                "text": "Application of planar motion segmentation for scene text extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "An approach for extracting scene text from a sequence of images with relative motion between the camera and the scene by combining the motion models of multiple planar surfaces to improve the reliability and robustness of the estimates, which are used to perform perspective correction on the individual surfaces."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 15th International Conference on Pattern Recognition. ICPR-2000"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48764203"
                        ],
                        "name": "Victor Wu",
                        "slug": "Victor-Wu",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Victor Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758550"
                        ],
                        "name": "R. Manmatha",
                        "slug": "R.-Manmatha",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Manmatha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Manmatha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31338632"
                        ],
                        "name": "E. Riseman",
                        "slug": "E.-Riseman",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Riseman",
                            "middleNames": [
                                "M."
                            ],
                            "suffix": ""
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Riseman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 1830124,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e5c342ba0edbebadc7c95c7e59d1bef87d7e4add",
            "isKey": false,
            "numCitedBy": 451,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "A robust system is proposed to automatically detect and extract text in images from different sources, including video, newspapers, advertisements, stock certificates, photographs, and checks. Text is first detected using multiscale texture segmentation and spatial cohesion constraints, then cleaned up and extracted using a histogram-based binarization algorithm. An automatic performance evaluation scheme is also proposed."
            },
            "slug": "TextFinder:-An-Automatic-System-to-Detect-and-Text-Wu-Manmatha",
            "title": {
                "fragments": [],
                "text": "TextFinder: An Automatic System to Detect and Recognize Text In Images"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A robust system is proposed to automatically detect and extract text in images from different sources, including video, newspapers, advertisements, stock certificates, photographs, and checks."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116415943"
                        ],
                        "name": "B. Yu",
                        "slug": "B.-Yu",
                        "structuredName": {
                            "firstName": "Bin",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Yu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 58
                            }
                        ],
                        "text": "The TIE problem can be divided into the following sub-problems: (i) detection, (ii) localization, (iii) tracking, (iv) extraction and enhancement, and (v) recognition (OCR) (Fig."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 46138594,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b39beea0f761152e65fac0e498af387821d887f1",
            "isKey": false,
            "numCitedBy": 251,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "Transforming a paper document to its electronic version in a form suitable for efficient storage, retrieval, and interpretation continues to be a challenging problem. An efficient representation scheme for document images is necessary to solve this problem. Document representation involves techniques of thresholding, skew detection, geometric layout analysis, and logical layout analysis. The derived representation can then be used in document storage and retrieval. Page segmentation is an important stage in representing document images obtained by scanning journal pages. The performance of a document understanding system greatly depends on the correctness of page segmentation and labeling of different regions such as text, tables, images, drawings, and rulers. We use the traditional bottom-up approach based on the connected component extraction to efficiently implement page segmentation and region identification. A new document model which preserves top-down generation information is proposed based on which a document is logically represented for interactive editing, storage, retrieval, transfer, and logical analysis. Our algorithm has a high accuracy and takes approximately 1.4 seconds on a SGI Indy workstation for model creation, including orientation estimation, segmentation, and labeling (text, table, image, drawing, and ruler) for a 2550/spl times/3300 image of a typical journal page scanned at 300 dpi. This method is applicable to documents from various technical journals and can accommodate moderate amounts of skew and noise."
            },
            "slug": "Document-Representation-and-Its-Application-to-Page-Jain-Yu",
            "title": {
                "fragments": [],
                "text": "Document Representation and Its Application to Page Decomposition"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A new document model which preserves top-down generation information is proposed based on which a document is logically represented for interactive editing, storage, retrieval, transfer, and logical analysis."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31446769"
                        ],
                        "name": "M. Sawaki",
                        "slug": "M.-Sawaki",
                        "structuredName": {
                            "firstName": "Minako",
                            "lastName": "Sawaki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Sawaki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "82910116"
                        ],
                        "name": "H. Murase",
                        "slug": "H.-Murase",
                        "structuredName": {
                            "firstName": "Hiroshi",
                            "lastName": "Murase",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Murase"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781078"
                        ],
                        "name": "N. Hagita",
                        "slug": "N.-Hagita",
                        "structuredName": {
                            "firstName": "Norihiro",
                            "lastName": "Hagita",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Hagita"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[65] proposed a method for adaptively acquiring templates of degraded characters in scene images involving the automatic creation of \u2018context-based image templates\u2019 from text line images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 38279239,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6c572f4c48f2d26f23957a013c8780261444f422",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "Proposes a method for adaptively acquiring templates for degraded characters in scene images. Characters in scene images are often degraded because of poor printing and viewing conditions. To cope with the degradation problem, we proposed the idea of \"context-based image templates\" which include neighboring characters of parts thereof and so represent more contextual information than single-letter templates. However, our previous method manually selects the learning samples to make the context-based image templates and is time-consuming. Therefore, we attempt to make the context-based image templates automatically from single-letter templates and learning text-line images. The context-based image templates are iteratively created using the k-nearest neighbor rule. Experiments with 3,467 alpha-numeric characters in nine bookshelf images show that the high recognition rates for test samples possible with this method asymptotically approach those achieved with manual selection."
            },
            "slug": "Automatic-acquisition-of-context-based-images-for-Sawaki-Murase",
            "title": {
                "fragments": [],
                "text": "Automatic acquisition of context-based images templates for degraded character recognition in scene images"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work attempts to make the context-based image templates automatically from single-letter templates and learning text-line images and is iteratively created using the k-nearest neighbor rule."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 15th International Conference on Pattern Recognition. ICPR-2000"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2673919"
                        ],
                        "name": "V. Mariano",
                        "slug": "V.-Mariano",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Mariano",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Mariano"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3110392"
                        ],
                        "name": "R. Kasturi",
                        "slug": "R.-Kasturi",
                        "structuredName": {
                            "firstName": "Rangachar",
                            "lastName": "Kasturi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kasturi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[22] used a formal evaluation method for TIE, and selected Ave algorithms [29,57\u201359,71] as promising."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 195348929,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "1d4a5e70874d828bbd6c16300511fa2744f0b2fd",
            "isKey": false,
            "numCitedBy": 107,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper a method is proposed for locating horizontal, uniform-colored text in video frames. It was observed that when a row of pixels across such a text region is clustered in perceptually uniform L*a*b* color space, the pixels of one of these clusters would belong to the text strokes. These pixels would appeal as a line of short streaks on the row since a typical text region has many vertical and diagonal strokes. The proposed method examines every third row of the the image and checks whether this row passes through a horizontal text region. For a given row R, the pixels of R are hierarchically clustered in L*a*b* space and each cluster is tested whether similar-colored pixels in R's vicinity are possibly part of a text region. Candidate text blocks are marked by heuristics using information about the cluster's line of shell streaks. The detected text blocks are fused with the text regions. The method was tested on key frames of several video sequences and was able to locate a wide variety of text."
            },
            "slug": "Locating-uniform-colored-text-in-video-frames-Mariano-Kasturi",
            "title": {
                "fragments": [],
                "text": "Locating uniform-colored text in video frames"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The proposed method examines every third row of the the image and checks whether this row passes through a horizontal text region, and detects text blocks marked by heuristics using information about the cluster's line of shell streaks."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 15th International Conference on Pattern Recognition. ICPR-2000"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784196"
                        ],
                        "name": "Datong Chen",
                        "slug": "Datong-Chen",
                        "structuredName": {
                            "firstName": "Datong",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Datong Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144040003"
                        ],
                        "name": "K. Shearer",
                        "slug": "K.-Shearer",
                        "structuredName": {
                            "firstName": "Kim",
                            "lastName": "Shearer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Shearer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733733"
                        ],
                        "name": "H. Bourlard",
                        "slug": "H.-Bourlard",
                        "structuredName": {
                            "firstName": "Herv\u00e9",
                            "lastName": "Bourlard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bourlard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17316443,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e581b09dd70f70c95571bab6ec02fb58b4edf3da",
            "isKey": false,
            "numCitedBy": 73,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Stripes are common sub-structures of text characters, and the scale of these stripes varies little within a word. This scale consistency thus provides us with a useful feature for text detection and segmentation. A new form of filter is derived from the Gabor filter, and it is shown that this filter can efficiently estimate the scales of these stripes. The contrast of text in video can then be increased by enhancing the edges of only those stripes found to correspond to a suitable scale. More specifically the algorithm presented here enhances the stripes in three pre-selected scale ranges. The resulting enhancement yields much better performance from the binarization process, which is the step required before character recognition."
            },
            "slug": "Text-enhancement-with-asymmetric-filter-for-video-Chen-Shearer",
            "title": {
                "fragments": [],
                "text": "Text enhancement with asymmetric filter for video OCR"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A new form of filter is derived from the Gabor filter, and it is shown that this filter can efficiently estimate the scales of these stripes and enhance the edges of only those stripes found to correspond to a suitable scale."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 11th International Conference on Image Analysis and Processing"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1692339"
                        ],
                        "name": "B. Yeo",
                        "slug": "B.-Yeo",
                        "structuredName": {
                            "firstName": "Boon-Lock",
                            "lastName": "Yeo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Yeo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108661679"
                        ],
                        "name": "Bede Liu",
                        "slug": "Bede-Liu",
                        "structuredName": {
                            "firstName": "Bede",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bede Liu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62189337,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ac83336e50496b9a75714f1024531fe7d698d33b",
            "isKey": false,
            "numCitedBy": 47,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "Embedded captions in TV programs such as news broadcasts, documentaries and coverage of sports events provide important information on the underlying events. In digital video libraries, such captions represent a highly condensed form of key information on the contents of the video. In this paper we propose a scheme to automatically detect the presence of captions embedded in video frames. The proposed method operates on reduced image sequences which are efficiently reconstructed from compressed MPEG video and thus does not require full frame decompression. The detection, extraction and analysis of embedded captions help to capture the highlights of visual contents in video documents for better organization of video, to present succinctly the important messages embedded in the images, and to facilitate browsing, searching and retrieval of relevant clips."
            },
            "slug": "Visual-content-highlighting-via-automatic-of-on-Yeo-Liu",
            "title": {
                "fragments": [],
                "text": "Visual content highlighting via automatic extraction of embedded captions on MPEG compressed video"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A scheme to automatically detect the presence of captions embedded in video frames which operates on reduced image sequences which are efficiently reconstructed from compressed MPEG video and thus does not require full frame decompression."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "121267347"
                        ],
                        "name": "K. Jung",
                        "slug": "K.-Jung",
                        "structuredName": {
                            "firstName": "Keechul",
                            "lastName": "Jung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Jung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144602022"
                        ],
                        "name": "K. Kim",
                        "slug": "K.-Kim",
                        "structuredName": {
                            "firstName": "Kwang",
                            "lastName": "Kim",
                            "middleNames": [
                                "In"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2146355817"
                        ],
                        "name": "Junghyun Han",
                        "slug": "Junghyun-Han",
                        "structuredName": {
                            "firstName": "Junghyun",
                            "lastName": "Han",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Junghyun Han"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[83] applied a simple extension of their text localization method for wide text strings such as banners."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15194582,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "03b6bfddb3109cdf7d383f316fc639e1435af6c5",
            "isKey": false,
            "numCitedBy": 35,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a text scanner which detects wide text strings in a sequence of scene images. For scene text detection, we use a multiple-CAMShift algorithm on a text probability image produced by a multi-layer perceptron. To provide enhanced resolution of the extracted text images, we perform the text detection process after generating a mosaic image in a fast and robust image registration method."
            },
            "slug": "Text-scanner-with-text-detection-technology-on-Jung-Kim",
            "title": {
                "fragments": [],
                "text": "Text scanner with text detection technology on image sequences"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "A text scanner which detects wide text strings in a sequence of scene images by using a multiple-CAMShift algorithm on a text probability image produced by a multi-layer perceptron."
            },
            "venue": {
                "fragments": [],
                "text": "Object recognition supported by user interaction for service robots"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115196978"
                        ],
                        "name": "Huiping Li",
                        "slug": "Huiping-Li",
                        "structuredName": {
                            "firstName": "Huiping",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huiping Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48471936"
                        ],
                        "name": "D. Doermann",
                        "slug": "D.-Doermann",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Doermann",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Doermann"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Lastly, a super-resolution-based text enhancement scheme is also presented for de-blurring scene text, involving a projection onto convex sets (POCS)-based method [69]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5452218,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2d4e472e87cb932c52b5f59abc6007ba904e648a",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a superresolution-based text enhancement scheme to improve optical character recognition (OCR) and readability of text in digital video. The quality of video text is degraded by factors such as low resolution, antialiasing, clutter and blurring. We use the fact that the same text string often exists in consecutive frames to explore the temporal information and enhance the text image. For graphic text, we register text blocks to subpixel accuracy and fuse them to a new block with high resolution and a cleaner background. We use a projection onto convex sets based method to deblur scene text to improve readability. Experimental results show our scheme can improve OCR for graphic text and readability for scene text significantly."
            },
            "slug": "Superresolution-based-enhancement-of-text-in-video-Li-Doermann",
            "title": {
                "fragments": [],
                "text": "Superresolution-based enhancement of text in digital video"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "Experimental results show the scheme can improve OCR for graphic text and readability for scene text significantly and register text blocks to subpixel accuracy and fuse them to a new block with high resolution and a cleaner background."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 15th International Conference on Pattern Recognition. ICPR-2000"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2798041"
                        ],
                        "name": "I. Haritaoglu",
                        "slug": "I.-Haritaoglu",
                        "structuredName": {
                            "firstName": "Ismail",
                            "lastName": "Haritaoglu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Haritaoglu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Haritaoglu [75] also demonstrated his TIE system on a hand-held device."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8742498,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f5c4a0067f01260c9d91fca79473b740552e19a3",
            "isKey": false,
            "numCitedBy": 65,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a scene text extraction system for handheld devices to provide enhanced information perception services to the user. It uses a color camera attached to a personal digital assistant as an input device to capture scene images from the real world and it employs image enhancement and segmentation methods to extract written information from the scene, convert them to text information and show them to the user so that he/she can see both the real world and information together. We implemented a prototype application: an automatic sign/text language translation for foreign travelers, where people can use the system whenever they want to see text or signs in their own language where they are originally written in a foreign language in the scene."
            },
            "slug": "Scene-text-extraction-and-translation-for-handheld-Haritaoglu",
            "title": {
                "fragments": [],
                "text": "Scene text extraction and translation for handheld devices"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "An automatic sign/text language translation for foreign travelers, where people can use the system whenever they want to see text or signs in their own language where they are originally written in a foreign language in the scene."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120640002"
                        ],
                        "name": "Wenyin Liu",
                        "slug": "Wenyin-Liu",
                        "structuredName": {
                            "firstName": "Wenyin",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wenyin Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144964966"
                        ],
                        "name": "D. Dori",
                        "slug": "D.-Dori",
                        "structuredName": {
                            "firstName": "Dov",
                            "lastName": "Dori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Dori"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 112
                            }
                        ],
                        "text": "[72] proposed a performance evaluation protocol for text localization algorithms based on Liu and Dori\u2019s method [73] of text segmentation from engineering drawings."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11000656,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c9adca5cac518016f32f59e4fb2ac20d130f34ad",
            "isKey": false,
            "numCitedBy": 32,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose an objective, comprehensive, and complexity independent metric for performance evaluation of graphics/text separation (text segmentation) algorithms. The metric includes a positive set and a negative set of indices, at both the character and the character string (text) levels, _and it evaluates the detection accuracy of the location, width, height, orientation, skew, string length, and the fragmentation of both characters and strings. Assigning a Segmentation Difficulty (SD) value to the ground truth characters, the performance indices are normalized with respect to the character SD and are therefore independent of the ground truth complexity. The evaluation provides an overall, objective, and comprehensive metric of the text segmentation capability of various algorithms aimed at performing this task."
            },
            "slug": "A-Proposed-Scheme-for-Performance-Evaluation-of-Liu-Dori",
            "title": {
                "fragments": [],
                "text": "A Proposed Scheme for Performance Evaluation of Graphics/Text Separation Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "The evaluation provides an overall, objective, and comprehensive metric of the text segmentation capability of various algorithms aimed at performing this task."
            },
            "venue": {
                "fragments": [],
                "text": "GREC"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712991"
                        ],
                        "name": "M. Flickner",
                        "slug": "M.-Flickner",
                        "structuredName": {
                            "firstName": "Myron",
                            "lastName": "Flickner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Flickner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733393"
                        ],
                        "name": "H. Sawhney",
                        "slug": "H.-Sawhney",
                        "structuredName": {
                            "firstName": "Harpreet",
                            "lastName": "Sawhney",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Sawhney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152883679"
                        ],
                        "name": "J. Ashley",
                        "slug": "J.-Ashley",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Ashley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ashley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1391129943"
                        ],
                        "name": "Qian Huang",
                        "slug": "Qian-Huang",
                        "structuredName": {
                            "firstName": "Qian",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qian Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786444"
                        ],
                        "name": "B. Dom",
                        "slug": "B.-Dom",
                        "structuredName": {
                            "firstName": "Byron",
                            "lastName": "Dom",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Dom"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2087139"
                        ],
                        "name": "M. Gorkani",
                        "slug": "M.-Gorkani",
                        "structuredName": {
                            "firstName": "Monika",
                            "lastName": "Gorkani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gorkani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39311329"
                        ],
                        "name": "J. Hafner",
                        "slug": "J.-Hafner",
                        "structuredName": {
                            "firstName": "Jim",
                            "lastName": "Hafner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hafner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2499047"
                        ],
                        "name": "Denis Lee",
                        "slug": "Denis-Lee",
                        "structuredName": {
                            "firstName": "Denis",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Denis Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143867341"
                        ],
                        "name": "D. Petkovic",
                        "slug": "D.-Petkovic",
                        "structuredName": {
                            "firstName": "Dragutin",
                            "lastName": "Petkovic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Petkovic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144028064"
                        ],
                        "name": "David Steele",
                        "slug": "David-Steele",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Steele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Steele"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70341848"
                        ],
                        "name": "P. Yanker",
                        "slug": "P.-Yanker",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Yanker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Yanker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 110716,
            "fieldsOfStudy": [
                "Computer Science",
                "Art"
            ],
            "id": "dc139f901c869f80b54b41f89d5b7f35c7dfa3c7",
            "isKey": false,
            "numCitedBy": 4257,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Research on ways to extend and improve query methods for image databases is widespread. We have developed the QBIC (Query by Image Content) system to explore content-based retrieval methods. QBIC allows queries on large image and video databases based on example images, user-constructed sketches and drawings, selected color and texture patterns, camera and object motion, and other graphical information. Two key properties of QBIC are (1) its use of image and video content-computable properties of color, texture, shape and motion of images, videos and their objects-in the queries, and (2) its graphical query language, in which queries are posed by drawing, selecting and other graphical means. This article describes the QBIC system and demonstrates its query capabilities. QBIC technology is part of several IBM products. >"
            },
            "slug": "Query-by-Image-and-Video-Content:-The-QBIC-System-Flickner-Sawhney",
            "title": {
                "fragments": [],
                "text": "Query by Image and Video Content: The QBIC System"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "The QBIC system is described and its query capabilities are demonstrated, which allows queries on large image and video databases based on example images, user-constructed sketches and drawings, selected color and texture patterns, camera and object motion, and other graphical information."
            },
            "venue": {
                "fragments": [],
                "text": "Computer"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152736800"
                        ],
                        "name": "M. Smith",
                        "slug": "M.-Smith",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Smith",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Smith"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[34] 1994 Adaptive thresholding and relaxation operations Color, scene text (train, signboard, skew and curved), localization and recognition Lee and 1995 Coarse search using edge information, followed by Scene text (cargo container), Kankanhalli [35] connected component (CC) generation localization and recognition Smith and Kanade [7] 1995 3\u00d7 3 Alter seeking vertical edges Caption text, localization Zhong et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Studies on semantic image content in the form of text, face, vehicle, and human action have also attracted some recent interest [7\u201316]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Smith and Kanade [7] deAned a scene change based on the di2erence between two consecutive frames and then used this scene-change information for text detection."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Text regions are detected and localized using Smith and Kanade\u2019s method [7], and sub-pixel linear interpolation is applied to obtain higher resolution images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15525071,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "94c4141cdd7615e8e6fccbfa864abd518a62efd8",
            "isKey": true,
            "numCitedBy": 241,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Digital video is rapidly becoming an important source for information, entertainment and a host of multimedia applications. With the size of these collections growing to thousands of hours, technology is needed to effectively browse segments in a short time without losing the content of the video. We propose a method to extract the significant audio and video information and create a \u201cskim\u201d video which represents a short synopsis of the original. The extraction of significant information, such as specific objects, audio keywords and relevant video structure, is made possible through the integration of techniques in image and language understanding. The resulting skim is much smaller, and retains the essential content of the original segment. This research is sponsored by the National Science Foundation under grant no. IRI9411299, the National Space and Aeronautics Administration, and the Advanced Research Projects Agency. The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing official policies or endorsements, either expressed or implied, of the United States Government."
            },
            "slug": "Video-Skimming-for-Quick-Browsing-based-on-Audio-Smith",
            "title": {
                "fragments": [],
                "text": "Video Skimming for Quick Browsing based on Audio and Image Characterization"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The extraction of significant information, such as specific objects, audio keywords and relevant video structure, is made possible through the integration of techniques in image and language understanding and a \u201cskim\u201d video is proposed which represents a short synopsis of the original."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145007805"
                        ],
                        "name": "Y. Hasan",
                        "slug": "Y.-Hasan",
                        "structuredName": {
                            "firstName": "Yassin",
                            "lastName": "Hasan",
                            "middleNames": [
                                "M.",
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Hasan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47209857"
                        ],
                        "name": "Lina Karam",
                        "slug": "Lina-Karam",
                        "structuredName": {
                            "firstName": "Lina",
                            "lastName": "Karam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lina Karam"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1635,
                                "start": 1620
                            }
                        ],
                        "text": "These two approaches\nTable 2 A brief survey of TIE\nAuthor Year Approach Features\nOhya et al. [34] 1994 Adaptive thresholding and relaxation operations Color, scene text (train, signboard, skew and curved), localization and recognition Lee and 1995 Coarse search using edge information, followed by Scene text (cargo container), Kankanhalli [35] connected component (CC) generation localization and recognition Smith and Kanade [7] 1995 3\u00d7 3 Alter seeking vertical edges Caption text, localization Zhong et al. [36] 1995 CC-based method after color reduction, local spatial Scene text (CD covers), localization variance-based method, and hybrid method Yeo and Liu [56] 1996 Localization based on large inter-frame di2erence in Caption text, localization MPEG compressed image Shim et al. [21] 1998 Gray-level di2erence between pairs of pixels Caption text, localization Jain and Yu 1998 CC-based method after multi-valued color image Color (book cover, Web image, video [38] decomposition frame), localization Sato et al. [11] 1998 Smith and Kanade\u2019s localization method and Recognition recognition-based character extraction Chun et al. [54] 1999 Filtering using neural network after FFT Caption text, localization Antani et al. [28] 1999 Multiple algorithms in functional parallelism Scene text, recognition Messelodi and 1999 CC generation, followed by text line selection using Scene images (book covers, slanted) Modena [39] divisive hierarchical clustering procedure localization Wu et al. [45] 1999 Localization based on multi-scale texture segmentation Video and scene images (newspaper, advertisement) recognition Hasan and Karam [42] 2000 Morphological approach Scene text, localization Li et al. [51] 2000 Wavelet-based feature extraction and neural network for Scene text (slanted), localization, enhancement, texture analysis and tracking Lim et al. [33] 2000 Text detection and localization using DCT coeCcient Caption text, MPEG compressed and macroblock type information video, localization Zhong et al. [27] 2000 Texture analysis in DCT compressed domain Caption text, JPEG and I-frames of MPEG, localization Jung [49] 2001 Gabor Alter-like multi-layer perceptron for texture Color, caption text, localization analysis Chen et al. [43] 2001 Text detection in edge-enhanced image Caption text, localization and recognition Strouthopoulos 2002 Page layout analysis after adaptive color reduction Color document image, localization et al. [16]\nwork in a bottom-up fashion; by identifying sub-structures, such as CCs or edges, and then merging these sub-structures to mark bounding boxes for text."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 16
                            }
                        ],
                        "text": "Hasan and Karam [42] 2000 Morphological approach Scene text, localization"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 16
                            }
                        ],
                        "text": "Hasan and Karam [42] presented a morphological approach for text extraction."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5778124,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "ab09f0ac0f2700c47b3a0a303f13edc76177e66b",
            "isKey": false,
            "numCitedBy": 174,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a morphological technique for text extraction from images. The proposed morphological technique is insensitive to noise, skew and text orientation. It is also free from artifacts that are usually introduced by both fixed/optimal global thresholding and fixed-size block-based local thresholding. Examples are presented to illustrate the performance of the proposed method."
            },
            "slug": "Morphological-text-extraction-from-images-Hasan-Karam",
            "title": {
                "fragments": [],
                "text": "Morphological text extraction from images"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "The proposed morphological technique is insensitive to noise, skew and text orientation, and is also free from artifacts that are usually introduced by both fixed/optimal global thresholding and fixed-size block-based local thresholding."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Image Process."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39319377"
                        ],
                        "name": "Yu Zhong",
                        "slug": "Yu-Zhong",
                        "structuredName": {
                            "firstName": "Yu",
                            "lastName": "Zhong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yu Zhong"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 50
                            }
                        ],
                        "text": "Image content can be divided into two main categories: perceptual content and semantic content [1]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "E-mail address: kcjung@ssu.ac.kr (K. Jung)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 39821816,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "14371d83ac90edc11b3604d4bf64915f99e45678",
            "isKey": false,
            "numCitedBy": 220,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Page-segmentation-using-tecture-analysis-Jain-Zhong",
            "title": {
                "fragments": [],
                "text": "Page segmentation using tecture analysis"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2061646861"
                        ],
                        "name": "Stephan Fischer",
                        "slug": "Stephan-Fischer",
                        "structuredName": {
                            "firstName": "Stephan",
                            "lastName": "Fischer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephan Fischer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144739319"
                        ],
                        "name": "R. Lienhart",
                        "slug": "R.-Lienhart",
                        "structuredName": {
                            "firstName": "Rainer",
                            "lastName": "Lienhart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lienhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1750165"
                        ],
                        "name": "W. Effelsberg",
                        "slug": "W.-Effelsberg",
                        "structuredName": {
                            "firstName": "Wolfgang",
                            "lastName": "Effelsberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Effelsberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 123
                            }
                        ],
                        "text": "\u2212 Video content analysis: Extracted text regions or the output of character recognition can be useful in genre recognition [1, 79]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3168975,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "96840df6de2afaf6ede9e788c0eb54a9171d1aab",
            "isKey": false,
            "numCitedBy": 198,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Film genres in digital video can be detected automatically. In a three-step approach we analyze first the syntactic properties of digital films: color statistics, cut detection, camera motion, object motion and audio. In a second step we use these statistics to derive at a more abstract level film style attributes such as camera panning and zooming, speech and music. These are distinguishing properties for film genres, e.g. newscasts vs. sports vs. commercials. In the third and final step we map the detected style attributes to film genres. Algorithms for the three steps are presented in detail, and we report on initial experience with real videos. It is our goal to automatically classify the large body of existing video for easier access in digital video-on-demand databases."
            },
            "slug": "Automatic-recognition-of-film-genres-Fischer-Lienhart",
            "title": {
                "fragments": [],
                "text": "Automatic recognition of film genres"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is the goal to automatically classify the large body of existing video for easier access in digital video-on-demand databases."
            },
            "venue": {
                "fragments": [],
                "text": "MULTIMEDIA '95"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708785"
                        ],
                        "name": "J. Ohya",
                        "slug": "J.-Ohya",
                        "structuredName": {
                            "firstName": "Jun",
                            "lastName": "Ohya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ohya"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2019875"
                        ],
                        "name": "A. Shio",
                        "slug": "A.-Shio",
                        "structuredName": {
                            "firstName": "Akio",
                            "lastName": "Shio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Shio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49052113"
                        ],
                        "name": "S. Akamatsu",
                        "slug": "S.-Akamatsu",
                        "structuredName": {
                            "firstName": "Shigeru",
                            "lastName": "Akamatsu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Akamatsu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 339,
                                "start": 328
                            }
                        ],
                        "text": "These two approaches\nTable 2 A brief survey of TIE\nAuthor Year Approach Features\nOhya et al. [34] 1994 Adaptive thresholding and relaxation operations Color, scene text (train, signboard, skew and curved), localization and recognition Lee and 1995 Coarse search using edge information, followed by Scene text (cargo container), Kankanhalli [35] connected component (CC) generation localization and recognition Smith and Kanade [7] 1995 3\u00d7 3 Alter seeking vertical edges Caption text, localization Zhong et al. [36] 1995 CC-based method after color reduction, local spatial Scene text (CD covers), localization variance-based method, and hybrid method Yeo and Liu [56] 1996 Localization based on large inter-frame di2erence in Caption text, localization MPEG compressed image Shim et al. [21] 1998 Gray-level di2erence between pairs of pixels Caption text, localization Jain and Yu 1998 CC-based method after multi-valued color image Color (book cover, Web image, video [38] decomposition frame), localization Sato et al. [11] 1998 Smith and Kanade\u2019s localization method and Recognition recognition-based character extraction Chun et al. [54] 1999 Filtering using neural network after FFT Caption text, localization Antani et al. [28] 1999 Multiple algorithms in functional parallelism Scene text, recognition Messelodi and 1999 CC generation, followed by text line selection using Scene images (book covers, slanted) Modena [39] divisive hierarchical clustering procedure localization Wu et al. [45] 1999 Localization based on multi-scale texture segmentation Video and scene images (newspaper, advertisement) recognition Hasan and Karam [42] 2000 Morphological approach Scene text, localization Li et al. [51] 2000 Wavelet-based feature extraction and neural network for Scene text (slanted), localization, enhancement, texture analysis and tracking Lim et al. [33] 2000 Text detection and localization using DCT coeCcient Caption text, MPEG compressed and macroblock type information video, localization Zhong et al. [27] 2000 Texture analysis in DCT compressed domain Caption text, JPEG and I-frames of MPEG, localization Jung [49] 2001 Gabor Alter-like multi-layer perceptron for texture Color, caption text, localization analysis Chen et al. [43] 2001 Text detection in edge-enhanced image Caption text, localization and recognition Strouthopoulos 2002 Page layout analysis after adaptive color reduction Color document image, localization et al. [16]\nwork in a bottom-up fashion; by identifying sub-structures, such as CCs or edges, and then merging these sub-structures to mark bounding boxes for text."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[34] presented a four-stage method: (i) binarization based on local thresholding, (ii) tentative character component detection using gray-level di2erence, (iii) character recognition for calculating the similarities between the character candidates and the standard patterns stored in a database, and (iv) relaxation operation to update the similarities."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 0
                            }
                        ],
                        "text": "Lee and Kankanhalli [35] applied a CC-based method to the detection and recognition of text on cargo containers, which can have uneven lighting conditions and characters with di2erent sizes and shapes."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[34] 1994 Adaptive thresholding and relaxation operations Color, scene text (train, signboard, skew and curved), localization and recognition Lee and 1995 Coarse search using edge information, followed by Scene text (cargo container), Kankanhalli [35] connected component (CC) generation localization and recognition Smith and Kanade [7] 1995 3\u00d7 3 Alter seeking vertical edges Caption text, localization Zhong et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 20
                            }
                        ],
                        "text": "Lee and Kankanhalli [34] apply a CC-based method for cargo container veriAcation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1565945,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e94d1ff801fce49eea8d8aa51a477b130ca755de",
            "isKey": true,
            "numCitedBy": 263,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "An effective algorithm for character recognition in scene images is studied. Scene images are segmented into regions by an image segmentation method based on adaptive thresholding. Character candidate regions are detected by observing gray-level differences between adjacent regions. To ensure extraction of multisegment characters as well as single-segment characters, character pattern candidates are obtained by associating the detected regions according to their positions and gray levels. A character recognition process selects patterns with high similarities by calculating the similarities between character pattern candidates and the standard patterns in a dictionary and then comparing the similarities to the thresholds. A relaxational approach to determine character patterns updates the similarities by evaluating the interactions between categories of patterns, and finally character patterns and their recognition results are obtained. Highly promising experimental results have been obtained using the method on 100 images involving characters of different sizes and formats under uncontrolled lighting. >"
            },
            "slug": "Recognizing-Characters-in-Scene-Images-Ohya-Shio",
            "title": {
                "fragments": [],
                "text": "Recognizing Characters in Scene Images"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "An effective algorithm for character recognition in scene images is studied and highly promising experimental results have been obtained using the method on 100 images involving characters of different sizes and formats under uncontrolled lighting."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1950475"
                        ],
                        "name": "Zhaoyang Lu",
                        "slug": "Zhaoyang-Lu",
                        "structuredName": {
                            "firstName": "Zhaoyang",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhaoyang Lu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 171
                            }
                        ],
                        "text": "Gray-scale document images: (a) single-column text from a book, (b) two-column page from a journal (IEEE Transactions on PAMI), and (c) an electrical drawing (courtesy Lu [24])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6192673,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e4732d06f10440f980a0af8758db17819ad88ba6",
            "isKey": false,
            "numCitedBy": 65,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "An algorithm for text/graphics separation is presented in this paper. The basic principle of the algorithm is to erase nontext regions from mixed text and graphics engineering drawings, rather than extract text regions directly. This algorithm can be used to extract both Chinese and Western characters, dimensions, and symbols and has few limitations on the kind of engineering drawings and noise level. It is robust to text-graphics touching, text fonts, and written orientations."
            },
            "slug": "Detection-of-Text-Regions-From-Digital-Engineering-Lu",
            "title": {
                "fragments": [],
                "text": "Detection of Text Regions From Digital Engineering Drawings"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "An algorithm for text/graphics separation is presented that can be used to extract both Chinese and Western characters, dimensions, and symbols and has few limitations on the kind of engineering drawings and noise level."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2530942"
                        ],
                        "name": "H. Hase",
                        "slug": "H.-Hase",
                        "structuredName": {
                            "firstName": "Hiroyuki",
                            "lastName": "Hase",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hase"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3188516"
                        ],
                        "name": "T. Shinokawa",
                        "slug": "T.-Shinokawa",
                        "structuredName": {
                            "firstName": "Toshiyuki",
                            "lastName": "Shinokawa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Shinokawa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "71114145"
                        ],
                        "name": "M. Yoneda",
                        "slug": "M.-Yoneda",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Yoneda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Yoneda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713795"
                        ],
                        "name": "C. Suen",
                        "slug": "C.-Suen",
                        "structuredName": {
                            "firstName": "Ching",
                            "lastName": "Suen",
                            "middleNames": [
                                "Yee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Suen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[84] proposed a CC-based method for color documents."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 40333068,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2b24ea59f58374750894ad050dbafe88c81ed54f",
            "isKey": false,
            "numCitedBy": 53,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Character-string-extraction-from-color-documents-Hase-Shinokawa",
            "title": {
                "fragments": [],
                "text": "Character string extraction from color documents"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2530942"
                        ],
                        "name": "H. Hase",
                        "slug": "H.-Hase",
                        "structuredName": {
                            "firstName": "Hiroyuki",
                            "lastName": "Hase",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hase"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3188516"
                        ],
                        "name": "T. Shinokawa",
                        "slug": "T.-Shinokawa",
                        "structuredName": {
                            "firstName": "Toshiyuki",
                            "lastName": "Shinokawa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Shinokawa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "71114145"
                        ],
                        "name": "M. Yoneda",
                        "slug": "M.-Yoneda",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Yoneda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Yoneda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052190554"
                        ],
                        "name": "Mitsuru Sakai",
                        "slug": "Mitsuru-Sakai",
                        "structuredName": {
                            "firstName": "Mitsuru",
                            "lastName": "Sakai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mitsuru Sakai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49770410"
                        ],
                        "name": "H. Maruyama",
                        "slug": "H.-Maruyama",
                        "structuredName": {
                            "firstName": "Hiroshi",
                            "lastName": "Maruyama",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Maruyama"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "images and string extraction by multi-stage relaxation, which was presented by the same authors previously [ 85 ], is"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5991845,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1f75ea0cc4168aa592c63691afa8b6e4c062f280",
            "isKey": false,
            "numCitedBy": 11,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "An extraction algorithm for character strings is proposed. We first obtain a set of eight-connected components from a document image. For the components, we apply a relaxation method. The method makes mutual connections between components increase or decrease depending on the state of the neighboring components. While applying the relaxation method several times, the process proceeds from a local connection to a global connection, and finally character strings are extracted. We call this process multi stage relaxation. The advantages of this algorithm are that it does not need to nominate character components from an image beforehand, it is adaptive for character size and font, and it can also cope with a document which includes strings with various orientations. In our experiments we use a color image of a magazine cover and a monochromatic image of a graph. For the color image, the multi stage relaxation was executed for each binary image obtained by color segmentation. Lastly, we show the results of the experiments and discuss the effectiveness of our method."
            },
            "slug": "Character-string-extraction-by-multi-stage-Hase-Shinokawa",
            "title": {
                "fragments": [],
                "text": "Character string extraction by multi-stage relaxation"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "An extraction algorithm for character strings that does not need to nominate character components from an image beforehand, it is adaptive for character size and font, and it can also cope with a document which includes strings with various orientations is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Fourth International Conference on Document Analysis and Recognition"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679880"
                        ],
                        "name": "H. Wactlar",
                        "slug": "H.-Wactlar",
                        "structuredName": {
                            "firstName": "Howard",
                            "lastName": "Wactlar",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Wactlar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733113"
                        ],
                        "name": "T. Kanade",
                        "slug": "T.-Kanade",
                        "structuredName": {
                            "firstName": "Takeo",
                            "lastName": "Kanade",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanade"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116645471"
                        ],
                        "name": "Michael A. Smith",
                        "slug": "Michael-A.-Smith",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Smith",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael A. Smith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48817314"
                        ],
                        "name": "S. Stevens",
                        "slug": "S.-Stevens",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Stevens",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Stevens"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 196,
                                "start": 194
                            }
                        ],
                        "text": "While comprehensive surveys of related problems such as face detection, document analysis, and image & video indexing can be found, the problem of text information extraction is not well surveyed."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8345108,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3b4d170b81ffc895e5b7960040a3f44a044d6bb8",
            "isKey": false,
            "numCitedBy": 372,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Carnegie Mellon's Informedia Digital Video Library project will establish a large, on-line digital video library featuring full-content and knowledge-based search and retrieval. Intelligent, automatic mechanisms will be developed to populate the library. Search and retrieval from digital video, audio, and text libraries will take place via desktop computer over local, metropolitan, and wide-area networks. The project's approach applies several techniques for content-based searching and video-sequence retrieval. Content is conveyed in both the narrative (speech and language) and the image. Only by the collaborative interaction of image, speech, and natural language understanding technology is it possible to successfully populate, segment, index, and search diverse video collections with satisfactory recall and precision. This collaborative interaction approach uniquely compensates for problems of interpretation and search in error-ridden and ambiguous data sets. The authors have focused the work on two corpuses. One is science documentaries and lectures, the other is broadcast news content with partial closed-captions. Further work will continue to improve the accuracy and performance of the underlying processing as well as explore performance issues related to Web-based access and interoperability with other digital video resources."
            },
            "slug": "Intelligent-Access-to-Digital-Video:-Informedia-Wactlar-Kanade",
            "title": {
                "fragments": [],
                "text": "Intelligent Access to Digital Video: Informedia Project"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "Carnegie Mellon's Informedia Digital Video Library project will establish a large, on-line digital video library featuring full-content and knowledge-based search and retrieval, and focused the work on two corpuses."
            },
            "venue": {
                "fragments": [],
                "text": "Computer"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2898481"
                        ],
                        "name": "Wei-Song Qi",
                        "slug": "Wei-Song-Qi",
                        "structuredName": {
                            "firstName": "Wei-Song",
                            "lastName": "Qi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei-Song Qi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3062639"
                        ],
                        "name": "L. Gu",
                        "slug": "L.-Gu",
                        "structuredName": {
                            "firstName": "Lie",
                            "lastName": "Gu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152631223"
                        ],
                        "name": "Hao Jiang",
                        "slug": "Hao-Jiang",
                        "structuredName": {
                            "firstName": "Hao",
                            "lastName": "Jiang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hao Jiang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46772671"
                        ],
                        "name": "Xiangrong Chen",
                        "slug": "Xiangrong-Chen",
                        "structuredName": {
                            "firstName": "Xiangrong",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiangrong Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108698841"
                        ],
                        "name": "HongJiang Zhang",
                        "slug": "HongJiang-Zhang",
                        "structuredName": {
                            "firstName": "HongJiang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "HongJiang Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Studies on semantic image content in the form of text, face, vehicle, and human action have also attracted some recent interest [7-16]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15472062,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b9bab6c1c2270e91e94d393e82d86fa1e05d2b61",
            "isKey": false,
            "numCitedBy": 90,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a system developed for content-based broadcast news video browsing for home users. There are three main factors that distinguish our work from other similar ones. First, we have integrated the image and audio analysis results in identifying news segments. Second, we use the video OCR technology to detect text from frames, which provides a good source of textual information for story classification when transcripts and close captions are not available. Finally, natural language processing (NLP) technologies are used to perform automated categorization of news stories based on the texts obtained from close caption or video OCR process. Based on these video structure and content analysis technologies, we have developed two advanced video browsers for home users: intelligent highlight player and HTML-based video browser."
            },
            "slug": "Integrating-visual,-audio-and-text-analysis-for-Qi-Gu",
            "title": {
                "fragments": [],
                "text": "Integrating visual, audio and text analysis for news video"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Two advanced video browsers for home users are developed: intelligent highlight player and HTML-based video browser that perform automated categorization of news stories based on the texts obtained from close caption or video OCR process."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 2000 International Conference on Image Processing (Cat. No.00CH37101)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2854896"
                        ],
                        "name": "A. Yoshitaka",
                        "slug": "A.-Yoshitaka",
                        "structuredName": {
                            "firstName": "Atsuo",
                            "lastName": "Yoshitaka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yoshitaka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737975"
                        ],
                        "name": "T. Ichikawa",
                        "slug": "T.-Ichikawa",
                        "structuredName": {
                            "firstName": "Tadao",
                            "lastName": "Ichikawa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Ichikawa"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 188,
                                "start": 184
                            }
                        ],
                        "text": "While comprehensive surveys of related problems such as face detection, document analysis, and image & video indexing can be found, the problem of text information extraction is not well surveyed."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16766019,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bc4b948b1a0f91525bc3d47e9e192b392bf790ed",
            "isKey": false,
            "numCitedBy": 378,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "Conventional database systems are designed for managing textual and numerical data, and retrieving such data is often based on simple comparisons of text/numerical values. However, this simple method of retrieval is no longer adequate for multimedia data, since the digitized representation of images, video, or data itself does not convey the reality of these media items. In addition, composite data consisting of heterogeneous types of data also associates with the semantic content acquired by a user's recognition. Therefore, content-based retrieval for multimedia data is realized taking such intrinsic features of multimedia data into account. Implementation of the content-based retrieval facility is not based on a single fundamental, but is closely related to an underlying data model, a priori knowledge of the area of interest, and the scheme for representing queries. This paper surveys recent studies on content-based retrieval for multimedia databases from the point of view of three fundamental issues. Throughout the discussion, we assume databases that manage only nontextual/numerical data, such as image or video, are also in the category of multimedia databases."
            },
            "slug": "A-Survey-on-Content-Based-Retrieval-for-Multimedia-Yoshitaka-Ichikawa",
            "title": {
                "fragments": [],
                "text": "A Survey on Content-Based Retrieval for Multimedia Databases"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper surveys recent studies on content-based retrieval for multimedia databases from the point of view of three fundamental issues: an underlying data model, a priori knowledge of the area of interest, and the scheme for representing queries."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Knowl. Data Eng."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39702442"
                        ],
                        "name": "Y. Tang",
                        "slug": "Y.-Tang",
                        "structuredName": {
                            "firstName": "Yuan",
                            "lastName": "Tang",
                            "middleNames": [
                                "Yan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Tang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50112753"
                        ],
                        "name": "Seong-Whan Lee",
                        "slug": "Seong-Whan-Lee",
                        "structuredName": {
                            "firstName": "Seong-Whan",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Seong-Whan Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713795"
                        ],
                        "name": "C. Suen",
                        "slug": "C.-Suen",
                        "structuredName": {
                            "firstName": "Ching",
                            "lastName": "Suen",
                            "middleNames": [
                                "Yee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Suen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 29737518,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "92df66c37211e55faafdf879f95185fa86f196a6",
            "isKey": false,
            "numCitedBy": 140,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Automatic-document-processing:-A-survey-Tang-Lee",
            "title": {
                "fragments": [],
                "text": "Automatic document processing: A survey"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2413848"
                        ],
                        "name": "B. Shahraray",
                        "slug": "B.-Shahraray",
                        "structuredName": {
                            "firstName": "Behzad",
                            "lastName": "Shahraray",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Shahraray"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2387879"
                        ],
                        "name": "D. Gibbon",
                        "slug": "D.-Gibbon",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Gibbon",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Gibbon"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "\u2212 Text-based image indexing: This involves automatic text-based video structuring methods using caption data [11, 78]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 62152523,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fe60d1f5b811688f3b40014ad6108b493f7f3e82",
            "isKey": false,
            "numCitedBy": 92,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "An automatic authoring system for the generation of pictorial transcripts of video programs which are accompanied by closed caption information is presented. A number of key frames, each of which represents the visual information in a segment of the video (i.e., a scene), are selected automatically by performing a content-based sampling of the video program. The textual information is recovered from the closed caption signal and is initially segmented based on its implied temporal relationship with the video segments. The text segmentation boundaries are then adjusted, based on lexical analysis and/or caption control information, to account for synchronization errors due to possible delays in the detection of scene boundaries or the transmission of the caption information. The closed caption text is further refined through linguistic processing for conversion to lower- case with correct capitalization. The key frames and the related text generate a compact multimedia presentation of the contents of the video program which lends itself to efficient storage and transmission. This compact representation can be viewed on a computer screen, or used to generate the input to a commercial text processing package to generate a printed version of the program."
            },
            "slug": "Automatic-generation-of-pictorial-transcripts-of-Shahraray-Gibbon",
            "title": {
                "fragments": [],
                "text": "Automatic generation of pictorial transcripts of video programs"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "An automatic authoring system for the generation of pictorial transcripts of video programs which are accompanied by closed caption information is presented, which generates a compact multimedia presentation of the contents of the video program which lends itself to efficient storage and transmission."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145211604"
                        ],
                        "name": "K. Karu",
                        "slug": "K.-Karu",
                        "structuredName": {
                            "firstName": "Kalle",
                            "lastName": "Karu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Karu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 501,
                                "start": 496
                            }
                        ],
                        "text": "These two approaches\nTable 2 A brief survey of TIE\nAuthor Year Approach Features\nOhya et al. [34] 1994 Adaptive thresholding and relaxation operations Color, scene text (train, signboard, skew and curved), localization and recognition Lee and 1995 Coarse search using edge information, followed by Scene text (cargo container), Kankanhalli [35] connected component (CC) generation localization and recognition Smith and Kanade [7] 1995 3\u00d7 3 Alter seeking vertical edges Caption text, localization Zhong et al. [36] 1995 CC-based method after color reduction, local spatial Scene text (CD covers), localization variance-based method, and hybrid method Yeo and Liu [56] 1996 Localization based on large inter-frame di2erence in Caption text, localization MPEG compressed image Shim et al. [21] 1998 Gray-level di2erence between pairs of pixels Caption text, localization Jain and Yu 1998 CC-based method after multi-valued color image Color (book cover, Web image, video [38] decomposition frame), localization Sato et al. [11] 1998 Smith and Kanade\u2019s localization method and Recognition recognition-based character extraction Chun et al. [54] 1999 Filtering using neural network after FFT Caption text, localization Antani et al. [28] 1999 Multiple algorithms in functional parallelism Scene text, recognition Messelodi and 1999 CC generation, followed by text line selection using Scene images (book covers, slanted) Modena [39] divisive hierarchical clustering procedure localization Wu et al. [45] 1999 Localization based on multi-scale texture segmentation Video and scene images (newspaper, advertisement) recognition Hasan and Karam [42] 2000 Morphological approach Scene text, localization Li et al. [51] 2000 Wavelet-based feature extraction and neural network for Scene text (slanted), localization, enhancement, texture analysis and tracking Lim et al. [33] 2000 Text detection and localization using DCT coeCcient Caption text, MPEG compressed and macroblock type information video, localization Zhong et al. [27] 2000 Texture analysis in DCT compressed domain Caption text, JPEG and I-frames of MPEG, localization Jung [49] 2001 Gabor Alter-like multi-layer perceptron for texture Color, caption text, localization analysis Chen et al. [43] 2001 Text detection in edge-enhanced image Caption text, localization and recognition Strouthopoulos 2002 Page layout analysis after adaptive color reduction Color document image, localization et al. [16]\nwork in a bottom-up fashion; by identifying sub-structures, such as CCs or edges, and then merging these sub-structures to mark bounding boxes for text."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 123
                            }
                        ],
                        "text": "[44] use a learning-based approach for license plate extraction, which is similar to a texture-based text detection method [47,49]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "Zhong et al. [36] use the local spatial variations in a gray-scale image to locate text regions with a high variance."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "Zhong et al. [27] and Antani et al. [28] performed text localization on compressed images, which resulted in a faster performance."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "Zhong et al. [36] fused the CC-based approach with the texture-based approach."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 61
                            }
                        ],
                        "text": "The input conAguration is the same as that in Jain and Zhong [47]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "Zhong et al. [27] presented a method for localizing captions in JPEG images and I-frames of MPEG compressed videos."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 31
                            }
                        ],
                        "text": "Jain and Zhong [17] and others [47,48] use a learning-based texture discrimination method to separate text, graphics, and halftone image regions in documents."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "Zhong et al. [36] used a CC-based method using color reduction."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 18
                            }
                        ],
                        "text": "In another paper, Zhong [27] presents a similar texture-based method for JPEG images and I-frames of MPEG compressed videos."
                    },
                    "intents": []
                }
            ],
            "corpusId": 20133451,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bfbafbc47d9ad9722047bf7c15d88d9bf4762744",
            "isKey": false,
            "numCitedBy": 216,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Texture segmentation using multichannel filtering involves applying a set of masks to an input image, and then grouping the pixels based on the responses to these masks. We solve the problem of finding an optimal set of masks by designing a neural network which is trained to maximize a relevant function. Two algorithms, the centroid algorithm and the gradient descent algorithm, are used to train the network. Experimental results on segmenting two natural textures and extracting barcodes in an image are reported, and the error rates compared for both the algorithms with different network configurations. The centroid algorithm gives better results in small parameter spaces, whereas the gradient descent algorithm works better with more parameters. Our method of automatically generating texture discrimination masks not only results in a good segmentation performance, but also reduces the dimensionality of the feature space compared to previously published multichannel filtering methods.<<ETX>>"
            },
            "slug": "Learning-texture-discrimination-masks-Jain-Karu",
            "title": {
                "fragments": [],
                "text": "Learning Texture Discrimination Masks"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The method of automatically generating texture discrimination masks not only results in a good segmentation performance, but also reduces the dimensionality of the feature space compared to previously published multichannel filtering methods."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "121267347"
                        ],
                        "name": "K. Jung",
                        "slug": "K.-Jung",
                        "structuredName": {
                            "firstName": "Keechul",
                            "lastName": "Jung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Jung"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 5
                            }
                        ],
                        "text": "Jung [49] 2001 Gabor Alter-like multi-layer perceptron for texture Color, caption text, localization"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 123
                            }
                        ],
                        "text": "[44] use a learning-based approach for license plate extraction, which is similar to a texture-based text detection method [47,49]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15034932,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "93c8a2963be6088dd55959bfe8cd92916891fb66",
            "isKey": false,
            "numCitedBy": 112,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Neural-network-based-text-location-in-color-images-Jung",
            "title": {
                "fragments": [],
                "text": "Neural network-based text location in color images"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit. Lett."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115439468"
                        ],
                        "name": "Yuntao Cui",
                        "slug": "Yuntao-Cui",
                        "structuredName": {
                            "firstName": "Yuntao",
                            "lastName": "Cui",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuntao Cui"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1391129943"
                        ],
                        "name": "Qian Huang",
                        "slug": "Qian-Huang",
                        "structuredName": {
                            "firstName": "Qian",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qian Huang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "also attracted some recent interest [ 7-16 ]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "assumptions have been made regarding the image acquisition process (camera and vehicle position and direction, illumination, character types, and color) and geometric attributes of the text. Cui and Huang [ 9 ] model the extraction of"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "applications including page segmentation [17, 18], address block location [19], license plate location [ 9 , 20], and content-"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2459300,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1e75b09dff3656243fde5cacf9baf1c42a4ea972",
            "isKey": false,
            "numCitedBy": 122,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present a new approach to extract characters on a license plate of a moving vehicle given a sequence of perspective distortion corrected license plate images. We model the extraction of characters as a Markov random field (MRF). With the MRF modeling, the extraction of characters is formulated as the problem of maximizing the a posteriori probability based on given prior and observations. A genetic algorithm with local greedy mutation operator is employed to optimize the objective function. Experiments and comparison study were conducted. It is shown that our approach provides better performance than other single frame methods."
            },
            "slug": "Character-extraction-of-license-plates-from-video-Cui-Huang",
            "title": {
                "fragments": [],
                "text": "Character extraction of license plates from video"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "It is shown that this approach to extract characters on a license plate of a moving vehicle given a sequence of perspective distortion corrected license plate images provides better performance than other single frame methods."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784196"
                        ],
                        "name": "Datong Chen",
                        "slug": "Datong-Chen",
                        "structuredName": {
                            "firstName": "Datong",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Datong Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678373"
                        ],
                        "name": "J. Luettin",
                        "slug": "J.-Luettin",
                        "structuredName": {
                            "firstName": "Juergen",
                            "lastName": "Luettin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Luettin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 146
                            }
                        ],
                        "text": "\u2026of text in images\nProperty Variants or sub-classes\nSize Regularity in size of text Horizontal/vertical Straight line with skew (implies vertical direction)Geometry Alignment Curves 3D perspective distortion\nInter-character distance Aggregation of characters with uniform distance\nGrayColor\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "E-mail address: kcjung@ssu.ac.kr (K. Jung)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 61028857,
            "fieldsOfStudy": [
                "Computer Science",
                "Physics",
                "Art"
            ],
            "id": "53757cc24a70c812a97b5869df45a78d3ab97f74",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "A Survey of Text Detection and Recognition in Images and Videos, including the state-of-the-art methods and systems."
            },
            "slug": "A-Survey-of-Text-Detection-and-Recognition-in-and-Chen-Luettin",
            "title": {
                "fragments": [],
                "text": "A Survey of Text Detection and Recognition in Images and Videos"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A Survey of Text Detection and Recognition in Images and Videos, including the state-of-the-art methods and systems."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118021384"
                        ],
                        "name": "Sunghoon Kim",
                        "slug": "Sunghoon-Kim",
                        "structuredName": {
                            "firstName": "Sunghoon",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sunghoon Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111889373"
                        ],
                        "name": "D. Kim",
                        "slug": "D.-Kim",
                        "structuredName": {
                            "firstName": "Dae",
                            "lastName": "Kim",
                            "middleNames": [
                                "Sung"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3354836"
                        ],
                        "name": "Younbok Ryu",
                        "slug": "Younbok-Ryu",
                        "structuredName": {
                            "firstName": "Younbok",
                            "lastName": "Ryu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Younbok Ryu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2157691706"
                        ],
                        "name": "Gyeonghwan Kim",
                        "slug": "Gyeonghwan-Kim",
                        "structuredName": {
                            "firstName": "Gyeonghwan",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gyeonghwan Kim"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[88] use gradient information to extract license plates."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14088001,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0a049d297a842c150503b4f432dd9db111c519a6",
            "isKey": false,
            "numCitedBy": 162,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "A robust approach for extracting car license plate from images with complex background and relatively poor quality is presented. The approach focuses on dealing with images taken under weak lighting condition. The proposed method is divided into two steps: 1) searching candidate areas from the input image using gradient information, and 2) determining the plate area among the candidates and adjusting the boundary of the area by introducing a plate template. A set of experiments has been performed to prove the robustness and accuracy of the approach. For many images collected from a large underground parking place the result shows that 90% of them are correctly segmented."
            },
            "slug": "A-robust-license-plate-extraction-method-under-Kim-Kim",
            "title": {
                "fragments": [],
                "text": "A robust license-plate extraction method under complex image conditions"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "A robust approach for extracting car license plate from images with complex background and relatively poor quality is presented and shows that 90% of images collected from a large underground parking place are correctly segmented."
            },
            "venue": {
                "fragments": [],
                "text": "Object recognition supported by user interaction for service robots"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144973459"
                        ],
                        "name": "HongJiang Zhang",
                        "slug": "HongJiang-Zhang",
                        "structuredName": {
                            "firstName": "HongJiang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "HongJiang Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144768792"
                        ],
                        "name": "Yihong Gong",
                        "slug": "Yihong-Gong",
                        "structuredName": {
                            "firstName": "Yihong",
                            "lastName": "Gong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yihong Gong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743808"
                        ],
                        "name": "S. Smoliar",
                        "slug": "S.-Smoliar",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Smoliar",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Smoliar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153635925"
                        ],
                        "name": "Shuang Yeo Tan",
                        "slug": "Shuang-Yeo-Tan",
                        "structuredName": {
                            "firstName": "Shuang",
                            "lastName": "Tan",
                            "middleNames": [
                                "Yeo"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shuang Yeo Tan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "applications including page segmentation [17, 18], address block location [19], license plate location [9, 20], and content- based image/video indexing [ 5 , 21]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "their relations. A number of studies on the use of relatively low-level perceptual content [ 2-6 ] for image and video indexing"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 35961225,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "05edc783def10174ca1d9952deb444f1e1c5baa1",
            "isKey": false,
            "numCitedBy": 207,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Video content parsing is possible when one has an a priori model of a video's structure based on domain knowledge. This paper presents work on using domain knowledge to parse content of news video programs. Approaches to locating and identifying frame structure models based on temporal and spatial structure of news video data, along with algorithms to apply these models in parsing news video, have been developed and are presented in detail in this paper. Experimental results are also discussed in detail to evaluate the approaches and algorithms. Finally, proposals for future work are summarized.<<ETX>>"
            },
            "slug": "Automatic-parsing-of-news-video-Zhang-Gong",
            "title": {
                "fragments": [],
                "text": "Automatic parsing of news video"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Approaches to locating and identifying frame structure models based on temporal and spatial structure of news video data, along with algorithms to apply these models in parsing news video, have been developed and are presented in detail in this paper."
            },
            "venue": {
                "fragments": [],
                "text": "1994 Proceedings of IEEE International Conference on Multimedia Computing and Systems"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1406741841"
                        ],
                        "name": "P. L'assainato",
                        "slug": "P.-L'assainato",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "L'assainato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. L'assainato"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2833202"
                        ],
                        "name": "P. Gamba",
                        "slug": "P.-Gamba",
                        "structuredName": {
                            "firstName": "Paolo",
                            "lastName": "Gamba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Gamba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1406741847"
                        ],
                        "name": "A. Mccocci",
                        "slug": "A.-Mccocci",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Mccocci",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Mccocci"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[61] used the vanishing point information to recover the 3D information."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 62739188,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7360dc2616b4a07ec94f7b602093e6e30c1b682f",
            "isKey": false,
            "numCitedBy": 2,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method to extract characters from images of outdoor scenes by means of a suitable grouping procedure. The candidate characters are recognized by simple low-level segmentation techniques, and a further elaboration allows to group them in consistent chains by means of their relation to the vanishing points of the scene. The method has been trained and tested using two different sets of images, recorded without any particular attention to the light conditions and the tilt angle of the camera, and very good results are obtained."
            },
            "slug": "Character-recognition-in-external-scenes-by-means-L'assainato-Gamba",
            "title": {
                "fragments": [],
                "text": "Character recognition in external scenes by means of vanishing point grouping"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A method to extract characters from images of outdoor scenes by means of a suitable grouping procedure, trained and tested using two different sets of images, and very good results are obtained."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 13th International Conference on Digital Signal Processing"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1767238"
                        ],
                        "name": "Y. Nakajima",
                        "slug": "Y.-Nakajima",
                        "structuredName": {
                            "firstName": "Yasuyuki",
                            "lastName": "Nakajima",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Nakajima"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144415144"
                        ],
                        "name": "A. Yoneyama",
                        "slug": "A.-Yoneyama",
                        "structuredName": {
                            "firstName": "Akio",
                            "lastName": "Yoneyama",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yoneyama"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796620"
                        ],
                        "name": "H. Yanagihara",
                        "slug": "H.-Yanagihara",
                        "structuredName": {
                            "firstName": "Hiromasa",
                            "lastName": "Yanagihara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Yanagihara"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1769639"
                        ],
                        "name": "M. Sugano",
                        "slug": "M.-Sugano",
                        "structuredName": {
                            "firstName": "Masaru",
                            "lastName": "Sugano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Sugano"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 58750105,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bcbac7ea4875d3e56886d4f1a4423bd3af30b2ef",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a method of moving object detection directly from MPEG coded data. Since motion information in MPEG coded data is determined in terms of coding efficiency point of view, it does not always provide real motion information of objects. We use a wide variety of coding information including motion vectors and DCT coefficients to estimate real object motion. Since such information can be directly obtained from coded bitstream, very fast operation can be expected. Moving objects are detected basically analyzing motion vectors and spatio-temporal correlation of motion in P-, and B-pictures. Moving objects are also detected in intra macroblocks by analyzing coding characteristics of intra macroblocks in P- and B-pictures and by investigating temporal motion continuity in I-pictures. The simulation results show that successful moving object detection has been performed on macroblock level using several test sequences. Since proposed method is very simple and requires much less computational power than the conventional object detection methods, it has a significant advantage as motion analysis tool."
            },
            "slug": "Moving-object-detection-from-MPEG-coded-data-Nakajima-Yoneyama",
            "title": {
                "fragments": [],
                "text": "Moving-object detection from MPEG coded data"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Simulation results show that successful moving object detection has been performed on macroblock level using several test sequences and proposed method has a significant advantage as motion analysis tool."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111414836"
                        ],
                        "name": "Dong-Su Kim",
                        "slug": "Dong-Su-Kim",
                        "structuredName": {
                            "firstName": "Dong-Su",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dong-Su Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3681448"
                        ],
                        "name": "S. Chien",
                        "slug": "S.-Chien",
                        "structuredName": {
                            "firstName": "Sung-Il",
                            "lastName": "Chien",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Chien"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 17
                            }
                        ],
                        "text": "Perceptual content includes attributes such as color, intensity, shape, texture, and their temporal changes, whereas semantic content means objects, events, and their relations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 111074479,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "98091fc9ce8396aa302b4ccb1555757b653cc05a",
            "isKey": false,
            "numCitedBy": 140,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a method which evaluates symmetry of plate corners and extracts car license plates captured from the arbitrary directions. The generalized symmetry transform (GST) produces continuous features of symmetry between two points by combining locality constraint and reflectional symmetry. Also, we propose scan line based GST to improve time complexity of the GST significantly due to selective attention to particular distortions in case of a large searching window. The corners of a plate are detected from our modified scan line based GST with selective directions using the fact that a corner is formed by two neighboring straight lines acquired in a symmetry map of GST. However, a rotated or perspectively distorted car license plate image is very difficult for segmentation of interior characters. Image normalization by image warping is adopted to make such segmentation of license plate and later identification much easier. We also adopt the verifier which evaluates a candidate license plate to enhance extraction rate. Our experiments show that the proposed method for extracting perspectively distorted license plates is fairly reliable."
            },
            "slug": "Automatic-car-license-plate-extraction-using-and-Kim-Chien",
            "title": {
                "fragments": [],
                "text": "Automatic car license plate extraction using modified generalized symmetry transform and image warping"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "A method which evaluates symmetry of plate corners and extracts car license plates captured from the arbitrary directions and adopts the verifier which evaluates a candidate license plate to enhance extraction rate is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "ISIE 2001. 2001 IEEE International Symposium on Industrial Electronics Proceedings (Cat. No.01TH8570)"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145573734"
                        ],
                        "name": "F. Lebourgeois",
                        "slug": "F.-Lebourgeois",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Lebourgeois",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Lebourgeois"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "for TIE, and selected five algorithms [29, 57, 58,  59 , 71] as promising."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 29939979,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0a8754ab68589b8e893d6962eb92c56300ecb764",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper presents a general robust OCR system designed for practical use and suited to unconstrained gray-level images grabbed from a CCD camera. The system works with minimum assumptions on font, text location, size, color and the background scene. The text blocks localization in complex scenes using a specific filter which enhances any text from the background without binarization. A special stage is designed to separate characters, even touched by using gray-level information. The authors also extract gray-level features which make the algorithm more reliable, in particular under poor printing conditions or bad contrast digitization."
            },
            "slug": "Robust-multifont-OCR-system-from-gray-level-images-Lebourgeois",
            "title": {
                "fragments": [],
                "text": "Robust multifont OCR system from gray level images"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "The paper presents a general robust OCR system designed for practical use and suited to unconstrained gray-level images grabbed from a CCD camera, with minimum assumptions on font, text location, size, color and the background scene."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Fourth International Conference on Document Analysis and Recognition"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2622803"
                        ],
                        "name": "G. Feng",
                        "slug": "G.-Feng",
                        "structuredName": {
                            "firstName": "Guotong",
                            "lastName": "Feng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Feng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745655"
                        ],
                        "name": "C. Bouman",
                        "slug": "C.-Bouman",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Bouman",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Bouman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "146876323"
                        ],
                        "name": "Hui Cheng",
                        "slug": "Hui-Cheng",
                        "structuredName": {
                            "firstName": "Hui",
                            "lastName": "Cheng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hui Cheng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "quality. Feng et al. [ 76 ] and Cheng et al. [77] apply adaptive dithering after segmenting a document into several"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6231649,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d0fa827ea59f7f1a67a37daf57e3d3e677d801e0",
            "isKey": false,
            "numCitedBy": 25,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "The mixed raster content (MRC) model can be used to implement highly effective document compression algorithms. MRC document coders are typically based on the use of a binary mask layer that efficiently encodes the text and graphic content. However, while many MRC-based methods can yield much higher compression ratios than conventional color image compression methods, the binary representation tends to distort fine document details, such as thin lines and text edges. In this paper, we propose a method for encoding and decoding the binary mask layer that substantially improves the decoded document fidelity of text and graphics at a fixed bit rate. This method, which we call resolution-enhanced rendering (RER), works by adaptively dithering the encoded binary mask, and then applying a nonlinear predictor to decode a gray level mask at the same resolution. Both the dithering and nonlinear prediction algorithms are jointly optimized to produce the minimal distortion rendering. In addition, we introduce a second method, interpolative RER (IRER), which incorporates interpolation into the MRC decoder. The IRER method increases the compression ratio by allowing a high-resolution document to be coded at lower resolutions. We present experimental results illustrating the performance of our RER/IRER methods and comparing them to some existing MRC-based compression algorithms"
            },
            "slug": "High-Quality-MRC-Document-Coding-Feng-Bouman",
            "title": {
                "fragments": [],
                "text": "High-Quality MRC Document Coding"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper proposes a method for encoding and decoding the binary mask layer that substantially improves the decoded document fidelity of text and graphics at a fixed bit rate, and introduces a second method, interpolative RER (IRER), which incorporates interpolation into the MRC decoder."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Image Processing"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1925316"
                        ],
                        "name": "M. Pilu",
                        "slug": "M.-Pilu",
                        "structuredName": {
                            "firstName": "Maurizio",
                            "lastName": "Pilu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pilu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1543016,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "69a84b03133a766cf0f60f6bab2d76b2fb0c93d0",
            "isKey": false,
            "numCitedBy": 52,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a simple and effective method to determine global camera motion using raw MPEG-1 motion vectors information obtained straight form real MPEG-1 streams such as those of the new HITACHI MP-EG1A digital camcorder. The simple approach we have experimented with robustly fits a global affine optic flow model to the motion vectors. Other more robust methods are also proposed. In order to cope with the group-of-frames (GOF) discontinuity of the MPEG stream, B frames are used backward to determine the 'missing link' to a previous GOF thereby ensuring continuity of the motion estimation across a reasonable number of frames. As a tested, we have applied the method to the image mosaicing problem, for which interesting results have been obtained. Although several other methods exists to perform camera motion estimation, the approach presented here is particularly interesting because exploits 'free' information present in MPEG streams and bypass the highly expensive correlation process."
            },
            "slug": "Using-raw-MPEG-motion-vectors-to-determine-global-Pilu",
            "title": {
                "fragments": [],
                "text": "Using raw MPEG motion vectors to determine global camera motion"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This paper presents a simple and effective method to determine global camera motion using raw MPEG-1 motion vectors information obtained straight form real MPEG- 1 streams such as those of the new HITACHI MP-EG1A digital camcorder."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48189908"
                        ],
                        "name": "Yasuhiko Watanabe",
                        "slug": "Yasuhiko-Watanabe",
                        "structuredName": {
                            "firstName": "Yasuhiko",
                            "lastName": "Watanabe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yasuhiko Watanabe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2090132835"
                        ],
                        "name": "Yoshihiro Okada",
                        "slug": "Yoshihiro-Okada",
                        "structuredName": {
                            "firstName": "Yoshihiro",
                            "lastName": "Okada",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshihiro Okada"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2146300157"
                        ],
                        "name": "Yeun-Bae Kim",
                        "slug": "Yeun-Bae-Kim",
                        "structuredName": {
                            "firstName": "Yeun-Bae",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yeun-Bae Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105963409"
                        ],
                        "name": "Tetsuya Takeda",
                        "slug": "Tetsuya-Takeda",
                        "structuredName": {
                            "firstName": "Tetsuya",
                            "lastName": "Takeda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tetsuya Takeda"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Watanabe\u2019s [74] translation camera can detect text in a scene image and translate Japanese text into English after performing character recognition."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10167499,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "61ae735749334af00d99a78a71df2595913ea5b4",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose a camera system which translates Japanese texts in a scene. The system is portable and consists of four components: digital camera, character image extraction process, character recognition process, and translation process. The system extracts character strings from a region which a user specifies, and translates them into English."
            },
            "slug": "Translation-camera-Watanabe-Okada",
            "title": {
                "fragments": [],
                "text": "Translation camera"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "A camera system which translates Japanese texts in a scene using a digital camera, which extracts character strings from a region which a user specifies, and translates them into English."
            },
            "venue": {
                "fragments": [],
                "text": "MTSUMMIT"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2015495"
                        ],
                        "name": "Young Kug Ham",
                        "slug": "Young-Kug-Ham",
                        "structuredName": {
                            "firstName": "Young",
                            "lastName": "Ham",
                            "middleNames": [
                                "Kug"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Young Kug Ham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111622683"
                        ],
                        "name": "Min-Seok Kang",
                        "slug": "Min-Seok-Kang",
                        "structuredName": {
                            "firstName": "Min-Seok",
                            "lastName": "Kang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Min-Seok Kang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3082212"
                        ],
                        "name": "H. K. Chung",
                        "slug": "H.-K.-Chung",
                        "structuredName": {
                            "firstName": "Hong",
                            "lastName": "Chung",
                            "middleNames": [
                                "Kyu"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. K. Chung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102442672"
                        ],
                        "name": "Rae-Hong Park",
                        "slug": "Rae-Hong-Park",
                        "structuredName": {
                            "firstName": "Rae-Hong",
                            "lastName": "Park",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rae-Hong Park"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112332245"
                        ],
                        "name": "Gwi-Tae Park",
                        "slug": "Gwi-Tae-Park",
                        "structuredName": {
                            "firstName": "Gwi-Tae",
                            "lastName": "Park",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gwi-Tae Park"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122339003,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "834458aac8e2ecb58cc2a37c1b498bba958bcb7e",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The recognition of raised alphanumeric markings on rubber tires for their automatic classification is presented. Raised alphanumeric markings on rubber tires show different characteristics from those of printed characters. In the preprocessing step of the proposed method, we first determine the slope of an arc, along which alphanumerics are marked, using the Hough transform, and align them horizontally. Then we separate each character using vertical and horizontal projections. In the recognition step, to recognize characters hierarchically we use several effective features, such as width of a character, number of cross points, partial projections, and distance features. Computer simulation results show that the proposed system can be successfully applied to automatic classification of rubber tires."
            },
            "slug": "Recognition-of-raised-characters-for-automatic-of-Ham-Kang",
            "title": {
                "fragments": [],
                "text": "Recognition of raised characters for automatic classification of rubber tires"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Computer simulation results show that the proposed system can be successfully applied to automatic classification of rubber tires."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116415943"
                        ],
                        "name": "B. Yu",
                        "slug": "B.-Yu",
                        "structuredName": {
                            "firstName": "Bin",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153701268"
                        ],
                        "name": "M. Mohiuddin",
                        "slug": "M.-Mohiuddin",
                        "structuredName": {
                            "firstName": "Marzia",
                            "lastName": "Mohiuddin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mohiuddin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "applications including page segmentation [17, 18], address block location [ 19 ], license plate location [9, 20], and content-"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 61109635,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "e9f890d2ce8b6ef47301312a768366fec32ba1a0",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "Unlike simple letter envelopes which have a high degree of global spatial structure among a limited number of entities, many mail pieces such as magazines usually have an address block printed on a label which can be pasted in an arbitrary position and orientation among text, graphics and images on a magazine cover, which is often shrink wrapped. This work concentrates on address block location for complex mail pieces with an arbitrary layout of printed entities. Using a bottom-up approach, a pyramid model is created for the address block. Based on this model, some features are extracted for assigning a confidence value to the located address blocks. The typical processing time for locating a destination address block is 0.1 seconds on a SGI Indy workstation."
            },
            "slug": "Address-block-location-on-complex-mail-pieces-Yu-Jain",
            "title": {
                "fragments": [],
                "text": "Address block location on complex mail pieces"
            },
            "tldr": {
                "abstractSimilarityScore": 36,
                "text": "Using a bottom-up approach, a pyramid model is created for the address block and some features are extracted for assigning a confidence value to the located address blocks."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Fourth International Conference on Document Analysis and Recognition"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1405374270"
                        ],
                        "name": "M. Schaar-Mitrea",
                        "slug": "M.-Schaar-Mitrea",
                        "structuredName": {
                            "firstName": "Mihaela",
                            "lastName": "Schaar-Mitrea",
                            "middleNames": [
                                "van",
                                "der"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Schaar-Mitrea"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 21
                            }
                        ],
                        "text": "Schaar-Mitrea and de With\u2019s algorithm [58] was originally developed for the classiAcation of graphics and video."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 111
                            }
                        ],
                        "text": "They utilize algorithms by Gargi et al. [29,32] (see Section 2.2.3), Chaddha et al. [57], Schaar-Mitrea and de With [58], and LeBourgeois [59]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 75
                            }
                        ],
                        "text": "[22] used a formal evaluation method for TIE, and selected five algorithms [29, 57, 58, 59, 71] as promising."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 32
                            }
                        ],
                        "text": "[57], Schaar-Mitrea and de With [58], and LeBourgeois [59]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 38
                            }
                        ],
                        "text": "Schaar-Mitrea and de With\u2019s algorithm [58] was originally"
                    },
                    "intents": []
                }
            ],
            "corpusId": 62607491,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4155a9423e46ed278ee1acde33c1a0e3054170a5",
            "isKey": true,
            "numCitedBy": 15,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The diversity in TV images has augmented with the increased application of computer graphics. In this paper we study z coding system that supports both the lossless coding of such graphics data and regular lossy video compression. The lossless coding techniques are based on runlength and arithmetical coding. For video compression, we introduce a simple block predictive coding technique featuring individual pixel access, so that it enables a gradual shift from lossless coding of graphics to the lossy coding of video. An overall bit rate control completes the system. Computer simulations show a very high quality with a compression factor between 2-3.\u00a9 (1998) COPYRIGHT SPIE--The International Society for Optical Engineering. Downloading of the abstract is permitted for personal use only."
            },
            "slug": "Compression-of-mixed-video-and-graphics-images-for-Schaar-Mitrea",
            "title": {
                "fragments": [],
                "text": "Compression of mixed video and graphics images for TV systems"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "Z coding system that supports both the lossless coding of such graphics data and regular lossy video compression is studied and a simple block predictive coding technique featuring individual pixel access is introduced, so that it enables a gradual shift from lossed coding of graphics to the lossy coding of video."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1643905305"
                        ],
                        "name": "YangMing-Hsuan",
                        "slug": "YangMing-Hsuan",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "YangMing-Hsuan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "YangMing-Hsuan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1643925435"
                        ],
                        "name": "J. KriegmanDavid",
                        "slug": "J.-KriegmanDavid",
                        "structuredName": {
                            "firstName": "J",
                            "lastName": "KriegmanDavid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. KriegmanDavid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1644015411"
                        ],
                        "name": "AhujaNarendra",
                        "slug": "AhujaNarendra",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "AhujaNarendra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "AhujaNarendra"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 216011979,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "acf80e577f03e0856ff4b1220547b60b2474df64",
            "isKey": false,
            "numCitedBy": 60,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Images containing faces are essential to intelligent vision-based human computer interaction, and research efforts in face processing include face recognition, face tracking, pose estimation, and e..."
            },
            "slug": "Detecting-Faces-in-Images-YangMing-Hsuan-KriegmanDavid",
            "title": {
                "fragments": [],
                "text": "Detecting Faces in Images"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "Images containing faces are essential to intelligent vision-based human computer interaction, and research efforts in face processing include face recognition, face tracking, pose estimation, and pose estimation."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1405336900"
                        ],
                        "name": "M. van der Schaar-Mitrea",
                        "slug": "M.-van-der-Schaar-Mitrea",
                        "structuredName": {
                            "firstName": "Mihaela",
                            "lastName": "van der Schaar-Mitrea",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. van der Schaar-Mitrea"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "122835730"
                        ],
                        "name": "P. D. De with",
                        "slug": "P.-D.-De-with",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "De with",
                            "middleNames": [
                                "H.",
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. D. De with"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 173186085,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "19c7816c3f7cd5512d2e69fd5e246a6b2a2fa663",
            "isKey": false,
            "numCitedBy": 2,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The diversity in TV images has augmented with the increased application of computer graphics. In this paper we study z coding system that supports both the lossless coding of such graphics data and regular lossy video compression. The lossless coding techniques are based on runlength and arithmetical coding. For video compression, we introduce a simple block predictive coding technique featuring individual pixel access, so that it enables a gradual shift from lossless coding of graphics to the lossy coding of video. An overall bit rate control completes the system. Computer simulations show a very high quality with a compression factor between 2-3."
            },
            "slug": "Compression-of-mixed-video-and-graphics-images-for-Schaar-Mitrea-with",
            "title": {
                "fragments": [],
                "text": "Compression of mixed video and graphics images for TV systems"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "This paper studies z coding system that supports both the lossless coding of such graphics data and regular lossy video compression, and introduces a simple block predictive coding technique featuring individual pixel access that enables a gradual shift from lossed coding of graphics to the lossy coding of video."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "89125003"
                        ],
                        "name": "Andr\u00e9 Marion",
                        "slug": "Andr\u00e9-Marion",
                        "structuredName": {
                            "firstName": "Andr\u00e9",
                            "lastName": "Marion",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andr\u00e9 Marion"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 10
                            }
                        ],
                        "text": "Niblack\u2019s [70] adaptive thresholding method is used to Alter out non-text regions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6825818,
            "fieldsOfStudy": [
                "Mathematics",
                "Art"
            ],
            "id": "731cc6cf4f60a6cf6dc96b925940c16b59858a0e",
            "isKey": false,
            "numCitedBy": 334,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "1 The image as an analogue signal.- 2 Scanning of an image by an aperture.- 3 Extension of the aperture notion.- 4 Photographic images.- 5 Digitizing and reconstructing images.- 6 Basic techniques of digital image processing.- 7 Algebraic operations between images.- 8 Coloured images.- 9 Linear processing of signals and images."
            },
            "slug": "Introduction-to-Image-Processing-Marion",
            "title": {
                "fragments": [],
                "text": "Introduction to Image Processing"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "The image as an analogue signal, scanning of an image by an aperture, and extension of the aperture notion are illustrated."
            },
            "venue": {
                "fragments": [],
                "text": "Springer US"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115290599"
                        ],
                        "name": "Se Hyun Park",
                        "slug": "Se-Hyun-Park",
                        "structuredName": {
                            "firstName": "Se",
                            "lastName": "Park",
                            "middleNames": [
                                "Hyun"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Se Hyun Park"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144602022"
                        ],
                        "name": "K. Kim",
                        "slug": "K.-Kim",
                        "structuredName": {
                            "firstName": "Kwang",
                            "lastName": "Kim",
                            "middleNames": [
                                "In"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "121267347"
                        ],
                        "name": "K. Jung",
                        "slug": "K.-Jung",
                        "structuredName": {
                            "firstName": "Keechul",
                            "lastName": "Jung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Jung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70261685"
                        ],
                        "name": "Hang-Joon Kim",
                        "slug": "Hang-Joon-Kim",
                        "structuredName": {
                            "firstName": "Hang-Joon",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hang-Joon Kim"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[44] use a learning-based approach for license plate extraction, which is similar to a texture-based text detection method [47,49]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 123666990,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e1b5738d6d86f53cae7cc12c00db68ea81de0b1e",
            "isKey": false,
            "numCitedBy": 158,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors present a method for locating car license plates using neural networks. Neural networks are used as filters for analysing small windows of an image and deciding whether each window contains a license plate. A post-processor combines these filtered images and gives the final location of the license plates. The method offers robustness when dealing with noisy images. Tests with car images travelling on the road and at the entrance of a car park showed extraction rates of 99 and 97.5%, respectively. These results suggest that the proposed method works well with real-world situations."
            },
            "slug": "Locating-car-license-plates-using-neural-networks-Park-Kim",
            "title": {
                "fragments": [],
                "text": "Locating car license plates using neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Tests with car images travelling on the road and at the entrance of a car park showed extraction rates of 99 and 97.5%, respectively, suggesting that the proposed method works well with real-world situations."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144300261"
                        ],
                        "name": "C. Colombo",
                        "slug": "C.-Colombo",
                        "structuredName": {
                            "firstName": "Carlo",
                            "lastName": "Colombo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Colombo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8196487"
                        ],
                        "name": "A. Bimbo",
                        "slug": "A.-Bimbo",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Bimbo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bimbo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1767957"
                        ],
                        "name": "P. Pala",
                        "slug": "P.-Pala",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Pala",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Pala"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 174
                            }
                        ],
                        "text": "While comprehensive surveys of related problems such as face detection, document analysis, and image & video indexing can be found, the problem of text information extraction is not well surveyed."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10063800,
            "fieldsOfStudy": [
                "Computer Science",
                "Psychology"
            ],
            "id": "108652cc6c3cdfd754ac8622794d85e945996b3c",
            "isKey": false,
            "numCitedBy": 255,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "A compositional approach increases the level of representation that can be automatically extracted and used in a visual information retrieval system. Visual information at the perceptual level is aggregated according to a set of rules. These rules reflect the specific context and transform perceptual words into phrases capturing pictorial content at a higher, and closer to the human, semantic level."
            },
            "slug": "Semantics-in-Visual-Information-Retrieval-Colombo-Bimbo",
            "title": {
                "fragments": [],
                "text": "Semantics in Visual Information Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A compositional approach increases the level of representation that can be automatically extracted and used in a visual information retrieval system."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Multim."
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[36] fused the CC-based approach with the texture-based approach."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[36] 1995 CC-based method after color reduction, local spatial Scene text (CD covers), localization variance-based method, and hybrid method Yeo and Liu [56] 1996 Localization based on large inter-frame di2erence in Caption text, localization MPEG compressed image Shim et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[36] use the local spatial variations in a gray-scale image to locate text regions with a high variance."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[36] used a CC-based method using color reduction."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 35696139,
            "fieldsOfStudy": [],
            "id": "d68b95534860e2bddd17d17ef7f362d16c550bde",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Locating text in complex color images"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 339,
                                "start": 328
                            }
                        ],
                        "text": "These two approaches\nTable 2 A brief survey of TIE\nAuthor Year Approach Features\nOhya et al. [34] 1994 Adaptive thresholding and relaxation operations Color, scene text (train, signboard, skew and curved), localization and recognition Lee and 1995 Coarse search using edge information, followed by Scene text (cargo container), Kankanhalli [35] connected component (CC) generation localization and recognition Smith and Kanade [7] 1995 3\u00d7 3 Alter seeking vertical edges Caption text, localization Zhong et al. [36] 1995 CC-based method after color reduction, local spatial Scene text (CD covers), localization variance-based method, and hybrid method Yeo and Liu [56] 1996 Localization based on large inter-frame di2erence in Caption text, localization MPEG compressed image Shim et al. [21] 1998 Gray-level di2erence between pairs of pixels Caption text, localization Jain and Yu 1998 CC-based method after multi-valued color image Color (book cover, Web image, video [38] decomposition frame), localization Sato et al. [11] 1998 Smith and Kanade\u2019s localization method and Recognition recognition-based character extraction Chun et al. [54] 1999 Filtering using neural network after FFT Caption text, localization Antani et al. [28] 1999 Multiple algorithms in functional parallelism Scene text, recognition Messelodi and 1999 CC generation, followed by text line selection using Scene images (book covers, slanted) Modena [39] divisive hierarchical clustering procedure localization Wu et al. [45] 1999 Localization based on multi-scale texture segmentation Video and scene images (newspaper, advertisement) recognition Hasan and Karam [42] 2000 Morphological approach Scene text, localization Li et al. [51] 2000 Wavelet-based feature extraction and neural network for Scene text (slanted), localization, enhancement, texture analysis and tracking Lim et al. [33] 2000 Text detection and localization using DCT coeCcient Caption text, MPEG compressed and macroblock type information video, localization Zhong et al. [27] 2000 Texture analysis in DCT compressed domain Caption text, JPEG and I-frames of MPEG, localization Jung [49] 2001 Gabor Alter-like multi-layer perceptron for texture Color, caption text, localization analysis Chen et al. [43] 2001 Text detection in edge-enhanced image Caption text, localization and recognition Strouthopoulos 2002 Page layout analysis after adaptive color reduction Color document image, localization et al. [16]\nwork in a bottom-up fashion; by identifying sub-structures, such as CCs or edges, and then merging these sub-structures to mark bounding boxes for text."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 0
                            }
                        ],
                        "text": "Lee and Kankanhalli [35] applied a CC-based method to the detection and recognition of text on cargo containers, which can have uneven lighting conditions and characters with di2erent sizes and shapes."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 162
                            }
                        ],
                        "text": "[34] 1994 Adaptive thresholding and relaxation operations Color, scene text (train, signboard, skew and curved), localization and recognition Lee and Kankanhalli [35] 1995 Coarse search using edge information, followed by connected component (CC) generation Scene text (cargo container), localization and recognition Smith and Kanade [7] 1995 3\u00d73 filter seeking vertical edges Caption text, localization"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 0
                            }
                        ],
                        "text": "Lee and Kankanhalli [34] apply a CC-based method for cargo container veriAcation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 20
                            }
                        ],
                        "text": "Lee and Kankanhalli [35] applied a CC-based method to the detection and recognition of text on cargo containers, which can have uneven lighting conditions and characters with different sizes and shapes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Automatic Extraction of Characters in Complex Images, International Journal  of Pattern Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Artificial Intelligence,"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[28] 1999 Multiple algorithms in functional parallelism Scene text, recognition Messelodi and Modena [39] 1999 CC generation, followed by text line selection using divisive hierarchical clustering procedure Scene images (book covers, slanted) localization"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 0
                            }
                        ],
                        "text": "[23, 28] proposed a multi-pronged approach for text detection, localization, tracking, extraction, and recognition."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 92
                            }
                        ],
                        "text": "Table 1 shows a list of properties that have been utilized in recently published algorithms [25-30]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[28] performed text localization on compressed images, which resulted in a faster performance."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Extraction of Text in Video, Technical Report of Department of Computer Science and Engineering, Penn"
            },
            "venue": {
                "fragments": [],
                "text": "State University,"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116415943"
                        ],
                        "name": "B. Yu",
                        "slug": "B.-Yu",
                        "structuredName": {
                            "firstName": "Bin",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Yu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 12
                            }
                        ],
                        "text": "Jain and Yu [38] apply a CC-based method after preprocessing, which includes bit dropping, color clustering, multi-valued image decomposition, and foreground image generation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 186,
                                "start": 182
                            }
                        ],
                        "text": "[21] 1998 Gray-level di2erence between pairs of pixels Caption text, localization Jain and Yu 1998 CC-based method after multi-valued color image Color (book cover, Web image, video [38] decomposition frame), localization Sato et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 115
                            }
                        ],
                        "text": "A multi-colored image and its element images: (a) color input image; (b) nine element images (courtesy Jain and Yu [38])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 34993677,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "73bf7b2fdb26498c05896d99fee4b8f3608c3bd6",
            "isKey": false,
            "numCitedBy": 101,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Automatic-text-location-in-images-and-video-frames-Jain-Yu",
            "title": {
                "fragments": [],
                "text": "Automatic text location in images and video frames"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144080449"
                        ],
                        "name": "L. Dorst",
                        "slug": "L.-Dorst",
                        "structuredName": {
                            "firstName": "Leo",
                            "lastName": "Dorst",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Dorst"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144638781"
                        ],
                        "name": "A. Smeulders",
                        "slug": "A.-Smeulders",
                        "structuredName": {
                            "firstName": "Arnold",
                            "lastName": "Smeulders",
                            "middleNames": [
                                "W.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Smeulders"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 195706206,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "425480b863236a657c19f5beddc91151fcd6426a",
            "isKey": false,
            "numCitedBy": 176,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "An-introduction-to-image-processing-Dorst-Smeulders",
            "title": {
                "fragments": [],
                "text": "An introduction to image processing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721328"
                        ],
                        "name": "Sameer Kiran Antani",
                        "slug": "Sameer-Kiran-Antani",
                        "structuredName": {
                            "firstName": "Sameer",
                            "lastName": "Antani",
                            "middleNames": [
                                "Kiran"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sameer Kiran Antani"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Later, in his thesis, Antani [23] gave more consideration to the merging strategy."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "[23, 28] proposed a multi-pronged approach for text detection, localization, tracking, extraction, and recognition."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Some researchers like to use the term \u2018graphics text\u2019 for scene text, and \u2018superimposed text\u2019 or \u2018artificial text\u2019 for caption text [22, 23]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 64187558,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6aeb3e2ccc1fbb277f24ba449063ce1d4c241d83",
            "isKey": false,
            "numCitedBy": 10,
            "numCiting": 80,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Reliable-Extraction-of-Text-from-Video-Antani",
            "title": {
                "fragments": [],
                "text": "Reliable Extraction of Text from Video"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2122942"
                        ],
                        "name": "B. Ripley",
                        "slug": "B.-Ripley",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Ripley",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Ripley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107314775"
                        ],
                        "name": "C. C. Taylor",
                        "slug": "C.-C.-Taylor",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Taylor",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. C. Taylor"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 1625830,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "ca23e7a71ace53d6a5b2a553ff37c63365d22b8a",
            "isKey": false,
            "numCitedBy": 5720,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Pattern-Recognition-Ripley-Taylor",
            "title": {
                "fragments": [],
                "text": "Pattern Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1968
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39319377"
                        ],
                        "name": "Yu Zhong",
                        "slug": "Yu-Zhong",
                        "structuredName": {
                            "firstName": "Yu",
                            "lastName": "Zhong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yu Zhong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38188346"
                        ],
                        "name": "Hong-jiang Zhang",
                        "slug": "Hong-jiang-Zhang",
                        "structuredName": {
                            "firstName": "Hong-jiang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hong-jiang Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "E-mail address: kcjung@ssu.ac.kr (K. Jung)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 207871522,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "291d916a267c640f30d9b891441433ee3be9ffc6",
            "isKey": false,
            "numCitedBy": 131,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method to automatically localize captions in JPEG compressed images and the I-frames of MPEG compressed videos. Caption text regions are segmented from background images using their distinguishing texture characteristics. Unlike previously published methods which fully decompress the video sequence before extracting the text regions, this method locates candidate caption text regions directly in the DCT compressed domain using the intensity variation information encoded in the DCT domain. Therefore, only a very small amount of decoding is required. The proposed algorithm takes about 0.006 second to process a 240/spl times/350 image and achieves a recall rate of 99.17 percent while falsely accepting about 1.87 percent nontext DCT blocks on a variety of MPEG compressed videos containing more than 2,300 I-frames."
            },
            "slug": "Automatic-Caption-Localization-in-Compressed-Video-Zhong-Zhang",
            "title": {
                "fragments": [],
                "text": "Automatic Caption Localization in Compressed Video"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "A method to automatically localize captions in JPEG compressed images and the I-frames of MPEG compressed videos and locates candidate caption text regions directly in the DCT compressed domain using the intensity variation information encoded in theDCT domain."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 174
                            }
                        ],
                        "text": "They use character recognition results to make decisions on the segmentation and positions of individual characters, thereby improving the accuracy of character segmentation [41]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 34435085,
            "fieldsOfStudy": [],
            "id": "29a98da1ded1b7d8e849b95c0490013490b1b306",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A New Methodology for Gray-Scale Character Segmentation and Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143677016"
                        ],
                        "name": "K. Mohiuddin",
                        "slug": "K.-Mohiuddin",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Mohiuddin",
                            "middleNames": [
                                "Moidin"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Mohiuddin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 195867354,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "15dedaa657ff7cbb283aaf96d9be2f4c5dcea694",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "International-Conference-On-Document-Analysis-and-Mohiuddin",
            "title": {
                "fragments": [],
                "text": "International Conference On Document Analysis and Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 3rd International Conference on Document Analysis and Recognition"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Kim Automatic Text Extraction in Digital Videos using FFT and Neural Network"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. of IEEE International Fuzzy Systems Conference"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[40] used cluster-based templates for filtering out non-character components for multi-segment characters to alleviate the difficulty in defining heuristics for filtering out non-text components."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Automatic Text Region Extraction Using Cluster-based Templates"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. of International Conference on Advances in Pattern Recognition and Digital Techniques,"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Automatic identi\u00ffcation and skew estimation of text lines in real scene images, Pattern Recogn"
            },
            "venue": {
                "fragments": [],
                "text": "Automatic identi\u00ffcation and skew estimation of text lines in real scene images, Pattern Recogn"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Proposed Scheme for Performance Evaluation of Graphics/Text Separation Algorithm, Graphics Recognition \u2013 Algorithms and Systems"
            },
            "venue": {
                "fragments": [],
                "text": "Lecture Notes in Computer Science"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Intelligent Access to Digital Video"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 112
                            }
                        ],
                        "text": "[72] proposed a performance evaluation protocol for text localization algorithms based on Liu and Dori\u2019s method [73] of text segmentation from engineering drawings."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Proposed Scheme for Performance Evaluation of Graphics/Text Separation Algorithm, Graphics Recognition \u2013 Algorithms and Systems, K"
            },
            "venue": {
                "fragments": [],
                "text": "Tombre and A. Chhabra (eds.), Lecture Notes in Computer Science,"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "[23,28] proposed a multi-pronged approach for text detection, localization, tracking, extraction, and recognition."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[28] 1999 Multiple algorithms in functional parallelism Scene text, recognition"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[28] performed text localization on compressed images, which resulted in a faster performance."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Extraction of text in video"
            },
            "venue": {
                "fragments": [],
                "text": "Technical Report, Department of Computer Science and Engineering, Pennsylvania State University, CSE-99-016, August 30"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Object localization using color"
            },
            "venue": {
                "fragments": [],
                "text": "texture, and shape, Pattern Recognition 33 "
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[16] presented a method using page layout analysis (PLA) after adaptive color reduction."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 128
                            }
                        ],
                        "text": "Studies on semantic image content in the form of text, face, vehicle, and human action have also attracted some recent interest [7-16]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[16] 2002 Page layout analysis after adaptive color reduction Color document image, localization"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Atsalakis, Text Extraction in Complex Color Document"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognition,"
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 10
                            }
                        ],
                        "text": "Niblack\u2019s [70] adaptive thresholding method is used to filter out non-text regions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An Introduction to Image Processing, Englewood Cliffs, N"
            },
            "venue": {
                "fragments": [],
                "text": "J.:Prentice Hall,"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Integrating visual"
            },
            "venue": {
                "fragments": [],
                "text": "audio, and text analysis for news video, Proceedings of IEEE International Conference on Image Processing, Vancouver, BC Canada"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Automatic recognition of \u00fflm genres"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of ACM Multimedia'95"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 75
                            }
                        ],
                        "text": "[22] used a formal evaluation method for TIE, and selected five algorithms [29, 57, 58, 59, 71] as promising."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Locating Uniform-Colored Text in Digital Videos"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. of International Conference on Pattern Recognition,"
            },
            "year": 2000
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 37,
            "methodology": 51
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 104,
        "totalPages": 11
    },
    "page_url": "https://www.semanticscholar.org/paper/Text-information-extraction-in-images-and-video:-a-Jung-Kim/cedf72be1fe814ef2ee9d65633dc3226f80f0785?sort=total-citations"
}