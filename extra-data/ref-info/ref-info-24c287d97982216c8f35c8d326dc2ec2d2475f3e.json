{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35238678"
                        ],
                        "name": "D. Lowe",
                        "slug": "D.-Lowe",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Lowe",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lowe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2989237,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7f3d6c191d658229e97ed42828c67cc0ddb11585",
            "isKey": false,
            "numCitedBy": 319,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Nearest-neighbor interpolation algorithms have many useful properties for applications to learning, but they often exhibit poor generalization. In this paper, it is shown that much better generalization can be obtained by using a variable interpolation kernel in combination with conjugate gradient optimization of the similarity metric and kernel size. The resulting method is called variable-kernel similarity metric (VSM) learning. It has been tested on several standard classification data sets, and on these problems it shows better generalization than backpropagation and most other learning methods. The number of parameters that must be determined through optimization are orders of magnitude less than for backpropagation or radial basis function (RBF) networks, which may indicate that the method better captures the essential degrees of variation in learning. Other features of VSM learning are discussed that make it relevant to models for biological learning in the brain."
            },
            "slug": "Similarity-Metric-Learning-for-a-Variable-Kernel-Lowe",
            "title": {
                "fragments": [],
                "text": "Similarity Metric Learning for a Variable-Kernel Classifier"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Much better generalization can be obtained by using a variable interpolation kernel in combination with conjugate gradient optimization of the similarity metric and kernel size to create a variable-kernel similarity metric (VSM) learning."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1400556488"
                        ],
                        "name": "Aharon Bar-Hillel",
                        "slug": "Aharon-Bar-Hillel",
                        "structuredName": {
                            "firstName": "Aharon",
                            "lastName": "Bar-Hillel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aharon Bar-Hillel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1774536"
                        ],
                        "name": "T. Hertz",
                        "slug": "T.-Hertz",
                        "structuredName": {
                            "firstName": "Tomer",
                            "lastName": "Hertz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Hertz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1787804"
                        ],
                        "name": "N. Shental",
                        "slug": "N.-Shental",
                        "structuredName": {
                            "firstName": "Noam",
                            "lastName": "Shental",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Shental"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1789171"
                        ],
                        "name": "D. Weinshall",
                        "slug": "D.-Weinshall",
                        "structuredName": {
                            "firstName": "Daphna",
                            "lastName": "Weinshall",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Weinshall"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 374,
                                "start": 371
                            }
                        ],
                        "text": "Many approaches exist for linear dimensionality reduction, ranging from purely unsupervised approaches (such as factor analysis, principal components analysis and independent components analysis) to methods which make use of class labels in addition to input features such as linear discriminant analysis (LDA)[3] possibly combined with relevant components analysis (RCA)[1]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 28
                            }
                        ],
                        "text": "Two related methods are RCA [1] and a convex optimization based algorithm [7]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 48
                            }
                        ],
                        "text": "The results under the PCA, LDA, LDA followed by RCA and NCA transformations (using K=1) appear in figure 1."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "RCA is implicitly assuming a Gaussian distribution for each class (so it can be described using only the first two moments of the class-conditional distribution)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 232,
                                "start": 229
                            }
                        ],
                        "text": "We compared the NCA transformation obtained from optimizingf (for squareA) on the training set with the default Euclidean distanceA = I, the \u201cwhitening\u201d transformation ,A = \u03a3\u2212 1 2 (where\u03a3 is the sample data covariance matrix), and the RCA [1] transformationA = \u03a3 \u2212 1 2\nw (where\u03a3w is the average of the within-class covariance matrices)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 243,
                                "start": 240
                            }
                        ],
                        "text": "We compared the NCA transformation obtained from optimizingf (for squareA) on the training set with the default Euclidean distance A = I, the \u201cwhitening\u201d transformation , A = \u03a3 1 2 (where\u03a3 is the sample data covariance matrix), and the RCA [1] transformation A = \u03a3 \u2212 1 2 w (where\u03a3w is the average of the within-class covariance matrices)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6865208,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0029cf474bce70cfab2e944c4be01f99e741f1f4",
            "isKey": true,
            "numCitedBy": 504,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the problem of learning distance metrics using side-information in the form of groups of \"similar\" points. We propose to use the RCA algorithm, which is a simple and efficient algorithm for learning a full ranked Mahalanobis metric (Shental et al., 2002). We first show that RCA obtains the solution to an interesting optimization problem, founded on an information theoretic basis. If the Mahalanobis matrix is allowed to be singular, we show that Fisher's linear discriminant followed by RCA is the optimal dimensionality reduction algorithm under the same criterion. We then show how this optimization problem is related to the criterion optimized by another recent algorithm for metric learning (Xing et al., 2002), which uses the same kind of side information. We empirically demonstrate that learning a distance metric using the RCA algorithm significantly improves clustering performance, similarly to the alternative algorithm. Since the RCA algorithm is much more efficient and cost effective than the alternative, as it only uses closed form expressions of the data, it seems like a preferable choice for the learning of full rank Mahalanobis distances."
            },
            "slug": "Learning-Distance-Functions-using-Equivalence-Bar-Hillel-Hertz",
            "title": {
                "fragments": [],
                "text": "Learning Distance Functions using Equivalence Relations"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "It is empirically demonstrate that learning a distance metric using the RCA algorithm significantly improves clustering performance, similarly to the alternative algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701121"
                        ],
                        "name": "Y. Koren",
                        "slug": "Y.-Koren",
                        "structuredName": {
                            "firstName": "Yehuda",
                            "lastName": "Koren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Koren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2550943"
                        ],
                        "name": "L. Carmel",
                        "slug": "L.-Carmel",
                        "structuredName": {
                            "firstName": "Liran",
                            "lastName": "Carmel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Carmel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[5], [2]) make the transformation more robust to outliers an d to numerical instability when not enough datapoints are available."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6192316,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c9106fb451ac4970bdd6f9fcacb79a0188cbd301",
            "isKey": false,
            "numCitedBy": 148,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel family of data-driven linear transformations, aimed at finding low-dimensional embeddings of multivariate data, in a way that optimally preserves the structure of the data. The well-studied PCA and Fisher's LDA are shown to be special members in this family of transformations, and we demonstrate how to generalize these two methods such as to enhance their performance. Furthermore, our technique is the only one, to the best of our knowledge, that reflects in the resulting embedding both the data coordinates and pairwise relationships between the data elements. Even more so, when information on the clustering (labeling) decomposition of the data is known, this information can also be integrated in the linear transformation, resulting in embeddings that clearly show the separation between the clusters, as well as their internal structure. All of this makes our technique very flexible and powerful, and lets us cope with kinds of data that other techniques fail to describe properly."
            },
            "slug": "Robust-linear-dimensionality-reduction-Koren-Carmel",
            "title": {
                "fragments": [],
                "text": "Robust linear dimensionality reduction"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "A novel family of data-driven linear transformations, aimed at finding low-dimensional embeddings of multivariate data, in a way that optimally preserves the structure of the data is presented."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Visualization and Computer Graphics"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109298448"
                        ],
                        "name": "Li-Fen Chen",
                        "slug": "Li-Fen-Chen",
                        "structuredName": {
                            "firstName": "Li-Fen",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li-Fen Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704678"
                        ],
                        "name": "H. Liao",
                        "slug": "H.-Liao",
                        "structuredName": {
                            "firstName": "Hong-Yuan",
                            "lastName": "Liao",
                            "middleNames": [
                                "Mark"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Liao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145689992"
                        ],
                        "name": "M. Ko",
                        "slug": "M.-Ko",
                        "structuredName": {
                            "firstName": "Ming-Tat",
                            "lastName": "Ko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Ko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39819603"
                        ],
                        "name": "Ja-Chen Lin",
                        "slug": "Ja-Chen-Lin",
                        "structuredName": {
                            "firstName": "Ja-Chen",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ja-Chen Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40467932"
                        ],
                        "name": "Gwo-Jong Yu",
                        "slug": "Gwo-Jong-Yu",
                        "structuredName": {
                            "firstName": "Gwo-Jong",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gwo-Jong Yu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 5
                            }
                        ],
                        "text": "[5], [2]) make the transformation more robust to outliers an d to numerical instability when not enough datapoints are available."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 146
                            }
                        ],
                        "text": "LDA also suffers from a small sample size problem when dealing wi th high-dimensional data when the within-class scatter matrix is nearly singular[2] ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10243856,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "995982d44cd79e28001123caadec4fca4e9a4274",
            "isKey": false,
            "numCitedBy": 1436,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-new-LDA-based-face-recognition-system-which-can-Chen-Liao",
            "title": {
                "fragments": [],
                "text": "A new LDA-based face recognition system which can solve the small sample size problem"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056361"
                        ],
                        "name": "J. Friedman",
                        "slug": "J.-Friedman",
                        "structuredName": {
                            "firstName": "Jerome",
                            "lastName": "Friedman",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Friedman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696575"
                        ],
                        "name": "J. Bentley",
                        "slug": "J.-Bentley",
                        "structuredName": {
                            "firstName": "Jon",
                            "lastName": "Bentley",
                            "middleNames": [
                                "Louis"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bentley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2236311"
                        ],
                        "name": "R. Finkel",
                        "slug": "R.-Finkel",
                        "structuredName": {
                            "firstName": "Raphael",
                            "lastName": "Finkel",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Finkel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 218,
                                "start": 215
                            }
                        ],
                        "text": "(Storage can potentially be reduced by \u201cediting\u201d or \u201cthinni g\u201d the training data; and in low dimensional input spaces, the search problem can be mitigat ed by employing data structures such as KD-trees or ball-trees[4]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10811510,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cab3c73f1b2140231b98944c720100b356d91b28",
            "isKey": false,
            "numCitedBy": 2965,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "An algorithm and data structure are presented for searching a file containing N records, each described by k real valued keys, for the m closest matches or nearest neighbors to a given query record. The computation required to organize the file is proportional to kNlogN. The expected number of records examined in each search is independent of the file size. The expected computation to perform each search is proportional to logN. Empirical evidence suggests that except for very small files, this algorithm is considerably faster than other methods."
            },
            "slug": "An-Algorithm-for-Finding-Best-Matches-in-Expected-Friedman-Bentley",
            "title": {
                "fragments": [],
                "text": "An Algorithm for Finding Best Matches in Logarithmic Expected Time"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "An algorithm and data structure are presented for searching a file containing N records, each described by k real valued keys, for the m closest matches or nearest neighbors to a given query record."
            },
            "venue": {
                "fragments": [],
                "text": "TOMS"
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49353544"
                        ],
                        "name": "R. Fisher",
                        "slug": "R.-Fisher",
                        "structuredName": {
                            "firstName": "Rory",
                            "lastName": "Fisher",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fisher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 309,
                                "start": 306
                            }
                        ],
                        "text": "Many approaches exist for linear dimensionality reduction, ranging from purely unsupervised approaches (such as factor analysis, principal components analysis and independent components analysis) to methods which make use of class labels in addition to input features such as linear discriminant analysis (LDA)[3] possibly combined with relevant components analysis (RCA)[1]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 93
                            }
                        ],
                        "text": "The classes are not convex and cannot be linearly separated, hence the results obtained from LDA will be inappropriate (as shown in figure 2)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 259,
                                "start": 256
                            }
                        ],
                        "text": "In summary, we have found that when labeled data is available, NCA performs better both in terms of classification performance in the projected representation and in terms of visualization of class separation as compared to the standard methods of PCA and LDA."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 196,
                                "start": 193
                            }
                        ],
                        "text": "As can be seen in figure 2 when a two-dimensional projection is used, the classes are consistently much better separated by the NCA transformation than by either PCA (which is unsupervised) or LDA (which has access to the class labels)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 27
                            }
                        ],
                        "text": "The results under the PCA, LDA, LDA followed by RCA and NCA transformations (using K=1) appear in figure 1."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 25
                            }
                        ],
                        "text": "The classic LDA algorithm[3] is optimal if all class distributions are Gaussian with a single shared covariance; this assumption, however is rarely true."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "LDA also suffers from a small sample size problem when dealing with high-dimensional data when the within-class scatter matrix is nearly singular[2]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 316,
                                "start": 313
                            }
                        ],
                        "text": "Many approaches exist for linear dimensionality reduction, ranging from purely unsuper vis d approaches (such as factor analysis, principal components analysis and independent c omponents analysis) to methods which make use of class labels in addition to input features s uch as linear discriminant analysis (LDA)[3] possibly combined with relevant compone nts analysis (RCA)[1]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 19
                            }
                        ],
                        "text": "Recent variants of LDA (e.g. [5], [2]) make the transformation more robust to outliers and to numerical instability when not enough datapoints are available."
                    },
                    "intents": []
                }
            ],
            "corpusId": 29084021,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "ab21376e43ac90a4eafd14f0f02a0c87502b6bbf",
            "isKey": true,
            "numCitedBy": 13266,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "THE-USE-OF-MULTIPLE-MEASUREMENTS-IN-TAXONOMIC-Fisher",
            "title": {
                "fragments": [],
                "text": "THE USE OF MULTIPLE MEASUREMENTS IN TAXONOMIC PROBLEMS"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1936
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 370,
                                "start": 367
                            }
                        ],
                        "text": "Many approaches exist for linear dimensionality reduction, ranging from purely unsupervised approaches (such as factor analysis, principal components analysis and independent components analysis) to methods which make use of class labels in addition to input features such as linear discriminant analysis (LDA)[3] possibly combined with relevant components analysis (RCA)[1]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 28
                            }
                        ],
                        "text": "Two related methods are RCA [1] and a convex optimization based a lgorithm [7]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 24
                            }
                        ],
                        "text": "Two related methods are RCA [1] and a convex optimization based algorithm [7]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 48
                            }
                        ],
                        "text": "The results under the PCA, LDA, LDA followed by RCA and NCA transformations (using K=1) appear in figure 1."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 378,
                                "start": 375
                            }
                        ],
                        "text": "Many approaches exist for linear dimensionality reduction, ranging from purely unsuper vis d approaches (such as factor analysis, principal components analysis and independent c omponents analysis) to methods which make use of class labels in addition to input features s uch as linear discriminant analysis (LDA)[3] possibly combined with relevant compone nts analysis (RCA)[1]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 244,
                                "start": 241
                            }
                        ],
                        "text": "We compared the NCA transf ormation obtained from optimizingf (for squareA) on the training set with the default Euclidean distance A = I, the \u201cwhitening\u201d transformation , A = \u03a3 1 2 (where\u03a3 is the sample data covariance matrix), and the RCA [1] transformation A = \u03a3 \u2212 1 2 w (where\u03a3w is the average of the within-class covariance matrices)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "RCA is implicitly assuming a Gaussian distribution for each class (so it can be described using only the first two moments of the class-conditional distribution)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 232,
                                "start": 229
                            }
                        ],
                        "text": "We compared the NCA transformation obtained from optimizingf (for squareA) on the training set with the default Euclidean distanceA = I, the \u201cwhitening\u201d transformation ,A = \u03a3\u2212 1 2 (where\u03a3 is the sample data covariance matrix), and the RCA [1] transformationA = \u03a3 \u2212 1 2\nw (where\u03a3w is the average of the within-class covariance matrices)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning distanc e functions using equivalence relation"
            },
            "venue": {
                "fragments": [],
                "text": "InInternational Conference on Machine Learning"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 370,
                                "start": 367
                            }
                        ],
                        "text": "Many approaches exist for linear dimensionality reduction, ranging from purely unsupervised approaches (such as factor analysis, principal components analysis and independent components analysis) to methods which make use of class labels in addition to input features such as linear discriminant analysis (LDA)[3] possibly combined with relevant components analysis (RCA)[1]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 28
                            }
                        ],
                        "text": "Two related methods are RCA [1] and a convex optimization based a lgorithm [7]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 24
                            }
                        ],
                        "text": "Two related methods are RCA [1] and a convex optimization based algorithm [7]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 48
                            }
                        ],
                        "text": "The results under the PCA, LDA, LDA followed by RCA and NCA transformations (using K=1) appear in figure 1."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 378,
                                "start": 375
                            }
                        ],
                        "text": "Many approaches exist for linear dimensionality reduction, ranging from purely unsuper vis d approaches (such as factor analysis, principal components analysis and independent c omponents analysis) to methods which make use of class labels in addition to input features s uch as linear discriminant analysis (LDA)[3] possibly combined with relevant compone nts analysis (RCA)[1]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 244,
                                "start": 241
                            }
                        ],
                        "text": "We compared the NCA transf ormation obtained from optimizingf (for squareA) on the training set with the default Euclidean distance A = I, the \u201cwhitening\u201d transformation , A = \u03a3 1 2 (where\u03a3 is the sample data covariance matrix), and the RCA [1] transformation A = \u03a3 \u2212 1 2 w (where\u03a3w is the average of the within-class covariance matrices)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "RCA is implicitly assuming a Gaussian distribution for each class (so it can be described using only the first two moments of the class-conditional distribution)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 232,
                                "start": 229
                            }
                        ],
                        "text": "We compared the NCA transformation obtained from optimizingf (for squareA) on the training set with the default Euclidean distanceA = I, the \u201cwhitening\u201d transformation ,A = \u03a3\u2212 1 2 (where\u03a3 is the sample data covariance matrix), and the RCA [1] transformationA = \u03a3 \u2212 1 2\nw (where\u03a3w is the average of the within-class covariance matrices)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Le  arning distance functions using equivalence relation"
            },
            "venue": {
                "fragments": [],
                "text": "InInternational Conference on Machine Learning"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Similarity metric learning for a variable kerne  l classifier. InNeural Computation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 75
                            }
                        ],
                        "text": "Two related methods are RCA [1] and a convex optimization based a lgorithm [7]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Distance learning metric"
            },
            "venue": {
                "fragments": [],
                "text": "InProc. of Neural Information Processing Systems"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 75
                            }
                        ],
                        "text": "Two related methods are RCA [1] and a convex optimization based a lgorithm [7]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Distance learnin  g metric"
            },
            "venue": {
                "fragments": [],
                "text": "InProc. of Neural Information Processing Systems"
            },
            "year": 2003
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 4,
            "methodology": 6
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 11,
        "totalPages": 2
    },
    "page_url": "https://www.semanticscholar.org/paper/Neighbourhood-Components-Analysis-Goldberger-Roweis/24c287d97982216c8f35c8d326dc2ec2d2475f3e?sort=total-citations"
}