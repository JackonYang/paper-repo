{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703537"
                        ],
                        "name": "Y. Freund",
                        "slug": "Y.-Freund",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Freund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6644398,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4ba566223e426677d12a9a18418c023a4deec77e",
            "isKey": false,
            "numCitedBy": 13116,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "In the first part of the paper we consider the problem of dynamically apportioning resources among a set of options in a worst-case on-line framework. The model we study can be interpreted as a broad, abstract extension of the well-studied on-line prediction model to a general decision-theoretic setting. We show that the multiplicative weight-update Littlestone?Warmuth rule can be adapted to this model, yielding bounds that are slightly weaker in some cases, but applicable to a considerably more general class of learning problems. We show how the resulting learning algorithm can be applied to a variety of problems, including gambling, multiple-outcome prediction, repeated games, and prediction of points in Rn. In the second part of the paper we apply the multiplicative weight-update technique to derive a new boosting algorithm. This boosting algorithm does not require any prior knowledge about the performance of the weak learning algorithm. We also study generalizations of the new boosting algorithm to the problem of learning functions whose range, rather than being binary, is an arbitrary finite set or a bounded segment of the real line."
            },
            "slug": "A-decision-theoretic-generalization-of-on-line-and-Freund-Schapire",
            "title": {
                "fragments": [],
                "text": "A decision-theoretic generalization of on-line learning and an application to boosting"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "The model studied can be interpreted as a broad, abstract extension of the well-studied on-line prediction model to a general decision-theoretic setting, and it is shown that the multiplicative weight-update Littlestone?Warmuth rule can be adapted to this model, yielding bounds that are slightly weaker in some cases, but applicable to a considerably more general class of learning problems."
            },
            "venue": {
                "fragments": [],
                "text": "EuroCOLT"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 32
                            }
                        ],
                        "text": "Freund and Schapire (1996b) and Schapire and Singer (1998) provide some theory to support their algorithms, in the form of upper bounds on generalization error."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 0
                            }
                        ],
                        "text": "Schapire and Singer (1998) motivate e\u2212yF x as a differentiable upper bound\nto misclassification error 1 yF 0 (see Figure 2)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 16
                            }
                        ],
                        "text": "In this context Schapire and Singer (1998) define J responses yj for a J class problem, each taking values in \u22121 1 ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 132
                            }
                        ],
                        "text": "Result 1 requires minor modifications to accommodate f x \u2208 R, as in the generalized AdaBoost algorithms [Freund and Schapire (1996b), Schapire and Singer (1998)]; the estimate for cm differs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 1
                            }
                        ],
                        "text": "\u2737\nSchapire and Singer (1998) give the interpretation that the weights are updated to make the new weighted problem maximally difficult for the next weak learner."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 108
                            }
                        ],
                        "text": "This is a natural choice and is especially appropriate when observations can belong to more than one class [Schapire and Singer (1998)]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 108
                            }
                        ],
                        "text": "A generalization of Discrete AdaBoost appeared in Freund and Schapire (1996b), and was developed further in Schapire and Singer (1998), that uses real-valued \u201cconfidence-rated\u201d predictions rather than the \u22121 1 of Discrete AdaBoost."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 182
                            }
                        ],
                        "text": "The base classifier in Discrete AdaBoost produces a classification rule fm x \u2192 \u22121 1 , where is the domain of the predictive features x. Freund and Schapire (1996b), Breiman (1998a) and Schapire and Singer (1998) have suggested various modifications to improve the boosting algorithms."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 40
                            }
                        ],
                        "text": "\u2737\nThis exponential criterion appeared in Schapire and Singer (1998), motivated as an upper bound on misclassification error."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 0
                            }
                        ],
                        "text": "Schapire and Singer (1998) provide several generalizations of AdaBoost for the multiclass case, and also refer to other proposals [Freund and Schapire (1997), Schapire (1997)]; we describe their AdaBoost."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 73
                            }
                        ],
                        "text": "Parts of this derivation for AdaBoost can be found in Breiman (1997) and Schapire and Singer (1998), but without making the connection to additive logistic regression models."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2329907,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "14e53403a0055dbe5faaf9f1f3be96ca0e692a4d",
            "isKey": true,
            "numCitedBy": 1914,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe several improvements to Freund and Schapire's AdaBoost boosting algorithm, particularly in a setting in which hypotheses may assign confidences to each of their predictions. We give a simplified analysis of AdaBoost in this setting, and we show how this analysis can be used to find improved parameter settings as well as a refined criterion for training weak hypotheses. We give a specific method for assigning confidences to the predictions of decision trees, a method closely related to one used by Quinlan. This method also suggests a technique for growing decision trees which turns out to be identical to one proposed by Kearns and Mansour. We focus next on how to apply the new boosting algorithms to multiclass classification problems, particularly to the multi-label case in which each example may belong to more than one class. We give two boosting methods for this problem, plus a third method based on output coding. One of these leads to a new method for handling the single-label case which is simpler but as effective as techniques suggested by Freund and Schapire. Finally, we give some experimental results comparing a few of the algorithms discussed in this paper."
            },
            "slug": "Improved-Boosting-Algorithms-Using-Confidence-rated-Schapire-Singer",
            "title": {
                "fragments": [],
                "text": "Improved Boosting Algorithms Using Confidence-rated Predictions"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "Several improvements to Freund and Schapire's AdaBoost boosting algorithm are described, particularly in a setting in which hypotheses may assign confidences to each of their predictions."
            },
            "venue": {
                "fragments": [],
                "text": "COLT' 98"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703537"
                        ],
                        "name": "Y. Freund",
                        "slug": "Y.-Freund",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Freund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 0
                            }
                        ],
                        "text": "Freund and Schapire (1996b) and Schapire and Singer (1998) provide some theory to support their algorithms, in the form of upper bounds on generalization error."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 54
                            }
                        ],
                        "text": "This led to the more adaptive and realistic AdaBoost [Freund and Schapire (1996b)] and its offspring, where this assumption was dropped."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 103
                            }
                        ],
                        "text": "Result 1 requires minor modifications to accommodate f x \u2208 R, as in the generalized AdaBoost algorithms [Freund and Schapire (1996b), Schapire and Singer (1998)]; the estimate for cm differs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 225,
                                "start": 203
                            }
                        ],
                        "text": "Direct multiclass generalizations based on multinomial likelihood are derived that exhibit performance comparable to other recently proposed multiclass generalizations of boosting in most situations, and far superior in some."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 69
                            }
                        ],
                        "text": "Other theories attempting to explain boosting come from game theory [Freund and Schapire (1996a), Breiman (1997)] and VC theory [Schapire, Freund, Bartlett and Lee (1998)]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 98
                            }
                        ],
                        "text": "Results 1 and 2 show that both Discrete and Real AdaBoost, as well as the Generalized AdaBoost of Freund and Schapire (1996b), can be motivated as iterative algorithms for optimizing the (population based) exponential criterion."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 50
                            }
                        ],
                        "text": "A generalization of Discrete AdaBoost appeared in Freund and Schapire (1996b), and was developed further in Schapire and Singer (1998), that uses real-valued \u201cconfidence-rated\u201d predictions rather than the \u22121 1 of Discrete AdaBoost."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 133
                            }
                        ],
                        "text": "The base classifier in Discrete AdaBoost produces a classification rule fm x \u2192 \u22121 1 , where is the domain of the predictive features x. Freund and Schapire (1996b), Breiman (1998a) and Schapire and Singer (1998) have suggested various modifications to improve the boosting algorithms."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1836349,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "68c1bfe375dde46777fe1ac8f3636fb651e3f0f8",
            "isKey": true,
            "numCitedBy": 8625,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "In an earlier paper, we introduced a new \"boosting\" algorithm called AdaBoost which, theoretically, can be used to significantly reduce the error of any learning algorithm that con- sistently generates classifiers whose performance is a little better than random guessing. We also introduced the related notion of a \"pseudo-loss\" which is a method for forcing a learning algorithm of multi-label concepts to concentrate on the labels that are hardest to discriminate. In this paper, we describe experiments we carried out to assess how well AdaBoost with and without pseudo-loss, performs on real learning problems. We performed two sets of experiments. The first set compared boosting to Breiman's \"bagging\" method when used to aggregate various classifiers (including decision trees and single attribute- value tests). We compared the performance of the two methods on a collection of machine-learning benchmarks. In the second set of experiments, we studied in more detail the performance of boosting using a nearest-neighbor classifier on an OCR problem."
            },
            "slug": "Experiments-with-a-New-Boosting-Algorithm-Freund-Schapire",
            "title": {
                "fragments": [],
                "text": "Experiments with a New Boosting Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper describes experiments carried out to assess how well AdaBoost with and without pseudo-loss, performs on real learning problems and compared boosting to Breiman's \"bagging\" method when used to aggregate various classifiers."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056361"
                        ],
                        "name": "J. Friedman",
                        "slug": "J.-Friedman",
                        "structuredName": {
                            "firstName": "Jerome",
                            "lastName": "Friedman",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Friedman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 156
                            }
                        ],
                        "text": "We see here a simple example in L2 boosting for regression where overfitting occurs easily in contrast to classification; similar phenomena are reported in Friedman (1999)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 49
                            }
                        ],
                        "text": "Details of these generalizations may be found in Friedman (1999)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 117
                            }
                        ],
                        "text": "More generally, one can consider an expansion of the decision boundary function in a functional ANOVA decomposition [Friedman (1991)]"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 994,
                                "start": 173
                            }
                        ],
                        "text": "Even if the decision boundaries separating all class pairs are relatively simple, pooling classes can produce complex decision boundaries that are difficult to approximate [Friedman (1996)]. By considering all of the classes simultaneously, the symmetric multiclass model is better able to take advantage of simple pairwise boundaries when they exist [Hastie and Tibshirani (1998)]. As noted above, the pairwise boundaries induced by (41) and (42) are simple when viewed in the context of additive modeling, whereas the pooled boundaries are more complex; they cannot be well approximated by functions that are additive in the original predictor variables. The decision boundaries associated with these examples were deliberately chosen to be geometrically complex in an attempt to elicit performance differences among the methods being tested. Such complicated boundaries are not likely to often occur in practice. Many practical problems involve comparatively simple boundaries [Holte (1993)]; in such cases performance differences will still be situation dependent, but correspondingly less pronounced."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 27
                            }
                        ],
                        "text": "The backfitting algorithm [Friedman and Stuetzle (1981), Buja, Hastie and Tibshirani (1989)] is a convenient modular \u201cGauss\u2013Seidel\u201d algorithm for fitting additive models."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 189,
                                "start": 173
                            }
                        ],
                        "text": "Even if the decision boundaries separating all class pairs are relatively simple, pooling classes can produce complex decision boundaries that are difficult to approximate [Friedman (1996)]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 264,
                                "start": 249
                            }
                        ],
                        "text": "\u2026function estimator fm \u00b7 which is para-\nmeterized as fm x = \u03b2b x \u03b3 (as in FHT):\n\u03b2m \u03b3m = argmin\u03b2 \u03b3 n\u2211 i=1 ( Yi \u2212Fm\u22121 Xi \u2212 \u03b2b Xi \u03b3 )2\nSet Fm \u00b7 = Fm\u22121 \u00b7 + fm \u00b7 (c) Output the function estimator\nFM \u00b7 = M\u2211 m=1 fm \u00b7\nThis algorithm is indicated by FHT [formula (6)] and was also given by Friedman (1999)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 381,
                                "start": 173
                            }
                        ],
                        "text": "Even if the decision boundaries separating all class pairs are relatively simple, pooling classes can produce complex decision boundaries that are difficult to approximate [Friedman (1996)]. By considering all of the classes simultaneously, the symmetric multiclass model is better able to take advantage of simple pairwise boundaries when they exist [Hastie and Tibshirani (1998)]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 39450643,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1679beddda3a183714d380e944fe6bf586c083cd",
            "isKey": false,
            "numCitedBy": 13764,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Function estimation/approximation is viewed from the perspective of numerical optimization in function space, rather than parameter space. A connection is made between stagewise additive expansions and steepest-descent minimization. A general gradient descent boosting paradigm is developed for additive expansions based on any fitting criterion. Specific algorithms are presented for least-squares, least absolute deviation, and Huber-M loss functions for regression, and multiclass logistic likelihood for classification. Special enhancements are derived for the particular case where the individual additive components are regression trees, and tools for interpreting such TreeBoost models are presented. Gradient boosting of regression trees produces competitive, highly robust, interpretable procedures for both regression and classification, especially appropriate for mining less than clean data. Connections between this approach and the boosting methods of Freund and Shapire and Friedman, Hastie and Tibshirani are discussed."
            },
            "slug": "Greedy-function-approximation:-A-gradient-boosting-Friedman",
            "title": {
                "fragments": [],
                "text": "Greedy function approximation: A gradient boosting machine."
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A general gradient descent boosting paradigm is developed for additive expansions based on any fitting criterion, and specific algorithms are presented for least-squares, least absolute deviation, and Huber-M loss functions for regression, and multiclass logistic likelihood for classification."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2187850"
                        ],
                        "name": "Adam J. Grove",
                        "slug": "Adam-J.-Grove",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Grove",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adam J. Grove"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714772"
                        ],
                        "name": "Dale Schuurmans",
                        "slug": "Dale-Schuurmans",
                        "structuredName": {
                            "firstName": "Dale",
                            "lastName": "Schuurmans",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dale Schuurmans"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1556809,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "639057ad00ddaf8bbed3fa0dbd9663dfeb663d62",
            "isKey": false,
            "numCitedBy": 282,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "The \"minimum margin\" of an ensemble classifier on a given training set is, roughly speaking, the smallest vote it gives to any correct training label. Recent work has shown that the Adaboost algorithm is particularly effective at producing ensembles with large minimum margins, and theory suggests that this may account for its success at reducing generalization error. We note, however, that the problem of finding good margins is closely related to linear programming, and we use this connection to derive and test new \"LPboosting\" algorithms that achieve better minimum margins than Adaboost.However, these algorithms do not always yield better generalization performance. In fact, more often the opposite is true. We report on a series of controlled experiments which show that no simple version of the minimum-margin story can be complete. We conclude that the crucial question as to why boosting works so well in practice, and how to further improve upon it, remains mostly open.Some of our experiments are interesting for another reason: we show that Adaboost sometimes does overfit--eventually. This may take a very long time to occur, however, which is perhaps why this phenomenon has gone largely unnoticed."
            },
            "slug": "Boosting-in-the-Limit:-Maximizing-the-Margin-of-Grove-Schuurmans",
            "title": {
                "fragments": [],
                "text": "Boosting in the Limit: Maximizing the Margin of Learned Ensembles"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The crucial question as to why boosting works so well in practice, and how to further improve upon it, remains mostly open, and it is concluded that no simple version of the minimum-margin story can be complete."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI/IAAI"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1830755"
                        ],
                        "name": "D. Denison",
                        "slug": "D.-Denison",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Denison",
                            "middleNames": [
                                "G.",
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Denison"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 18609598,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e00fa749a24120d0f0ed646440c425f58a3e6184",
            "isKey": false,
            "numCitedBy": 6,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Boosting is a new, powerful method for classification. It is an iterative procedure which successively classifies a weighted version of the sample, and then reweights this sample dependent on how successful the classification was. In this paper we review some of the commonly used methods for performing boosting and show how they can be fit into a Bayesian setup at each iteration of the algorithm. We demonstrate how this formulation gives rise to a new splitting criterion when using a domain-partitioning classification method such as a decision tree. Further we can improve the predictive performance of simple decision trees, known as stumps, by using a posterior weighted average of them to classify at each step of the algorithm, rather than just a single stump. The main advantage of this approach is to reduce the number of boosting iterations required to produce a good classifier with only a minimal increase in the computational complexity of the algorithm."
            },
            "slug": "Boosting-with-Bayesian-stumps-Denison",
            "title": {
                "fragments": [],
                "text": "Boosting with Bayesian stumps"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper reviews some of the commonly used methods for performing boosting and shows how they can be fit into a Bayesian setup at each iteration of the algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "Stat. Comput."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703537"
                        ],
                        "name": "Y. Freund",
                        "slug": "Y.-Freund",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Freund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2022386739"
                        ],
                        "name": "Peter Barlett",
                        "slug": "Peter-Barlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Barlett",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter Barlett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740222"
                        ],
                        "name": "Wee Sun Lee",
                        "slug": "Wee-Sun-Lee",
                        "structuredName": {
                            "firstName": "Wee",
                            "lastName": "Lee",
                            "middleNames": [
                                "Sun"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wee Sun Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 573509,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4d19272112b50547614479a0c409fca66e3b05f7",
            "isKey": false,
            "numCitedBy": 2844,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the surprising recurring phenomena observed in experiments with boosting is that the test error of the generated classifier usually does not increase as its size becomes very large, and often is observed to decrease even after the training error reaches zero. In this paper, we show that this phenomenon is related to the distribution of margins of the training examples with respect to the generated voting classification rule, where the margin of an example is simply the difference between the number of correct votes and the maximum number of votes received by any incorrect label. We show that techniques used in the analysis of Vapnik's support vector classifiers and of neural networks with small weights can be applied to voting methods to relate the margin distribution to the test error. We also show theoretically and experimentally that boosting is especially effective at increasing the margins of the training examples. Finally, we compare our explanation to those based on the bias-variance"
            },
            "slug": "Boosting-the-margin:-A-new-explanation-for-the-of-Schapire-Freund",
            "title": {
                "fragments": [],
                "text": "Boosting the margin: A new explanation for the effectiveness of voting methods"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown that techniques used in the analysis of Vapnik's support vector classifiers and of neural networks with small weights can be applied to voting methods to relate the margin distribution to the test error."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2050470845"
                        ],
                        "name": "H. Drucker",
                        "slug": "H.-Drucker",
                        "structuredName": {
                            "firstName": "Harris",
                            "lastName": "Drucker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Drucker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145115014"
                        ],
                        "name": "Corinna Cortes",
                        "slug": "Corinna-Cortes",
                        "structuredName": {
                            "firstName": "Corinna",
                            "lastName": "Cortes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Corinna Cortes"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 1266014,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a1dfeb731fc0c79e04523cd655413c223f6fa102",
            "isKey": false,
            "numCitedBy": 280,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a constructive, incremental learning system for regression problems that models data by means of locally linear experts. In contrast to other approaches, the experts are trained independently and do not compete for data during learning. Only when a prediction for a query is required do the experts cooperate by blending their individual predictions. Each expert is trained by minimizing a penalized local cross validation error using second order methods. In this way, an expert is able to find a local distance metric by adjusting the size and shape of the receptive field in which its predictions are valid, and also to detect relevant input features by adjusting its bias on the importance of individual input dimensions. We derive asymptotic results for our method. In a variety of simulations the properties of the algorithm are demonstrated with respect to interference, learning speed, prediction accuracy, feature detection, and task oriented incremental learning."
            },
            "slug": "Boosting-Decision-Trees-Drucker-Cortes",
            "title": {
                "fragments": [],
                "text": "Boosting Decision Trees"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A constructive, incremental learning system for regression problems that models data by means of locally linear experts that does not compete for data during learning and derives asymptotic results for this method."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 142
                            }
                        ],
                        "text": "Schapire and Singer (1998) provide several generalizations of AdaBoost for the multiclass case, and also refer to other proposals [Freund and Schapire (1997), Schapire (1997)]; we describe their AdaBoost."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 114
                            }
                        ],
                        "text": "Boosting was proposed in the computational learning theory literature [Schapire (1990), Freund (1995), Freund and Schapire (1997)] and has since received much attention."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2685539,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cc1374bcd952032dabe891114f29092b868e01b8",
            "isKey": false,
            "numCitedBy": 319,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a new technique for solv- ing multiclass learning problems by combining Freund and Schapire's boosting algorithm with the main ideas of Diet- terich and Bakiri's method of error-correcting output codes (ECOC). Boosting is a general method of improving the ac- curacy of a given base or \"weak\" learning algorithm. ECOC is a robust method of solving multiclass learning problems by reducing to a sequence of two-class problems. We show that our new hybrid method has advantages of both: Like ECOC, our method only requires that the base learning al- gorithm work on binary-labeled data. Like boosting, we prove that the method comes with strong theoretical guar- antees on the training and generalization error of the final combined hypothesis assuming only that the base learning algorithm perform slightly better than random guessing. Although previous methods were known for boosting multi- class problems, the new method may be significantly faster and require less programming effort in creating the base learning algorithm. We also compare the new algorithm experimentally to other voting methods."
            },
            "slug": "Using-output-codes-to-boost-multiclass-learning-Schapire",
            "title": {
                "fragments": [],
                "text": "Using output codes to boost multiclass learning problems"
            },
            "tldr": {
                "abstractSimilarityScore": 87,
                "text": "This paper describes a new technique for multiclass learning problems by combining Freund and Schapire's boosting algorithm with the main ideas of Diet- terich and Bakiri's method of error-correcting output codes (ECOC), and shows that the new hybrid method has advantages of both."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703537"
                        ],
                        "name": "Y. Freund",
                        "slug": "Y.-Freund",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Freund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Freund"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 51
                            }
                        ],
                        "text": "The original boosting techniques [Schapire (1990), Freund (1995)] provably improved or \u201cboosted\u201d the performance of a single classifier by producing a \u201cmajority vote\u201d of similar classifiers."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 88
                            }
                        ],
                        "text": "Boosting was proposed in the computational learning theory literature [Schapire (1990), Freund (1995), Freund and Schapire (1997)] and has since received much attention."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 151
                            }
                        ],
                        "text": "Despite its apparent simplicity, this approach does not appear to be in common use [although similar ideas have been proposed before: Schapire (1990), Freund (1995)]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 0
                            }
                        ],
                        "text": "Freund (1995) proposed a \u201cboost by majority\u201d variation which combined many weak learners simultaneously and improved the performance of the simple boosting algorithm of Schapire."
                    },
                    "intents": []
                }
            ],
            "corpusId": 19728033,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b824cb051ffbdd81b529c4b82379a3af270fb6f7",
            "isKey": true,
            "numCitedBy": 1278,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract We present an algorithm for improving the accuracy of algorithms for learning binary concepts. The improvement is achieved by combining a large number of hypotheses, each of which is generated by training the given learning algorithm on a different set of examples. Our algorithm is based on ideas presented by Schapire and represents an improvement over his results, The analysis of our algorithm provides general upper bounds on the resources required for learning in Valiant\u2032s polynomial PAC learning framework, which are the best general upper bounds known today. We show that the number of hypotheses that are combined by our algorithm is the smallest number possible. Other outcomes of our analysis are results regarding the representational power of threshold circuits, the relation between learnability and compression, and a method for parallelizing PAC learning algorithms. We provide extensions of our algorithms to cases in which the concepts are not binary and to the case where the accuracy of the learning algorithm depends on the distribution of the instances."
            },
            "slug": "Boosting-a-weak-learning-algorithm-by-majority-Freund",
            "title": {
                "fragments": [],
                "text": "Boosting a weak learning algorithm by majority"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "An algorithm for improving the accuracy of algorithms for learning binary concepts by combining a large number of hypotheses, each of which is generated by training the given learning algorithm on a different set of examples, is presented."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '90"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703537"
                        ],
                        "name": "Y. Freund",
                        "slug": "Y.-Freund",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Freund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2586148"
                        ],
                        "name": "L. Mason",
                        "slug": "L.-Mason",
                        "structuredName": {
                            "firstName": "Llew",
                            "lastName": "Mason",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Mason"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "A new boosting algorithm based on this radical approach was recently proposed by  Freund (1999) ."
                    },
                    "intents": []
                }
            ],
            "corpusId": 3772657,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "29b7eebc893acd2c2596de227333480e7a118af8",
            "isKey": false,
            "numCitedBy": 819,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "The application of boosting procedures to decision tree algorithms has been shown to produce very accurate classi ers. These classiers are in the form of a majority vote over a number of decision trees. Unfortunately, these classi ers are often large, complex and di\u00c6cult to interpret. This paper describes a new type of classi cation rule, the alternating decision tree, which is a generalization of decision trees, voted decision trees and voted decision stumps. At the same time classi ers of this type are relatively easy to interpret. We present a learning algorithm for alternating decision trees that is based on boosting. Experimental results show it is competitive with boosted decision tree algorithms such as C5.0, and generates rules that are usually smaller in size and thus easier to interpret. In addition these rules yield a natural measure of classi cation con dence which can be used to improve the accuracy at the cost of abstaining from predicting examples that are hard to classify."
            },
            "slug": "The-Alternating-Decision-Tree-Learning-Algorithm-Freund-Mason",
            "title": {
                "fragments": [],
                "text": "The Alternating Decision Tree Learning Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A new type of classi cation rule, the alternating decision tree, which is a generalization of decision trees, voted decision trees and voted decision stumps and generates rules that are usually smaller in size and thus easier to interpret."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2423230"
                        ],
                        "name": "L. Breiman",
                        "slug": "L.-Breiman",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Breiman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Breiman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 9
                            }
                        ],
                        "text": "In fact, Breiman (1996) (referring to a NIPS workshop) called AdaBoost with trees the \u201cbest off-the-shelf classifier in the world\u201d [see also Breiman (1998b)]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 101
                            }
                        ],
                        "text": "In the context of boosting L2 regression, we compare boosting with another ensemble scheme, bagging [Breiman (1996)], for which we have gained some understanding recently [Bu\u0308hlmann and Yu (2000)]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 72
                            }
                        ],
                        "text": "Note that other committee approaches to classification such as bagging [Breiman (1996)] and randomized trees [Dietterich (1998)] while admitting parallel implementations, cannot take advantage of this approach to reduce computation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 204,
                                "start": 190
                            }
                        ],
                        "text": "The concepts developed in this paper suggest that there is very little, if any, connection between (deterministic) weighted boosting and other (randomized) ensemble methods such as bagging [Breiman (1996)] and randomized trees [Dietterich (1998)]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 43
                            }
                        ],
                        "text": "Included in the figure is the bagged tree [Breiman (1996)] which averages trees grown on bootstrap resampled versions of the training data."
                    },
                    "intents": []
                }
            ],
            "corpusId": 7732239,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d414438926b73bde0313948d8b074cb5360a0e6f",
            "isKey": true,
            "numCitedBy": 637,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent work has shown that combining multiple versions of unstable classifiers such as trees or neural nets results in reduced test set error. To study this, the concepts of bias and variance of a classifier are defined. Unstable classifiers can have universally low bias. Their problem is high variance. Combining multiple versions is a variance reducing device. One of the most effective is bagging (Breiman [1996a]) Here, modified training sets are formed by resampling from the original training set, classifiers constructed using these training sets and then combined by voting. Freund and Schapire [1995,1996] propose an algorithm the basis of which is to adaptively resample and combine (hence the acronym-arcing) so that the weights in the resampling are increased for those cases most often missclassified and the combining is done by weighted voting. Arcing is more sucessful than bagging in variance reduction. We explore two arcing algorithms, compare them to each other and to bagging, and try to understand how arcing works."
            },
            "slug": "Bias,-Variance-,-And-Arcing-Classifiers-Breiman",
            "title": {
                "fragments": [],
                "text": "Bias, Variance , And Arcing Classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This work explores two arcing algorithms, compares them to each other and to bagging, and tries to understand how arcing works, which is more sucessful than bagging in variance reduction."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145797806"
                        ],
                        "name": "G. Ridgeway",
                        "slug": "G.-Ridgeway",
                        "structuredName": {
                            "firstName": "Greg",
                            "lastName": "Ridgeway",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Ridgeway"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14337989,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1aac6453fbb8333ee638b6d8b2bb2aff06c3654b",
            "isKey": false,
            "numCitedBy": 362,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "In many problem domains, combining the predictions of several models often results in a model with improved predictive performance. Boosting is one such method that has shown great promise. On the applied side, empirical studies have shown that combining models using boosting methods produces more accurate classification and regression models. These methods are extendible to the exponential family as well as proportional hazards regression models. This article shows that boosting, which is still new to statistics, is widely applicable. I will introduce boosting, discuss the current state of boosting, and show how these methods connect to more standard statistical practice."
            },
            "slug": "The-State-of-Boosting-\u2217-Ridgeway",
            "title": {
                "fragments": [],
                "text": "The State of Boosting \u2217"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This article shows that boosting, which is still new to statistics, is widely applicable and discusses the current state of boosting, and shows how these methods connect to more standard statistical practice."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703537"
                        ],
                        "name": "Y. Freund",
                        "slug": "Y.-Freund",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Freund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Freund"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 975467,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "be9a9efa52c1c0cc708ce3dc79c85433ebd08108",
            "isKey": false,
            "numCitedBy": 273,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a new boosting algorithm. This boosting algorithm is an adaptive version of the boost by majority algorithm and combines bounded goals of the boost by majority algorithm with the adaptivity of AdaBoost.The method used for making boost-by-majority adaptive is to consider the limit in which each of the boosting iterations makes an infinitesimally small contribution to the process as a whole. This limit can be modeled using the differential equations that govern Brownian motion. The new boosting algorithm, named BrownBoost, is based on finding solutions to these differential equations.The paper describes two methods for finding approximate solutions to the differential equations. The first is a method that results in a provably polynomial time algorithm. The second method, based on the Newton-Raphson minimization procedure, is much more efficient in practice but is not known to be polynomial."
            },
            "slug": "An-Adaptive-Version-of-the-Boost-by-Majority-Freund",
            "title": {
                "fragments": [],
                "text": "An Adaptive Version of the Boost by Majority Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The paper describes two methods for finding approximate solutions to the differential equations and a method that results in a provably polynomial time algorithm based on the Newton-Raphson minimization procedure, which is much more efficient in practice but is not known to bePolynomial."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '99"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056361"
                        ],
                        "name": "J. Friedman",
                        "slug": "J.-Friedman",
                        "structuredName": {
                            "firstName": "Jerome",
                            "lastName": "Friedman",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Friedman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 264,
                                "start": 249
                            }
                        ],
                        "text": "\u2026function estimator fm \u00b7 which is para-\nmeterized as fm x = \u03b2b x \u03b3 (as in FHT):\n\u03b2m \u03b3m = argmin\u03b2 \u03b3 n\u2211 i=1 ( Yi \u2212Fm\u22121 Xi \u2212 \u03b2b Xi \u03b3 )2\nSet Fm \u00b7 = Fm\u22121 \u00b7 + fm \u00b7 (c) Output the function estimator\nFM \u00b7 = M\u2211 m=1 fm \u00b7\nThis algorithm is indicated by FHT [formula (6)] and was also given by Friedman (1999)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 156
                            }
                        ],
                        "text": "We see here a simple example in L2 boosting for regression where overfitting occurs easily in contrast to classification; similar phenomena are reported in Friedman (1999)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 49
                            }
                        ],
                        "text": "Details of these generalizations may be found in Friedman (1999)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 16190154,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8bdda840ec8990e24c5a70db171edac330ebf650",
            "isKey": false,
            "numCitedBy": 4249,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Stochastic-gradient-boosting-Friedman",
            "title": {
                "fragments": [],
                "text": "Stochastic gradient boosting"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341779"
                        ],
                        "name": "J. R. Quinlan",
                        "slug": "J.-R.-Quinlan",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Quinlan",
                            "middleNames": [
                                "Ross"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. R. Quinlan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 937841,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "79ea6a5a68e05065f82acd11a478aa7eac5f6c06",
            "isKey": false,
            "numCitedBy": 1657,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Breiman's bagging and Freund and Schapire's boosting are recent methods for improving the predictive power of classifier learning systems. Both form a set of classifiers that are combined by voting, bagging by generating replicated bootstrap samples of the data, and boosting by adjusting the weights of training instances. This paper reports results of applying both techniques to a system that learns decision trees and testing on a representative collection of datasets. While both approaches substantially improve predictive accuracy, boosting shows the greater benefit. On the other hand, boosting also produces severe degradation on some datasets. A small change to the way that boosting combines the votes of learned classifiers reduces this downside and also leads to slightly better results on most of the datasets considered."
            },
            "slug": "Bagging,-Boosting,-and-C4.5-Quinlan",
            "title": {
                "fragments": [],
                "text": "Bagging, Boosting, and C4.5"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "Results of applying Breiman's bagging and Freund and Schapire's boosting to a system that learns decision trees and testing on a representative collection of datasets show boosting shows the greater benefit."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI/IAAI, Vol. 1"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4801654"
                        ],
                        "name": "Y. Amit",
                        "slug": "Y.-Amit",
                        "structuredName": {
                            "firstName": "Yali",
                            "lastName": "Amit",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Amit"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707642"
                        ],
                        "name": "D. Geman",
                        "slug": "D.-Geman",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Geman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Geman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 39
                            }
                        ],
                        "text": "This idea first appeared in a paper by Amit and Geman (1997)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 12470146,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "de5e95325e139fd0a46df1dd28aabecd0273b772",
            "isKey": false,
            "numCitedBy": 1152,
            "numCiting": 78,
            "paperAbstract": {
                "fragments": [],
                "text": "We explore a new approach to shape recognition based on a virtually infinite family of binary features (queries) of the image data, designed to accommodate prior information about shape invariance and regularity. Each query corresponds to a spatial arrangement of several local topographic codes (or tags), which are in themselves too primitive and common to be informative about shape. All the discriminating power derives from relative angles and distances among the tags. The important attributes of the queries are a natural partial ordering corresponding to increasing structure and complexity; semi-invariance, meaning that most shapes of a given class will answer the same way to two queries that are successive in the ordering; and stability, since the queries are not based on distinguished points and substructures. No classifier based on the full feature set can be evaluated, and it is impossible to determine a priori which arrangements are informative. Our approach is to select informative features and build tree classifiers at the same time by inductive learning. In effect, each tree provides an approximation to the full posterior where the features chosen depend on the branch that is traversed. Due to the number and nature of the queries, standard decision tree construction based on a fixed-length feature vector is not feasible. Instead we entertain only a small random sample of queries at each node, constrain their complexity to increase with tree depth, and grow multiple trees. The terminal nodes are labeled by estimates of the corresponding posterior distribution over shape classes. An image is classified by sending it down every tree and aggregating the resulting distributions. The method is applied to classifying handwritten digits and synthetic linear and nonlinear deformations of three hundred symbols. State-of-the-art error rates are achieved on the National Institute of Standards and Technology database of digits. The principal goal of the experiments on symbols is to analyze invariance, generalization error and related issues, and a comparison with artificial neural networks methods is presented in this context. Figure 1: LATEX Symbol"
            },
            "slug": "Shape-Quantization-and-Recognition-with-Randomized-Amit-Geman",
            "title": {
                "fragments": [],
                "text": "Shape Quantization and Recognition with Randomized Trees"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "A new approach to shape recognition based on a virtually infinite family of binary features (queries) of the image data, designed to accommodate prior information about shape invariance and regularity, and a comparison with artificial neural networks methods is presented."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784682"
                        ],
                        "name": "T. Hastie",
                        "slug": "T.-Hastie",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Hastie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Hastie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761784"
                        ],
                        "name": "R. Tibshirani",
                        "slug": "R.-Tibshirani",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Tibshirani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Tibshirani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1963591"
                        ],
                        "name": "A. Buja",
                        "slug": "A.-Buja",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Buja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Buja"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "By fitting an additive model of different and potentially simple functions, it expands the class of functions that can be approximated."
                    },
                    "intents": []
                }
            ],
            "corpusId": 122018810,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "03fea2fbb8e780fdebcce89bb19c6f4b87f73347",
            "isKey": false,
            "numCitedBy": 668,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract Fisher's linear discriminant analysis is a valuable tool for multigroup classification. With a large number of predictors, one can find a reduced number of discriminant coordinate functions that are \u201coptimal\u201d for separating the groups. With two such functions, one can produce a classification map that partitions the reduced space into regions that are identified with group membership, and the decision boundaries are linear. This article is about richer nonlinear classification schemes. Linear discriminant analysis is equivalent to multiresponse linear regression using optimal scorings to represent the groups. In this paper, we obtain nonparametric versions of discriminant analysis by replacing linear regression by any nonparametric regression method. In this way, any multiresponse regression technique (such as MARS or neural networks) can be postprocessed to improve its classification performance."
            },
            "slug": "Flexible-Discriminant-Analysis-by-Optimal-Scoring-Hastie-Tibshirani",
            "title": {
                "fragments": [],
                "text": "Flexible Discriminant Analysis by Optimal Scoring"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Nonparametric versions of discriminant analysis are obtained by replacing linear regression by any nonparametric regression method so that any multiresponse regression technique can be postprocessed to improve its classification performance."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752732"
                        ],
                        "name": "T. Cover",
                        "slug": "T.-Cover",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Cover",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Cover"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3108177"
                        ],
                        "name": "P. Hart",
                        "slug": "P.-Hart",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Hart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hart"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 139
                            }
                        ],
                        "text": "Classifiers are hurt less by overfitting than other function estimators [e.g., the famous risk bound of the 1-nearest-neighbor classifier, Cover and Hart (1967)]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 5246200,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "0efb841403aa6252b39ae6975c1cc5410554ef7b",
            "isKey": false,
            "numCitedBy": 10766,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "The nearest neighbor decision rule assigns to an unclassified sample point the classification of the nearest of a set of previously classified points. This rule is independent of the underlying joint distribution on the sample points and their classifications, and hence the probability of error R of such a rule must be at least as great as the Bayes probability of error R^{\\ast} --the minimum probability of error over all decision rules taking underlying probability structure into account. However, in a large sample analysis, we will show in the M -category case that R^{\\ast} \\leq R \\leq R^{\\ast}(2 --MR^{\\ast}/(M-1)) , where these bounds are the tightest possible, for all suitably smooth underlying distributions. Thus for any number of categories, the probability of error of the nearest neighbor rule is bounded above by twice the Bayes probability of error. In this sense, it may be said that half the classification information in an infinite sample set is contained in the nearest neighbor."
            },
            "slug": "Nearest-neighbor-pattern-classification-Cover-Hart",
            "title": {
                "fragments": [],
                "text": "Nearest neighbor pattern classification"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "The nearest neighbor decision rule assigns to an unclassified sample point the classification of the nearest of a set of previously classified points, so it may be said that half the classification information in an infinite sample set is contained in the nearest neighbor."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1967
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1960946"
                        ],
                        "name": "H. Chipman",
                        "slug": "H.-Chipman",
                        "structuredName": {
                            "firstName": "Hugh",
                            "lastName": "Chipman",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Chipman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2347035"
                        ],
                        "name": "E. George",
                        "slug": "E.-George",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "George",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. George"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2840298"
                        ],
                        "name": "R. McCulloch",
                        "slug": "R.-McCulloch",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "McCulloch",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. McCulloch"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 122667720,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "2d1e0a5d5309354d4948fa88b01232e59eee94dc",
            "isKey": false,
            "numCitedBy": 674,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract In this article we put forward a Bayesian approach for finding classification and regression tree (CART) models. The two basic components of this approach consist of prior specification and stochastic search. The basic idea is to have the prior induce a posterior distribution that will guide the stochastic search toward more promising CART models. As the search proceeds, such models can then be selected with a variety of criteria, such as posterior probability, marginal likelihood, residual sum of squares or misclassification rates. Examples are used to illustrate the potential superiority of this approach over alternative methods."
            },
            "slug": "Bayesian-CART-Model-Search-Chipman-George",
            "title": {
                "fragments": [],
                "text": "Bayesian CART Model Search"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "A Bayesian approach for finding classification and regression tree (CART) models by having the prior induce a posterior distribution that will guide the stochastic search toward more promising CART models."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2423230"
                        ],
                        "name": "L. Breiman",
                        "slug": "L.-Breiman",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Breiman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Breiman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 3
                            }
                        ],
                        "text": "In Breiman (1999a), empirical evidence is given to show that AdaBoost is a random forest."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 104
                            }
                        ],
                        "text": "To check on my hypothesis that AdaBoost worked so well because it is an equalizer, using the methods in Breiman (1999b), I devised an algorithm called arc-equal which sequentially minimizes Avn prk n \u2212 t 2, where t is an adjustable parameter."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 60
                            }
                        ],
                        "text": "It is also much faster computationally, making it more suitable to large-scale data mining applications."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 3
                            }
                        ],
                        "text": "In Breiman (1999a), I assume an ensemble of classifiers h x \u03b8 \u03b8 \u2208 3 ."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14488820,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a92684c164b0c46020a371ae5116df74bb37a412",
            "isKey": true,
            "numCitedBy": 551,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "The theory behind the success of adaptive reweighting and combining algorithms (arcing) such as Adaboost (Freund & Schapire, 1996a, 1997) and others in reducing generalization error has not been well understood. By formulating prediction as a game where one player makes a selection from instances in the training set and the other a convex linear combination of predictors from a finite set, existing arcing algorithms are shown to be algorithms for finding good game strategies. The minimax theorem is an essential ingredient of the convergence proofs. An arcing algorithm is described that converges to the optimal strategy. A bound on the generalization error for the combined predictors in terms of their maximum error is proven that is sharper than bounds to date. Schapire, Freund, Bartlett, and Lee (1997) offered an explanation of why Adaboost works in terms of its ability to produce generally high margins. The empirical comparison of Adaboost to the optimal arcing algorithm shows that their explanation is not complete."
            },
            "slug": "Prediction-Games-and-Arcing-Algorithms-Breiman",
            "title": {
                "fragments": [],
                "text": "Prediction Games and Arcing Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "The theory behind the success of adaptive reweighting and combining algorithms (arcing) such as Adaboost and others in reducing generalization error has not been well understood, and an explanation of whyAdaboost works in terms of its ability to produce generally high margins is offered."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784682"
                        ],
                        "name": "T. Hastie",
                        "slug": "T.-Hastie",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Hastie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Hastie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761784"
                        ],
                        "name": "R. Tibshirani",
                        "slug": "R.-Tibshirani",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Tibshirani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Tibshirani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 189,
                                "start": 161
                            }
                        ],
                        "text": "By considering all of the classes simultaneously, the symmetric multiclass model is better able to take advantage of simple pairwise boundaries when they exist [Hastie and Tibshirani (1998)]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 10097148,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "f642a692da944604a7df590e9f9fa06089b7991a",
            "isKey": false,
            "numCitedBy": 1574,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We discuss a strategy for polychotomous classification that involves estimating class probabilities for each pair of classes, and then coupling the estimates together. The coupling model is similar to the Bradley-Terry method for paired comparisons. We study the nature of the class probability estimates that arise, and examine the performance of the procedure in simulated datasets. The classifiers used include linear discriminants and nearest neighbors: application to support vector machines is also briefly described."
            },
            "slug": "Classification-by-Pairwise-Coupling-Hastie-Tibshirani",
            "title": {
                "fragments": [],
                "text": "Classification by Pairwise Coupling"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "A strategy for polychotomous classification that involves estimating class probabilities for each pair of classes, and then coupling the estimates together is discussed, similar to the Bradley-Terry method for paired comparisons."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2423230"
                        ],
                        "name": "L. Breiman",
                        "slug": "L.-Breiman",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Breiman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Breiman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 3
                            }
                        ],
                        "text": "In Breiman (1999a), empirical evidence is given to show that AdaBoost is a random forest."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 104
                            }
                        ],
                        "text": "To check on my hypothesis that AdaBoost worked so well because it is an equalizer, using the methods in Breiman (1999b), I devised an algorithm called arc-equal which sequentially minimizes Avn prk n \u2212 t 2, where t is an adjustable parameter."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 3
                            }
                        ],
                        "text": "In Breiman (1999a), I assume an ensemble of classifiers h x \u03b8 \u03b8 \u2208 3 ."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1221586,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "636c243ae176bcfa9766f8d4fca7eb441819e21d",
            "isKey": false,
            "numCitedBy": 103,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Breiman[1996] showed that bagging could effectively reduce the variance of regression predictors, while leaving the bias unchanged. A new form of bagging we call adaptive bagging is effective in reducing both bias and variance. The procedure works in stages-the first stage is bagging. Based on the outcomes of the first stage, the output values are altered and a second stage of bagging is carried out using the altered output values. This is repeated until a specified noise level is reached. We give the background theory, and test the method using both trees and nearest neighbor regression methods. Application to two class classification data gives some interesting results."
            },
            "slug": "USING-ADAPTIVE-BAGGING-TO-DEBIAS-REGRESSIONS-Breiman",
            "title": {
                "fragments": [],
                "text": "USING ADAPTIVE BAGGING TO DEBIAS REGRESSIONS"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A new form of bagging that is effective in reducing both bias and variance and applied to two class classification data gives some interesting results."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341779"
                        ],
                        "name": "J. R. Quinlan",
                        "slug": "J.-R.-Quinlan",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Quinlan",
                            "middleNames": [
                                "Ross"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. R. Quinlan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "uncanny ability to improve prediction accuracy [ Quinlan (1996) , Bauer and"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Ratsch, Onoda and Muller (2000), \u201cRecent studies with highly noisy patterns [ Quinlan (1996) , Grove and Schuurmans (1998), Ratsch (1998)] showed that"
                    },
                    "intents": []
                }
            ],
            "corpusId": 17589883,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7e1d105273bae6a9c0fe37c5cd709490282fe83a",
            "isKey": false,
            "numCitedBy": 112,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Several empirical studies have confirmed that boosting classifier-learning systems can lead to substantial improvements in predictive accuracy. This paper reports early experimental results from applying boosting to ffoil, a first-order system that constructs definitions of functional relations. Although the evidence is less convincing than that for propositional-level learning systems, it suggests that boosting will also prove beneficial for first-order induction."
            },
            "slug": "Boosting-First-Order-Learning-Quinlan",
            "title": {
                "fragments": [],
                "text": "Boosting First-Order Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Early experimental results from applying boosting to ffoil, a first-order system that constructs definitions of functional relations, suggest that boosting will also prove beneficial for first- order induction."
            },
            "venue": {
                "fragments": [],
                "text": "ALT"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3444394"
                        ],
                        "name": "E. Ziegel",
                        "slug": "E.-Ziegel",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Ziegel",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Ziegel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "By fitting an additive model of different and potentially simple functions, it expands the class of functions that can be approximated."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 43
                            }
                        ],
                        "text": "In particular, linear logistic regression [McCullagh and Nelder (1989), e.g.] and additive logistic regression [Hastie and Tibshirani (1990)] are popular."
                    },
                    "intents": []
                }
            ],
            "corpusId": 7218290,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "2b461250c014b460e7c97b6138a3ee811f198f43",
            "isKey": false,
            "numCitedBy": 11571,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "This is the \u008e rst book on generalized linear models written by authors not mostly associated with the biological sciences. Subtitled \u201cWith Applications in Engineering and the Sciences,\u201d this book\u2019s authors all specialize primarily in engineering statistics. The \u008e rst author has produced several recent editions of Walpole, Myers, and Myers (1998), the last reported by Ziegel (1999). The second author has had several editions of Montgomery and Runger (1999), recently reported by Ziegel (2002). All of the authors are renowned experts in modeling. The \u008e rst two authors collaborated on a seminal volume in applied modeling (Myers and Montgomery 2002), which had its recent revised edition reported by Ziegel (2002). The last two authors collaborated on the most recent edition of a book on regression analysis (Montgomery, Peck, and Vining (2001), reported by Gray (2002), and the \u008e rst author has had multiple editions of his own regression analysis book (Myers 1990), the latest of which was reported by Ziegel (1991). A comparable book with similar objectives and a more speci\u008e c focus on logistic regression, Hosmer and Lemeshow (2000), reported by Conklin (2002), presumed a background in regression analysis and began with generalized linear models. The Preface here (p. xi) indicates an identical requirement but nonetheless begins with 100 pages of material on linear and nonlinear regression. Most of this will probably be a review for the readers of the book. Chapter 2, \u201cLinear Regression Model,\u201d begins with 50 pages of familiar material on estimation, inference, and diagnostic checking for multiple regression. The approach is very traditional, including the use of formal hypothesis tests. In industrial settings, use of p values as part of a risk-weighted decision is generally more appropriate. The pedagologic approach includes formulas and demonstrations for computations, although computing by Minitab is eventually illustrated. Less-familiar material on maximum likelihood estimation, scaled residuals, and weighted least squares provides more speci\u008e c background for subsequent estimation methods for generalized linear models. This review is not meant to be disparaging. The authors have packed a wealth of useful nuggets for any practitioner in this chapter. It is thoroughly enjoyable to read. Chapter 3, \u201cNonlinear Regression Models,\u201d is arguably less of a review, because regression analysis courses often give short shrift to nonlinear models. The chapter begins with a great example on the pitfalls of linearizing a nonlinear model for parameter estimation. It continues with the effective balancing of explicit statements concerning the theoretical basis for computations versus the application and demonstration of their use. The details of maximum likelihood estimation are again provided, and weighted and generalized regression estimation are discussed. Chapter 4 is titled \u201cLogistic and Poisson Regression Models.\u201d Logistic regression provides the basic model for generalized linear models. The prior development for weighted regression is used to motivate maximum likelihood estimation for the parameters in the logistic model. The algebraic details are provided. As in the development for linear models, some of the details are pushed into an appendix. In addition to connecting to the foregoing material on regression on several occasions, the authors link their development forward to their following chapter on the entire family of generalized linear models. They discuss score functions, the variance-covariance matrix, Wald inference, likelihood inference, deviance, and overdispersion. Careful explanations are given for the values provided in standard computer software, here PROC LOGISTIC in SAS. The value in having the book begin with familiar regression concepts is clearly realized when the analogies are drawn between overdispersion and nonhomogenous variance, or analysis of deviance and analysis of variance. The authors rely on the similarity of Poisson regression methods to logistic regression methods and mostly present illustrations for Poisson regression. These use PROC GENMOD in SAS. The book does not give any of the SAS code that produces the results. Two of the examples illustrate designed experiments and modeling. They include discussion of subset selection and adjustment for overdispersion. The mathematic level of the presentation is elevated in Chapter 5, \u201cThe Family of Generalized Linear Models.\u201d First, the authors unify the two preceding chapters under the exponential distribution. The material on the formal structure for generalized linear models (GLMs), likelihood equations, quasilikelihood, the gamma distribution family, and power functions as links is some of the most advanced material in the book. Most of the computational details are relegated to appendixes. A discussion of residuals returns one to a more practical perspective, and two long examples on gamma distribution applications provide excellent guidance on how to put this material into practice. One example is a contrast to the use of linear regression with a log transformation of the response, and the other is a comparison to the use of a different link function in the previous chapter. Chapter 6 considers generalized estimating equations (GEEs) for longitudinal and analogous studies. The \u008e rst half of the chapter presents the methodology, and the second half demonstrates its application through \u008e ve different examples. The basis for the general situation is \u008e rst established using the case with a normal distribution for the response and an identity link. The importance of the correlation structure is explained, the iterative estimation procedure is shown, and estimation for the scale parameters and the standard errors of the coef\u008e cients is discussed. The procedures are then generalized for the exponential family of distributions and quasi-likelihood estimation. Two of the examples are standard repeated-measures illustrations from biostatistical applications, but the last three illustrations are all interesting reworkings of industrial applications. The GEE computations in PROC GENMOD are applied to account for correlations that occur with multiple measurements on the subjects or restrictions to randomizations. The examples show that accounting for correlation structure can result in different conclusions. Chapter 7, \u201cFurther Advances and Applications in GLM,\u201d discusses several additional topics. These are experimental designs for GLMs, asymptotic results, analysis of screening experiments, data transformation, modeling for both a process mean and variance, and generalized additive models. The material on experimental designs is more discursive than prescriptive and as a result is also somewhat theoretical. Similar comments apply for the discussion on the quality of the asymptotic results, which wallows a little too much in reports on various simulation studies. The examples on screening and data transformations experiments are again reworkings of analyses of familiar industrial examples and another obvious motivation for the enthusiasm that the authors have developed for using the GLM toolkit. One can hope that subsequent editions will similarly contain new examples that will have caused the authors to expand the material on generalized additive models and other topics in this chapter. Designating myself to review a book that I know I will love to read is one of the rewards of being editor. I read both of the editions of McCullagh and Nelder (1989), which was reviewed by Schuenemeyer (1992). That book was not fun to read. The obvious enthusiasm of Myers, Montgomery, and Vining and their reliance on their many examples as a major focus of their pedagogy make Generalized Linear Models a joy to read. Every statistician working in any area of applied science should buy it and experience the excitement of these new approaches to familiar activities."
            },
            "slug": "Generalized-Linear-Models-Ziegel",
            "title": {
                "fragments": [],
                "text": "Generalized Linear Models"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "This is the \u008e rst book on generalized linear models written by authors not mostly associated with the biological sciences, and it is thoroughly enjoyable to read."
            },
            "venue": {
                "fragments": [],
                "text": "Technometrics"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784682"
                        ],
                        "name": "T. Hastie",
                        "slug": "T.-Hastie",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Hastie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Hastie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761784"
                        ],
                        "name": "R. Tibshirani",
                        "slug": "R.-Tibshirani",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Tibshirani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Tibshirani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 189,
                                "start": 161
                            }
                        ],
                        "text": "By considering all of the classes simultaneously, the symmetric multiclass model is better able to take advantage of simple pairwise boundaries when they exist [Hastie and Tibshirani (1998)]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 18204444,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "af3ba188294d059af6a66be9d591612ecc872b65",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "We discuss a strategy for polychotomous classi cation that involves estimating class probabilities for each pair of classes and then cou pling the estimates together The coupling model is similar to the Bradley Terry method for paired comparisons We study the na ture of the class probability estimates that arise and examine the performance of the procedure in real and simulated datasets Clas si ers used include linear discriminants nearest neighbors and the support vector machine"
            },
            "slug": "Classi-cation-by-Pairwise-Coupling-Hastie-Tibshirani",
            "title": {
                "fragments": [],
                "text": "Classi cation by Pairwise Coupling"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784682"
                        ],
                        "name": "T. Hastie",
                        "slug": "T.-Hastie",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Hastie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Hastie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761784"
                        ],
                        "name": "R. Tibshirani",
                        "slug": "R.-Tibshirani",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Tibshirani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Tibshirani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 189,
                                "start": 172
                            }
                        ],
                        "text": "By considering all of the classes simultaneously, the symmetric multiclass model is better able to take advantage of simple pairwise boundaries when they exist [Hastie and Tibshirani (1998)]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 8895862,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "17880e1de5fe2826bec3b7e34daba2e77d43b4f9",
            "isKey": false,
            "numCitedBy": 53,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose general procedures for posterior sampling from additive and generalized additive models. The procedure is a stochastic generalization of the well-known backfitting algorithm for fitting additive models. One chooses a linear operator (\u201csmoother\u201d) for each predictor, and the algorithm requires only the application of the operator and its square root. The procedure is general and modular, and we describe its application to nonparametric, semiparametric and mixed models."
            },
            "slug": "Bayesian-Backfitting-Hastie-Tibshirani",
            "title": {
                "fragments": [],
                "text": "Bayesian Backfitting"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "The procedure is a stochastic generalization of the well-known backfitting algorithm for fitting additive models and describes its application to nonparametric, semiparametric and mixed models."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117118"
                        ],
                        "name": "S. Sperlich",
                        "slug": "S.-Sperlich",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Sperlich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sperlich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69932841"
                        ],
                        "name": "J. Zelinka",
                        "slug": "J.-Zelinka",
                        "structuredName": {
                            "firstName": "Ji\u00e9r\u00ed",
                            "lastName": "Zelinka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Zelinka"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 203,
                                "start": 175
                            }
                        ],
                        "text": "If we knew that the decision boundary could be well approximated by an additive function, we would feel more comfortable by fitting an additive model (with backfitting) as in Hastie and Tibshirani (1990), rather than boosting stumps."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 0
                            }
                        ],
                        "text": "Hastie and Tibshirani (1990) use a similar derivation of the local scoring algorithm used in fitting generalized additive models."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 66
                            }
                        ],
                        "text": "A generalized version of backfitting (2), called \u201cLocal Scoring\u201d in Hastie and Tibshirani (1990), can be used to fit the additive logistic model by maximum likelihood."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "By fitting an additive model of different and potentially simple functions, it expands the class of functions that can be approximated."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "By Jerome Friedman,1 Trevor Hastie2 3 and Robert Tibshirani2 4"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 99
                            }
                        ],
                        "text": "This is just \u201cfitting of residuals\u201d and is commonly used in linear regression and additive modeling [Hastie and Tibshirani (1990)]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 112
                            }
                        ],
                        "text": "In particular, linear logistic regression [McCullagh and Nelder (1989), e.g.] and additive logistic regression [Hastie and Tibshirani (1990)] are popular."
                    },
                    "intents": []
                }
            ],
            "corpusId": 61047344,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "87083d0fd9fb1af9c7171b26c1653f835b2bb9a3",
            "isKey": true,
            "numCitedBy": 353,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "In Chapter 8 we discussed additive models (AM) of the form \n \n$$ E(Y|X) = c + \\sum\\limits_{\\alpha = 1}^d {g_\\alpha (x_\\alpha )} . $$ \n \n(1) \n \nNote that we put EY = c and E(g \u03b1 (X \u03b1 ) = 0 for identification."
            },
            "slug": "Generalized-Additive-Models-Sperlich-Zelinka",
            "title": {
                "fragments": [],
                "text": "Generalized Additive Models"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "In Chapter 8 the authors discussed additive models (AM) of the form E(Y|X) = c + \\sum\\limits_{alpha = 1}^d {g_alpha (x_\\alpha )} ."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703537"
                        ],
                        "name": "Y. Freund",
                        "slug": "Y.-Freund",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Freund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 0
                            }
                        ],
                        "text": "Freund and Schapire (1996b) and Schapire and Singer (1998) provide some theory to support their algorithms, in the form of upper bounds on generalization error."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 54
                            }
                        ],
                        "text": "This led to the more adaptive and realistic AdaBoost [Freund and Schapire (1996b)] and its offspring, where this assumption was dropped."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 103
                            }
                        ],
                        "text": "Result 1 requires minor modifications to accommodate f x \u2208 R, as in the generalized AdaBoost algorithms [Freund and Schapire (1996b), Schapire and Singer (1998)]; the estimate for cm differs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 69
                            }
                        ],
                        "text": "Other theories attempting to explain boosting come from game theory [Freund and Schapire (1996a), Breiman (1997)] and VC theory [Schapire, Freund, Bartlett and Lee (1998)]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 98
                            }
                        ],
                        "text": "Results 1 and 2 show that both Discrete and Real AdaBoost, as well as the Generalized AdaBoost of Freund and Schapire (1996b), can be motivated as iterative algorithms for optimizing the (population based) exponential criterion."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 50
                            }
                        ],
                        "text": "A generalization of Discrete AdaBoost appeared in Freund and Schapire (1996b), and was developed further in Schapire and Singer (1998), that uses real-valued \u201cconfidence-rated\u201d predictions rather than the \u22121 1 of Discrete AdaBoost."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 133
                            }
                        ],
                        "text": "The base classifier in Discrete AdaBoost produces a classification rule fm x \u2192 \u22121 1 , where is the domain of the predictive features x. Freund and Schapire (1996b), Breiman (1998a) and Schapire and Singer (1998) have suggested various modifications to improve the boosting algorithms."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 31
                            }
                        ],
                        "text": "It is also much faster computationally, making it more suitable to large-scale data mining applications."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1638095,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "888c09de60ce427669fe5a264fa3e787803eb9d2",
            "isKey": true,
            "numCitedBy": 397,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the close connections between game theory, on-line prediction and boosting. After a brief review of game theory, we describe an algorithm for learning to play repeated games based on the on-line prediction methods of Littlestone and Warmuth. The analysis of this algorithm yields a simple proof of von Neumann\u2019s famous minmax theorem, as well as a provable method of approximately solving a game. We then show that the on-line prediction model is obtained by applying this gameplaying algorithm to an appropriate choice of game and that boosting is obtained by applying the same algorithm to the \u201cdual\u201d of this game."
            },
            "slug": "Game-theory,-on-line-prediction-and-boosting-Freund-Schapire",
            "title": {
                "fragments": [],
                "text": "Game theory, on-line prediction and boosting"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "An algorithm for learning to play repeated games based on the on-line prediction methods of Littlestone and Warmuth is described, which yields a simple proof of von Neumann\u2019s famous minmax theorem, as well as a provable method of approximately solving a game."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '96"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741124"
                        ],
                        "name": "L. Valiant",
                        "slug": "L.-Valiant",
                        "structuredName": {
                            "firstName": "Leslie",
                            "lastName": "Valiant",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Valiant"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 93
                            }
                        ],
                        "text": "Schapire (1990) developed the first simple boosting procedure in the PAC-learning framework [Valiant (1984), Kearns and Vazirani (1994)]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 35
                            }
                        ],
                        "text": "Boosting works by sequentially applying a classification algorithm to reweighted versions of the training data and then taking a weighted majority vote of the sequence of classifiers thus produced."
                    },
                    "intents": []
                }
            ],
            "corpusId": 59712,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "10ddb646feddc12337b5a755c72e153e37088c02",
            "isKey": false,
            "numCitedBy": 4184,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Humans appear to be able to learn new concepts without needing to be programmed explicitly in any conventional sense. In this paper we regard learning as the phenomenon of knowledge acquisition in the absence of explicit programming. We give a precise methodology for studying this phenomenon from a computational viewpoint. It consists of choosing an appropriate information gathering mechanism, the learning protocol, and exploring the class of concepts that can be learnt using it in a reasonable (polynomial) number of steps. We find that inherent algorithmic complexity appears to set serious limits to the range of concepts that can be so learnt. The methodology and results suggest concrete principles for designing realistic learning systems."
            },
            "slug": "A-theory-of-the-learnable-Valiant",
            "title": {
                "fragments": [],
                "text": "A theory of the learnable"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper regards learning as the phenomenon of knowledge acquisition in the absence of explicit programming, and gives a precise methodology for studying this phenomenon from a computational viewpoint."
            },
            "venue": {
                "fragments": [],
                "text": "STOC '84"
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5162221"
                        ],
                        "name": "Ilaria Dimatteo",
                        "slug": "Ilaria-Dimatteo",
                        "structuredName": {
                            "firstName": "Ilaria",
                            "lastName": "Dimatteo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilaria Dimatteo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2264370"
                        ],
                        "name": "C. Genovese",
                        "slug": "C.-Genovese",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Genovese",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Genovese"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1840562"
                        ],
                        "name": "R. Kass",
                        "slug": "R.-Kass",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Kass",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kass"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16373571,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "4e5e0335247e077dc26496960cd172c205885151",
            "isKey": false,
            "numCitedBy": 446,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a Bayesian method, for fitting curves to data drawn from an exponential family, that uses splines for which the number and locations of knots are free parameters. The method uses reversible jump Markov chain Monte Carlo to change the knot configurations and a locality heuristic to speed up mixing. For nonnormal models, we approximate the integrated likelihood ratios needed to compute acceptance probabilities by using the Bayesian information criterion, BIC, under priors that make this approximation accurate. Our technique is based on a marginalised chain on the knot number and locations, but we provide methods for inference about the regression coefficients, and functions of them, in both normal and nonnormal models. Simulation results suggest that the method performs well, and we illustrate the method in two neuroscience applications."
            },
            "slug": "Bayesian-curve-fitting-with-free-knot-splines-Dimatteo-Genovese",
            "title": {
                "fragments": [],
                "text": "Bayesian curve-fitting with free-knot splines"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "A Bayesian method, for fitting curves to data drawn from an exponential family, that uses splines for which the number and locations of knots are free parameters, which performs well and is illustrated in two neuroscience applications."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746242"
                        ],
                        "name": "S. Mallat",
                        "slug": "S.-Mallat",
                        "structuredName": {
                            "firstName": "St\u00e9phane",
                            "lastName": "Mallat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mallat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109019649"
                        ],
                        "name": "Zhifeng Zhang",
                        "slug": "Zhifeng-Zhang",
                        "structuredName": {
                            "firstName": "Zhifeng",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhifeng Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 2
                            }
                        ],
                        "text": "2 Schapire & Singer (1998) give the interpretation that the weights are updated to make the new weighted problem maximally di cult for the next weak learner."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 29
                            }
                        ],
                        "text": "This is the approach used by Mallat and Zhang (1993) in \u201cmatching pursuit,\u201d where the b x \u03b3 are selected from an over-complete dictionary of wavelet bases."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14427335,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2210a7157565422261b03cf2cdf4e91b583df5a0",
            "isKey": false,
            "numCitedBy": 8850,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors introduce an algorithm, called matching pursuit, that decomposes any signal into a linear expansion of waveforms that are selected from a redundant dictionary of functions. These waveforms are chosen in order to best match the signal structures. Matching pursuits are general procedures to compute adaptive signal representations. With a dictionary of Gabor functions a matching pursuit defines an adaptive time-frequency transform. They derive a signal energy distribution in the time-frequency plane, which does not include interference terms, unlike Wigner and Cohen class distributions. A matching pursuit isolates the signal structures that are coherent with respect to a given dictionary. An application to pattern extraction from noisy signals is described. They compare a matching pursuit decomposition with a signal expansion over an optimized wavepacket orthonormal basis, selected with the algorithm of Coifman and Wickerhauser see (IEEE Trans. Informat. Theory, vol. 38, Mar. 1992). >"
            },
            "slug": "Matching-pursuits-with-time-frequency-dictionaries-Mallat-Zhang",
            "title": {
                "fragments": [],
                "text": "Matching pursuits with time-frequency dictionaries"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "The authors introduce an algorithm, called matching pursuit, that decomposes any signal into a linear expansion of waveforms that are selected from a redundant dictionary of functions, chosen in order to best match the signal structures."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Signal Process."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056361"
                        ],
                        "name": "J. Friedman",
                        "slug": "J.-Friedman",
                        "structuredName": {
                            "firstName": "Jerome",
                            "lastName": "Friedman",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Friedman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1684300"
                        ],
                        "name": "W. Stuetzle",
                        "slug": "W.-Stuetzle",
                        "structuredName": {
                            "firstName": "Werner",
                            "lastName": "Stuetzle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Stuetzle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 95
                            }
                        ],
                        "text": "Interestingly, one popular method for joint fitting, the backfitting algorithm, was devised by Friedman and Stuetzle (1981) to more closely achieve joint optimality in the otherwise greedy fitting procedure of projection pursuit regression (PPR)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 27
                            }
                        ],
                        "text": "The backfitting algorithm [Friedman and Stuetzle (1981), Buja, Hastie and Tibshirani (1989)] is a convenient modular \u201cGauss\u2013Seidel\u201d algorithm for fitting additive models."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14183758,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "589b8659007e1124f765a5d1bd940b2bf4d79054",
            "isKey": false,
            "numCitedBy": 2177,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract A new method for nonparametric multiple regression is presented. The procedure models the regression surface as a sum of general smooth functions of linear combinations of the predictor variables in an iterative manner. It is more general than standard stepwise and stagewise regression procedures, does not require the definition of a metric in the predictor space, and lends itself to graphical interpretation."
            },
            "slug": "Projection-Pursuit-Regression-Friedman-Stuetzle",
            "title": {
                "fragments": [],
                "text": "Projection Pursuit Regression"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38857503"
                        ],
                        "name": "J. Heikkinen",
                        "slug": "J.-Heikkinen",
                        "structuredName": {
                            "firstName": "Juha",
                            "lastName": "Heikkinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Heikkinen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 19024808,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "e5f77874647d8e6b792c6f8a24b6c00ccb34858b",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter describes a nonparametric Bayesian approach to the estimation of curves and surfaces that act as parameters in statistical models. The approach is based on mixing variable dimensional piecewise constant approximations, whose \u2018smoothness\u2019 is regulated by a Markov random field prior. Random partitions of the domain are defined by Voronoi tessellations of random generating point patterns. Variable dimension Markov chain Monte Carlo methods are proposed for the numerical estimation, and a detailed algorithm is specified for one special case. General applicability of the approach is discussed in the context of density estimation, regression and interpolation problems, and an application to the intensity estimation for a spatial Poisson point process is presented."
            },
            "slug": "Curve-and-Surface-Estimation-Using-Dynamic-Step-Heikkinen",
            "title": {
                "fragments": [],
                "text": "Curve and Surface Estimation Using Dynamic Step Functions"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "This chapter describes a nonparametric Bayesian approach to the estimation of curves and surfaces that act as parameters in statistical models based on mixing variable dimensional piecewise constant approximations, whose \u2018smoothness\u2019 is regulated by a Markov random field prior."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056642528"
                        ],
                        "name": "M. Kearns",
                        "slug": "M.-Kearns",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Kearns",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kearns"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46753437"
                        ],
                        "name": "U. Vazirani",
                        "slug": "U.-Vazirani",
                        "structuredName": {
                            "firstName": "Umesh",
                            "lastName": "Vazirani",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Vazirani"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 109
                            }
                        ],
                        "text": "Schapire (1990) developed the first simple boosting procedure in the PAC-learning framework [Valiant (1984), Kearns and Vazirani (1994)]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 51
                            }
                        ],
                        "text": "Boosting works by sequentially applying a classification algorithm to reweighted versions of the training data and then taking a weighted majority vote of the sequence of classifiers thus produced."
                    },
                    "intents": []
                }
            ],
            "corpusId": 44944785,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "97e14147f2e61456bba016f720488410393f9e48",
            "isKey": false,
            "numCitedBy": 1786,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The probably approximately correct learning model Occam's razor the Vapnik-Chervonenkis dimension weak and strong learning learning in the presence of noise inherent unpredictability reducibility in PAC learning learning finite automata by experimentation appendix - some tools for probabilistic analysis."
            },
            "slug": "An-Introduction-to-Computational-Learning-Theory-Kearns-Vazirani",
            "title": {
                "fragments": [],
                "text": "An Introduction to Computational Learning Theory"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "The probably approximately correct learning model Occam's razor the Vapnik-Chervonenkis dimension weak and strong learning learning in the presence of noise inherent unpredictability reducibility in PAC learning learning finite automata is described."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744288"
                        ],
                        "name": "P. Gill",
                        "slug": "P.-Gill",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Gill",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Gill"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143873253"
                        ],
                        "name": "W. Murray",
                        "slug": "W.-Murray",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Murray",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Murray"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685738"
                        ],
                        "name": "M. H. Wright",
                        "slug": "M.-H.-Wright",
                        "structuredName": {
                            "firstName": "Margaret",
                            "lastName": "Wright",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. H. Wright"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 20611582,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b8d9abd1c078573188b13d36c1b1efb7cb2fa865",
            "isKey": false,
            "numCitedBy": 7626,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Practical Optimization MethodsFree eBook: Practical Aspects of Structural Optimization [1701.01450] Practical optimization for hybrid quantum Practical Optimization | 4e70c9cf5faf796993a61adc9f46f2c5Acces PDF Practical OptimizationPractical Bayesian Optimization of Machine Learning Particle Swarm Optimization (PSO) An Overview Practical Issues Optimization Algorithms in Physics Practical Mathematical Optimization Universit T BremenA Practical Price Optimization Approach for Omnichannel A Gentle Introduction to Stochastic Optimization AlgorithmsApplied Sciences | Free Full-Text | Evolutionary 0387986316 Practical Optimization Methods: with A Lecture on Model Predictive ControlPractical Optimization : Algorithms and Engineering Wiley Series in Discrete Mathematics and Optimization Ser PRACTICAL OPTIMIZATION uCozEvolutionary practical optimization | DeepDyveA Practical Guide To Hyperparameter Optimization.Blood platelet production: a novel approach for practical [PDF] Practical Bilevel Optimization Download and Read Stability and Sample-based Approximations of Composite Practical portfolio optimization in Python (2/3) machine (PDF) Practical Financial Optimization. Decision making A Multiobjective Optimization Model for Prevention and Particle swarm optimization WikipediaPractical Methods Of Optimization|RPractical Portfolio Optimization London Business SchoolBao: Making Learned Query Optimization PracticalApache Spark Core Practical Optimization DatabricksPractical Methods of Optimization by R. FletcherChapter 11 Nonlinear Optimization Examples4.7 Applied Optimization Problems \u2013 Calculus Volume 1Practical bayesian optimization using Goptuna | by Masashi Practical Optimization Methods For 4th Generation Cellular Facility location problems \u2014 Mathematical Optimization Practical optimization (2004 edition) | Open Library[J726.Ebook] PDF Download Practical Optimization of Multi-objective Exploration for Practical Optimization Practical Optimization: a Gentle Introduction has moved!?Practical Rod Pumping Optimization on Apple Books(PDF) Practical Optimization with MATLAB The Free StudyPractical portfolio optimization in Python (3/3) code (PDF) Practical, Fast and Robust Point Cloud Registration Numerical Optimization Stanford UniversityPractical Optimization Methods with Mathematica ApplicationsPractical Optimization | 4e70c9cf5faf796993a61adc9f46f2c5Search Engine Optimization: Practical Marketing TechniquesLagout.orgMeter Placement in Active Distribution System using Manual: Practical guide to optimization for mobiles Unity"
            },
            "slug": "Practical-optimization-Gill-Murray",
            "title": {
                "fragments": [],
                "text": "Practical optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "This ebook Practical Optimization by Philip E. Gill is presented in pdf format and the full version of this ebook in DjVu, ePub, doc, txt, PDF forms is presented."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1423933025"
                        ],
                        "name": "C. A. Roberts",
                        "slug": "C.-A.-Roberts",
                        "structuredName": {
                            "firstName": "Clark",
                            "lastName": "Roberts",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. A. Roberts"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 208217294,
            "fieldsOfStudy": [],
            "id": "01f71050daa8940df85213618e14834895f360a5",
            "isKey": false,
            "numCitedBy": 128,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "tarian limitations. In these manuals, \"military necessity\" is denned and discussed, and its limitations recognized. I note that Mr. Parks has criticized the codification of the principle of proportionality in Protocol I. Proportionality, however, is an essential ingredient of the principle of military necessity and is not so easily ignored. I hope that the revised military manuals will provide guidance on how the principle of proportionality is to be applied in the Conduct of hostilities. Another area of concern is Mr. Parks' disapproval of the regime established by Protocol I to protect military aircraft. The provisions for agreed flight plans and the resolutions establishing electronic and other means of identifying medical aircraft are necessitated by the fact that the Red Cross symbol is almost completely ineffective, given the striking distance of modern weapons. MR. PARKS: I will offer a few observations on the issue of proportionality. First, the United States is looking at the practice of states with regard to this concept. This review is not limited to but nevertheless emphasizes examination of the concept of proportionality as it applies to civilian casualties. The picture that is emerging is that proportionality is not a hard and fast rule. Proportionality as a principle is easier to apply on the theater level than on lower levels because of the greater flexibility in selecting and defining the military objective to be obtained. Proportionality aids the attacked, and unscrupulous defenders frequently have played upon our emphasis on proportionality by using their civilians as a shield from attack in the attempt to gain a military advantage. Despite the ambiguities attendant in this principle, and the potential for abuse, however, proportionality does play an important role in military planning. As a participant in the planning for the Libyan air raids, I can attest that proportionality did play an important role not only in selecting targets but in establishing attack parameters."
            },
            "slug": "Discussion-*-Roberts",
            "title": {
                "fragments": [],
                "text": "Discussion *"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the ASIL Annual Meeting"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105451749"
                        ],
                        "name": "SwitzerlandBin YuBell",
                        "slug": "SwitzerlandBin-YuBell",
                        "structuredName": {
                            "firstName": "SwitzerlandBin",
                            "lastName": "YuBell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "SwitzerlandBin YuBell"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 93
                            }
                        ],
                        "text": "We use \u201csub-bagging\u201d for computational efficiency and it has a performance similar to bagging [Bu\u0308hlmann and Yu (2000)]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 172
                            }
                        ],
                        "text": "In the context of boosting L2 regression, we compare boosting with another ensemble scheme, bagging [Breiman (1996)], for which we have gained some understanding recently [Bu\u0308hlmann and Yu (2000)]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 56
                            }
                        ],
                        "text": "The precise (asymptotic) smoothing function is given in Bu\u0308hlmann and Yu (2000), which also characterizes its variance reduction effect in simple yet canonical cases."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 143
                            }
                        ],
                        "text": "Boosting is done with 6 and bag-boosting with 4 iterations.\nexplained by the fact that a bagged stump is a smooth rather than a step function [Bu\u0308hlmann and Yu (2000)]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 234,
                                "start": 212
                            }
                        ],
                        "text": "Instead of a classical bagging step, we actually use \u201csub-bagging\u201d or bagging on subsamples of size [n/2] (resampling without replacement) which is computationally cheaper while still being as accurate as bagging [Bu\u0308hlmann and Yu (2000)]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 19
                            }
                        ],
                        "text": "From our own work [Bu\u0308hlmann and Yu (2000)] we know that stumps evaluated at x have high variances for x in a whole region of the covariate space."
                    },
                    "intents": []
                }
            ],
            "corpusId": 9947235,
            "fieldsOfStudy": [],
            "id": "4255a8a0619ddb1247632e8b1c5c4d096596869a",
            "isKey": true,
            "numCitedBy": 17,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Explaining-Bagging-YuBell",
            "title": {
                "fragments": [],
                "text": "Explaining Bagging"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 36
                            }
                        ],
                        "text": "The distinction becomes less clear [Breiman (1998a)] when boosting is implemented by finite weighted random sampling instead of weighted optimization."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 139
                            }
                        ],
                        "text": "In fact, Breiman (1996) (referring to a NIPS workshop) called AdaBoost with trees the \u201cbest off-the-shelf classifier in the world\u201d [see also Breiman (1998b)]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1807,
                                "start": 18
                            }
                        ],
                        "text": "Discrete AdaBoost(Freund & Schapire 1996) 1. Start with weights wi = 1=N , i = 1; : : : ; N . 2. Repeat for m = 1; 2; : : : ;M : (a) Fit the classi er fm(x) 2 f 1; 1g using weights wi on the training data. (b) Compute em = Ew[1(y 6=fm(x))], cm = log((1 em)=em). (c) Set wi wi exp[cm 1(yi 6=fm(xi))]; i = 1; 2; : : : N , and renormalize so that Piwi = 1. 3. Output the classi er sign[PMm=1 cmfm(x)] Algorithm 1: Ew represents expectation over the training data with weights w = (w1; w2; : : : wn). At each iteration AdaBoost increases the weights of the observations misclassi ed by fm(x) by a factor that depends on the weighted training error. variance-reduction technique, and since trees tend to have high variance, bagging often produces good results. Early versions of AdaBoost used a resampling scheme to implement step 2 of Algorithm 1, by weighted importance sampling from the training data. This suggested a connection with bagging, and that a major component of the success of boosting has to do with variance reduction. However, boosting performs comparably well when: a weighted tree-growing algorithm is used in step 2 rather than weighted resampling, where each training observation is assigned its weight wi. This removes the randomization component essential in bagging. \\stumps\" are used for the weak learners. Stumps are single-split trees with only two terminal nodes. These typically have low variance but high bias. Bagging performs very poorly with stumps (Fig. 1[top-right panel].) These observations suggest that boosting is capable of both bias and variance reduction, and thus di ers fundamentally from bagging. The base classi er in Discrete AdaBoost produces a classi cation rule fm(x) : X 7! f 1; 1g, where X is the domain of the predictive features x. Freund & Schapire (1996), Breiman (1996b) and Schapire & Singer (1998) have suggested various modi cations to improve the boosting algorithms."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 48
                            }
                        ],
                        "text": "generalization of Discrete AdaBoost appeared in Freund & Schapire (1996), and was developed further in Schapire & Singer (1998), that uses realvalued \\con dence-rated\" predictions rather than the f 1; 1g of Discrete AdaBoost."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 162
                            }
                        ],
                        "text": "The base classifier in Discrete AdaBoost produces a classification rule fm x \u2192 \u22121 1 , where is the domain of the predictive features x. Freund and Schapire (1996b), Breiman (1998a) and Schapire and Singer (1998) have suggested various modifications to improve the boosting algorithms."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1728,
                                "start": 48
                            }
                        ],
                        "text": "generalization of Discrete AdaBoost appeared in Freund & Schapire (1996), and was developed further in Schapire & Singer (1998), that uses realvalued \\con dence-rated\" predictions rather than the f 1; 1g of Discrete AdaBoost. The weak learner for this generalized boosting produces a mapping fm(x) : X 7! R; the sign of fm(x) gives the classi cation, and jfm(x)j a measure of the \\con dence\" in the prediction. This real valued contribution is combined with the previous contributions with a multiplier cm as before, and a slightly di erent recipe for cm is provided. We present a generalized version of AdaBoost, which we call Real AdaBoost in Algorithm 2, in which the weak learner returns a class probability estimate pm(x) = P\u0302w(y = 1jx) 2 [0; 1]. The contribution to the nal classi er is half the logit-transform of this probability estimate. One form of Schapire and Singer's generalized AdaBoost coincides with Real AdaBoost, in the special case where the weak learner is a decision tree. Real AdaBoost tends to perform the best in our simulated examples in Fig. 1, especially with stumps, although we see with 100 node trees Discrete AdaBoost overtakes Real AdaBoost after 200 iterations. Real AdaBoost 1. Start with weights wi = 1=N , i = 1; 2; : : : ; N . 2. Repeat for m = 1; 2; : : : ;M : (a) Fit the class probability estimate pm(x) = P\u0302w(y = 1jx) 2 [0; 1] using weights wi on the training data. (b) Set fm(x) 1 2 log pm(x) 1 pm(x) 2 R. (c) Set wi wi exp[ yifm(xi)]; i = 1; 2; : : : N , and renormalize so that Pi wi = 1. 3. Output the classi er sign[PMm=1 fm(x)] Algorithm 2: The Real AdaBoost algorithm uses class probability estimates pm(x) to construct real-valued contributions fm(x). Freund & Schapire (1996) and Schapire & Singer (1998) provide some theory to support their algorithms, in the form of upper bounds on generalization error."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Combining predictors, Technical report, Statistics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 43
                            }
                        ],
                        "text": "Early versions of boosting \\weak learners\" (Schapire 1990) are far simpler 5"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 34
                            }
                        ],
                        "text": "The original boosting techniques [Schapire (1990), Freund (1995)] provably improved or \u201cboosted\u201d the performance of a single classifier by producing a \u201cmajority vote\u201d of similar classifiers."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 0
                            }
                        ],
                        "text": "Schapire (1990) developed the first simple boosting procedure in the PAC-learning framework [Valiant (1984), Kearns and Vazirani (1994)]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 12
                            }
                        ],
                        "text": "In addition Schapire & Singer (1998) motivate e yF (x) as a di erentiable upperbound to misclassi cation error 1[yF<0] (see Fig."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 12
                            }
                        ],
                        "text": "This theory (Schapire 1990) has evolved in the machine learning community, initially based on the concepts of PAC learning (Kearns & Vazirani 1994), and later from game theory (Freund 1995, Breiman 1997)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 71
                            }
                        ],
                        "text": "Boosting was proposed in the computational learning theory literature [Schapire (1990), Freund (1995), Freund and Schapire (1997)] and has since received much attention."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 16
                            }
                        ],
                        "text": "In this context Schapire & Singer (1998) de ne J responses yj for a J class problem, each taking values in f 1; 1g."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 134
                            }
                        ],
                        "text": "Despite its apparent simplicity, this approach does not appear to be in common use [although similar ideas have been proposed before: Schapire (1990), Freund (1995)]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 0
                            }
                        ],
                        "text": "Schapire & Singer (1998) provide several generalizations of AdaBoost for the multiclass case; we describe their AdaBoost."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "`The strength of weak learnability', Machine Learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 9
                            }
                        ],
                        "text": "In fact, Breiman (1996) (referring to a NIPS workshop) called AdaBoost with trees the \u201cbest off-the-shelf classifier in the world\u201d [see also Breiman (1998b)]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 101
                            }
                        ],
                        "text": "In the context of boosting L2 regression, we compare boosting with another ensemble scheme, bagging [Breiman (1996)], for which we have gained some understanding recently [Bu\u0308hlmann and Yu (2000)]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 72
                            }
                        ],
                        "text": "Note that other committee approaches to classification such as bagging [Breiman (1996)] and randomized trees [Dietterich (1998)] while admitting parallel implementations, cannot take advantage of this approach to reduce computation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 110
                            }
                        ],
                        "text": "A notable exception is when trees are used as the regression method, and in fact this is the approach used by Breiman et al. (1984). Logistic regression is a popular approach used in statistics for overcoming these problems."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 56
                            }
                        ],
                        "text": "2 Parts of this derivation for AdaBoost can be found in Breiman (1997) and Schapire & Singer (1998), but without making the connection to additive logistic regression models."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 204,
                                "start": 190
                            }
                        ],
                        "text": "The concepts developed in this paper suggest that there is very little, if any, connection between (deterministic) weighted boosting and other (randomized) ensemble methods such as bagging [Breiman (1996)] and randomized trees [Dietterich (1998)]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 26
                            }
                        ],
                        "text": "Freund & Schapire (1996), Breiman (1996b) and Schapire & Singer (1998) have suggested various modi cations to improve the boosting algorithms."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 43
                            }
                        ],
                        "text": "Included in the figure is the bagged tree [Breiman (1996)] which averages trees grown on bootstrap resampled versions of the training data."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "1996a), `Bagging predictors"
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 32
                            }
                        ],
                        "text": "Freund and Schapire (1996b) and Schapire and Singer (1998) provide some theory to support their algorithms, in the form of upper bounds on generalization error."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 0
                            }
                        ],
                        "text": "Schapire and Singer (1998) motivate e\u2212yF x as a differentiable upper bound\nto misclassification error 1 yF 0 (see Figure 2)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 16
                            }
                        ],
                        "text": "In this context Schapire and Singer (1998) define J responses yj for a J class problem, each taking values in \u22121 1 ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 132
                            }
                        ],
                        "text": "Result 1 requires minor modifications to accommodate f x \u2208 R, as in the generalized AdaBoost algorithms [Freund and Schapire (1996b), Schapire and Singer (1998)]; the estimate for cm differs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 1
                            }
                        ],
                        "text": "\u2737\nSchapire and Singer (1998) give the interpretation that the weights are updated to make the new weighted problem maximally difficult for the next weak learner."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 108
                            }
                        ],
                        "text": "This is a natural choice and is especially appropriate when observations can belong to more than one class [Schapire and Singer (1998)]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 108
                            }
                        ],
                        "text": "A generalization of Discrete AdaBoost appeared in Freund and Schapire (1996b), and was developed further in Schapire and Singer (1998), that uses real-valued \u201cconfidence-rated\u201d predictions rather than the \u22121 1 of Discrete AdaBoost."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 182
                            }
                        ],
                        "text": "The base classifier in Discrete AdaBoost produces a classification rule fm x \u2192 \u22121 1 , where is the domain of the predictive features x. Freund and Schapire (1996b), Breiman (1998a) and Schapire and Singer (1998) have suggested various modifications to improve the boosting algorithms."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 40
                            }
                        ],
                        "text": "\u2737\nThis exponential criterion appeared in Schapire and Singer (1998), motivated as an upper bound on misclassification error."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 0
                            }
                        ],
                        "text": "Schapire and Singer (1998) provide several generalizations of AdaBoost for the multiclass case, and also refer to other proposals [Freund and Schapire (1997), Schapire (1997)]; we describe their AdaBoost."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 73
                            }
                        ],
                        "text": "Parts of this derivation for AdaBoost can be found in Breiman (1997) and Schapire and Singer (1998), but without making the connection to additive logistic regression models."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Improved boosting algorithms using"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 0
                            }
                        ],
                        "text": "Freund and Schapire (1996b) and Schapire and Singer (1998) provide some theory to support their algorithms, in the form of upper bounds on generalization error."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 54
                            }
                        ],
                        "text": "This led to the more adaptive and realistic AdaBoost [Freund and Schapire (1996b)] and its offspring, where this assumption was dropped."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 103
                            }
                        ],
                        "text": "Result 1 requires minor modifications to accommodate f x \u2208 R, as in the generalized AdaBoost algorithms [Freund and Schapire (1996b), Schapire and Singer (1998)]; the estimate for cm differs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 69
                            }
                        ],
                        "text": "Other theories attempting to explain boosting come from game theory [Freund and Schapire (1996a), Breiman (1997)] and VC theory [Schapire, Freund, Bartlett and Lee (1998)]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 98
                            }
                        ],
                        "text": "Results 1 and 2 show that both Discrete and Real AdaBoost, as well as the Generalized AdaBoost of Freund and Schapire (1996b), can be motivated as iterative algorithms for optimizing the (population based) exponential criterion."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 50
                            }
                        ],
                        "text": "A generalization of Discrete AdaBoost appeared in Freund and Schapire (1996b), and was developed further in Schapire and Singer (1998), that uses real-valued \u201cconfidence-rated\u201d predictions rather than the \u22121 1 of Discrete AdaBoost."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 133
                            }
                        ],
                        "text": "The base classifier in Discrete AdaBoost produces a classification rule fm x \u2192 \u22121 1 , where is the domain of the predictive features x. Freund and Schapire (1996b), Breiman (1998a) and Schapire and Singer (1998) have suggested various modifications to improve the boosting algorithms."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Experiments with a new boosting"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 9
                            }
                        ],
                        "text": "In fact, Breiman (1996) (referring to a NIPS workshop) called AdaBoost with trees the \u201cbest off-the-shelf classifier in the world\u201d [see also Breiman (1998b)]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 101
                            }
                        ],
                        "text": "In the context of boosting L2 regression, we compare boosting with another ensemble scheme, bagging [Breiman (1996)], for which we have gained some understanding recently [Bu\u0308hlmann and Yu (2000)]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 72
                            }
                        ],
                        "text": "Note that other committee approaches to classification such as bagging [Breiman (1996)] and randomized trees [Dietterich (1998)] while admitting parallel implementations, cannot take advantage of this approach to reduce computation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 204,
                                "start": 190
                            }
                        ],
                        "text": "The concepts developed in this paper suggest that there is very little, if any, connection between (deterministic) weighted boosting and other (randomized) ensemble methods such as bagging [Breiman (1996)] and randomized trees [Dietterich (1998)]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The Annals of Statistics 2000, Vol. 28, No. 2, 337\u2013407\nSPECIAL INVITED PAPER"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 43
                            }
                        ],
                        "text": "Included in the figure is the bagged tree [Breiman (1996)] which averages trees grown on bootstrap resampled versions of the training data."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bagging predictors', Machine Learning 26"
            },
            "venue": {
                "fragments": [],
                "text": "Bagging predictors', Machine Learning 26"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 65
                            }
                        ],
                        "text": "Many practical problems involve comparatively simple boundaries [Holte (1993)]; in such cases performance differences will still be situation dependent, but correspondingly less pronounced."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 94
                            }
                        ],
                        "text": "Alternatively, it may be that their underlying decision boundaries are all relatively simple [Holte (1993)] so that all reasonable methods exhibit similar performance."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 88
                            }
                        ],
                        "text": "The fact that the boosting folks haven\u2019t hit on this the hard way is probably due to the Holte (1993) effect, cited a couple of times by FHT: \u201cVery simple classification rules perform well on most commonly used datasets.\u201d"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 97
                            }
                        ],
                        "text": "Alternatively, it may be that the their underlying decision boundaries are all relatively simple (Holte 1993) so that all reasonable methods exhibit similar performance."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 64
                            }
                        ],
                        "text": "Many practical problems involve comparatively simple boundaries (Holte 1993); in such cases performance di erences will still be situation dependent, but correspondingly less pronounced."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "`Very simple classi cation rules perform well on most"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 90
                            }
                        ],
                        "text": "\u201d Boosting was proposed in the computational learning theory literature [Schapire (1990), Freund (1995), Freund and Schapire (1997)] and has since received much attention."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 105
                            }
                        ],
                        "text": "Result 1 requires minor modifications to accommodate f x \u2208 R, as in the generalized AdaBoost algorithms [Freund and Schapire (1996b), Schapire and Singer (1998)]; the estimate for cm differs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 98
                            }
                        ],
                        "text": "Results 1 and 2 show that both Discrete and Real AdaBoost, as well as the Generalized AdaBoost of Freund and Schapire (1996b), can be motivated as iterative algorithms for optimizing the (population based) exponential criterion."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 151
                            }
                        ],
                        "text": "Despite its apparent simplicity, this approach does not appear to be in common use [although similar ideas have been proposed before: Schapire (1990), Freund (1995)]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 131
                            }
                        ],
                        "text": "Schapire and Singer (1998) provide several generalizations of AdaBoost for the multiclass case, and also refer to other proposals [Freund and Schapire (1997), Schapire (1997)]; we describe their AdaBoost."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "REFERENCES Breiman, L"
            },
            "venue": {
                "fragments": [],
                "text": "(1998). Arcing classifiers. Ann. Statist. 26 801\u2013849. Freund, Y. (1999). An adaptive version of the boost by majority algorithm. In Proceedings of the Twelfth Annual Conference on Computational Learning Theory."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143985550"
                        ],
                        "name": "Nagi B. Kumar",
                        "slug": "Nagi-B.-Kumar",
                        "structuredName": {
                            "firstName": "Nagi",
                            "lastName": "Kumar",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nagi B. Kumar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4083939"
                        ],
                        "name": "D. E. Bostow",
                        "slug": "D.-E.-Bostow",
                        "structuredName": {
                            "firstName": "Darrel",
                            "lastName": "Bostow",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. E. Bostow"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 107
                            }
                        ],
                        "text": "Note that other committee approaches to classi cation such as bagging (Breiman 1996a) and randomized trees (Dietterich 1998), while admitting parallel implementations, cannot take advantage of this approach to reduce computation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 110
                            }
                        ],
                        "text": "Note that other committee approaches to classification such as bagging [Breiman (1996)] and randomized trees [Dietterich (1998)] while admitting parallel implementations, cannot take advantage of this approach to reduce computation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 245,
                                "start": 228
                            }
                        ],
                        "text": "The concepts developed in this paper suggest that there is very little, if any, connection between (deterministic) weighted boosting and other (randomized) ensemble methods such as bagging [Breiman (1996)] and randomized trees [Dietterich (1998)]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 63974961,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "b41bfcad6e31a13a0c8850cadfac63ca4d6140c8",
            "isKey": false,
            "numCitedBy": 7,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "An-experimental-comparison-of-three-methods-of-in-Kumar-Bostow",
            "title": {
                "fragments": [],
                "text": "An experimental comparison of three methods of instruction in health education for cancer prevention: traditional paper prose text, passive non-interactive computer presentation and overt-interactive computer presentation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1927644"
                        ],
                        "name": "F. R. Forst",
                        "slug": "F.-R.-Forst",
                        "structuredName": {
                            "firstName": "Fred",
                            "lastName": "Forst",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. R. Forst"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 129
                            }
                        ],
                        "text": "However, other loss functions might have benefits, for example, tapered squared error based on Huber\u2019s robust influence function [Huber (1964)]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 61846277,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "c87d57da3b1f2b467ef4995d30df832ee2281107",
            "isKey": false,
            "numCitedBy": 3971,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-robust-estimation-of-the-location-parameter-Forst",
            "title": {
                "fragments": [],
                "text": "On robust estimation of the location parameter"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1980
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Boosting the mar"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 117
                            }
                        ],
                        "text": "More generally, one can consider an expansion of the decision boundary function in a functional ANOVA decomposition [Friedman (1991)]\nB x =\u2211 j fj xj + \u2211 j k fjk xj xk + \u2211 j k l fjkl xj xk xl + \u00b7 \u00b7 \u00b7 (48)\nThe first sum represents the closest function to B x that is additive in the original features,\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "interesting. It shares many common elements with MARS [ Friedman (1991) ],"
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Multivariate adaptive regression splines (with discussion)"
            },
            "venue": {
                "fragments": [],
                "text": "Ann. Statist"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian CART"
            },
            "venue": {
                "fragments": [],
                "text": "Bayesian CART"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Classiication and Regression Trees"
            },
            "venue": {
                "fragments": [],
                "text": "Classiication and Regression Trees"
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The state of boosting Interface Foundation of North America"
            },
            "venue": {
                "fragments": [],
                "text": "In Computing Science and Statistics"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 189,
                                "start": 161
                            }
                        ],
                        "text": "By considering all of the classes simultaneously, the symmetric multiclass model is better able to take advantage of simple pairwise boundaries when they exist [Hastie and Tibshirani (1998)]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Classiication by pairwise coupling"
            },
            "venue": {
                "fragments": [],
                "text": "Annals of Statistics"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 139
                            }
                        ],
                        "text": "In fact, Breiman (1996) (referring to a NIPS workshop) called AdaBoost with trees the \u201cbest off-the-shelf classifier in the world\u201d [see also Breiman (1998b)]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 162
                            }
                        ],
                        "text": "The base classifier in Discrete AdaBoost produces a classification rule fm x \u2192 \u22121 1 , where is the domain of the predictive features x. Freund and Schapire (1996b), Breiman (1998a) and Schapire and Singer (1998) have suggested various modifications to improve the boosting algorithms."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 36
                            }
                        ],
                        "text": "The distinction becomes less clear [Breiman (1998a)] when boosting is implemented by finite weighted random sampling instead of weighted optimization."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Arcing classifiers (with discussion)"
            },
            "venue": {
                "fragments": [],
                "text": "Ann. Statist"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 116
                            }
                        ],
                        "text": "This suggested a connection with bagging and that a major component of the success of boosting has to do with variance reduction."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Linear smoothers and additive models (with discussion)"
            },
            "venue": {
                "fragments": [],
                "text": "Ann. Statist"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "E-mail buhlmann@stat.math.ethz.ch Department of Statistics University of California Berkeley, California 94720-3860 REFERENCES Breimann Bagging predictors"
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 188,
                                "start": 173
                            }
                        ],
                        "text": "Even if the decision boundaries separating all class pairs are relatively simple, pooling classes can produce complex decision boundaries that are difficult to approximate [Friedman (1996)]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Another approach to polychotomous classification"
            },
            "venue": {
                "fragments": [],
                "text": "Another approach to polychotomous classification"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "`Flexible discriminant analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 117
                            }
                        ],
                        "text": "More generally, one can consider an expansion of the decision boundary function in a functional ANOVA decomposition [Friedman (1991)]\nB x =\u2211 j fj xj + \u2211 j k fjk xj xk + \u2211 j k l fjkl xj xk xl + \u00b7 \u00b7 \u00b7 (48)\nThe first sum represents the closest function to B x that is additive in the original features,\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 123
                            }
                        ],
                        "text": "More generally, one can consider an expansion of the of the decision boundary function in a functional ANOVA decomposition (Friedman 1991) B(x) =Xj fj(xj) +Xj;k fjk(xj ; xk) +X j;k;l fjkl(xj ; xk; xl) + ::: (46) The rst sum represents the closest function to B(x) that is additive in the original features, the rst two represent the closest approximation 33"
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "`Multivariate adaptive regression splines (with discus"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A computer program that plays a hunch"
            },
            "venue": {
                "fragments": [],
                "text": "A computer program that plays a hunch"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "With the extension of the FHT interpretation to generalized additive regression, it seems most natural to interpret stagewise LS regression as the regression version of boosting."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 188,
                                "start": 173
                            }
                        ],
                        "text": "Even if the decision boundaries separating all class pairs are relatively simple, pooling classes can produce complex decision boundaries that are difficult to approximate [Friedman (1996)]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Another approach to polychotomous classiication"
            },
            "venue": {
                "fragments": [],
                "text": "Another approach to polychotomous classiication"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian CART model search (with discussion )"
            },
            "venue": {
                "fragments": [],
                "text": "J. Amer. Statist. Assoc"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A computer program that plays a hunch"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Ensemble learning methods for classification"
            },
            "venue": {
                "fragments": [],
                "text": "Ensemble learning methods for classification"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 101
                            }
                        ],
                        "text": "Under fairly general conditions, back tting can be shown to converge to the minimizer of E(y F (x))2 (Buja et al. 1989)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "`Linear smoothers and additive"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Boosting and na\u00a8na\u00a8\u0131ve Bayes learning"
            },
            "venue": {
                "fragments": [],
                "text": "Boosting and na\u00a8na\u00a8\u0131ve Bayes learning"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 117
                            }
                        ],
                        "text": "More generally, one can consider an expansion of the decision boundary function in a functional ANOVA decomposition [Friedman (1991)]\nB x =\u2211 j fj xj + \u2211 j k fjk xj xk + \u2211 j k l fjkl xj xk xl + \u00b7 \u00b7 \u00b7 (48)\nThe first sum represents the closest function to B x that is additive in the original features,\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "interesting. It shares many common elements with MARS [ Friedman (1991) ],"
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Multivariate adaptive regression splines (with discussion)"
            },
            "venue": {
                "fragments": [],
                "text": "Ann. Statist"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 139
                            }
                        ],
                        "text": "In fact, Breiman (1996) (referring to a NIPS workshop) called AdaBoost with trees the \u201cbest off-the-shelf classifier in the world\u201d [see also Breiman (1998b)]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 162
                            }
                        ],
                        "text": "The base classifier in Discrete AdaBoost produces a classification rule fm x \u2192 \u22121 1 , where is the domain of the predictive features x. Freund and Schapire (1996b), Breiman (1998a) and Schapire and Singer (1998) have suggested various modifications to improve the boosting algorithms."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 36
                            }
                        ],
                        "text": "The distinction becomes less clear [Breiman (1998a)] when boosting is implemented by finite weighted random sampling instead of weighted optimization."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Combining predictors"
            },
            "venue": {
                "fragments": [],
                "text": "Combining predictors"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 170
                            }
                        ],
                        "text": "Even if the decision boundaries separating all class pairs are relatively simple, pooling classes can produce complex decision boundaries that are di cult to approximate (Friedman 1996)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 188,
                                "start": 173
                            }
                        ],
                        "text": "Even if the decision boundaries separating all class pairs are relatively simple, pooling classes can produce complex decision boundaries that are difficult to approximate [Friedman (1996)]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Another approach to polychotomous"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 76
                            }
                        ],
                        "text": "Test error for Bagging, Discrete AdaBoost and Real AdaBoost on a simulated two-class nested spheres problem (see Section 6)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 18
                            }
                        ],
                        "text": "We even teach our students that fitting one term at a time and not adjusting the others is not the way to fit a model; we teach them that changing one term in a model affects all others in a joint model fit, assuming that the joint model fit is the norm."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Classiication and Regression Trees, W adsworth"
            },
            "venue": {
                "fragments": [],
                "text": "Classiication and Regression Trees, W adsworth"
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "`A decision-theoretic generalization"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {},
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 72,
        "totalPages": 8
    },
    "page_url": "https://www.semanticscholar.org/paper/Special-Invited-Paper-Additive-logistic-regression:-Friedman/6f4493eff2531536a7aeb3fc11d62c30a8f487f6?sort=total-citations"
}