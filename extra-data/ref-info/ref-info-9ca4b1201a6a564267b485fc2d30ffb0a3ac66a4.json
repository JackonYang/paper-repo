{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2463974"
                        ],
                        "name": "Z. Saidane",
                        "slug": "Z.-Saidane",
                        "structuredName": {
                            "firstName": "Zohra",
                            "lastName": "Saidane",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Saidane"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144723337"
                        ],
                        "name": "Christophe Garcia",
                        "slug": "Christophe-Garcia",
                        "structuredName": {
                            "firstName": "Christophe",
                            "lastName": "Garcia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christophe Garcia"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 56
                            }
                        ],
                        "text": "Different from the previous methods, Saidane and Garcia [10] used a machine learning technique, convolutional neural networks, for segmentation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 42259700,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dcd810121ca82672a0c40f70ba60f59469ac5dab",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents an automatic segmentation system for characters in text color images cropped from natural images or videos based on a new neuronal architecture insuring fast processing and robustness against noise, variations in illumination, complex background and low resolution. An off-line training phase on a set of synthetic text color images, where the exact character positions are known, allows adjusting the neural parameters and thus building an optimal non linear filter which extracts the best features in order to robustly detect the border positions between characters. The proposed method is tested on a set of synthetic text images to precisely evaluate its performance according to noise, and on a set of complex text images collected from video frames and web pages to evaluate its performance on real images. The results are encouraging with a good segmentation rate of 89.12% and a recognition rate of 81.94% on a set of difficult text images collected from video frames and from web pages."
            },
            "slug": "An-Automatic-Method-for-Video-Character-Saidane-Garcia",
            "title": {
                "fragments": [],
                "text": "An Automatic Method for Video Character Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "An automatic segmentation system for characters in text color images cropped from natural images or videos based on a new neuronal architecture insuring fast processing and robustness against noise, variations in illumination, complex background and low resolution is presented."
            },
            "venue": {
                "fragments": [],
                "text": "ICIAR"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46422831"
                        ],
                        "name": "Xiaodong Huang",
                        "slug": "Xiaodong-Huang",
                        "structuredName": {
                            "firstName": "Xiaodong",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaodong Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144258295"
                        ],
                        "name": "Huadong Ma",
                        "slug": "Huadong-Ma",
                        "structuredName": {
                            "firstName": "Huadong",
                            "lastName": "Ma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huadong Ma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2153528457"
                        ],
                        "name": "He Zhang",
                        "slug": "He-Zhang",
                        "structuredName": {
                            "firstName": "He",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "He Zhang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 141
                            }
                        ],
                        "text": "Heuristic rules were proposed to further split and merge the segmented regions based on assumptions about the characters\u2019 widths and heights [6], [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 495763,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "854ebdee0457499c319c97169aa06890bdffa55d",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Video text provides precise and meaningful information about video content. Video text extraction is a crucial step to retrieve video text characters. Most of papers perform video text extraction in a whole text row. Compared with whole text row extraction, single character extraction can achieve higher accuracy because the background of single character is relatively simple. However, character segmentation is difficult because of the low resolution of the characters in video. Therefore, we propose a character segmentation method, which can accurately locate the character boundary in the text row. In the text extraction, we perform k-means clustering in single character and define a method to quickly select one of the images as the final binary image. Experimental results on TV news video show the encouraging performance of the proposed algorithm."
            },
            "slug": "A-new-video-text-extraction-approach-Huang-Ma",
            "title": {
                "fragments": [],
                "text": "A new video text extraction approach"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work proposes a character segmentation method, which can accurately locate the character boundary in the text row of video text extraction and defines a method to quickly select one of the images as the final binary image."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE International Conference on Multimedia and Expo"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744575"
                        ],
                        "name": "P. Shivakumara",
                        "slug": "P.-Shivakumara",
                        "structuredName": {
                            "firstName": "Palaiahnakote",
                            "lastName": "Shivakumara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Shivakumara"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715066"
                        ],
                        "name": "T. Phan",
                        "slug": "T.-Phan",
                        "structuredName": {
                            "firstName": "Trung",
                            "lastName": "Phan",
                            "middleNames": [
                                "Quy"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Phan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679749"
                        ],
                        "name": "C. Tan",
                        "slug": "C.-Tan",
                        "structuredName": {
                            "firstName": "Chew",
                            "lastName": "Tan",
                            "middleNames": [
                                "Lim"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Tan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 33
                            }
                        ],
                        "text": "We use our text detection method [2], which is capable of detecting both horizontal and non-horizontal text, to extract the text lines from a video frame."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 55
                            }
                        ],
                        "text": "Hence, in this paper, we use our text detection method [2] to get multi-oriented text lines in a video frame and focus on the fourth step, extraction and enhancement, to improve the performance of OCR for video text."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 90
                            }
                        ],
                        "text": "Since there is no standard dataset for video text, we have used our text detection method [2] to extract a variety of text lines from TRECVID videos [18], including news programmes, commercials and movie clips."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 196066575,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1755d99d89dee5915df1df4a7991b87138e93c78",
            "isKey": false,
            "numCitedBy": 296,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose a method based on the Laplacian in the frequency domain for video text detection. Unlike many other approaches which assume that text is horizontally-oriented, our method is able to handle text of arbitrary orientation. The input image is first filtered with Fourier-Laplacian. K-means clustering is then used to identify candidate text regions based on the maximum difference. The skeleton of each connected component helps to separate the different text strings from each other. Finally, text string straightness and edge density are used for false positive elimination. Experimental results show that the proposed method is able to handle graphics text and scene text of both horizontal and nonhorizontal orientation."
            },
            "slug": "A-Laplacian-Approach-to-Multi-Oriented-Text-in-Shivakumara-Phan",
            "title": {
                "fragments": [],
                "text": "A Laplacian Approach to Multi-Oriented Text Detection in Video"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Experimental results show that the proposed method is able to handle graphics text and scene text of both horizontal and nonhorizontal orientation."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9311673"
                        ],
                        "name": "S. Kopf",
                        "slug": "S.-Kopf",
                        "structuredName": {
                            "firstName": "Stephan",
                            "lastName": "Kopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276016"
                        ],
                        "name": "T. Haenselmann",
                        "slug": "T.-Haenselmann",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Haenselmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Haenselmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1750165"
                        ],
                        "name": "W. Effelsberg",
                        "slug": "W.-Effelsberg",
                        "structuredName": {
                            "firstName": "Wolfgang",
                            "lastName": "Effelsberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Effelsberg"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 33
                            }
                        ],
                        "text": "Kopf, Haenselmann and Effelsberg [8] used Dijkstra\u2019s algorithm to perform path finding from the top row to the bottom row of the input image."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 63
                            }
                        ],
                        "text": "For comparison purpose, we have implemented an existing method [8], denoted as Kopf\u2019s method."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 118964791,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e45a4c5a309abae017da667d0fd5690268456281",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Although OCR techniques work very reliably for high-resolution documents, the recognition of superimposed text in low-resolution images or videos with a complex background is still a challenge. Three major parts characterize our system for recognition of superimposed text in images and videos: localization of text regions, segmentation (binarization) of characters, and recognition. We use standard approaches to locate text regions and focus in this paper on the last two steps. Many approaches (e.g., projection profiles, k-mean clustering) do not work very well for separating characters with very small font sizes. We apply in a vertical direction a shortest-path algorithm to separate the characters in a text line. The recognition of characters is based on the curvature scale space (CSS) approach which smoothes the contour of a character with a Gaussian kernel and tracks its inflection points. A major drawback of the CSS method is its poor representation of convex segments: Convex objects cannot be represented at all due to missing inflection points. We have extended the CSS approach to generate feature points for concave and convex segments of a contour. This generic approach is not only applicable to text characters but to arbitrary objects as well. In the experimental results, we compare our approach against a pattern matching algorithm, two classification algorithms based on contour analysis, and a commercial OCR system. The overall recognition results are good enough even for the indexing of low resolution images and videos."
            },
            "slug": "Robust-Character-Recognition-in-Low-Resolution-and-Kopf-Haenselmann",
            "title": {
                "fragments": [],
                "text": "Robust Character Recognition in Low-Resolution Images and Videos"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper uses standard approaches to locate text regions, segmentation (binarization) of characters, and recognition of characters based on the curvature scale space approach to generate feature points for concave and convex segments of a contour."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144739319"
                        ],
                        "name": "R. Lienhart",
                        "slug": "R.-Lienhart",
                        "structuredName": {
                            "firstName": "Rainer",
                            "lastName": "Lienhart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lienhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32544716"
                        ],
                        "name": "Axel Wernicke",
                        "slug": "Axel-Wernicke",
                        "structuredName": {
                            "firstName": "Axel",
                            "lastName": "Wernicke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Axel Wernicke"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 76
                            }
                        ],
                        "text": "A common video character segmentation method is projection profile analysis [5]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 143774,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8387d4998f810cd2b60bd81545cb993087bc8788",
            "isKey": false,
            "numCitedBy": 467,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Many images, especially those used for page design on Web pages, as well as videos contain visible text. If these text occurrences could be detected, segmented, and recognized automatically, they would be a valuable source of high-level semantics for indexing and retrieval. We propose a novel method for localizing and segmenting text in complex images and videos. Text lines are identified by using a complex-valued multilayer feed-forward network trained to detect text at a fixed scale and position. The network's output at all scales and positions is integrated into a single text-saliency map, serving as a starting point for candidate text lines. In the case of video, these candidate text lines are refined by exploiting the temporal redundancy of text in video. Localized text lines are then scaled to a fixed height of 100 pixels and segmented into a binary image with black characters on white background. For videos, temporal redundancy is exploited to improve segmentation performance. Input images and videos can be of any size due to a true multiresolution approach. Moreover, the system is not only able to locate and segment text occurrences into large binary images, but is also able to track each text line with sub-pixel accuracy over the entire occurrence in a video, so that one text bitmap is created for all instances of that text line. Therefore, our text segmentation results can also be used for object-based video encoding such as that enabled by MPEG-4."
            },
            "slug": "Localizing-and-segmenting-text-in-images-and-videos-Lienhart-Wernicke",
            "title": {
                "fragments": [],
                "text": "Localizing and segmenting text in images and videos"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work proposes a novel method for localizing and segmenting text in complex images and videos that is not only able to locate and segment text occurrences into large binary images, but is also able to track each text line with sub-pixel accuracy over the entire occurrence in a video."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Circuits Syst. Video Technol."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784196"
                        ],
                        "name": "Datong Chen",
                        "slug": "Datong-Chen",
                        "structuredName": {
                            "firstName": "Datong",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Datong Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719610"
                        ],
                        "name": "J. Odobez",
                        "slug": "J.-Odobez",
                        "structuredName": {
                            "firstName": "Jean-Marc",
                            "lastName": "Odobez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Odobez"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17254211,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bcb4a08129301a35473c27b7092f08c1cc93ce4e",
            "isKey": false,
            "numCitedBy": 56,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Video-text-recognition-using-sequential-Monte-Carlo-Chen-Odobez",
            "title": {
                "fragments": [],
                "text": "Video text recognition using sequential Monte Carlo and error voting methods"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit. Lett."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145900372"
                        ],
                        "name": "Wonjun Kim",
                        "slug": "Wonjun-Kim",
                        "structuredName": {
                            "firstName": "Wonjun",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wonjun Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145568138"
                        ],
                        "name": "Changick Kim",
                        "slug": "Changick-Kim",
                        "structuredName": {
                            "firstName": "Changick",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Changick Kim"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10712944,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "61c41f1cea644ea2d65455f9c3277ffe3e35aff2",
            "isKey": false,
            "numCitedBy": 149,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Overlay text brings important semantic clues in video content analysis such as video information retrieval and summarization, since the content of the scene or the editor's intention can be well represented by using inserted text. Most of the previous approaches to extracting overlay text from videos are based on low-level features, such as edge, color, and texture information. However, existing methods experience difficulties in handling texts with various contrasts or inserted in a complex background. In this paper, we propose a novel framework to detect and extract the overlay text from the video scene. Based on our observation that there exist transient colors between inserted text and its adjacent background, a transition map is first generated. Then candidate regions are extracted by a reshaping method and the overlay text regions are determined based on the occurrence of overlay text in each candidate. The detected overlay text regions are localized accurately using the projection of overlay text pixels in the transition map and the text extraction is finally conducted. The proposed method is robust to different character size, position, contrast, and color. It is also language independent. Overlay text region update between frames is also employed to reduce the processing time. Experiments are performed on diverse videos to confirm the efficiency of the proposed method."
            },
            "slug": "A-New-Approach-for-Overlay-Text-Detection-and-From-Kim-Kim",
            "title": {
                "fragments": [],
                "text": "A New Approach for Overlay Text Detection and Extraction From Complex Video Scene"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This paper proposes a novel framework to detect and extract the overlay text from the video scene that is robust to different character size, position, contrast, and color, and is also language independent."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Image Processing"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2143720530"
                        ],
                        "name": "Jin Wang",
                        "slug": "Jin-Wang",
                        "structuredName": {
                            "firstName": "Jin",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jin Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2130202"
                        ],
                        "name": "J. Jean",
                        "slug": "J.-Jean",
                        "structuredName": {
                            "firstName": "Jack",
                            "lastName": "Jean",
                            "middleNames": [
                                "S.",
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Jean"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 73
                            }
                        ],
                        "text": "Inspired by a method for segmenting merged characters in document images [16], we formulate character segmentation as a minimum cost path finding problem where from the top row, it costs less to go through a gap and reach the bottom row than cutting through a character."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 42514009,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9023408aec11ec87fc420442a8e0396f5a172409",
            "isKey": false,
            "numCitedBy": 50,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Segmentation-of-merged-characters-by-neural-and-Wang-Jean",
            "title": {
                "fragments": [],
                "text": "Segmentation of merged characters by neural networks and shortest path"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "121267347"
                        ],
                        "name": "K. Jung",
                        "slug": "K.-Jung",
                        "structuredName": {
                            "firstName": "Keechul",
                            "lastName": "Jung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Jung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144602022"
                        ],
                        "name": "K. Kim",
                        "slug": "K.-Kim",
                        "structuredName": {
                            "firstName": "Kwang",
                            "lastName": "Kim",
                            "middleNames": [
                                "In"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 13
                            }
                        ],
                        "text": "According to [1], a video text detection and recognition system consists of five steps: (1) Detection, (2) Localization, (3) Tracking, (4) Extraction and enhancement, and (5) Recognition."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5999466,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cedf72be1fe814ef2ee9d65633dc3226f80f0785",
            "isKey": false,
            "numCitedBy": 936,
            "numCiting": 100,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Text-information-extraction-in-images-and-video:-a-Jung-Kim",
            "title": {
                "fragments": [],
                "text": "Text information extraction in images and video: a survey"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1795547"
                        ],
                        "name": "Bolan Su",
                        "slug": "Bolan-Su",
                        "structuredName": {
                            "firstName": "Bolan",
                            "lastName": "Su",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bolan Su"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1771189"
                        ],
                        "name": "Shijian Lu",
                        "slug": "Shijian-Lu",
                        "structuredName": {
                            "firstName": "Shijian",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shijian Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679749"
                        ],
                        "name": "C. Tan",
                        "slug": "C.-Tan",
                        "structuredName": {
                            "firstName": "Chew",
                            "lastName": "Tan",
                            "middleNames": [
                                "Lim"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Tan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 111
                            }
                        ],
                        "text": "To show that character segmentation helps to improve the recognition rate, we use a recent binarization method [17], which outperforms traditional methods such as Otsu\u2019s method and Niblack\u2019s method on the dataset of the Document Image Binarization Contest 2009, at two different levels: the text line level and the character level (i."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12214593,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "93eec9dfcdf1f60bde72704de367fae3a11bbde8",
            "isKey": false,
            "numCitedBy": 246,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a new document image binarization technique that segments the text from badly degraded historical document images. The proposed technique makes use of the image contrast that is defined by the local image maximum and minimum. Compared with the image gradient, the image contrast evaluated by the local maximum and minimum has a nice property that it is more tolerant to the uneven illumination and other types of document degradation such as smear. Given a historical document image, the proposed technique first constructs a contrast image and then detects the high contrast image pixels which usually lie around the text stroke boundary. The document text is then segmented by using local thresholds that are estimated from the detected high contrast pixels within a local neighborhood window. The proposed technique has been tested over the dataset that is used in the recent Document Image Binarization Contest (DIBCO) 2009. Experiments show its superior performance."
            },
            "slug": "Binarization-of-historical-document-images-using-Su-Lu",
            "title": {
                "fragments": [],
                "text": "Binarization of historical document images using the local maximum and minimum"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "A new document image binarization technique that segments the text from badly degraded historical document images by using local thresholds that are estimated from the detected high contrast pixels within a local neighborhood window."
            },
            "venue": {
                "fragments": [],
                "text": "DAS '10"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34896449"
                        ],
                        "name": "R. Casey",
                        "slug": "R.-Casey",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Casey",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Casey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794025"
                        ],
                        "name": "\u00c9. Lecolinet",
                        "slug": "\u00c9.-Lecolinet",
                        "structuredName": {
                            "firstName": "\u00c9ric",
                            "lastName": "Lecolinet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u00c9. Lecolinet"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 20
                            }
                        ],
                        "text": "Casey and Lecolinet [4] provided a comprehensive literature survey of character segmentation methods for document images."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2762290,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ef143fd02fc50dd93dacf1a805dd072bf3a0d71f",
            "isKey": false,
            "numCitedBy": 923,
            "numCiting": 143,
            "paperAbstract": {
                "fragments": [],
                "text": "Character segmentation has long been a critical area of the OCR process. The higher recognition rates for isolated characters vs. those obtained for words and connected character strings well illustrate this fact. A good part of recent progress in reading unconstrained printed and written text may be ascribed to more insightful handling of segmentation. This paper provides a review of these advances. The aim is to provide an appreciation for the range of techniques that have been developed, rather than to simply list sources. Segmentation methods are listed under four main headings. What may be termed the \"classical\" approach consists of methods that partition the input image into subimages, which are then classified. The operation of attempting to decompose the image into classifiable units is called \"dissection.\" The second class of methods avoids dissection, and segments the image either explicitly, by classification of prespecified windows, or implicitly by classification of subsets of spatial features collected from the image as a whole. The third strategy is a hybrid of the first two, employing dissection together with recombination rules to define potential segments, but using classification to select from the range of admissible segmentation possibilities offered by these subimages. Finally, holistic approaches that avoid segmentation by recognizing entire character strings as units are described."
            },
            "slug": "A-Survey-of-Methods-and-Strategies-in-Character-Casey-Lecolinet",
            "title": {
                "fragments": [],
                "text": "A Survey of Methods and Strategies in Character Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "H holistic approaches that avoid segmentation by recognizing entire character strings as units are described, including methods that partition the input image into subimages, which are then classified."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113759578"
                        ],
                        "name": "M. Mori",
                        "slug": "M.-Mori",
                        "structuredName": {
                            "firstName": "Minoru",
                            "lastName": "Mori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mori"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[3], is used instead of a commercial OCR."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14178864,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6e885c6c1d87fa111b742e5654fb46bd805c8922",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "When recognizing multiple fonts, geometric features,such as the directional information of strokes, are generallyrobust against deformation but are weak against degradation.This paper describes a category-dependent feature extractionmethod that uses a feature compensation techniqueto overcome this weakness. Our proposed method estimatesthe degree of degradation of an input pattern by comparingthe input pattern and the template of each category. Thisestimation enables us to compensate the degradation in featurevalues. We apply the proposed method to the recognitionof video text suffering from degradation and deformation.Recognition experiments using characters extractedfrom videos show that the proposed method is superior tothe conventional alternatives in resisting degradation."
            },
            "slug": "Video-text-recognition-using-feature-compensation-Mori",
            "title": {
                "fragments": [],
                "text": "Video text recognition using feature compensation as category-dependent feature extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A category-dependent feature extraction method that uses a feature compensation techniqueto overcome this weakness in strength against degradation and is superior to the conventional alternatives in resisting degradation."
            },
            "venue": {
                "fragments": [],
                "text": "Seventh International Conference on Document Analysis and Recognition, 2003. Proceedings."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2150022"
                        ],
                        "name": "Guangyi Miao",
                        "slug": "Guangyi-Miao",
                        "structuredName": {
                            "firstName": "Guangyi",
                            "lastName": "Miao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guangyi Miao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145377023"
                        ],
                        "name": "Guangyu Zhu",
                        "slug": "Guangyu-Zhu",
                        "structuredName": {
                            "firstName": "Guangyu",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guangyu Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696610"
                        ],
                        "name": "Shuqiang Jiang",
                        "slug": "Shuqiang-Jiang",
                        "structuredName": {
                            "firstName": "Shuqiang",
                            "lastName": "Jiang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shuqiang Jiang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689702"
                        ],
                        "name": "Qingming Huang",
                        "slug": "Qingming-Huang",
                        "structuredName": {
                            "firstName": "Qingming",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qingming Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145194969"
                        ],
                        "name": "Changsheng Xu",
                        "slug": "Changsheng-Xu",
                        "structuredName": {
                            "firstName": "Changsheng",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Changsheng Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2153576035"
                        ],
                        "name": "Wen Gao",
                        "slug": "Wen-Gao",
                        "structuredName": {
                            "firstName": "Wen",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wen Gao"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 146
                            }
                        ],
                        "text": "Heuristic rules were proposed to further split and merge the segmented regions based on assumptions about the characters\u2019 widths and heights [6], [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5804470,
            "fieldsOfStudy": [
                "Education",
                "Computer Science"
            ],
            "id": "88f9a88ad8fd330e84e3ba4e3bf830e02221bc01",
            "isKey": false,
            "numCitedBy": 7,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "For broadcast sports video, score information is an effective mid-level representation to facilitate high-level video content analysis. In this paper, we propose a real-time approach to detect score region and recognize scores in broadcast basketball sports video. The flow chart of our proposed approach has two major modules: score detection and score recognition. Using our approach, we can locate score regions automatically and reliably without detecting all the texts in the frame, and recognize the scores without training process. The running speed of our algorithm is fast."
            },
            "slug": "The-Demo:-A-Real-Time-Score-Detection-and-Approach-Miao-Zhu",
            "title": {
                "fragments": [],
                "text": "The Demo: A Real-Time Score Detection and Recognition Approach in Broadcast Basketball Sports Video"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A real-time approach to detect score region and recognize scores in broadcast basketball sports video is proposed and can locate score regions automatically and reliably without detecting all the texts in the frame, and recognize the scores without training process."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE International Conference on Multimedia and Expo"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1765604"
                        ],
                        "name": "Junaed Sattar",
                        "slug": "Junaed-Sattar",
                        "structuredName": {
                            "firstName": "Junaed",
                            "lastName": "Sattar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Junaed Sattar"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 104
                            }
                        ],
                        "text": "The propagation is done by minimizing the following energy functional:\n| | | | (1) where , , , , is the GVF field and , is the edge map of the input image [14]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 4
                            }
                        ],
                        "text": "GVF [14] is a popular method that is often used together with active contour [15] for non-rigid registration and motion tracking."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "GVF helps to overcome this problem by propagating the gradient information, i.e. the magnitude and the direction, into homogenous regions."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "GVF is able to detect pixels in the gaps between consecutive characters."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 23
                            }
                        ],
                        "text": "The first step employs GVF to identify pixels that are potentially part of non-vertical cuts."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 28
                            }
                        ],
                        "text": "On the other hand, by using GVF and backward path verification, the proposed method is able to stay as far as possible from the character edges (to allow room for errors) and remove the majority of the false cuts."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 74
                            }
                        ],
                        "text": "On the other hand, the proposed method defines the cost function based on GVF and also employs path verification."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 155
                            }
                        ],
                        "text": "The propagation is done by minimizing the following energy functional: | | | | (1) where , , , , is the GVF field and , is the edge map of the input image [14]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "GVF is used in a novel way to identify candidate cut pixels."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 20
                            }
                        ],
                        "text": "In other words, the GVF vector at pixel (x, y) should point to the left hand side, the GVF vector at pixel (x + 1, y) should point to the right hand side and the angle between these two vectors should be sufficiently large, e.g. 15 degrees."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 32
                            }
                        ],
                        "text": "In this paper, we propose using GVF in a novel way for character segmentation (instead of for registration or tracking)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 48
                            }
                        ],
                        "text": "The cost function employs gradient vector flow (GVF) to define the criteria of a good path based on the gradient information."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 33
                            }
                        ],
                        "text": "With this motivation, we use the GVF field to identify candidate cut pixels."
                    },
                    "intents": []
                }
            ],
            "corpusId": 17304911,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4e9498322979ee4aa286b7aed222240d789efd02",
            "isKey": true,
            "numCitedBy": 1813,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "A novel method for finding active contours, or snakes as developed by Xu and Prince [1] is presented in this paper. The approach uses a regularization based technique and calculus of variations to find what the authors call a Gradient Vector Field or GVF in binary-values or grayscale images. The GVF is in turn applied to \u2019pull\u2019 the snake towards the required feature. The approach presented here differs from other snake algorithms in its ability to extend into object concavities and its robust initialization technique. Although their algorithm works better than existing active contour algorithms, it suffers from computational complexity and associated costs in execution, resulting in slow execution time."
            },
            "slug": "Snakes-,-Shapes-and-Gradient-Vector-Flow-Sattar",
            "title": {
                "fragments": [],
                "text": "Snakes , Shapes and Gradient Vector Flow"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Although the authors\u2019 algorithm works better than existing active contour algorithms, it suffers from computational complexity and associated costs in execution, resulting in slow execution time."
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143602141"
                        ],
                        "name": "M. Kass",
                        "slug": "M.-Kass",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Kass",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kass"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1809905"
                        ],
                        "name": "A. Witkin",
                        "slug": "A.-Witkin",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Witkin",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Witkin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1750924"
                        ],
                        "name": "Demetri Terzopoulos",
                        "slug": "Demetri-Terzopoulos",
                        "structuredName": {
                            "firstName": "Demetri",
                            "lastName": "Terzopoulos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Demetri Terzopoulos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 77
                            }
                        ],
                        "text": "GVF [14] is a popular method that is often used together with active contour [15] for non-rigid registration and motion tracking."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12849354,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "9394a5d5adcb626128b6a42c8810b9505a3c6487",
            "isKey": false,
            "numCitedBy": 15501,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "A snake is an energy-minimizing spline guided by external constraint forces and influenced by image forces that pull it toward features such as lines and edges. Snakes are active contour models: they lock onto nearby edges, localizing them accurately. Scale-space continuation can be used to enlarge the capture region surrounding a feature. Snakes provide a unified account of a number of visual problems, including detection of edges, lines, and subjective contours; motion tracking; and stereo matching. We have used snakes successfully for interactive interpretation, in which user-imposed constraint forces guide the snake near features of interest."
            },
            "slug": "Snakes:-Active-contour-models-Kass-Witkin",
            "title": {
                "fragments": [],
                "text": "Snakes: Active contour models"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This work uses snakes for interactive interpretation, in which user-imposed constraint forces guide the snake near features of interest, and uses scale-space continuation to enlarge the capture region surrounding a feature."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 4,
            "methodology": 10
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 15,
        "totalPages": 2
    },
    "page_url": "https://www.semanticscholar.org/paper/A-Gradient-Vector-Flow-Based-Method-for-Video-Phan-Shivakumara/9ca4b1201a6a564267b485fc2d30ffb0a3ac66a4?sort=total-citations"
}