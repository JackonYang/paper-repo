{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792884"
                        ],
                        "name": "Charles M. Bishop",
                        "slug": "Charles-M.-Bishop",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Bishop",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charles M. Bishop"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2662552"
                        ],
                        "name": "M. Svens\u00e9n",
                        "slug": "M.-Svens\u00e9n",
                        "structuredName": {
                            "firstName": "Markus",
                            "lastName": "Svens\u00e9n",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Svens\u00e9n"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14042409,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "539cf46331ebdbb078914240af1e94accbbef32e",
            "isKey": false,
            "numCitedBy": 175,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "The Self-Organizing Map (SOM) algorithm has been extensively studied and has been applied with considerable success to a wide variety of problems. However, the algorithm is derived from heuristic ideas and this leads to a number of significant limitations. In this paper, we consider the problem of modelling the probability density of data in a space of several dimensions in terms of a smaller number of latent, or hidden, variables. We introduce a novel form of latent variable model, which we call the GTM algorithm (for Generative Topographic Mapping), which allows general non-linear transformations from latent space to data space, and which is trained using the EM (expectation-maximization) algorithm. Our approach overcomes the limitations of the SOM, while introducing no significant disadvantages. We demonstrate the performance of the GTM algorithm on simulated data from flow diagnostics for a multi-phase oil pipeline."
            },
            "slug": "GTM:-A-Principled-Alternative-to-the-Map-Bishop-Svens\u00e9n",
            "title": {
                "fragments": [],
                "text": "GTM: A Principled Alternative to the Self-Organizing Map"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A novel form of latent variable model is introduced, which is called the GTM algorithm (for Generative Topographic Mapping), which allows general non-linear transformations from latent space to data space, and which is trained using the EM (expectation-maximization) algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792884"
                        ],
                        "name": "Charles M. Bishop",
                        "slug": "Charles-M.-Bishop",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Bishop",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charles M. Bishop"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2662552"
                        ],
                        "name": "M. Svens\u00e9n",
                        "slug": "M.-Svens\u00e9n",
                        "structuredName": {
                            "firstName": "Markus",
                            "lastName": "Svens\u00e9n",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Svens\u00e9n"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2438937,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bba91dff07f67d95362c9ba8c358e60d0a27503f",
            "isKey": false,
            "numCitedBy": 61,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "The generative topographic mapping (GTM) algorithm of C.M. Bishop et al. (1996) has been introduced as a principled alternative to the self-organizing map (SOM). As well as avoiding a number of deficiencies in the SOM, the GTM algorithm has the key property that the smoothness properties of the model are decoupled from the reference vectors, and are described by a continuous mapping from a lower-dimensional latent space into the data space. Magnification factors, which are approximated by the difference between code-book vectors in SOMs, can therefore be evaluated for the GTM model as continuous functions of the latent variables using the techniques of differential geometry. They play an important role in data visualization by highlighting the boundaries between data clusters, and are illustrated here for both a toy data set, and a problem involving the identification of crab species from morphological data."
            },
            "slug": "Magnification-factors-for-the-GTM-algorithm-Bishop-Svens\u00e9n",
            "title": {
                "fragments": [],
                "text": "Magnification factors for the GTM algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "The generative topographic mapping (GTM) algorithm has the key property that the smoothness properties of the model are decoupled from the reference vectors, and are described by a continuous mapping from a lower-dimensional latent space into the data space."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792884"
                        ],
                        "name": "Charles M. Bishop",
                        "slug": "Charles-M.-Bishop",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Bishop",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charles M. Bishop"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2662552"
                        ],
                        "name": "M. Svens\u00e9n",
                        "slug": "M.-Svens\u00e9n",
                        "structuredName": {
                            "firstName": "Markus",
                            "lastName": "Svens\u00e9n",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Svens\u00e9n"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5951140,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "83f8a437d77f854049e09d53688a7af8dc61d44d",
            "isKey": false,
            "numCitedBy": 201,
            "numCiting": 64,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Developments-of-the-generative-topographic-mapping-Bishop-Svens\u00e9n",
            "title": {
                "fragments": [],
                "text": "Developments of the generative topographic mapping"
            },
            "venue": {
                "fragments": [],
                "text": "Neurocomputing"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113861474"
                        ],
                        "name": "C. Bishop",
                        "slug": "C.-Bishop",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Bishop",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Bishop"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2662552"
                        ],
                        "name": "M. Svens\u00e9n",
                        "slug": "M.-Svens\u00e9n",
                        "structuredName": {
                            "firstName": "Markus",
                            "lastName": "Svens\u00e9n",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Svens\u00e9n"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 140704080,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ea92efd922531e95ffa6569ad9d4db3a06c6c6ea",
            "isKey": false,
            "numCitedBy": 51,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Magnification factors specify the extent to which the area of a small patch of the latent (or `feature') space of a topographic mapping is magnified on projection to the data space, and are of considerable interest in both neuro-biological and data analysis contexts. Previous attempts to consider magnification factors for the self-organizing map (SOM) algorithm have been hindered because the mapping is only defined at discrete points (given by the reference vectors). In this paper we consider the batch version of SOM, for which a continuous mapping can be defined, as well as the Generative Topographic Mapping (GTM) algorithm of Bishop et al. (1997) which has been introduced as a probabilistic formulation of the SOM. We show how the techniques of differential geometry can be used to determine magnification factors as continuous functions of the latent space coordinates. The results are illustrated here using a problem involving the identification of crab species from morphological data."
            },
            "slug": "Magnification-factors-for-the-SOM-and-GTM-Bishop-Svens\u00e9n",
            "title": {
                "fragments": [],
                "text": "Magnification factors for the SOM and GTM algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This paper considers the batch version of SOM, for which a continuous mapping can be defined, as well as the Generative Topographic Mapping (GTM) algorithm which has been introduced as a probabilistic formulation of the SOM."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686971"
                        ],
                        "name": "T. Graepel",
                        "slug": "T.-Graepel",
                        "structuredName": {
                            "firstName": "Thore",
                            "lastName": "Graepel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Graepel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743272"
                        ],
                        "name": "K. Obermayer",
                        "slug": "K.-Obermayer",
                        "structuredName": {
                            "firstName": "Klaus",
                            "lastName": "Obermayer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Obermayer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 17783978,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1e43166400c7d128215433a6141190dacc1659af",
            "isKey": false,
            "numCitedBy": 95,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "We derive an efficient algorithm for topographic mapping of proximity data (TMP), which can be seen as an extension of Kohonen's self-organizing map to arbitrary distance measures. The TMP cost function is derived in a Baysian framework of folded Markov chains for the description of autoencoders. It incorporates the data by a dissimilarity matrix and the topographic neighborhood by a matrix of transition probabilities. From the principle of maximum entropy, a nonfactorizing Gibbs distribution is obtained, which is approximated in a mean-field fashion. This allows for maximum likelihood estimation using an expectation-maximization algorithm. In analogy to the transition from topographic vector quantization to the self-organizing map, we suggest an approximation to TMP that is computationally more efficient. In order to prevent convergence to local minima, an annealing scheme in the temperature parameter is introduced, for which the critical temperature of the first phase transition is calculated in terms of and . Numerical results demonstrate the working of the algorithm and confirm the analytical results. Finally, the algorithm is used to generate a connection map of areas of the cat's cerebral cortex."
            },
            "slug": "A-Stochastic-Self-Organizing-Map-for-Proximity-Data-Graepel-Obermayer",
            "title": {
                "fragments": [],
                "text": "A Stochastic Self-Organizing Map for Proximity Data"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "An efficient algorithm is derived for topographic mapping of proximity data (TMP), which can be seen as an extension of Kohonen's self-organizing map to arbitrary distance measures and is used to generate a connection map of areas of the cat's cerebral cortex."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30258243"
                        ],
                        "name": "H. Ritter",
                        "slug": "H.-Ritter",
                        "structuredName": {
                            "firstName": "Helge",
                            "lastName": "Ritter",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ritter"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 213,
                                "start": 200
                            }
                        ],
                        "text": "The technique of parametrized self-organizing maps (PSOMs) involves first fitting a standard SOM model to a data set and then finding a manifold in data space which interpolates the reference vectors (Ritter 1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 207,
                                "start": 195
                            }
                        ],
                        "text": "The technique of parameterized self-organizing maps (PSOMs) involves rst tting a standard SOM model to a data set and then nding a manifold in data space that interpolates the reference vectors (Ritter, 1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60593952,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "f1019624ad3cf37ac1e34c1204f3b0560c9bfe11",
            "isKey": false,
            "numCitedBy": 67,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "The Self-organizing Map creates a dimension reducing mapping from a feature space onto a non-linear map manifold. In the basic mapping algorithm, this mapping is discretized and for higher-dimensional map manifolds a very large number of reference vectors must be used to obtain a good resolution. To overcome this limitation, we present an approach in which the map manifold is built from a small set of basis manifolds. The resulting parametrized self-organizing maps (\u201cPSOM\u201d) require only very few reference vectors for their construction. They can be considered as recurrent networks with a continuous attractor manifold. We illustrate the approach with the construction of a PSOM for the six-dimensional configuration manifold of a puma manipulator, embedded in a 15-dimensional variable space."
            },
            "slug": "Parametrized-Self-Organizing-Maps-Ritter",
            "title": {
                "fragments": [],
                "text": "Parametrized Self-Organizing Maps"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work presents an approach in which the map manifold is built from a small set of basis manifolds, and the resulting parametrized self-organizing maps (\u201cPSOM\u201d) require only very few reference vectors for their construction."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744700"
                        ],
                        "name": "Zoubin Ghahramani",
                        "slug": "Zoubin-Ghahramani",
                        "structuredName": {
                            "firstName": "Zoubin",
                            "lastName": "Ghahramani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zoubin Ghahramani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 61577,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7a64be9fb3f1bce1dada1c32bfac42164cf3cdec",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We first describe a hierarchical, generative model that can be viewed as a non-linear generalisation of factor analysis and can be implemented in a neural network. The model performs perceptual inference in a probabilistically consistent manner by using top-down, bottom-up and lateral connections. These connections can be learned using simple rules that require only locally available information. We then show how to incorporate lateral connections into the generative model. The model extracts a sparse, distributed, hierarchical representation of depth from simplified random-dot stereograms and the localised disparity detectors in the first hidden layer form a topographic map. When presented with image patches from natural scenes, the model develops topographically organised local feature detectors."
            },
            "slug": "Hierarchical-Non-linear-Factor-Analysis-and-Maps-Ghahramani-Hinton",
            "title": {
                "fragments": [],
                "text": "Hierarchical Non-linear Factor Analysis and Topographic Maps"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "A hierarchical, generative model that can be viewed as a non-linear generalisation of factor analysis and can be implemented in a neural network that performs perceptual inference in a probabilistically consistent manner by using top-down, bottom-up and lateral connections."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1400347470"
                        ],
                        "name": "M. A. Carreira-Perpi\u00f1\u00e1n",
                        "slug": "M.-A.-Carreira-Perpi\u00f1\u00e1n",
                        "structuredName": {
                            "firstName": "Miguel",
                            "lastName": "Carreira-Perpi\u00f1\u00e1n",
                            "middleNames": [
                                "\u00c1."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. A. Carreira-Perpi\u00f1\u00e1n"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145086187"
                        ],
                        "name": "S. Renals",
                        "slug": "S.-Renals",
                        "structuredName": {
                            "firstName": "Steve",
                            "lastName": "Renals",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Renals"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6634936,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9d29c7802bc435ee5fbfdf930d53196cd63d473b",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Dimensionality-reduction-of-electropalatographic-Carreira-Perpi\u00f1\u00e1n-Renals",
            "title": {
                "fragments": [],
                "text": "Dimensionality reduction of electropalatographic data using latent variable models"
            },
            "venue": {
                "fragments": [],
                "text": "Speech Commun."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50859190"
                        ],
                        "name": "M. Girolami",
                        "slug": "M.-Girolami",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Girolami",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Girolami"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145683892"
                        ],
                        "name": "A. Cichocki",
                        "slug": "A.-Cichocki",
                        "structuredName": {
                            "firstName": "Andrzej",
                            "lastName": "Cichocki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Cichocki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10671312,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a47c9401a6a3fd49ba903ac9459dbd2bd1175514",
            "isKey": false,
            "numCitedBy": 55,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents the derivation of an unsupervised learning algorithm, which enables the identification and visualization of latent structure within ensembles of high-dimensional data. This provides a linear projection of the data onto a lower dimensional subspace to identify the characteristic structure of the observations independent latent causes. The algorithm is shown to be a very promising tool for unsupervised exploratory data analysis and data visualization. Experimental results confirm the attractiveness of this technique for exploratory data analysis and an empirical comparison is made with the recently proposed generative topographic mapping (GTM) and standard principal component analysis (PCA). Based on standard probability density models a generic nonlinearity is developed which allows both 1) identification and visualization of dichotomised clusters inherent in the observed data and 2) separation of sources with arbitrary distributions from mixtures, whose dimensionality may be greater than that of number of sources. The resulting algorithm is therefore also a generalized neural approach to independent component analysis (ICA) and it is considered to be a promising method for analysis of real-world data that will consist of sub- and super-Gaussian components such as biomedical signals."
            },
            "slug": "A-common-neural-network-model-for-unsupervised-data-Girolami-Cichocki",
            "title": {
                "fragments": [],
                "text": "A common neural-network model for unsupervised exploratory data analysis and independent component analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "The derivation of an unsupervised learning algorithm, which enables the identification and visualization of latent structure within ensembles of high-dimensional data, which is considered to be a promising method for analysis of real-world data that will consist of sub- and super-Gaussian components such as biomedical signals."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "92459994"
                        ],
                        "name": "Z. Meng",
                        "slug": "Z.-Meng",
                        "structuredName": {
                            "firstName": "Z",
                            "lastName": "Meng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Meng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144763324"
                        ],
                        "name": "Y. Pao",
                        "slug": "Y.-Pao",
                        "structuredName": {
                            "firstName": "Yoh-Han",
                            "lastName": "Pao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Pao"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 911678,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8da4d99cc683cb45c8aac777adfeba1d31165595",
            "isKey": false,
            "numCitedBy": 27,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "A new approach to dimension-reduction mapping of multidimensional pattern data is presented. The motivation for this work is to provide a computationally efficient method for visualizing large bodies of complex multidimensional data as a relatively \"topologically correct\" lower dimensional approximation. Examples of the use of this approach in obtaining meaningful two-dimensional (2-D) maps and comparisons with those obtained by the self-organizing map (SOM) and the neural-net implementation of Sammon's approach are also presented and discussed. In this method, the mapping equalizes and orthogonalizes the lower dimensional outputs by reducing the covariance matrix of the outputs to the form of a constant times the identity matrix. This new method is computationally efficient and \"topologically correct\" in interesting and useful ways."
            },
            "slug": "Visualization-and-self-organization-of-data-through-Meng-Pao",
            "title": {
                "fragments": [],
                "text": "Visualization and self-organization of multidimensional data through equalized orthogonal mapping"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "A new approach to dimension-reduction mapping of multidimensional pattern data that equalizes and orthogonalizes the lower dimensional outputs by reducing the covariance matrix of the outputs to the form of a constant times the identity matrix."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks Learn. Syst."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792884"
                        ],
                        "name": "Charles M. Bishop",
                        "slug": "Charles-M.-Bishop",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Bishop",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charles M. Bishop"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2831141"
                        ],
                        "name": "Michael E. Tipping",
                        "slug": "Michael-E.-Tipping",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Tipping",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael E. Tipping"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 59
                            }
                        ],
                        "text": "To see this, we rst use equation 4.7 to show that P j Fijw l(xj) D w l(xi)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2403387,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "de3ec11fe51e9abc6a6c1a639044ae5a70c43e72",
            "isKey": false,
            "numCitedBy": 240,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Visualization has proven to be a powerful and widely-applicable tool for the analysis and interpretation of multivariate data. Most visualization algorithms aim to find a projection from the data space down to a two-dimensional visualization space. However, for complex data sets living in a high-dimensional space, it is unlikely that a single two-dimensional projection can reveal all of the interesting structure. We therefore introduce a hierarchical visualization algorithm which allows the complete data set to be visualized at the top level, with clusters and subclusters of data points visualized at deeper levels. The algorithm is based on a hierarchical mixture of latent variable models, whose parameters are estimated using the expectation-maximization algorithm. We demonstrate the principle of the approach on a toy data set, and we then apply the algorithm to the visualization of a synthetic data set in 12 dimensions obtained from a simulation of multiphase flows in oil pipelines, and to data in 36 dimensions derived from satellite images."
            },
            "slug": "A-Hierarchical-Latent-Variable-Model-for-Data-Bishop-Tipping",
            "title": {
                "fragments": [],
                "text": "A Hierarchical Latent Variable Model for Data Visualization"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work introduces a hierarchical visualization algorithm which allows the complete data set to be visualized at the top level, with clusters and subclusters of data points visualization at deeper levels."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2716057"
                        ],
                        "name": "A. Utsugi",
                        "slug": "A.-Utsugi",
                        "structuredName": {
                            "firstName": "Akio",
                            "lastName": "Utsugi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Utsugi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10050011,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "373b98c92b0b2abe5ed628b360aa9ff82772f081",
            "isKey": false,
            "numCitedBy": 69,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "The self-organizing map (SOM) algorithm for finite data is derived as an approximate maximum a posteriori estimation algorithm for a gaussian mixture model with a gaussian smoothing prior, which is equivalent to a generalized deformable model (GDM). For this model, objective criteria for selecting hyperparameters are obtained on the basis of empirical Bayesian estimation and cross-validation, which are representative model selection methods. The properties of these criteria are compared by simulation experiments. These experiments show that the cross-validation methods favor more complex structures than the expected log likelihood supports, which is a measure of compatibility between a model and data distribution. On the other hand, the empirical Bayesian methods have the opposite bias."
            },
            "slug": "Hyperparameter-Selection-for-Self-Organizing-Maps-Utsugi",
            "title": {
                "fragments": [],
                "text": "Hyperparameter Selection for Self-Organizing Maps"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The self-organizing map (SOM) algorithm for finite data is derived as an approximate maximum a posteriori estimation algorithm for a gaussian mixture model with a Gaussian smoothing prior, which is equivalent to a generalized deformable model (GDM)."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792884"
                        ],
                        "name": "Charles M. Bishop",
                        "slug": "Charles-M.-Bishop",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Bishop",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charles M. Bishop"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2662552"
                        ],
                        "name": "M. Svens\u00e9n",
                        "slug": "M.-Svens\u00e9n",
                        "structuredName": {
                            "firstName": "Markus",
                            "lastName": "Svens\u00e9n",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Svens\u00e9n"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2857116,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b4ee1d186a3873bb0c4a72564bc0369110a38072",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "There is currently considerable interest in developing general nonlinear density models based on latent, or hidden, variables. Such models have the ability to discover the presence of a relatively small number of underlying 'causes' which, acting in combination, give rise to the apparent complexity of the observed data set. Unfortunately, to train such models generally requires large computational effort. In this paper we introduce a novel latent variable algorithm which retains the general non-linear capabilities of previous models but which uses a training procedure based on the EM algorithm. We demonstrate the performance of the model on a toy problem and on data from flow diagnostics for a multi-phase oil pipeline."
            },
            "slug": "EM-Optimization-of-Latent-Variables-Density-Models-Bishop-Svens\u00e9n",
            "title": {
                "fragments": [],
                "text": "EM Optimization of Latent-Variables Density Models"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper introduces a novel latent variable algorithm which retains the general non-linear capabilities of previous models but which uses a training procedure based on the EM algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694254"
                        ],
                        "name": "S. Luttrell",
                        "slug": "S.-Luttrell",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Luttrell",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Luttrell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 90
                            }
                        ],
                        "text": "In this context it has been shown how a reformulation of the vector quantization problem\n(Luttrell, 1990; Buhmann & Ku\u0308hnel 1993; Luttrell, 1994; Luttrell, 1995) can avoid many of the problems with the SOM procedure discussed earlier."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 0
                            }
                        ],
                        "text": "(Luttrell, 1990; Buhmann & K\u00fchnel 1993; Luttrell, 1994; Luttrell, 1995) can avoid many of the problems with the SOM procedure discussed earlier."
                    },
                    "intents": []
                }
            ],
            "corpusId": 9727118,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "46200b14c187e9e7450ad560c598b1af57b3a929",
            "isKey": false,
            "numCitedBy": 113,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "A novel derivation is presented of T. Kohonen's topographic mapping training algorithm (Self-Organization and Associative Memory, 1984), based upon an extension of the Linde-Buzo-Gray (LBG) algorithm for vector quantizer design. Thus a vector quantizer is designed by minimizing an L(2) reconstruction distortion measure, including an additional contribution from the effect of code noise which corrupts the output of the vector quantizer. The neighborhood updating scheme of Kohonen's topographic mapping training algorithm emerges as a special case of this code noise model. This formulation of Kohonen's algorithm is a specific instance of the robust hidden layer principle, which stabilizes the internal representations chosen by a network against anticipated noise or distortion processes."
            },
            "slug": "Derivation-of-a-class-of-training-algorithms-Luttrell",
            "title": {
                "fragments": [],
                "text": "Derivation of a class of training algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A novel derivation is presented of T. Kohonen's topographic mapping training algorithm, based upon an extension of the Linde-Buzo-Gray algorithm for vector quantizer design, which stabilizes the internal representations chosen by a network against anticipated noise or distortion processes."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790646"
                        ],
                        "name": "P. Dayan",
                        "slug": "P.-Dayan",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Dayan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Dayan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804104"
                        ],
                        "name": "R. Zemel",
                        "slug": "R.-Zemel",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Zemel",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Zemel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1890561,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "605402e235bd62437baf3c9ebefe77fb4d92ee95",
            "isKey": false,
            "numCitedBy": 1173,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "Discovering the structure inherent in a set of patterns is a fundamental aim of statistical inference or learning. One fruitful approach is to build a parameterized stochastic generative model, independent draws from which are likely to produce the patterns. For all but the simplest generative models, each pattern can be generated in exponentially many ways. It is thus intractable to adjust the parameters to maximize the probability of the observed patterns. We describe a way of finessing this combinatorial explosion by maximizing an easily computed lower bound on the probability of the observations. Our method can be viewed as a form of hierarchical self-supervised learning that may relate to the function of bottom-up and top-down cortical processing pathways."
            },
            "slug": "The-Helmholtz-Machine-Dayan-Hinton",
            "title": {
                "fragments": [],
                "text": "The Helmholtz Machine"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A way of finessing this combinatorial explosion by maximizing an easily computed lower bound on the probability of the observations is described, viewed as a form of hierarchical self-supervised learning that may relate to the function of bottom-up and top-down cortical processing pathways."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2716057"
                        ],
                        "name": "A. Utsugi",
                        "slug": "A.-Utsugi",
                        "structuredName": {
                            "firstName": "Akio",
                            "lastName": "Utsugi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Utsugi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11059447,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "235d2c71da7aa7a2c7325837fcbdd8808de72761",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "In the statistical approach for self-organizing maps (SOMs), learning is regarded as an estimation algorithm for a gaussian mixture model with a gaussian smoothing prior on the centroid parameters. The values of the hyperparameters and the topological structure are selected on the basis of a statistical principle. However, since the component selection probabilities are fixed to a common value, the centroids concentrate on areas with high data density. This deforms a coordinate system on an extracted manifold and makes smoothness evaluation for the manifold inaccurate. In this article, we study an extended SOM model whose component selection probabilities are variable. To stabilize the estimation, a smoothing prior on the component selection probabilities is introduced. An estimation algorithm for the parameters and the hyperparameters based on empirical Bayesian inference is obtained. The performance of density estimation by the new model and the SOM model is compared via simulation experiments."
            },
            "slug": "Density-Estimation-by-Mixture-Models-with-Smoothing-Utsugi",
            "title": {
                "fragments": [],
                "text": "Density Estimation by Mixture Models with Smoothing Priors"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "An extended SOM model whose component selection probabilities are variable is studied and an estimation algorithm for the parameters and the hyperparameters based on empirical Bayesian inference is obtained."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2000591"
                        ],
                        "name": "K. Kiviluoto",
                        "slug": "K.-Kiviluoto",
                        "structuredName": {
                            "firstName": "Kimmo",
                            "lastName": "Kiviluoto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kiviluoto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726997"
                        ],
                        "name": "E. Oja",
                        "slug": "E.-Oja",
                        "structuredName": {
                            "firstName": "Erkki",
                            "lastName": "Oja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Oja"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14058534,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8de9276a1444a5cb2f095b5d62d9a91831dd844a",
            "isKey": false,
            "numCitedBy": 27,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "The S-Map is a network with a simple learning algorithm that combines the self-organization capability of the Self-Organizing Map (SOM) and the probabilistic interpretability of the Generative Topographic Mapping (GTM). The simulations suggest that the S-Map algorithm has a stronger tendency to self-organize from random initial configuration than the GTM. The S-Map algorithm can be further simplified to employ pure Hebbian learning, without changing the qualitative behaviour of the network."
            },
            "slug": "S-Map:-A-Network-with-a-Simple-Self-Organization-Kiviluoto-Oja",
            "title": {
                "fragments": [],
                "text": "S-Map: A Network with a Simple Self-Organization Algorithm for Generative Topographic Mappings"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "The simulations suggest that the S-Map algorithm has a stronger tendency to self-organize from random initial configuration than the GTM."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2716057"
                        ],
                        "name": "A. Utsugi",
                        "slug": "A.-Utsugi",
                        "structuredName": {
                            "firstName": "Akio",
                            "lastName": "Utsugi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Utsugi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6611458,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "70c27536539cedb5d842be9d596940a37588bde1",
            "isKey": false,
            "numCitedBy": 10,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "A topology-selection method for self-organizing maps (SOMs) based on empirical Bayesian inference is presented. This method is a natural extension of the hyperparameter-selection method presented earlier, in which the SOM algorithm is regarded as an estimation algorithm for a Gaussian mixture model with a Gaussian smoothing prior on the centroid parameters, and optimal hyperparameters are obtained by maximizing their evidence. In the present paper, comparisons between models with different topologies are made possible by further specifying the prior of the centroid parameters with an additional hyperparameter. In addition, a fast hyperparameter-search algorithm using the derivatives of evidence is presented. The validity of the methods presented is confirmed by simulation experiments."
            },
            "slug": "Topology-selection-for-self-organizing-maps-Utsugi",
            "title": {
                "fragments": [],
                "text": "Topology selection for self-organizing maps"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "In the present paper, comparisons between models with different topologies are made possible by further specifying the prior of the centroid parameters with an additionalhyperparameter, and a fast hyperparameter-search algorithm using the derivatives of evidence is presented."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144902513"
                        ],
                        "name": "P. Baldi",
                        "slug": "P.-Baldi",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Baldi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Baldi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764952"
                        ],
                        "name": "K. Hornik",
                        "slug": "K.-Hornik",
                        "structuredName": {
                            "firstName": "Kurt",
                            "lastName": "Hornik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Hornik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14333248,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9552ac39a57daacf3d75865a268935b5a0df9bbb",
            "isKey": false,
            "numCitedBy": 1336,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Neural-networks-and-principal-component-analysis:-Baldi-Hornik",
            "title": {
                "fragments": [],
                "text": "Neural networks and principal component analysis: Learning from examples without local minima"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694254"
                        ],
                        "name": "S. Luttrell",
                        "slug": "S.-Luttrell",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Luttrell",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Luttrell"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 129
                            }
                        ],
                        "text": "In this context it has been shown how a reformulation of the vector quantization problem\n(Luttrell, 1990; Buhmann & Ku\u0308hnel 1993; Luttrell, 1994; Luttrell, 1995) can avoid many of the problems with the SOM procedure discussed earlier."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 0
                            }
                        ],
                        "text": "(Luttrell, 1990; Buhmann & K\u00fchnel 1993; Luttrell, 1994; Luttrell, 1995) can avoid many of the problems with the SOM procedure discussed earlier."
                    },
                    "intents": []
                }
            ],
            "corpusId": 9565103,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "76c19cb01ab0b07b12ca94bad29a416324a52575",
            "isKey": false,
            "numCitedBy": 153,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper Bayesian methods are used to analyze some of the properties of a special type of Markov chain. The forward transitions through the chain are followed by inverse transitions (using Bayes' theorem) backward through a copy of the same chain; this will be called a folded Markov chain. If an appropriately defined Euclidean error (between the original input and its reconstruction via Bayes' theorem) is minimized with respect to the choice of Markov chain transition probabilities, then the familiar theories of both vector quantizers and self-organizing maps emerge. This approach is also used to derive the theory of self-supervision, in which the higher layers of a multilayer network supervise the lower layers, even though overall there is no external teacher."
            },
            "slug": "A-Bayesian-Analysis-of-Self-Organizing-Maps-Luttrell",
            "title": {
                "fragments": [],
                "text": "A Bayesian Analysis of Self-Organizing Maps"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "Bayesian methods are used to analyze some of the properties of a special type of Markov chain and derive the theory of self-supervision, in which the higher layers of a multilayer network supervise the lower layers, even though overall there is no external teacher."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2436799"
                        ],
                        "name": "M. Kraaijveld",
                        "slug": "M.-Kraaijveld",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Kraaijveld",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kraaijveld"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4723637"
                        ],
                        "name": "J. Mao",
                        "slug": "J.-Mao",
                        "structuredName": {
                            "firstName": "Jianchang",
                            "lastName": "Mao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Mao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15443318,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1b14155e32679f77b7b0339d1f805575ecf0452b",
            "isKey": false,
            "numCitedBy": 248,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "A nonlinear projection method is presented to visualize high-dimensional data as a 2D image. The proposed method is based on the topology preserving mapping algorithm of Kohonen. The topology preserving mapping algorithm is used to train a 2D network structure. Then the interpoint distances in the feature space between the units in the network are graphically displayed to show the underlying structure of the data. Furthermore, we present and discuss a new method to quantify how well a topology preserving mapping algorithm maps the high-dimensional input data onto the network structure. This is used to compare our projection method with a well-known method of Sammon (1969). Experiments indicate that the performance of the Kohonen projection method is comparable or better than Sammon's method for the purpose of classifying clustered data. Its time-complexity only depends on the resolution of the output image, and not on the size of the dataset. A disadvantage, however, is the large amount of CPU time required."
            },
            "slug": "A-nonlinear-projection-method-based-on-Kohonen's-Kraaijveld-Mao",
            "title": {
                "fragments": [],
                "text": "A nonlinear projection method based on Kohonen's topology preserving maps"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Experiments indicate that the performance of the Kohonen projection method is comparable or better than Sammon's method for the purpose of classifying clustered data."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1815804"
                        ],
                        "name": "F. Mulier",
                        "slug": "F.-Mulier",
                        "structuredName": {
                            "firstName": "Filip",
                            "lastName": "Mulier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Mulier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145884505"
                        ],
                        "name": "V. Cherkassky",
                        "slug": "V.-Cherkassky",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Cherkassky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Cherkassky"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 18
                            }
                        ],
                        "text": "As pointed out by Mulier and Cherkassky (1995), the value of the neighborhood function hij(n) depends only on the identity of the winning node j and not on the value of the corresponding data vector tn."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 19477629,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "39759dccd71028b34da18c9b33d2df9fe4e73bd6",
            "isKey": false,
            "numCitedBy": 156,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Kohonen's self-organizing map, when described in a batch processing mode, can be interpreted as a statistical kernel smoothing problem. The batch SOM algorithm consists of two steps. First, the training data are partitioned according to the Voronoi regions of the map unit locations. Second, the units are updated by taking weighted centroids of the data falling into the Voronoi regions, with the weighing function given by the neighborhood. Then, the neighborhood width is decreased and steps 1, 2 are repeated. The second step can be interpreted as a statistical kernel smoothing problem where the neighborhood function corresponds to the kernel and neighborhood width corresponds to kernel span. To determine the new unit locations, kernel smoothing is applied to the centroids of the Voronoi regions in the topological space. This interpretation leads to some new insights concerning the role of the neighborhood and dimensionality reduction. It also strengthens the algorithm's connection with the Principal Curve algorithm. A generalized self-organizing algorithm is proposed, where the kernel smoothing step is replaced with an arbitrary nonparametric regression method."
            },
            "slug": "Self-Organization-as-an-Iterative-Kernel-Smoothing-Mulier-Cherkassky",
            "title": {
                "fragments": [],
                "text": "Self-Organization as an Iterative Kernel Smoothing Process"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "A generalized self-organizing algorithm is proposed, where the kernel smoothing step is replaced with an arbitrary nonparametric regression method, which strengthens the algorithm's connection with the Principal Curve algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9330607"
                        ],
                        "name": "S. Roweis",
                        "slug": "S.-Roweis",
                        "structuredName": {
                            "firstName": "Sam",
                            "lastName": "Roweis",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roweis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744700"
                        ],
                        "name": "Zoubin Ghahramani",
                        "slug": "Zoubin-Ghahramani",
                        "structuredName": {
                            "firstName": "Zoubin",
                            "lastName": "Ghahramani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zoubin Ghahramani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2590898,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "30755a7614148f1acf5edca72385832410c7c33a",
            "isKey": false,
            "numCitedBy": 992,
            "numCiting": 118,
            "paperAbstract": {
                "fragments": [],
                "text": "Factor analysis, principal component analysis, mixtures of gaussian clusters, vector quantization, Kalman filter models, and hidden Markov models can all be unified as variations of unsupervised learning under a single basic generative model. This is achieved by collecting together disparate observations and derivations made by many previous authors and introducing a new way of linking discrete and continuous state models using a simple nonlinearity. Through the use of other nonlinearities, we show how independent component analysis is also a variation of the same basic generative model. We show that factor analysis and mixtures of gaussians can be implemented in autoencoder neural networks and learned using squared error plus the same regularization term. We introduce a new model for static data, known as sensible principal component analysis, as well as a novel concept of spatially adaptive observation noise. We also review some of the literature involving global and local mixtures of the basic models and provide pseudocode for inference and learning for all the basic models."
            },
            "slug": "A-Unifying-Review-of-Linear-Gaussian-Models-Roweis-Ghahramani",
            "title": {
                "fragments": [],
                "text": "A Unifying Review of Linear Gaussian Models"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "A new model for static data is introduced, known as sensible principal component analysis, as well as a novel concept of spatially adaptive observation noise, which shows how independent component analysis is also a variation of the same basic generative model."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3332957"
                        ],
                        "name": "M. Khaikine",
                        "slug": "M.-Khaikine",
                        "structuredName": {
                            "firstName": "Maxim",
                            "lastName": "Khaikine",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Khaikine"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3144983"
                        ],
                        "name": "K. Holthausen",
                        "slug": "K.-Holthausen",
                        "structuredName": {
                            "firstName": "Klaus",
                            "lastName": "Holthausen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Holthausen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 27972302,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8a74d6d77a19ea920affd23861f54f49e17fc11c",
            "isKey": false,
            "numCitedBy": 2,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe an analytical framework for the adaptations of neural systems that adapt its internal structure on the basis of subjective probabilities constructed by computation of randomly received input signals. A principled approach is provided with the key property that it defines a probability density model that allows studying the convergence of the adaptation process. In particular, the derived algorithm can be applied for approximation problems such as the estimation of probability densitiesor the recognition of regression functions. These approximation algorithms can be easily extended to higher-dimensional cases. Certain neural network models can be derived from our approach (e.g., topological feature maps and associative networks)."
            },
            "slug": "A-General-Probability-Estimation-Approach-for-Khaikine-Holthausen",
            "title": {
                "fragments": [],
                "text": "A General Probability Estimation Approach for Neural Computation"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "An analytical framework for the adaptations of neural systems that adapt its internal structure on the basis of subjective probabilities constructed by computation of randomly received input signals with the key property that it defines a probability density model that allows studying the convergence of the adaptation process."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2831141"
                        ],
                        "name": "Michael E. Tipping",
                        "slug": "Michael-E.-Tipping",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Tipping",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael E. Tipping"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792884"
                        ],
                        "name": "Charles M. Bishop",
                        "slug": "Charles-M.-Bishop",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Bishop",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charles M. Bishop"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14159382,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "291c1274ee45bf78662ed60831ef1bdaf89bed94",
            "isKey": false,
            "numCitedBy": 1742,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Principal component analysis (PCA) is one of the most popular techniques for processing, compressing, and visualizing data, although its effectiveness is limited by its global linearity. While nonlinear variants of PCA have been proposed, an alternative paradigm is to capture data complexity by a combination of local linear PCA projections. However, conventional PCA does not correspond to a probability density, and so there is no unique way to combine PCA models. Therefore, previous attempts to formulate mixture models for PCA have been ad hoc to some extent. In this article, PCA is formulated within a maximum likelihood framework, based on a specific form of gaussian latent variable model. This leads to a well-defined mixture model for probabilistic principal component analyzers, whose parameters can be determined using an expectation-maximization algorithm. We discuss the advantages of this model in the context of clustering, density modeling, and local dimensionality reduction, and we demonstrate its application to image compression and handwritten digit recognition."
            },
            "slug": "Mixtures-of-Probabilistic-Principal-Component-Tipping-Bishop",
            "title": {
                "fragments": [],
                "text": "Mixtures of Probabilistic Principal Component Analyzers"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "PCA is formulated within a maximum likelihood framework, based on a specific form of gaussian latent variable model, which leads to a well-defined mixture model for probabilistic principal component analyzers, whose parameters can be determined using an expectation-maximization algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1705777"
                        ],
                        "name": "K. Haese",
                        "slug": "K.-Haese",
                        "structuredName": {
                            "firstName": "Karin",
                            "lastName": "Haese",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Haese"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 8213717,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d4e3c8f38d625f0c1815eedc337526f333217eaa",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents an extension of the self-organizing learning algorithm of feature maps in order to improve its convergence to neighborhood preserving maps. The Kohonen learning algorithm is controlled by two learning parameters, which have to be chosen empirically because there exists neither rules nor a method for their calculation. Consequently, often time consuming parameter studies have to precede before a neighborhood preserving feature map is obtained. To circumvent those lengthy numerical studies, here, a method is presented and incorporated into the learning algorithm which determines the learning parameters automatically. Therefore, system models of the learning and organizing process are developed in order to be followed and predicted by linear and extended Kalman filters. The learning parameters are optimal within the system models, so that the self-organizing process converges automatically to a neighborhood preserving feature map of the learning data."
            },
            "slug": "Self-organizing-feature-maps-with-self-adjusting-Haese",
            "title": {
                "fragments": [],
                "text": "Self-organizing feature maps with self-adjusting learning parameters"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "This paper presents an extension of the self-organizing learning algorithm of feature maps in order to improve its convergence to neighborhood preserving maps by incorporating a method into the learning algorithm which determines the learning parameters automatically."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2812486"
                        ],
                        "name": "P. Simard",
                        "slug": "P.-Simard",
                        "structuredName": {
                            "firstName": "Patrice",
                            "lastName": "Simard",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Simard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2080632378"
                        ],
                        "name": "B. Victorri",
                        "slug": "B.-Victorri",
                        "structuredName": {
                            "firstName": "Bernard",
                            "lastName": "Victorri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Victorri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747317"
                        ],
                        "name": "J. Denker",
                        "slug": "J.-Denker",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Denker",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Denker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2184474,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ff32cebbdb8a436ccd8ae797647428615ae32d74",
            "isKey": false,
            "numCitedBy": 286,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "In many machine learning applications, one has access, not only to training data, but also to some high-level a priori knowledge about the desired behavior of the system. For example, it is known in advance that the output of a character recognizer should be invariant with respect to small spatial distortions of the input images (translations, rotations, scale changes, etcetera). \n \nWe have implemented a scheme that allows a network to learn the derivative of its outputs with respect to distortion operators of our choosing. This not only reduces the learning time and the amount of training data, but also provides a powerful language for specifying what generalizations we wish the network to perform."
            },
            "slug": "Tangent-Prop-A-Formalism-for-Specifying-Selected-in-Simard-Victorri",
            "title": {
                "fragments": [],
                "text": "Tangent Prop - A Formalism for Specifying Selected Invariances in an Adaptive Network"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A scheme is implemented that allows a network to learn the derivative of its outputs with respect to distortion operators of their choosing, which not only reduces the learning time and the amount of training data, but also provides a powerful language for specifying what generalizations the authors wish the network to perform."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688681"
                        ],
                        "name": "T. Kohonen",
                        "slug": "T.-Kohonen",
                        "structuredName": {
                            "firstName": "Teuvo",
                            "lastName": "Kohonen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kohonen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "119559470"
                        ],
                        "name": "J. Hynninen",
                        "slug": "J.-Hynninen",
                        "structuredName": {
                            "firstName": "Jorma",
                            "lastName": "Hynninen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hynninen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145800809"
                        ],
                        "name": "J. Kangas",
                        "slug": "J.-Kangas",
                        "structuredName": {
                            "firstName": "Jari",
                            "lastName": "Kangas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kangas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708642"
                        ],
                        "name": "Jorma T. Laaksonen",
                        "slug": "Jorma-T.-Laaksonen",
                        "structuredName": {
                            "firstName": "Jorma",
                            "lastName": "Laaksonen",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jorma T. Laaksonen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60775240,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7bd2bb8319a75d9140fd4c30431c7283a6b25710",
            "isKey": false,
            "numCitedBy": 660,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "The Self-Organizing Map (SOM) represents the result of a vector quantization algorithm that places a number of reference or code-book vectors into a high-dimensional input data space to approximate to its data sets in an ordered fashion. The SOM PAK program package contains all programs necessary for the correct application of the Self-Organizing Map algorithm in the visualization of complex experimental data. The rst version 1.0 of this program package was published in 1992 and since then the package has been updated regularly to include latest improvements in the SOM implementations. This report that contains the last documentation was prepared for bibliographical purposes."
            },
            "slug": "SOM_PAK:-The-Self-Organizing-Map-Program-Package-Kohonen-Hynninen",
            "title": {
                "fragments": [],
                "text": "SOM_PAK: The Self-Organizing Map Program Package"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The SOM PAK program package contains all programs necessary for the correct application of the Self-Organizing Map algorithm in the visualization of complex experimental data."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110094"
                        ],
                        "name": "J. Bridle",
                        "slug": "J.-Bridle",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Bridle",
                            "middleNames": [
                                "Scott"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bridle"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59636530,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1f462943c8d0af69c12a09058251848324135e5a",
            "isKey": false,
            "numCitedBy": 1100,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "We are concerned with feed-forward non-linear networks (multi-layer perceptrons, or MLPs) with multiple outputs. We wish to treat the outputs of the network as probabilities of alternatives (e.g. pattern classes), conditioned on the inputs. We look for appropriate output non-linearities and for appropriate criteria for adaptation of the parameters of the network (e.g. weights). We explain two modifications: probability scoring, which is an alternative to squared error minimisation, and a normalised exponential (softmax) multi-input generalisation of the logistic non-linearity. The two modifications together result in quite simple arithmetic, and hardware implementation is not difficult either. The use of radial units (squared distance instead of dot product) immediately before the softmax output stage produces a network which computes posterior distributions over class labels based on an assumption of Gaussian within-class distributions. However the training, which uses cross-class information, can result in better performance at class discrimination than the usual within-class training method, unless the within-class distribution assumptions are actually correct."
            },
            "slug": "Probabilistic-Interpretation-of-Feedforward-Network-Bridle",
            "title": {
                "fragments": [],
                "text": "Probabilistic Interpretation of Feedforward Classification Network Outputs, with Relationships to Statistical Pattern Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Two modifications are explained: probability scoring, which is an alternative to squared error minimisation, and a normalised exponential (softmax) multi-input generalisation of the logistic non- linearity of feed-forward non-linear networks with multiple outputs."
            },
            "venue": {
                "fragments": [],
                "text": "NATO Neurocomputing"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702356"
                        ],
                        "name": "D. Husmeier",
                        "slug": "D.-Husmeier",
                        "structuredName": {
                            "firstName": "Dirk",
                            "lastName": "Husmeier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Husmeier"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11911655,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "19fdfde70999398329d4d11709ea50cae40b0a9e",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Training probability-density estimating neural networks with the expectation-maximization (EM) algorithm aims to maximize the likelihood of the training set and therefore leads to overfitting for sparse data. In this article, a regularization method for mixture models with generalized linear kernel centers is proposed, which adopts the Bayesian evidence approach and optimizes the hyperparameters of the prior by type II maximum likelihood. This includes a marginalization over the parameters, which is done by Laplace approximation and requires the derivation of the Hessian of the log-likelihood function. The incorporation of this approach into the standard training scheme leads to a modified form of the EM algorithm, which includes a regularization term and adapts the hyperparameters on-line after each EM cycle. The article presents applications of this scheme to classification problems, the prediction of stochastic time series, and latent space models."
            },
            "slug": "The-Bayesian-Evidence-Scheme-for-Regularizing-Husmeier",
            "title": {
                "fragments": [],
                "text": "The Bayesian Evidence Scheme for Regularizing Probability-Density Estimating Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A regularization method for mixture models with generalized linear kernel centers is proposed, which adopts the Bayesian evidence approach and optimizes the hyperparameters of the prior by type II maximum likelihood."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790646"
                        ],
                        "name": "P. Dayan",
                        "slug": "P.-Dayan",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Dayan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Dayan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1818565,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "1d510a685367dcc1ac1d1302221a9fda19c041d6",
            "isKey": false,
            "numCitedBy": 49,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a linear network that models correlations between real-valued visible variables using one or more real-valued hidden variablesa factor analysis model. This model can be seen as a linear version of the Helmholtz machine, and its parameters can be learned using the wake sleep method, in which learning of the primary generative model is as sisted by a recognition model, whose role is to fill in the values of hidden variables based on the values of visible variables. The generative and recognition models are jointly learned in wake and sleep phases, using just the delta rule. This learning procedure is comparable in simplicity to Hebbian learning, which produces a somewhat different representation of correlations in terms of principal components. We argue that the simplicity of wake-sleep learning makes factor analysis a plausible alternative to Hebbian learning as a model of activity-dependent cortical plasticity."
            },
            "slug": "Factor-Analysis-Using-Delta-Rule-Wake-Sleep-Neal-Dayan",
            "title": {
                "fragments": [],
                "text": "Factor Analysis Using Delta-Rule Wake-Sleep Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is argued that the simplicity of wake-sleep learning makes factor analysis a plausible alternative to Hebbian learning as a model of activity-dependent cortical plasticity."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14337779,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e1cc997a61e0b6da69a8512191daf339f785aefa",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "I define a latent variable model in the form of a neural network for which only target outputs are specified; the inputs are unspecified. Although the inputs are missing, it is still possible to train this model by placing a simple probability distribution on the unknown inputs and maximizing the probability of the data given the parameters. The model can then discover for itself a description of the data in terms of an underlying latent variable space of lower dimensionality. I present preliminary results of the application of these models to protein data."
            },
            "slug": "Density-Networks-and-their-Application-to-Protein-Mackay",
            "title": {
                "fragments": [],
                "text": "Density Networks and their Application to Protein Modelling"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "A latent variable model in the form of a neural network for which only target outputs are specified; the inputs are unspecified and the model can discover for itself a description of the data in terms of an underlying latent variable space of lower dimensionality."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47083033"
                        ],
                        "name": "T. Samad",
                        "slug": "T.-Samad",
                        "structuredName": {
                            "firstName": "Tariq",
                            "lastName": "Samad",
                            "middleNames": [],
                            "suffix": ""
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Samad"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685484"
                        ],
                        "name": "S. Harp",
                        "slug": "S.-Harp",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Harp",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Harp"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62600313,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "87ccf7ce7969f19a2143b27dba473d6420e53866",
            "isKey": false,
            "numCitedBy": 111,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "We show how the kohonen self-organizing feature map model can be extended so that partial training data can be utilized. Given input stimuli in which values for some elements or features are absent, the match computation and the weight updates are performed in the input subspace defined by the available values. Three examples, including an application to student modelling for intelligent tutoring systems in which data is inherently incomplete, demonstrate the effectiveness of the extension."
            },
            "slug": "Self\u2013organization-with-partial-data-Samad-Harp",
            "title": {
                "fragments": [],
                "text": "Self\u2013organization with partial data"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "It is shown how the kohonen self-organizing feature map model can be extended so that partial training data can be utilized, including an application to student modelling for intelligent tutoring systems in which data is inherently incomplete."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804104"
                        ],
                        "name": "R. Zemel",
                        "slug": "R.-Zemel",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Zemel",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Zemel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14493047,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2993bed6663f53a12584ea293bf8d487a91cd25a",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "The Minimum Description Length principle (MDL) can be used to train the hidden units of a neural network to extract a representation that is cheap to describe but nonetheless allows the input to be reconstructed accurately. We show how MDL can be used to develop highly redundant population codes. Each hidden unit has a location in a low-dimensional implicit space. If the hidden unit activities form a bump of a standard shape in this space, they can be cheaply encoded by the center ofthis bump. So the weights from the input units to the hidden units in an autoencoder are trained to make the activities form a standard bump. The coordinates of the hidden units in the implicit space are also learned, thus allowing flexibility, as the network develops a discontinuous topography when presented with different input classes. Population-coding in a space other than the input enables a network to extract nonlinear higher-order properties of the inputs. Most existing unsupervised learning algorithms can be understood using the Minimum Description Length (MDL) principle (Rissanen, 1989). Given an ensemble of input vectors, the aim of the learning algorithm is to find a method of coding each input vector that minimizes the total cost, in bits, of communicating the input vectors to a receiver. There are three terms in the total description length: \u2022 The code-cost is the number of bits required to communicate the code that the algorithm assigns to each input vector."
            },
            "slug": "Developing-Population-Codes-by-Minimizing-Length-Zemel-Hinton",
            "title": {
                "fragments": [],
                "text": "Developing Population Codes by Minimizing Description Length"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "The Minimum Description Length principle can be used to train the hidden units of a neural network to extract a representation that is cheap to describe but nonetheless allows the input to be reconstructed accurately."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46831169"
                        ],
                        "name": "G. Hinton",
                        "slug": "G.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744700"
                        ],
                        "name": "Zoubin Ghahramani",
                        "slug": "Zoubin-Ghahramani",
                        "structuredName": {
                            "firstName": "Zoubin",
                            "lastName": "Ghahramani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zoubin Ghahramani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17706343,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7e08b47eadfac97fab508485ed5fbef9dbbbd9a3",
            "isKey": false,
            "numCitedBy": 262,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a hierarchical, generative model that can be viewed as a nonlinear generalization of factor analysis and can be implemented in a neural network. The model uses bottom-up, top-down and lateral connections to perform Bayesian perceptual inference correctly. Once perceptual inference has been performed the connection strengths can be updated using a very simple learning rule that only requires locally available information. We demonstrate that the network learns to extract sparse, distributed, hierarchical representations."
            },
            "slug": "Generative-models-for-discovering-sparse-Hinton-Ghahramani",
            "title": {
                "fragments": [],
                "text": "Generative models for discovering sparse distributed representations."
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "A hierarchical, generative model that can be viewed as a nonlinear generalization of factor analysis and can be implemented in a neural network that learns to extract sparse, distributed, hierarchical representations is described."
            },
            "venue": {
                "fragments": [],
                "text": "Philosophical transactions of the Royal Society of London. Series B, Biological sciences"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786990"
                        ],
                        "name": "H. Attias",
                        "slug": "H.-Attias",
                        "structuredName": {
                            "firstName": "Hagai",
                            "lastName": "Attias",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Attias"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 746481,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2307fd6058ab4f7554a0b1f188507150ddb5b9a2",
            "isKey": false,
            "numCitedBy": 596,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce the independent factor analysis (IFA) method for recovering independent hidden sources from their observed mixtures. IFA generalizes and unifies ordinary factor analysis (FA), principal component analysis (PCA), and independent component analysis (ICA), and can handle not only square noiseless mixing but also the general case where the number of mixtures differs from the number of sources and the data are noisy. IFA is a two-step procedure. In the first step, the source densities, mixing matrix, and noise covariance are estimated from the observed data by maximum likelihood. For this purpose we present an expectation-maximization (EM) algorithm, which performs unsupervised learning of an associated probabilistic model of the mixing situation. Each source in our model is described by a mixture of gaussians; thus, all the probabilistic calculations can be performed analytically. In the second step, the sources are reconstructed from the observed data by an optimal nonlinear estimator. A variational approximation of this algorithm is derived for cases with a large number of sources, where the exact algorithm becomes intractable. Our IFA algorithm reduces to the one for ordinary FA when the sources become gaussian, and to an EM algorithm for PCA in the zero-noise limit. We derive an additional EM algorithm specifically for noiseless IFA. This algorithm is shown to be superior to ICA since it can learn arbitrary source densities from the data. Beyond blind separation, IFA can be used for modeling multidimensional data by a highly constrained mixture of gaussians and as a tool for nonlinear signal encoding."
            },
            "slug": "Independent-Factor-Analysis-Attias",
            "title": {
                "fragments": [],
                "text": "Independent Factor Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "An expectation-maximization (EM) algorithm is presented, which performs unsupervised learning of an associated probabilistic model of the mixing situation and is shown to be superior to ICA since it can learn arbitrary source densities from the data."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16543854,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b959164d1efca4b73986ba5d21e664aadbbc0457",
            "isKey": false,
            "numCitedBy": 2590,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "A quantitative and practical Bayesian framework is described for learning of mappings in feedforward networks. The framework makes possible (1) objective comparisons between solutions using alternative network architectures, (2) objective stopping rules for network pruning or growing procedures, (3) objective choice of magnitude and type of weight decay terms or additive regularizers (for penalizing large weights, etc.), (4) a measure of the effective number of well-determined parameters in a model, (5) quantified estimates of the error bars on network parameters and on network output, and (6) objective comparisons with alternative learning and interpolation models such as splines and radial basis functions. The Bayesian \"evidence\" automatically embodies \"Occam's razor,\" penalizing overflexible and overcomplex models. The Bayesian approach helps detect poor underlying assumptions in learning models. For learning models well matched to a problem, a good correlation between generalization ability and the Bayesian evidence is obtained."
            },
            "slug": "A-Practical-Bayesian-Framework-for-Backpropagation-Mackay",
            "title": {
                "fragments": [],
                "text": "A Practical Bayesian Framework for Backpropagation Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A quantitative and practical Bayesian framework is described for learning of mappings in feedforward networks that automatically embodies \"Occam's razor,\" penalizing overflexible and overcomplex models."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053227062"
                        ],
                        "name": "Richard Durbin",
                        "slug": "Richard-Durbin",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Durbin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Richard Durbin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40227361"
                        ],
                        "name": "D. Willshaw",
                        "slug": "D.-Willshaw",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Willshaw",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Willshaw"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 29
                            }
                        ],
                        "text": "The elastic net algorithm of Durbin and Willshaw (1987) can be viewed as a gaussian mixture density model, tted by penalized maximum likelihood."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 4321691,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a94be030ccd68f3a5a3bf9245137fe114c549819",
            "isKey": false,
            "numCitedBy": 855,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "The travelling salesman problem1 is a classical problem in the field of combinatorial optimization, concerned with efficient methods for maximizing or minimizing a function of many independent variables. Given the positions of N cities, which in the simplest case lie in the plane, what is the shortest closed tour in which each city can be visited once? We describe how a parallel analogue algorithm, derived from a formal model2\u20133 for the establishment of topographically ordered projections in the brain4\u201310, can be applied to the travelling salesman problem1,11,12. Using an iterative procedure, a circular closed path is gradually elongated non-uniformly until it eventually passes sufficiently near to all the cities to define a tour. This produces shorter tour lengths than another recent parallel analogue algorithm13, scales well with the size of the problem, and is naturally extendable to a large class of optimization problems involving topographic mappings between geometrical structures14."
            },
            "slug": "An-analogue-approach-to-the-travelling-salesman-an-Durbin-Willshaw",
            "title": {
                "fragments": [],
                "text": "An analogue approach to the travelling salesman problem using an elastic net method"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work describes how a parallel analogue algorithm, derived from a formal model for the establishment of topographically ordered projections in the brain, can be applied to the travelling salesman problem, and produces shorter tour lengths than another recent parallel analogue algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "150109870"
                        ],
                        "name": "G.R. De Haan",
                        "slug": "G.R.-De-Haan",
                        "structuredName": {
                            "firstName": "G.R.",
                            "lastName": "De Haan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G.R. De Haan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3720792"
                        ],
                        "name": "O. Egecioglu",
                        "slug": "O.-Egecioglu",
                        "structuredName": {
                            "firstName": "Omer",
                            "lastName": "Egecioglu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Egecioglu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 120120383,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "889196bdfcd62fb423edef576fa6710ce2b5fb14",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "A novel learning algorithm for self-organizing feature maps (SOFMs) is presented. The learning algorithm is based on an extension of vector quantization called weighted vector quantization (WVQ). WVQ distortion is a weighted sum of the distortion between an input vector and each of the codevectors in the codebook. A formulation of WVQ is given, as well as two optimality conditions which are analogous to the nearest neighbor and centroid conditions of vector quantization. The authors then incorporate the SOFM neighborhood mechanism into WVQ, and use the WVQ optimality conditions to derive the algorithm.<<ETX>>"
            },
            "slug": "Links-between-self-organizing-feature-maps-and-Haan-Egecioglu",
            "title": {
                "fragments": [],
                "text": "Links between self-organizing feature maps and weighted vector quantization"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "A novel learning algorithm for self-organizing feature maps (SOFMs) is presented, based on an extension ofvector quantization called weighted vector quantization (WVQ), which incorporates the SOFM neighborhood mechanism into WVQ, and uses the WVZ optimality conditions to derive the algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "[Proceedings] 1991 IEEE International Joint Conference on Neural Networks"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790646"
                        ],
                        "name": "P. Dayan",
                        "slug": "P.-Dayan",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Dayan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Dayan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2713302"
                        ],
                        "name": "M. Revow",
                        "slug": "M.-Revow",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Revow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Revow"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 996158,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "62f4d89a3c1441b47170c7e1380137fb388d0799",
            "isKey": false,
            "numCitedBy": 416,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes two new methods for modeling the manifolds of digitized images of handwritten digits. The models allow a priori information about the structure of the manifolds to be combined with empirical data. Accurate modeling of the manifolds allows digits to be discriminated using the relative probability densities under the alternative models. One of the methods is grounded in principal components analysis, the other in factor analysis. Both methods are based on locally linear low-dimensional approximations to the underlying data manifold. Links with other methods that model the manifold are discussed."
            },
            "slug": "Modeling-the-manifolds-of-images-of-handwritten-Hinton-Dayan",
            "title": {
                "fragments": [],
                "text": "Modeling the manifolds of images of handwritten digits"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "Two new methods for modeling the manifolds of digitized images of handwritten digits of principal components analysis and factor analysis are described, based on locally linear low-dimensional approximations to the underlying data manifold."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "12874899"
                        ],
                        "name": "J. Sammon",
                        "slug": "J.-Sammon",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Sammon",
                            "middleNames": [
                                "W."
                            ],
                            "suffix": "Jr."
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Sammon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 43151050,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "154f8a9906bcc99fca9b17aa521649b1c3734093",
            "isKey": false,
            "numCitedBy": 3461,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "An algorithm for the analysis of multivariate data is presented along with some experimental results. The algorithm is based upon a point mapping of N L-dimensional vectors from the L-space to a lower-dimensional space such that the inherent data \"structure\" is approximately preserved."
            },
            "slug": "A-Nonlinear-Mapping-for-Data-Structure-Analysis-Sammon",
            "title": {
                "fragments": [],
                "text": "A Nonlinear Mapping for Data Structure Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "An algorithm for the analysis of multivariate data is presented along with some experimental results that is based upon a point mapping of N L-dimensional vectors from the L-space to a lower-dimensional space such that the inherent data \"structure\" is approximately preserved."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Computers"
            },
            "year": 1969
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15366323,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fc053fd3feade79df85fd0612d7f817f5ae3cd44",
            "isKey": false,
            "numCitedBy": 168,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "It is shown that Bayesian training of backpropagation neural networks can feasibly be performed by the \\Hybrid Monte Carlo\" method. This approach allows the true predictive distribution for a test case given a set of training cases to be approximated arbitrarily closely, in contrast to previous approaches which approximate the posterior weight distribution by a Gaussian. In this work, the Hybrid Monte Carlo method is implemented in conjunction with simulated annealing, in order to speed relaxation to a good region of parameter space. The method has been applied to a test problem, demonstrating that it can produce good predictions, as well as an indication of the uncertainty of these predictions. Appropriate weight scaling factors are found automatically. By applying known techniques for calculation of \\free energy\" diierences, it should also be possible to compare the merits of diierent network architectures. The work described here should also be applicable to a wide variety of statistical models other than neural networks."
            },
            "slug": "Bayesian-training-of-backpropagation-networks-by-Neal",
            "title": {
                "fragments": [],
                "text": "Bayesian training of backpropagation networks by the hybrid Monte-Carlo method"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "It is shown that Bayesian training of backpropagation neural networks can feasibly be performed by the Hybrid Monte Carlo method, and the method has been applied to a test problem, demonstrating that it can produce good predictions, as well as an indication of the uncertainty of these predictions."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1423299574"
                        ],
                        "name": "F. O\u2019Sullivan",
                        "slug": "F.-O\u2019Sullivan",
                        "structuredName": {
                            "firstName": "Finbarr",
                            "lastName": "O\u2019Sullivan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. O\u2019Sullivan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17362921,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "c2d815514114a30d9e0977c0cd6cbdd815fde692",
            "isKey": false,
            "numCitedBy": 48,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract An approach to multidimensional smoothing is introduced that is based on a penalized likelihood with a modified discretized Laplacian penalty term. The choice of penalty simplifies computational difficulties associated with standard multidimensional Laplacian smoothing methods yet without compromising mean squared error characteristics, at least on the interior of the region of interest. For linear smoothing in hyper-rectangular domains, which has wide application in image reconstruction and restoration problems, computations are carried out using fast Fourier transforms. Nonlinear smoothing is accomplished by iterative application of the linear smoothing technique. The iterative procedure is shown to be convergent under general conditions. Adaptive choice of the amount of smoothing is based on approximate cross-validation type scores. An importance sampling technique is used to estimate the degrees of freedom of the smooth. The methods are implemented in one- and two-dimensional settings. Some i..."
            },
            "slug": "Discretized-Laplacian-Smoothing-by-Fourier-Methods-O\u2019Sullivan",
            "title": {
                "fragments": [],
                "text": "Discretized Laplacian Smoothing by Fourier Methods"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694254"
                        ],
                        "name": "S. Luttrell",
                        "slug": "S.-Luttrell",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Luttrell",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Luttrell"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 145
                            }
                        ],
                        "text": "In this context it has been shown how a reformulation of the vector quantization problem\n(Luttrell, 1990; Buhmann & Ku\u0308hnel 1993; Luttrell, 1994; Luttrell, 1995) can avoid many of the problems with the SOM procedure discussed earlier."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 0
                            }
                        ],
                        "text": "(Luttrell, 1990; Buhmann & K\u00fchnel 1993; Luttrell, 1994; Luttrell, 1995) can avoid many of the problems with the SOM procedure discussed earlier."
                    },
                    "intents": []
                }
            ],
            "corpusId": 62179769,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f5ab10dfd46d50188052f6f0620faf18b31fcf81",
            "isKey": false,
            "numCitedBy": 11,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "A model based approach to radar range profile classification is presented, and it is shown to be equivalent to training a topographic mapping neural network (T. Kohonen, 1984) on each of the range profile categories to be classified. The topographic mapping method is basically a Euclidean distance method of classifying range profiles. However, because it is model based, it offers much more flexibility, and will perform better in situations where there is little training data."
            },
            "slug": "Using-self-organising-maps-to-classify-radar-range-Luttrell",
            "title": {
                "fragments": [],
                "text": "Using self-organising maps to classify radar range profiles"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A model based approach to radar range profile classification is presented, and it is shown to be equivalent to training a topographic mapping neural network on each of the range profile categories to be classified."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2085687317"
                        ],
                        "name": "IItevor Hattie",
                        "slug": "IItevor-Hattie",
                        "structuredName": {
                            "firstName": "IItevor",
                            "lastName": "Hattie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "IItevor Hattie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14171464,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "75a46db5ee1b329836c5325f25c378f670123636",
            "isKey": false,
            "numCitedBy": 98,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Principal curves are smooth one dimensional curves that pass through the middle of a p dimensional data set. They minimise the distance from the points, and provide a non-linear summary of the data. The curves are non-parametric and their shape is suggested by the data. Similarly, principal surfaces are two dimensional surfaces that pass through the middle of the data. The curves and surfaces are found using an iterative procedure which starts with a linear summary such as the usual principal component line or plane. Each successive iteration is a smooth or local average of the p dimensional points, where local is based on the projections of the points onto the curve or surface of the previous iteration. A number of linear techniques, such as factor analysis and errors in variables regression, end up using the principal components as their estimates (after a suitable scaling of the co-ordinates). Principal curves and surfaces can be viewed as the estimates of non-linear generalisations of these procedures. We present some real data examples that illustrate these applications. Principal Curves (or surfaces) have a theoretical definition for distributions: they are the Self Consistent curves. A curve is self consistent if each point on the curve is the conditional mean of the points that project there. The main theorem proves that principal curves are critical values of the expected squared distance between the points and the curve. Linear principal components have this property as well; in fact, we prove that if a principal curve is straight, then it is a principal component. These results general&e the usual duality between conditional expectation and distance minimieation. We also examine two sources of bias in the procedures, which have the satisfactory property of partially cancelling each other. We compare the principal curve and surface procedures to other generalisations of principal components in the literature; the usual generalisations transform the space, whereas we transform the model. There are also strong ties with multidimensional scaling. l Work supported by the Department of Energy under contracts DE-AC03.76SF00515 and DEATOS-6l.ERlO645, and by the Office of Naval Research tmdcx contract ONR N00014.El-K-0340 and ONR N0014.63.K-0472, and by the U.S. Army Research Offics under contract DAAG29.62.K-0056."
            },
            "slug": "Principal-Curves-and-Surfaces-Hattie",
            "title": {
                "fragments": [],
                "text": "Principal Curves and Surfaces"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2302447"
                        ],
                        "name": "N. Kambhatla",
                        "slug": "N.-Kambhatla",
                        "structuredName": {
                            "firstName": "Nanda",
                            "lastName": "Kambhatla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Kambhatla"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3222903"
                        ],
                        "name": "T. Leen",
                        "slug": "T.-Leen",
                        "structuredName": {
                            "firstName": "Todd",
                            "lastName": "Leen",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Leen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8698778,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bd6184b1da59c5e0de08a6774368f4027a67c63c",
            "isKey": false,
            "numCitedBy": 128,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a fast algorithm for non-linear dimension reduction. The algorithm builds a local linear model of the data by merging PCA with clustering based on a new distortion measure. Experiments with speech and image data indicate that the local linear algorithm produces encodings with lower distortion than those built by five layer auto-associative networks. The local linear algorithm is also more than an order of magnitude faster to train."
            },
            "slug": "Fast-Non-Linear-Dimension-Reduction-Kambhatla-Leen",
            "title": {
                "fragments": [],
                "text": "Fast Non-Linear Dimension Reduction"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Experiments with speech and image data indicate that the local linear algorithm produces encodings with lower distortion than those built by five layer auto-associative networks."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145319478"
                        ],
                        "name": "Michael J. Jones",
                        "slug": "Michael-J.-Jones",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jones",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Jones"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 49743910,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "eae2430d9a984120bf511655a03c15089b007499",
            "isKey": false,
            "numCitedBy": 1366,
            "numCiting": 198,
            "paperAbstract": {
                "fragments": [],
                "text": "We had previously shown that regularization principles lead to approximation schemes that are equivalent to networks with one layer of hidden units, called regularization networks. In particular, standard smoothness functionals lead to a subclass of regularization networks, the well known radial basis functions approximation schemes. This paper shows that regularization networks encompass a much broader range of approximation schemes, including many of the popular general additive models and some of the neural networks. In particular, we introduce new classes of smoothness functionals that lead to different classes of basis functions. Additive splines as well as some tensor product splines can be obtained from appropriate classes of smoothness functionals. Furthermore, the same generalization that extends radial basis functions (RBF) to hyper basis functions (HBF) also leads from additive models to ridge approximation models, containing as special cases Breiman's hinge functions, some forms of projection pursuit regression, and several types of neural networks. We propose to use the term generalized regularization networks for this broad class of approximation schemes that follow from an extension of regularization. In the probabilistic interpretation of regularization, the different classes of basis functions correspond to different classes of prior probabilities on the approximating function spaces, and therefore to different types of smoothness assumptions. In summary, different multilayer networks with one hidden layer, which we collectively call generalized regularization networks, correspond to different classes of priors and associated smoothness functionals in a classical regularization principle. Three broad classes are (1) radial basis functions that can be generalized to hyper basis functions, (2) some tensor product splines, and (3) additive splines that can be generalized to schemes of the type of ridge approximation, hinge functions, and several perceptron-like neural networks with one hidden layer."
            },
            "slug": "Regularization-Theory-and-Neural-Networks-Girosi-Jones",
            "title": {
                "fragments": [],
                "text": "Regularization Theory and Neural Networks Architectures"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "This paper shows that regularization networks encompass a much broader range of approximation schemes, including many of the popular general additive models and some of the neural networks, and introduces new classes of smoothness functionals that lead to different classes of basis functions."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723612"
                        ],
                        "name": "A. Vellido",
                        "slug": "A.-Vellido",
                        "structuredName": {
                            "firstName": "Alfredo",
                            "lastName": "Vellido",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Vellido"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145408620"
                        ],
                        "name": "P. Lisboa",
                        "slug": "P.-Lisboa",
                        "structuredName": {
                            "firstName": "Paulo",
                            "lastName": "Lisboa",
                            "middleNames": [
                                "J.",
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Lisboa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2643080"
                        ],
                        "name": "Karon Meehan",
                        "slug": "Karon-Meehan",
                        "structuredName": {
                            "firstName": "Karon",
                            "lastName": "Meehan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karon Meehan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 58163527,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "4fda523d88938fb441042ccf49964927a17a683c",
            "isKey": false,
            "numCitedBy": 148,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Segmentation-of-the-on-line-shopping-market-using-Vellido-Lisboa",
            "title": {
                "fragments": [],
                "text": "Segmentation of the on-line shopping market using neural networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145034054"
                        ],
                        "name": "K. M\u00fcller",
                        "slug": "K.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Klaus-Robert",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. M\u00fcller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6674407,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "3f600e6c6cf93e78c9e6e690443d6d22c4bf18b9",
            "isKey": false,
            "numCitedBy": 7882,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "A new method for performing a nonlinear form of principal component analysis is proposed. By the use of integral operator kernel functions, one can efficiently compute principal components in high-dimensional feature spaces, related to input space by some nonlinear mapfor instance, the space of all possible five-pixel products in 16 16 images. We give the derivation of the method and present experimental results on polynomial feature extraction for pattern recognition."
            },
            "slug": "Nonlinear-Component-Analysis-as-a-Kernel-Eigenvalue-Sch\u00f6lkopf-Smola",
            "title": {
                "fragments": [],
                "text": "Nonlinear Component Analysis as a Kernel Eigenvalue Problem"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A new method for performing a nonlinear form of principal component analysis by the use of integral operator kernel functions is proposed and experimental results on polynomial feature extraction for pattern recognition are presented."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145833095"
                        ],
                        "name": "S. Kothari",
                        "slug": "S.-Kothari",
                        "structuredName": {
                            "firstName": "Suresh",
                            "lastName": "Kothari",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kothari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681982"
                        ],
                        "name": "H. Oh",
                        "slug": "H.-Oh",
                        "structuredName": {
                            "firstName": "Heekuck",
                            "lastName": "Oh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Oh"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 164
                            }
                        ],
                        "text": "The usual limitation of such models, however, is that the number of basis functions must typically grow exponentially with the dimensionality L of the input space (Bishop, 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 137
                            }
                        ],
                        "text": "However, our model consists of a mixture distribution which suggests that we might seek an EM algorithm (Dempster, Laird, & Rubin, 1977; Bishop, 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 101
                            }
                        ],
                        "text": "We shall see that, by contrast, the GTM model is de ned in terms of a mapping from the latent space into the data space."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 76
                            }
                        ],
                        "text": "The three phases in the pipe (oil, water, and gas) can belong to one of three different geometrical con gurations, corresponding to laminar, homogeneous, and annular ows, and the data set consists of 1000 points drawnwith equal probability from the three con gurations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 9
                            }
                        ],
                        "text": "Results from a toy problem for the case of a two-dimensional data space and a one-dimensional latent space are shown in Figure 4."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 186
                            }
                        ],
                        "text": "Since we are minimizing a differentiable cost function, given by equation 2.6, a sequential algorithm can be obtained by appealing to the Robbins-Monro procedure (Robbins & Monro, 1951; Bishop, 1995) to nd a zero of the objective function gradient."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 260,
                                "start": 248
                            }
                        ],
                        "text": "\u2026that other models for p(t |x) might also be appropriate, such as a Bernoulli for binary variables (with a sigmoid transformation of y) or a multinomial for mutually exclusive classes (with a softmax, or normalized exponential transformation of y [Bishop, 1995]), or even combinations of these."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 203,
                                "start": 190
                            }
                        ],
                        "text": "Jensen\u2019s inequality can be used to show that at each iteration of the algorithm, the objective function will increase unless it is already at a (local) maximum, as discussed, for example, in Bishop (1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 177751,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dbc0a468ab103ae29717703d4aa9f682f6a2b664",
            "isKey": true,
            "numCitedBy": 15339,
            "numCiting": 64,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Neural-Networks-for-Pattern-Recognition-Kothari-Oh",
            "title": {
                "fragments": [],
                "text": "Neural Networks for Pattern Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Adv. Comput."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2831141"
                        ],
                        "name": "Michael E. Tipping",
                        "slug": "Michael-E.-Tipping",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Tipping",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael E. Tipping"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792884"
                        ],
                        "name": "Charles M. Bishop",
                        "slug": "Charles-M.-Bishop",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Bishop",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charles M. Bishop"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15538672,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "ff4b3bbb455c9cc561ddec097a869140b3c1303d",
            "isKey": false,
            "numCitedBy": 3375,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": "Principal component analysis (PCA) is a ubiquitous technique for data analysis and processing, but one which is not based on a probability model. We demonstrate how the principal axes of a set of observed data vectors may be determined through maximum likelihood estimation of parameters in a latent variable model that is closely related to factor analysis. We consider the properties of the associated likelihood function, giving an EM algorithm for estimating the principal subspace iteratively, and discuss, with illustrative examples, the advantages conveyed by this probabilistic approach to PCA."
            },
            "slug": "Probabilistic-Principal-Component-Analysis-Tipping-Bishop",
            "title": {
                "fragments": [],
                "text": "Probabilistic Principal Component Analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3263426"
                        ],
                        "name": "I. Nabney",
                        "slug": "I.-Nabney",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Nabney",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Nabney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734566"
                        ],
                        "name": "D. Cornford",
                        "slug": "D.-Cornford",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Cornford",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Cornford"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11725474,
            "fieldsOfStudy": [
                "Environmental Science"
            ],
            "id": "64e90fa8443faa495a845beacff7e8874dd08b78",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Bayesian-inference-for-wind-field-retrieval-Nabney-Cornford",
            "title": {
                "fragments": [],
                "text": "Bayesian inference for wind field retrieval"
            },
            "venue": {
                "fragments": [],
                "text": "Neurocomputing"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2713302"
                        ],
                        "name": "M. Revow",
                        "slug": "M.-Revow",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Revow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Revow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790646"
                        ],
                        "name": "P. Dayan",
                        "slug": "P.-Dayan",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Dayan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Dayan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1785727,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9dea20c1e5bbb1f543ff08113ffde5380c679f1f",
            "isKey": false,
            "numCitedBy": 129,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "We construct a mixture of locally linear generative models of a collection of pixel-based images of digits, and use them for recognition. Different models of a given digit are used to capture different styles of writing, and new images are classified by evaluating their log-likelihoods under each model. We use an EM-based algorithm in which the M-step is computationally straightforward principal components analysis (PCA). Incorporating tangent-plane information [12] about expected local deformations only requires adding tangent vectors into the sample covariance matrices for the PCA, and it demonstrably improves performance."
            },
            "slug": "Recognizing-Handwritten-Digits-Using-Mixtures-of-Hinton-Revow",
            "title": {
                "fragments": [],
                "text": "Recognizing Handwritten Digits Using Mixtures of Linear Models"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "An EM-based algorithm in which the M-step is computationally straightforward principal components analysis (PCA), and incorporating tangent-plane information about expected local deformations only requires adding tangent vectors into the sample covariance matrices for the PCA, and it demonstrably improves performance."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 125432121,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1335b3997c95c7543691770651564c91e2517673",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "In this thesis I develop a method for recognizing isolated handprinted digits using trainable deformable models. Each digit is modelled by a cubic B-spline whose basic shape is defined by the \"home\" positions of the control points. A Gaussian distribution over displacements of the control points away from their home locations defines a probability distribution over shapes. The quality of the match of a spline model to an image is calculated as the likelihood of the data under a mixture of Gaussian \"ink generators\" placed along the length of the spline. Each spline model is adjusted to minimize an energy function that includes both the deformation energy of the model and the likelihood of the data, using a elastic matching procedure which is a generalization of the Expectation Maximization (EM) algorithm. I show that the matching procedure can be significantly speeded up by using a neural net to provide better starting points for the search. \nThe use of deformable models has a number of advantages. (1) After identifying the model most likely to have generated the data, the system not only produces a classification of the digit but also a rich description of the instantiation parameters. I have shown that these can be used to detect writing style consistency within a string of digits. (2) During the process of explaining the image, generative models can perform recognition-driven segmentation. (3) Unlike many other recognition schemes the method does not rely on some form of pre-normalization of input images, but can handle arbitrary scalings, translations and a limited degree of image rotation. The main disadvantage of the method is it requires much more computation than more standard optical character recognition techniques."
            },
            "slug": "Combining-deformable-models-and-neural-networks-for-Hinton-Williams",
            "title": {
                "fragments": [],
                "text": "Combining deformable models and neural networks for handprinted digit recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "A method for recognizing isolated handprinted digits using trainable deformable models that can handle arbitrary scalings, translations and a limited degree of image rotation and can be significantly speeded up by using a neural net to provide better starting points for the search."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2043869"
                        ],
                        "name": "K. J\u00f6reskog",
                        "slug": "K.-J\u00f6reskog",
                        "structuredName": {
                            "firstName": "Karl",
                            "lastName": "J\u00f6reskog",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. J\u00f6reskog"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 123231162,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "275db1c79e17a1811fc95044daaa679264f83f1b",
            "isKey": false,
            "numCitedBy": 808,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "A new computational method for the maximum likelihood solution in factor analysis is presented. This method takes into account the fact that the likelihood function may not have a maximum in a point of the parameter space where all unique variances are positive. Instead, the maximum may be attained on the boundary of the parameter space where one or more of the unique variances are zero. It is demonstrated that suchimproper (Heywood) solutions occur more often than is usually expected. A general procedure to deal with such improper solutions is proposed. The proposed methods are illustrated using two small sets of empirical data, and results obtained from the analyses of many other sets of data are reported. These analyses verify that the new computational method converges rapidly and that the maximum likelihood solution can be determined very accurately. A by-product obtained by the method is a large sample estimate of the variance-covariance matrix of the estimated unique variances. This can be used to set up approximate confidence intervals for communalities and unique variances."
            },
            "slug": "Some-contributions-to-maximum-likelihood-factor-J\u00f6reskog",
            "title": {
                "fragments": [],
                "text": "Some contributions to maximum likelihood factor analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1967
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145392702"
                        ],
                        "name": "P. Green",
                        "slug": "P.-Green",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Green",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Green"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122443027,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e1535c4f1e0c76a934598ec36c41b3759972dada",
            "isKey": false,
            "numCitedBy": 393,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "SUMMARY The EM algorithm is a popular approach to maximum likelihood estimation but has not been much used for penalized likelihood or maximum a posteriori estimation. This paper discusses properties of the EM algorithm in such contexts, concentrating on rates of conver- gence, and presents an alternative that is usually more practical and converges at least as quickly. The EM algorithm is a general approach to maximum likelihood estimation, rather than a specific algorithm. Dempster et al. (1977) discussed the method and derived basic properties, demonstrating that a variety of procedures previously developed rather informally could be unified. The common strand to problems where the approach is applicable is a notion of 'incomplete data'; this includes the conventional sense of 'missing data' but is much broader than that. The EM algorithm demon- strates its strength in situations where some hypothetical experiment yields data from which estimation is particularly convenient and economical: the 'incomplete' data actually at hand are regarded as observable functions of these 'complete' data. The resulting algorithms, while usually slow to converge, are often extremely simple and remain practical in large problems where no other approaches may be feasible. Dempster et al. (1977) briefly refer to the use of the same approach to the problem of finding the posterior mode (maximum a posteriori estimate) in a Bayesian estima-"
            },
            "slug": "On-Use-of-the-EM-Algorithm-for-Penalized-Likelihood-Green",
            "title": {
                "fragments": [],
                "text": "On Use of the EM Algorithm for Penalized Likelihood Estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Property of the EM algorithm in such contexts are discussed, concentrating on rates of conver- gence, and an alternative that is usually more practical and converges at least as quickly is presented."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46831169"
                        ],
                        "name": "G. Hinton",
                        "slug": "G.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790646"
                        ],
                        "name": "P. Dayan",
                        "slug": "P.-Dayan",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Dayan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Dayan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749650"
                        ],
                        "name": "B. Frey",
                        "slug": "B.-Frey",
                        "structuredName": {
                            "firstName": "Brendan",
                            "lastName": "Frey",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Frey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145572884"
                        ],
                        "name": "R. Neal",
                        "slug": "R.-Neal",
                        "structuredName": {
                            "firstName": "R",
                            "lastName": "Neal",
                            "middleNames": [
                                "M"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Neal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 871473,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6dd01cd9c17d1491ead8c9f97597fbc61dead8ea",
            "isKey": false,
            "numCitedBy": 1001,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "An unsupervised learning algorithm for a multilayer network of stochastic neurons is described. Bottom-up \"recognition\" connections convert the input into representations in successive hidden layers, and top-down \"generative\" connections reconstruct the representation in one layer from the representation in the layer above. In the \"wake\" phase, neurons are driven by recognition connections, and generative connections are adapted to increase the probability that they would reconstruct the correct activity vector in the layer below. In the \"sleep\" phase, neurons are driven by generative connections, and recognition connections are adapted to increase the probability that they would produce the correct activity vector in the layer above."
            },
            "slug": "The-\"wake-sleep\"-algorithm-for-unsupervised-neural-Hinton-Dayan",
            "title": {
                "fragments": [],
                "text": "The \"wake-sleep\" algorithm for unsupervised neural networks."
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "An unsupervised learning algorithm for a multilayer network of stochastic neurons is described, where bottom-up \"recognition\" connections convert the input into representations in successive hidden layers, and top-down \"generative\" connections reconstruct the representation in one layer from the representations in the layer above."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2428034"
                        ],
                        "name": "C. Bregler",
                        "slug": "C.-Bregler",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Bregler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Bregler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1808760"
                        ],
                        "name": "S. Omohundro",
                        "slug": "S.-Omohundro",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Omohundro",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Omohundro"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8783809,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "56efe7bf4bd52a6369d9ebbe55033e81e716f7d0",
            "isKey": false,
            "numCitedBy": 132,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Most connectionist research has focused on learning mappings from one space to another (eg. classification and regression). This paper introduces the more general task of learning constraint surfaces. It describes a simple but powerful architecture for learning and manipulating nonlinear surfaces from data. We demonstrate the technique on low dimensional synthetic surfaces and compare it to nearest neighbor approaches. We then show its utility in learning the space of lip images in a system for improving speech recognition by lip reading. This learned surface is used to improve the visual tracking performance during recognition."
            },
            "slug": "Surface-Learning-with-Applications-to-Lipreading-Bregler-Omohundro",
            "title": {
                "fragments": [],
                "text": "Surface Learning with Applications to Lipreading"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper describes a simple but powerful architecture for learning and manipulating nonlinear surfaces from data and shows its utility in learning the space of lip images in a system for improving speech recognition by lip reading."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 60809283,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "db869fa192a3222ae4f2d766674a378e47013b1b",
            "isKey": false,
            "numCitedBy": 3642,
            "numCiting": 92,
            "paperAbstract": {
                "fragments": [],
                "text": "Artificial \"neural networks\" are widely used as flexible models for classification and regression applications, but questions remain about how the power of these models can be safely exploited when training data is limited. This book demonstrates how Bayesian methods allow complex neural network models to be used without fear of the \"overfitting\" that can occur with traditional training methods. Insight into the nature of these complex Bayesian models is provided by a theoretical investigation of the priors over functions that underlie them. A practical implementation of Bayesian neural network learning using Markov chain Monte Carlo methods is also described, and software for it is freely available over the Internet. Presupposing only basic knowledge of probability and statistics, this book should be of interest to researchers in statistics, engineering, and artificial intelligence."
            },
            "slug": "Bayesian-Learning-for-Neural-Networks-Neal",
            "title": {
                "fragments": [],
                "text": "Bayesian Learning for Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Bayesian Learning for Neural Networks shows that Bayesian methods allow complex neural network models to be used without fear of the \"overfitting\" that can occur with traditional neural network learning methods."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145683892"
                        ],
                        "name": "A. Cichocki",
                        "slug": "A.-Cichocki",
                        "structuredName": {
                            "firstName": "Andrzej",
                            "lastName": "Cichocki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Cichocki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8896870"
                        ],
                        "name": "H. Yang",
                        "slug": "H.-Yang",
                        "structuredName": {
                            "firstName": "Howard",
                            "lastName": "Yang",
                            "middleNames": [
                                "Hua"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Yang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7941673,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fac0e753905d1498e0b3debf01431696e1f0c645",
            "isKey": false,
            "numCitedBy": 2220,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "A new on-line learning algorithm which minimizes a statistical dependency among outputs is derived for blind separation of mixed signals. The dependency is measured by the average mutual information (MI) of the outputs. The source signals and the mixing matrix are unknown except for the number of the sources. The Gram-Charlier expansion instead of the Edgeworth expansion is used in evaluating the MI. The natural gradient approach is used to minimize the MI. A novel activation function is proposed for the on-line learning algorithm which has an equivariant property and is easily implemented on a neural network like model. The validity of the new learning algorithm are verified by computer simulations."
            },
            "slug": "A-New-Learning-Algorithm-for-Blind-Signal-Amari-Cichocki",
            "title": {
                "fragments": [],
                "text": "A New Learning Algorithm for Blind Signal Separation"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "A new on-line learning algorithm which minimizes a statistical dependency among outputs is derived for blind separation of mixed signals and has an equivariant property and is easily implemented on a neural network like model."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "89161743"
                        ],
                        "name": "D. Lawley",
                        "slug": "D.-Lawley",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Lawley",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lawley"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 124543959,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "dde9b64a0bfe687d962056df0bfcb8580af627d9",
            "isKey": false,
            "numCitedBy": 372,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "1. When a battery of intelligence tests is administered to a set of persons it is a common practice among psychologists to \u201cexplain\u201d the scores obtained in terms of a number of \u201cfactors.\u201d Thus if we suppose that there are altogether t tests and that x i , denotes the score of any one person in the i th test, then it is assumed that where f, g , \u2026, h represent the person's measures in one or more general or group factors, and S i , is the person's specific ability in the i th test. It is further assumed that for a given and supposed infinite population of persons all the factors, specific and otherwise, are distributed normally and independently and that they are standardised, i.e. that their standard deviations are unity."
            },
            "slug": "VI.\u2014The-Estimation-of-Factor-Loadings-by-the-Method-Lawley",
            "title": {
                "fragments": [],
                "text": "VI.\u2014The Estimation of Factor Loadings by the Method of Maximum Likelihood"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1940
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "One way to view the role of the latent space samples {xi} is as a Monte Carlo approximation to the integral over x in (2) (MacKay 1995; Bishop, Svens\u00e9n, and Williams 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 132
                            }
                        ],
                        "text": "One way to view the role of the latent space samples fxig is as a Monte Carlo approximation to the integral over x in equation 2.2 (MacKay, 1995; Bishop, Svense\u0301n, & Williams, 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 38
                            }
                        ],
                        "text": "Finally, the density network model of MacKay (1995) involves transforming a simple distribution in latent space to a complex distribution in data space by propagation through a nonlinear network."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 122200499,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f2861a5e59de4d61bcd8a9ae4785978ac11fc9c1",
            "isKey": true,
            "numCitedBy": 159,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Bayesian-neural-networks-and-density-networks-Mackay",
            "title": {
                "fragments": [],
                "text": "Bayesian neural networks and density networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37367995"
                        ],
                        "name": "X. Giannakopoulos",
                        "slug": "X.-Giannakopoulos",
                        "structuredName": {
                            "firstName": "Xavier",
                            "lastName": "Giannakopoulos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Giannakopoulos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703769"
                        ],
                        "name": "J. Karhunen",
                        "slug": "J.-Karhunen",
                        "structuredName": {
                            "firstName": "Juha",
                            "lastName": "Karhunen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Karhunen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726997"
                        ],
                        "name": "E. Oja",
                        "slug": "E.-Oja",
                        "structuredName": {
                            "firstName": "Erkki",
                            "lastName": "Oja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Oja"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14713291,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5b57673b5da164e45836c114c50ca92c78afbbf0",
            "isKey": false,
            "numCitedBy": 103,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we compare the performance of five prominent neural or adaptive algorithms designed for Independent Component Analysis (ICA) and blind source separation (BSS). In the first part of the study, we use artificial data for comparing the accuracy, convergence speed, computational load, and other relevant properties of the algorithms. In the second part, the algorithms are applied to three different real-world data sets. The task is either blind source separation or finding interesting directions in the data for visualisation purposes. We develop criteria for selecting the most meaningful basis vectors of ICA and measuring the quality of the results. The comparison reveals characteristic differences between the studied ICA algorithms. The most important conclusions of our comparison are robustness of the ICA algorithms with respect to modest modeling imperfections, and the superiority of fixed-point algorithms with respect to the computational load."
            },
            "slug": "An-Experimental-Comparison-of-Neural-Algorithms-for-Giannakopoulos-Karhunen",
            "title": {
                "fragments": [],
                "text": "An Experimental Comparison of Neural Algorithms for Independent Component Analysis and Blind Separation"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "Comparing five prominent neural or adaptive algorithms designed for Independent Component Analysis and blind source separation reveals robustness of the ICA algorithms with respect to modest modeling imperfections, and the superiority of fixed-point algorithms withrespect to the computational load."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Neural Syst."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761784"
                        ],
                        "name": "R. Tibshirani",
                        "slug": "R.-Tibshirani",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Tibshirani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Tibshirani"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 49
                            }
                        ],
                        "text": "A modi\u008eed form of the principal curves algorithm (Tibshirani, 1992) introduces a generative distribution based on a mixture of gaussians, with a well-de\u008ened likelihood function, and is trained by the EM algorithm."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 50
                            }
                        ],
                        "text": "A modi ed form of the principal curves algorithm (Tibshirani, 1992) introduces a generative distribution based on a mixture of gaussians, with a well-de ned likelihood function, and is trained by the EM algorithm."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 121653378,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "07ce26d312a343d81331e34be7d8c73df4ef838e",
            "isKey": false,
            "numCitedBy": 246,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "A principal curve (Hastie and Stuetzle, 1989) is a smooth curve passing through the \u2018middle\u2019 of a distribution or data cloud, and is a generalization of linear principal components. We give an alternative definition of a principal curve, based on a mixture model. Estimation is carried out through an EM algorithm. Some comparisons are made to the Hastie-Stuetzle definition."
            },
            "slug": "Principal-curves-revisited-Tibshirani",
            "title": {
                "fragments": [],
                "text": "Principal curves revisited"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3472959"
                        ],
                        "name": "C. Rasmussen",
                        "slug": "C.-Rasmussen",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Rasmussen",
                            "middleNames": [
                                "Edward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Rasmussen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2877073,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "0e658618c9dad4d70dd7dcd5c519185ec4f845f5",
            "isKey": false,
            "numCitedBy": 1160,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "The Bayesian analysis of neural networks is difficult because a simple prior over weights implies a complex prior distribution over functions. In this paper we investigate the use of Gaussian process priors over functions, which permit the predictive Bayesian analysis for fixed values of hyperparameters to be carried out exactly using matrix operations. Two methods, using optimization and averaging (via Hybrid Monte Carlo) over hyperparameters have been tested on a number of challenging problems and have produced excellent results."
            },
            "slug": "Gaussian-Processes-for-Regression-Williams-Rasmussen",
            "title": {
                "fragments": [],
                "text": "Gaussian Processes for Regression"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper investigates the use of Gaussian process priors over functions, which permit the predictive Bayesian analysis for fixed values of hyperparameters to be carried out exactly using matrix operations."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053227062"
                        ],
                        "name": "Richard Durbin",
                        "slug": "Richard-Durbin",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Durbin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Richard Durbin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717841"
                        ],
                        "name": "R. Szeliski",
                        "slug": "R.-Szeliski",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Szeliski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Szeliski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145081362"
                        ],
                        "name": "A. Yuille",
                        "slug": "A.-Yuille",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Yuille",
                            "middleNames": [
                                "Loddon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yuille"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9262827,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "477db9d84d8875da57664d39af140d884d858222",
            "isKey": false,
            "numCitedBy": 209,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper analyzes the elastic net approach (Durbin and Willshaw 1987) to the traveling salesman problem of finding the shortest path through a set of cities. The elastic net approach jointly minimizes the length of an arbitrary path in the plane and the distance between the path points and the cities. The tradeoff between these two requirements is controlled by a scale parameter K. A global minimum is found for large K, and is then tracked to a small value. In this paper, we show that (1) in the small K limit the elastic path passes arbitrarily close to all the cities, but that only one path point is attracted to each city, (2) in the large K limit the net lies at the center of the set of cities, and (3) at a critical value of K the energy function bifurcates. We also show that this method can be interpreted in terms of extremizing a probability distribution controlled by K. The minimum at a given K corresponds to the maximum a posteriori (MAP) Bayesian estimate of the tour under a natural statistical interpretation. The analysis presented in this paper gives us a better understanding of the behavior of the elastic net, allows us to better choose the parameters for the optimization, and suggests how to extend the underlying ideas to other domains."
            },
            "slug": "An-Analysis-of-the-Elastic-Net-Approach-to-the-Durbin-Szeliski",
            "title": {
                "fragments": [],
                "text": "An Analysis of the Elastic Net Approach to the Traveling Salesman Problem"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The analysis presented in this paper gives a better understanding of the behavior of the elastic net, allows us to better choose the parameters for the optimization, and suggests how to extend the underlying ideas to other domains."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35106875"
                        ],
                        "name": "R. Duda",
                        "slug": "R.-Duda",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Duda",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Duda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3108177"
                        ],
                        "name": "P. Hart",
                        "slug": "P.-Hart",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Hart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hart"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 265,
                                "start": 245
                            }
                        ],
                        "text": "If we de ne the weighted means of the data vectors by\n\u00b5 i D P\nn RintnP n Rin , (4.5)\nthen we obtain\ny(xiI W) D X\nj\nFij\u00b5 j, (4.6)\nwhere we have introduced the effective kernel Fij given by\nFij D \u00c1T(xi) \u00b1 \u00a9TG\u00a9 \u00b2\u00a11 \u00c1(xj)Gjj. (4.7)\nNote that the effective kernel satis es P\nj Fij D 1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12946615,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b07ce649d6f6eb636872527104b0209d3edc8188",
            "isKey": true,
            "numCitedBy": 16928,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Provides a unified, comprehensive and up-to-date treatment of both statistical and descriptive methods for pattern recognition. The topics treated include Bayesian decision theory, supervised and unsupervised learning, nonparametric techniques, discriminant analysis, clustering, preprosessing of pictorial data, spatial filtering, shape description techniques, perspective transformations, projective invariants, linguistic procedures, and artificial intelligence techniques for scene analysis."
            },
            "slug": "Pattern-classification-and-scene-analysis-Duda-Hart",
            "title": {
                "fragments": [],
                "text": "Pattern classification and scene analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "The topics treated include Bayesian decision theory, supervised and unsupervised learning, nonparametric techniques, discriminant analysis, clustering, preprosessing of pictorial data, spatial filtering, shape description techniques, perspective transformations, projective invariants, linguistic procedures, and artificial intelligence techniques for scene analysis."
            },
            "venue": {
                "fragments": [],
                "text": "A Wiley-Interscience publication"
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2235217"
                        ],
                        "name": "D. Rubin",
                        "slug": "D.-Rubin",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Rubin",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rubin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69495445"
                        ],
                        "name": "Dorothy T. Thayer",
                        "slug": "Dorothy-T.-Thayer",
                        "structuredName": {
                            "firstName": "Dorothy",
                            "lastName": "Thayer",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dorothy T. Thayer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 123437256,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "5e9222ee44916c976c80f11303002e850de0c63e",
            "isKey": false,
            "numCitedBy": 579,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "The details of EM algorithms for maximum likelihood factor analysis are presented for both the exploratory and confirmatory models. The algorithm is essentially the same for both cases and involves only simple least squares regression operations; the largest matrix inversion required is for aq \u00d7q symmetric matrix whereq is the matrix of factors. The example that is used demonstrates that the likelihood for the factor analysis model may have multiple modes that are not simply rotations of each other; such behavior should concern users of maximum likelihood factor analysis and certainly should cast doubt on the general utility of second derivatives of the log likelihood as measures of precision of estimation."
            },
            "slug": "EM-algorithms-for-ML-factor-analysis-Rubin-Thayer",
            "title": {
                "fragments": [],
                "text": "EM algorithms for ML factor analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113861474"
                        ],
                        "name": "C. Bishop",
                        "slug": "C.-Bishop",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Bishop",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Bishop"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70332074"
                        ],
                        "name": "G. James",
                        "slug": "G.-James",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "James",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. James"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 146
                            }
                        ],
                        "text": "Our second example arises from the problem of determining the fraction of oil in a multiphase pipeline carrying a mixture of oil, water, and gas (Bishop & James, 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 121258161,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "8e62ba312ebbbf52ec1186f5aff581a416335c84",
            "isKey": false,
            "numCitedBy": 108,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Analysis-of-multiphase-flows-using-dual-energy-and-Bishop-James",
            "title": {
                "fragments": [],
                "text": "Analysis of multiphase flows using dual-energy gamma densitometry and neural networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682548"
                        ],
                        "name": "J. Buhmann",
                        "slug": "J.-Buhmann",
                        "structuredName": {
                            "firstName": "Joachim",
                            "lastName": "Buhmann",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Buhmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34528526"
                        ],
                        "name": "H. K\u00fchnel",
                        "slug": "H.-K\u00fchnel",
                        "structuredName": {
                            "firstName": "Hans",
                            "lastName": "K\u00fchnel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. K\u00fchnel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 90
                            }
                        ],
                        "text": "In this context it has been shown how a re-formulation of the vector quantization problem (Luttrell 1990; Buhmann and K\u00fchnel 1993; Luttrell 1994) can avoid many of the problems with the SOM procedure discussed earlier."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 28379134,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c829b01d6aac1825acae7ea90a00fee4fbc2689e",
            "isKey": false,
            "numCitedBy": 109,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "Vector quantization is a data compression method by which a set of data points is encoded by a reduced set of reference vectors: the codebook. A vector quantization strategy is discussed that jointly optimizes distortion errors and the codebook complexity, thereby determining the size of the codebook. A maximum entropy estimation of the cost function yields an optimal number of reference vectors, their positions, and their assignment probabilities. The dependence of the codebook density on the data density for different complexity functions is investigated in the limit of asymptotic quantization levels. How different complexity measures influence the efficiency of vector quantizers is studied for the task of image compression. The wavelet coefficients of gray-level images are quantized, and the reconstruction error is measured. The approach establishes a unifying framework for different quantization methods like K-means clustering and its fuzzy version, entropy constrained vector quantization or topological feature maps, and competitive neural networks. >"
            },
            "slug": "Vector-quantization-with-complexity-costs-Buhmann-K\u00fchnel",
            "title": {
                "fragments": [],
                "text": "Vector quantization with complexity costs"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The approach establishes a unifying framework for different quantization methods like K-means clustering and its fuzzy version, entropy constrained vector quantization or topological feature maps, and competitive neural networks."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "21476173"
                        ],
                        "name": "P. Pajunen",
                        "slug": "P.-Pajunen",
                        "structuredName": {
                            "firstName": "Petteri",
                            "lastName": "Pajunen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Pajunen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703769"
                        ],
                        "name": "J. Karhunen",
                        "slug": "J.-Karhunen",
                        "structuredName": {
                            "firstName": "Juha",
                            "lastName": "Karhunen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Karhunen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 16661657,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "2576649baa6d8d69450f3b5981944c9a5594644e",
            "isKey": false,
            "numCitedBy": 73,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "In the basic signal model of blind source separation (BSS), an unknown linear mixing process is assumed. While this ensures under mild conditions a sufficiently unique solution, it is desirable to extend the problem to nonlinear mixtures. Unfortunately the nonlinear case is much more difficult to handle, and brings serious indeterminacies to the solutions in the general case. In this paper we propose a new maximum likelihood approach to the nonlinear BSS problem. It is assumed that the source densities are known and that the mixing mapping is regularized. By finding a regular separating mapping which maximizes the likelihood, we show experimentally that the sources can often be separated."
            },
            "slug": "A-Maximum-Likelihood-Approach-to-Nonlinear-Blind-Pajunen-Karhunen",
            "title": {
                "fragments": [],
                "text": "A Maximum Likelihood Approach to Nonlinear Blind Source Separation"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A new maximum likelihood approach to the nonlinear BSS problem is proposed, assumed that the source densities are known and that the mixing mapping is regularized and it is shown experimentally that the sources can often be separated."
            },
            "venue": {
                "fragments": [],
                "text": "ICANN"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144686124"
                        ],
                        "name": "R. P. McDonald",
                        "slug": "R.-P.-McDonald",
                        "structuredName": {
                            "firstName": "Roderick",
                            "lastName": "McDonald",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. P. McDonald"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 119914603,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "4c4eca0fa3d2e8b2c822909014f7ab1fcb2a19a9",
            "isKey": false,
            "numCitedBy": 102,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "The method presented attempts to allow for nonlinear, possibly nonmonotonic relations between manifest and latent variates. An attempt is made to provide a workable criterion for choosing between alternative models on the basis of observable data as well as for constructing the appropriate function. An idealized numerical example is given."
            },
            "slug": "A-general-approach-to-nonlinear-factor-analysis-McDonald",
            "title": {
                "fragments": [],
                "text": "A general approach to nonlinear factor analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1962
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1400347470"
                        ],
                        "name": "M. A. Carreira-Perpi\u00f1\u00e1n",
                        "slug": "M.-A.-Carreira-Perpi\u00f1\u00e1n",
                        "structuredName": {
                            "firstName": "Miguel",
                            "lastName": "Carreira-Perpi\u00f1\u00e1n",
                            "middleNames": [
                                "\u00c1."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. A. Carreira-Perpi\u00f1\u00e1n"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8855100,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "737ee5560144fd006a74b6e2a23b343ea2fc4409",
            "isKey": false,
            "numCitedBy": 227,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "Gradient-quadratic and fixed-point iteration algorithms and appropriate values for their control parameters are derived for finding all modes of a Gaussian mixture, a problem with applications in clustering and regression. The significance of the modes found is quantified locally by Hessian-based error bars and globally by the entropy as sparseness measure."
            },
            "slug": "Mode-Finding-for-Mixtures-of-Gaussian-Distributions-Carreira-Perpi\u00f1\u00e1n",
            "title": {
                "fragments": [],
                "text": "Mode-Finding for Mixtures of Gaussian Distributions"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Gradient-quadratic and fixed-point iteration algorithms and appropriate values for their control parameters are derived for finding all modes of a Gaussian mixture, a problem with applications in clustering and regression."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49393083"
                        ],
                        "name": "B. Everitt",
                        "slug": "B.-Everitt",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Everitt",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Everitt"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 220,
                                "start": 206
                            }
                        ],
                        "text": "\u20264.7 can be interpreted as a weighted least-squares regression (Mardia, Kent, & Bibby, 1979) in which the target vectors are the \u00b5 i, and the weighting coef cients are given by Gjj.\nFigure 6 shows an example of the effective kernel for GTM corresponding to the oil ow problem discussed in section 3."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 122093866,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "345d4daa63202fedd6311ae295b298e216291af3",
            "isKey": true,
            "numCitedBy": 516,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "1 General introduction.- 1.1 Introduction.- 1.2 Latent variables and latent variable models.- 1.3 The role of models.- 1.4 The general latent model.- 1.5 A simple latent variable model.- 1.6 Estimation and goodness-of-fit.- 1.7 Path diagrams.- 1.8 Summary.- 2 Factor analysis.- 2.1 Introduction.- 2.2 Explanatory and confirmatory factor analysis.- 2.3 The factor analysis model.- 2.4 Identifiability of the factor analysis model.- 2.5 Estimating the parameters in the factor analysis model.- 2.6 Goodness-of-fit tests.- 2.7 Rotation of factors.- 2.8 Numerical examples.- 2.9 Confirmatory factor analysis.- 2.10 Summary.- 3 The LISREL model.- 3.1 Introduction.- 3.2 The LISREL model.- 3.3 Identification.- 3.4 Estimating the parameters in the LISREL model.- 3.5 Instrumental variables.- 3.6 Numerical examples.- 3.7 Assessing goodness-of-fit.- 3.8 Multigroup analysis.- 3.9 Summary.- 4 Latent variable models for categorical data.- 4.1 Introduction.- 4.2 Factor analysis of binary variables.- 4.3 Latent structure models.- 4.4 Summary.- 5 Some final comments.- 5.1 Introduction.- 5.2 Assessing the fit of latent variable models by cross-validation procedures.- 5.3 Latent variables - fact or fiction?.- 5.4 Summary.- Appendix A Estimating the parameters in latent variable models a brief account of computational procedures.- Appendix B Computer programs for latent variable models.- Exercises.- References."
            },
            "slug": "An-Introduction-to-Latent-Variable-Models-Everitt",
            "title": {
                "fragments": [],
                "text": "An Introduction to Latent Variable Models"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "Assessment of the fit of latent variable models by cross-validation procedures by estimating the parameters in latent variable model a brief account of computational procedures."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2172770680"
                        ],
                        "name": "N. L. Johnson",
                        "slug": "N.-L.-Johnson",
                        "structuredName": {
                            "firstName": "Norman",
                            "lastName": "Johnson",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. L. Johnson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4206943,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "182f77976b08d832b5cdc7debdaeacc300c8e723",
            "isKey": false,
            "numCitedBy": 5733,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "An Introduction to Multivariate Statistical AnalysisBy Prof. T. W. Anderson. (Wiley Publications in Mathematical Statistics.) Pp. xii + 374. (New York: John Wiley and Sons, Inc.; London: Chapman and Hall, Ltd., 1958.) 100s. net.Some Aspects of Multivariate AnalysisBy Prof. S. N. Roy. (Indian Statistical Series, No. 1.) Pp. viii + 214. (New York: John Wiley and Sons, Inc.; Calcutta: Indian Statistical Institute; London: Chapman and Hall, Ltd., 1957.) 64s. net.The Analysis of Multiple Time-SeriesBy M. H. Quenouille. (Griffin's Statistical Monographs and Courses, No. 1.) Pp. 105. (London: Charles Griffin and Co., Ltd., 1957.) 24s."
            },
            "slug": "Multivariate-Analysis-Johnson",
            "title": {
                "fragments": [],
                "text": "Multivariate Analysis"
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1958
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2713302"
                        ],
                        "name": "M. Revow",
                        "slug": "M.-Revow",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Revow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Revow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 57852830,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bbf4924123e798f32c028d184dd8ee051bef55a3",
            "isKey": false,
            "numCitedBy": 13,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Deformable models are an attractive way for characterizing handwritten digits since they have relatively few parameters, are able to capture many topological variations, and incorporate much prior knowledge. We have described a system [8] that uses learned digit models consisting of splines whose shape is governed by a small number of control points. Images can be classi ed by separately tting each digit model to the image, and using a simple neural network to decide which model ts best. We use an elastic matching algorithm to minimize an energy function that includes both the deformation energy of the digit model and the log probability that the model would generate the inked pixels in the image. The use of multiple models for each digit can characterize the population of handwritten digits better. We show how multiple models may be used without increasing the time required for elastic matching."
            },
            "slug": "Hand-printed-digit-recognition-using-deformable-Williams-Revow",
            "title": {
                "fragments": [],
                "text": "Hand-printed digit recognition using deformable models"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work uses an elastic matching algorithm to minimize an energy function that includes both the deformation energy of the digit model and the log probability that the model would generate the inked pixels in the image."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2713302"
                        ],
                        "name": "M. Revow",
                        "slug": "M.-Revow",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Revow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Revow"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "For the particular noise model p(t|x, W, b) given by equation 2.1, the distribution p(t|W, b) corresponds to a constrained gaussian mixture model ( Hinton, Williams, & Revow, 1992 ) since the centers of the gaussians, given by y(xiIW), cannot move independently but are related through the function y(xIW)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15052186,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "15034107f195f625922881ef197515a7997b4c0d",
            "isKey": false,
            "numCitedBy": 123,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Hand-printed digits can be modeled as splines that are governed by about 8 control points. For each known digit, the control points have preferred \"home\" locations, and deformations of the digit are generated by moving the control points away from their home locations. Images of digits can be produced by placing Gaussian ink generators uniformly along the spline. Real images can be recognized by finding the digit model most likely to have generated the data. For each digit model we use an elastic matching algorithm to minimize an energy function that includes both the deformation energy of the digit model and the log probability that the model would generate the inked pixels in the image. The model with the lowest total energy wins. If a uniform noise process is included in the model of image generation, some of the inked pixels can be rejected as noise as a digit model is fitting a poorly segmented image. The digit models learn by modifying the home locations of the control points."
            },
            "slug": "Adaptive-Elastic-Models-for-Hand-Printed-Character-Hinton-Williams",
            "title": {
                "fragments": [],
                "text": "Adaptive Elastic Models for Hand-Printed Character Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "An elastic matching algorithm is used to minimize an energy function that includes both the deformation energy of the digit model and the log probability that the model would generate the inked pixels in the image."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1399851969"
                        ],
                        "name": "J. Etezadi-Amoli",
                        "slug": "J.-Etezadi-Amoli",
                        "structuredName": {
                            "firstName": "Jamshid",
                            "lastName": "Etezadi-Amoli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Etezadi-Amoli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144686124"
                        ],
                        "name": "R. P. McDonald",
                        "slug": "R.-P.-McDonald",
                        "structuredName": {
                            "firstName": "Roderick",
                            "lastName": "McDonald",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. P. McDonald"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 120139738,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "fbaafd6f3c4ec3ec88803c81936213f3c1b7983d",
            "isKey": false,
            "numCitedBy": 96,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Nonlinear common factor models with polynomial regression functions, including interaction terms, are fitted by simultaneously estimating the factor loadings and common factor scores, using maximum-likelihood-ratio and ordinary-least-squares methods. A Monte Carlo study gives support to a conjecture about the form of the distribution of the likelihood-ratio criterion."
            },
            "slug": "A-second-generation-nonlinear-factor-analysis-Etezadi-Amoli-McDonald",
            "title": {
                "fragments": [],
                "text": "A second generation nonlinear factor analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30161601"
                        ],
                        "name": "Ignova",
                        "slug": "Ignova",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Ignova",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ignova"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "104941430"
                        ],
                        "name": "Montague",
                        "slug": "Montague",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Montague",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Montague"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2075763653"
                        ],
                        "name": "Ward",
                        "slug": "Ward",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Ward",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ward"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102650011"
                        ],
                        "name": "Glassey",
                        "slug": "Glassey",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Glassey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Glassey"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 27192399,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "2e13b67897610ba0b44cbfe4bfe3e56182ac321d",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Industrial fermentation processes operate under well defined operating conditions to attempt to minimise production variability. Variability occurs for many reasons but a long held belief is that variation in the state of the seed is highly influential. In this paper a seed stage (a batch process) of an industrial antibiotic fermentation is considered and the performance of the main production fermentations is correlated with the quality of the seed using an unsupervised Kohonen self-organising feature map (SOM). It is shown that using only seed information poor performance in the final stage fermentations can be predicted. Data from industrial penicillin G fermenters is used to demonstrate the procedure. Copyright 1999 John Wiley & Sons, Inc."
            },
            "slug": "Fermentation-seed-quality-analysis-with-neural-Ignova-Montague",
            "title": {
                "fragments": [],
                "text": "Fermentation seed quality analysis with self-organising neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is shown that using only seed information poor performance in the final stage fermentations can be predicted and the performance of the main production fermentations is correlated with the quality of the seed using an unsupervised Kohonen self-organising feature map (SOM)."
            },
            "venue": {
                "fragments": [],
                "text": "Biotechnology and bioengineering"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144686124"
                        ],
                        "name": "R. P. McDonald",
                        "slug": "R.-P.-McDonald",
                        "structuredName": {
                            "firstName": "Roderick",
                            "lastName": "McDonald",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. P. McDonald"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 120134941,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "aff9d2376390c31be1924f0594323ee8eafe162b",
            "isKey": false,
            "numCitedBy": 50,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "The basic concepts of nonlinear factor analysis are introduced and some extensions of the general theory are developed. An elementary account of the class of multiple-factor polynomial models is presented, using more elementary algebraic methods than have been employed in earlier accounts of this theory. Working formulas are developed for the multiple-factor polynomial model without product terms.Some empirical results are presented."
            },
            "slug": "Numerical-methods-for-polynomial-models-in-factor-McDonald",
            "title": {
                "fragments": [],
                "text": "Numerical methods for polynomial models in nonlinear factor analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1967
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2058094936"
                        ],
                        "name": "Michael LeBlanc",
                        "slug": "Michael-LeBlanc",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "LeBlanc",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael LeBlanc"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761784"
                        ],
                        "name": "R. Tibshirani",
                        "slug": "R.-Tibshirani",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Tibshirani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Tibshirani"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 110
                            }
                        ],
                        "text": "There are also similarities between GTM and principal curves and principal surfaces (Hastie & Stuetzle, 1989; LeBlanc & Tibshirani 1994), which again involve a two-stage algorithm consisting of projection followed by smoothing, although these are not generative models."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 84
                            }
                        ],
                        "text": "There are also similarities between GTM and principal curves and principal surfaces (Hastie and Stuetzle 1989; LeBlanc and Tibshirani 1994) which again involve a two-stage algorithm consisting of projection followed by smoothing, although these are not generative models."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 120720623,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "ce8ac91cbfc51900150064f44b4a178eeacf9b35",
            "isKey": false,
            "numCitedBy": 135,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract We develop a nonlinear generalization of principal components analysis. A principal surface of the data is constructed adaptively, using some ideas from the MARS procedure of Friedman. We explore applications to curve and surface reconstruction and to data summarization."
            },
            "slug": "Adaptive-Principal-Surfaces-LeBlanc-Tibshirani",
            "title": {
                "fragments": [],
                "text": "Adaptive Principal Surfaces"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2420980"
                        ],
                        "name": "N. Suga",
                        "slug": "N.-Suga",
                        "structuredName": {
                            "firstName": "Nobuo",
                            "lastName": "Suga",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Suga"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 39307955,
            "fieldsOfStudy": [
                "Biology",
                "Physics"
            ],
            "id": "7784380a3ee5655540f8cd95f4228c8bf484e377",
            "isKey": false,
            "numCitedBy": 222,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Cortical-computational-maps-for-auditory-imaging-Suga",
            "title": {
                "fragments": [],
                "text": "Cortical computational maps for auditory imaging"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144601452"
                        ],
                        "name": "A. Hasman",
                        "slug": "A.-Hasman",
                        "structuredName": {
                            "firstName": "Arie",
                            "lastName": "Hasman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hasman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61503518,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2a90c0588d1bf00e349abbb00dde009b6760866a",
            "isKey": false,
            "numCitedBy": 1114,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Probabilistic-reasoning-in-intelligent-systems:-of-Hasman",
            "title": {
                "fragments": [],
                "text": "Probabilistic reasoning in intelligent systems: Networks of plausible inference"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2328544"
                        ],
                        "name": "J. Kaas",
                        "slug": "J.-Kaas",
                        "structuredName": {
                            "firstName": "Jon",
                            "lastName": "Kaas",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kaas"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 26768510,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "09a25f926f95712cb85546a1969aa848132f9527",
            "isKey": false,
            "numCitedBy": 174,
            "numCiting": 80,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-functional-organization-of-somatosensory-cortex-Kaas",
            "title": {
                "fragments": [],
                "text": "The functional organization of somatosensory cortex in primates."
            },
            "venue": {
                "fragments": [],
                "text": "Annals of anatomy = Anatomischer Anzeiger : official organ of the Anatomische Gesellschaft"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144665059"
                        ],
                        "name": "Y. Linde",
                        "slug": "Y.-Linde",
                        "structuredName": {
                            "firstName": "Y.",
                            "lastName": "Linde",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Linde"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1805395"
                        ],
                        "name": "A. Buzo",
                        "slug": "A.-Buzo",
                        "structuredName": {
                            "firstName": "Andres",
                            "lastName": "Buzo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Buzo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144790332"
                        ],
                        "name": "R. Gray",
                        "slug": "R.-Gray",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Gray",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Gray"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18530691,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7c46799502bebfe6a9ae0f457b7b8b92248ec260",
            "isKey": false,
            "numCitedBy": 7891,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "An efficient and intuitive algorithm is presented for the design of vector quantizers based either on a known probabilistic model or on a long training sequence of data. The basic properties of the algorithm are discussed and demonstrated by examples. Quite general distortion measures and long blocklengths are allowed, as exemplified by the design of parameter vector quantizers of ten-dimensional vectors arising in Linear Predictive Coded (LPC) speech compression with a complicated distortion measure arising in LPC analysis that does not depend only on the error vector."
            },
            "slug": "An-Algorithm-for-Vector-Quantizer-Design-Linde-Buzo",
            "title": {
                "fragments": [],
                "text": "An Algorithm for Vector Quantizer Design"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "An efficient and intuitive algorithm is presented for the design of vector quantizers based either on a known probabilistic model or on a long training sequence of data."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Commun."
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35043531"
                        ],
                        "name": "A. Dempster",
                        "slug": "A.-Dempster",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "Dempster",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dempster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7890796"
                        ],
                        "name": "N. Laird",
                        "slug": "N.-Laird",
                        "structuredName": {
                            "firstName": "Nan",
                            "lastName": "Laird",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Laird"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2235217"
                        ],
                        "name": "D. Rubin",
                        "slug": "D.-Rubin",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Rubin",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rubin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 237,
                                "start": 216
                            }
                        ],
                        "text": "As a consequence there is a well-de ned objective function given by the log likelihood (see equation 2.6), and convergence to a (local) maximum of the objective function is guaranteed by the use of the EM algorithm (Dempster et al., 1977)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 254,
                                "start": 231
                            }
                        ],
                        "text": "In both examples, we choose the basis functions wj(x) to be radially symmetric gaussians whose centers are distributed on a uniform grid in x-space, with a common width parameter chosen equal to twice the separation of neighboring basis function centers."
                    },
                    "intents": []
                }
            ],
            "corpusId": 4193919,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "d36efb9ad91e00faa334b549ce989bfae7e2907a",
            "isKey": true,
            "numCitedBy": 48406,
            "numCiting": 134,
            "paperAbstract": {
                "fragments": [],
                "text": "Vibratory power unit for vibrating conveyers and screens comprising an asynchronous polyphase motor, at least one pair of associated unbalanced masses disposed on the shaft of said motor, with the first mass of a pair of said unbalanced masses being rigidly fastened to said shaft and with said second mass of said pair being movably arranged relative to said first mass, means for controlling and regulating the conveying rate during conveyer operation by varying the rotational speed of said motor between predetermined minimum and maximum values, said second mass being movably outwardly by centrifugal force against the pressure of spring means, said spring means being prestressed in such a manner that said second mass is, at rotational motor speeds lower than said minimum speed, held in its initial position, and at motor speeds between said lower and upper values in positions which are radially offset with respect to the axis of said motor to an extent depending on the value of said rotational motor speed."
            },
            "slug": "Maximum-likelihood-from-incomplete-data-via-the-EM-Dempster-Laird",
            "title": {
                "fragments": [],
                "text": "Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69381520"
                        ],
                        "name": "D. Lovelock",
                        "slug": "D.-Lovelock",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Lovelock",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lovelock"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69459550"
                        ],
                        "name": "H. Rund",
                        "slug": "H.-Rund",
                        "structuredName": {
                            "firstName": "Hanno",
                            "lastName": "Rund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Rund"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 60847341,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "dc7533aadfe464b9aae781b1455a7a1a7c15e2cc",
            "isKey": false,
            "numCitedBy": 491,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The aim of this book is to present a self-contained, reasonably modern account of tensor analysis and the calculus of exterior differential forms, adapted to the needs of physicists, engineers and applied mathematicians. In the later, increasingly sophisticated chapters, the interaction between the concept of invariance and the calculus of variations is examined. This interaction is of profound importance to all physical field theories."
            },
            "slug": "Tensors,-differential-forms,-and-variational-Lovelock-Rund",
            "title": {
                "fragments": [],
                "text": "Tensors, differential forms, and variational principles"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "A self-contained, reasonably modern account of tensor analysis and the calculus of exterior differential forms, adapted to the needs of physicists, engineers and applied mathematicians is presented."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1975
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "146720638"
                        ],
                        "name": "F. Krauss",
                        "slug": "F.-Krauss",
                        "structuredName": {
                            "firstName": "Flavia",
                            "lastName": "Krauss",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Krauss"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 118409087,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "89c1678393602fba0d3c1d62b97b1402922987c2",
            "isKey": false,
            "numCitedBy": 1037,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "Die Latent Structure Analysis (LSA) von LAZARSFELD wird hier im wesentlichen fur den wichtigen Spezialfall der Latent Class Analysis (LCA) vorgestellt. Ziel der LSA ist es, nicht direkt zu beobachtende latente Variablen aufzufinden, die den Zusammenhang der manifesten Variablen erklaren. Von diesem Ansatz her besteht eine Parallele zur Faktorenanalyse. Speiziell die LCA kann jedoch auch als nicht metrische Clusteranalyse aufgefast werden."
            },
            "slug": "Latent-Structure-Analysis-Krauss",
            "title": {
                "fragments": [],
                "text": "Latent Structure Analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 9584248,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "877a887e7af7daebcb685e4d7b5e80f764035581",
            "isKey": false,
            "numCitedBy": 4043,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Title Type pattern recognition with neural networks in c++ PDF pattern recognition and neural networks PDF neural networks for pattern recognition advanced texts in econometrics PDF neural networks for applied sciences and engineering from fundamentals to complex pattern recognition PDF an introduction to biological and artificial neural networks for pattern recognition spie tutorial text vol tt04 tutorial texts in optical engineering PDF"
            },
            "slug": "Pattern-Recognition-and-Neural-Networks-LeCun-Bengio",
            "title": {
                "fragments": [],
                "text": "Pattern Recognition and Neural Networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "14942773"
                        ],
                        "name": "M. Stone",
                        "slug": "M.-Stone",
                        "structuredName": {
                            "firstName": "Mervyn",
                            "lastName": "Stone",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Stone"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62698647,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "7b28610d2d681a11398eb614de0d70d7de41c20c",
            "isKey": false,
            "numCitedBy": 7503,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "SUMMARY A generalized form of the cross-validation criterion is applied to the choice and assessment of prediction using the data-analytic concept of a prescription. The examples used to illustrate the application are drawn from the problem areas of univariate estimation, linear regression and analysis of variance."
            },
            "slug": "Cross\u2010Validatory-Choice-and-Assessment-of-Stone",
            "title": {
                "fragments": [],
                "text": "Cross\u2010Validatory Choice and Assessment of Statistical Predictions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152591573"
                        ],
                        "name": "D. Titterington",
                        "slug": "D.-Titterington",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Titterington",
                            "middleNames": [
                                "Michael"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Titterington"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "15974963"
                        ],
                        "name": "A. F. Smith",
                        "slug": "A.-F.-Smith",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Smith",
                            "middleNames": [
                                "F.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. F. Smith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2580190"
                        ],
                        "name": "U. Makov",
                        "slug": "U.-Makov",
                        "structuredName": {
                            "firstName": "Udi",
                            "lastName": "Makov",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Makov"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Alternatively, a sequential form of the EM algorithm can be used ( Titterington, Smith, & Makov, 1985 )."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 124992180,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "54a1f6ab4cc6cb749c2b8d15c1dd3449e072362f",
            "isKey": false,
            "numCitedBy": 3447,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Statistical Problems. Applications of Finite Mixture Models. Mathematical Aspects of Mixtures. Learning About the Parameters of a Mixture. Learning About the Components of a Mixture. Sequential Problems and Procedures."
            },
            "slug": "Statistical-analysis-of-finite-mixture-Titterington-Smith",
            "title": {
                "fragments": [],
                "text": "Statistical analysis of finite mixture distributions"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "This course discusses Mathematical Aspects of Mixtures, Sequential Problems and Procedures, and Applications of Finite Mixture Models."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69924093"
                        ],
                        "name": "S. Hyakin",
                        "slug": "S.-Hyakin",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Hyakin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Hyakin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 125
                            }
                        ],
                        "text": "If we de ne the weighted means of the data vectors by\n\u00b5 i D P\nn RintnP n Rin , (4.5)\nthen we obtain\ny(xiI W) D X\nj\nFij\u00b5 j, (4.6)\nwhere we have introduced the effective kernel Fij given by\nFij D \u00c1T(xi) \u00b1 \u00a9TG\u00a9 \u00b2\u00a11 \u00c1(xj)Gjj. (4.7)\nNote that the effective kernel satis es P\nj Fij D 1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60577818,
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "id": "045310b06e8a3983a363a118cc9dcc3f292970b4",
            "isKey": true,
            "numCitedBy": 9900,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "Simon Haykin Neural Networks A Comprehensive Foundation. Neural Networks A Comprehensive Foundation Simon S. Neural Networks A Comprehensive Foundation Simon S. Neural Networks A Comprehensive Foundation. Neural Networks Association for Computing Machinery. Book Review Neural Networks A Comprehensive Foundation. Neural Networks A Comprehensive Foundation Pearson. Neural networks a comprehensive foundation. Neural Networks a Comprehensive Foundation AbeBooks. Neural networks a comprehensive foundation solutions. cdn preterhuman net. Neural Networks A Comprehensive Foundation Goodreads. Neural Networks A Comprehensive Foundation Amazon it. Neural Networks A Comprehensive Foundation Amazon co uk. Neural Networks A Comprehensive Foundation 3rd Edition. Neural Networks A Comprehensive Foundation Simon. Neural Networks A Comprehensive Foundation amazon com. Neural networks a comprehensive foundation Academia edu. Neural Networks A Comprehensive Foundation Amazon. neural networks a comprehensive foundation simon haykin. Simon Haykin Neural Networks A Comprehensive Foundation. Neural Networks A comprehensive Foundation 2 ed. Simon haykin neural networks a comprehensive foundation pdf. Buy Neural Networks A Comprehensive Foundation Book. Neural networks a comprehensive foundation 2e book. Neural Networks A Comprehensive Foundation. NEURAL NETWORKS A COMPREHENSIVE FOUNDATION SIMON. Neural Networks a Comprehensive Foundation by Haykin Simon. Neural Networks A Comprehensive Foundation pdf PDF Drive. Neural Networks A Comprehensive Foundation amazon ca. Simon Haykin Neural Networks A Comprehensive Foundation. NEURAL NETWORKS A Comprehensive Foundation PDF. Neural Networks A Comprehensive Foundation pdf PDF Drive. Neural Networks A Comprehensive Foundation by Haykin. Neural Networks A Comprehensive Foundation 3rd Edition. Neural Networks A Comprehensive Foundation Simon S. Neural Networks A Comprehensive Foundation. Neural networks a comprehensive foundation Book 1994. Neural Networks A Comprehensive Foundation 2nd Edition. Neural Networks A Comprehensive Foundation S S Haykin. Neural Networks A Comprehensive Foundation International. Neural Networks A Comprehensive Foundation 2 e Pearson. Download Neural Networks A Comprehensive Foundation 2Nd. Neural Networks A comprehensive foundation Aalto"
            },
            "slug": "Neural-Networks:-A-Comprehensive-Foundation-Hyakin",
            "title": {
                "fragments": [],
                "text": "Neural Networks: A Comprehensive Foundation"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "Simon Haykin Neural Networks A Comprehensive Foundation Simon S. Haykin neural networks a comprehensive foundation pdf PDF Drive."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30821484"
                        ],
                        "name": "E. Nadaraya",
                        "slug": "E.-Nadaraya",
                        "structuredName": {
                            "firstName": "Elizbar",
                            "lastName": "Nadaraya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Nadaraya"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 89
                            }
                        ],
                        "text": "The result (equation 4.2) is analogous to the Nadaraya-Watson kernel regression formula (Nadaraya, 1964; Watson, 1964) with the kernel functions given by\nKij D hijNjP j0 hij0 Nj0 ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "where Nj is the number of data vectors in group Gj. The result (22) is analogous to the Nadaraya-Watson kernel regression formula ( Nadaraya 1964;  Watson 1964) with the kernel functions given by"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 120067924,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "05175204318c3c01e3301fd864553071039605d2",
            "isKey": true,
            "numCitedBy": 3288,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "A study is made of certain properties of an approximation to the regression line on the basis of sampling data when the sample size increases unboundedly."
            },
            "slug": "On-Estimating-Regression-Nadaraya",
            "title": {
                "fragments": [],
                "text": "On Estimating Regression"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1964
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2078719392"
                        ],
                        "name": "Ayoub Ghriss",
                        "slug": "Ayoub-Ghriss",
                        "structuredName": {
                            "firstName": "Ayoub",
                            "lastName": "Ghriss",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ayoub Ghriss"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2070841583"
                        ],
                        "name": "R. Girard",
                        "slug": "R.-Girard",
                        "structuredName": {
                            "firstName": "Romain",
                            "lastName": "Girard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Girard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "72941158"
                        ],
                        "name": "Alexis Thual",
                        "slug": "Alexis-Thual",
                        "structuredName": {
                            "firstName": "Alexis",
                            "lastName": "Thual",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexis Thual"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 253,
                                "start": 231
                            }
                        ],
                        "text": "In the case of the radially symmetric gaussian given by equation 2.1, the model is closely related to principal component analysis since the maximum likelihood solution for W has columns given by the scaled principal eigenvectors (Tipping & Bishop, 1997)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 226298454,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "b08e2ea6f1ed75e14c74c8e303bf11c0abfedad4",
            "isKey": true,
            "numCitedBy": 132,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Let us formalize PCA in the following way. Given observed points {tn}, n \u2208 {1 . . . N}, of dimension d, it aims at finding a number q < d of orthonormal axes (thus forming a linear subspace of dimension q < d) such that the variance of the projection of the observed vectors onto this subspace is maximal. The idea is that the directions along which the variance of the observed data is maximal are those which carry the most information about the individual observations, and should therefore be preserved as such to discriminate the observations. On the other hand, the directions along which the observed variance is minimal give little information about the individual observations: all observed vectors are \u201droughly the same\u201d along this direction, hence this axe rather gives information about the structure of the problem; this information can be stored in the form of the lower-dimensional linear subspace, and forgotten at the level of individual observations."
            },
            "slug": "Mixtures-of-Probabilistic-Principal-Component-Ghriss-Girard",
            "title": {
                "fragments": [],
                "text": "Mixtures of Probabilistic Principal Component Analysers"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 105
                            }
                        ],
                        "text": "The result (equation 4.2) is analogous to the Nadaraya-Watson kernel regression formula (Nadaraya, 1964; Watson, 1964) with the kernel functions given by\nKij D hijNjP j0 hij0 Nj0 ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Smooth regression analysis. Sankhy\u00af a"
            },
            "venue": {
                "fragments": [],
                "text": "The Indian Journal of Statistics. Series A"
            },
            "year": 1964
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 74
                            }
                        ],
                        "text": "The rst is that in SOM, the vectors that are smoothed, de ned by equation 4.3, correspond to hard assignments of data points to nodes, whereas the corresponding vectors in GTM, given by equation 4.5, involve soft assignments, weighted by the posterior probabilities."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Factor Analysis as a Statstical Method"
            },
            "venue": {
                "fragments": [],
                "text": "Factor Analysis as a Statstical Method"
            },
            "year": 1971
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 132
                            }
                        ],
                        "text": "One way to view the role of the latent space samples fxig is as a Monte Carlo approximation to the integral over x in equation 2.2 (MacKay, 1995; Bishop, Svense\u0301n, & Williams, 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 38
                            }
                        ],
                        "text": "Finally, the density network model of MacKay (1995) involves transforming a simple distribution in latent space to a complex distribution in data space by propagation through a nonlinear network."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian neural networks and density n e t works. Nuclear Instruments and Methods in Physics Research, Section A"
            },
            "venue": {
                "fragments": [],
                "text": "Bayesian neural networks and density n e t works. Nuclear Instruments and Methods in Physics Research, Section A"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 105
                            }
                        ],
                        "text": "The result (equation 4.2) is analogous to the Nadaraya-Watson kernel regression formula (Nadaraya, 1964; Watson, 1964) with the kernel functions given by\nKij D hijNjP j0 hij0 Nj0 ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Smooth regression analysis. Sankhyy a"
            },
            "venue": {
                "fragments": [],
                "text": "The Indian Journal of Statistics. Series A"
            },
            "year": 1964
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 89
                            }
                        ],
                        "text": "The result (equation 4.2) is analogous to the Nadaraya-Watson kernel regression formula (Nadaraya, 1964; Watson, 1964) with the kernel functions given by\nKij D hijNjP j0 hij0 Nj0 ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On estimating regression. Theory of Probability and its Applications"
            },
            "venue": {
                "fragments": [],
                "text": "On estimating regression. Theory of Probability and its Applications"
            },
            "year": 1964
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48566708"
                        ],
                        "name": "G. S. Watson",
                        "slug": "G.-S.-Watson",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Watson",
                            "middleNames": [
                                "Stuart"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. S. Watson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 124218927,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "14821ac1bf09890a857fca2a6c324e8c85f2c0d0",
            "isKey": false,
            "numCitedBy": 2958,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Smooth-regression-analysis-Watson",
            "title": {
                "fragments": [],
                "text": "Smooth regression analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1964
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144686124"
                        ],
                        "name": "R. P. McDonald",
                        "slug": "R.-P.-McDonald",
                        "structuredName": {
                            "firstName": "Roderick",
                            "lastName": "McDonald",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. P. McDonald"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122365437,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "8689286efaad01b38df0f4cb5b60b788ad08344f",
            "isKey": false,
            "numCitedBy": 30,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-simultaneous-estimation-of-factor-loadings-and-McDonald",
            "title": {
                "fragments": [],
                "text": "The simultaneous estimation of factor loadings and scores"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "18478271"
                        ],
                        "name": "B. Lindsay",
                        "slug": "B.-Lindsay",
                        "structuredName": {
                            "firstName": "Bruce",
                            "lastName": "Lindsay",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Lindsay"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 122322120,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "518b1470704506e062b8a9ae06409a945917370f",
            "isKey": false,
            "numCitedBy": 635,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Geometry-of-Mixture-Likelihoods:-A-General-Lindsay",
            "title": {
                "fragments": [],
                "text": "The Geometry of Mixture Likelihoods: A General Theory"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "123962567"
                        ],
                        "name": "M. C. Seiler",
                        "slug": "M.-C.-Seiler",
                        "structuredName": {
                            "firstName": "Mary",
                            "lastName": "Seiler",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. C. Seiler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50678756"
                        ],
                        "name": "F. A. Seiler",
                        "slug": "F.-A.-Seiler",
                        "structuredName": {
                            "firstName": "Fritz",
                            "lastName": "Seiler",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. A. Seiler"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62717952,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8e980fbf251ecb28ba85eb092fc66ce284bb63be",
            "isKey": false,
            "numCitedBy": 13115,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Numerical-Recipes-in-C:-The-Art-of-Scientific-Seiler-Seiler",
            "title": {
                "fragments": [],
                "text": "Numerical Recipes in C: The Art of Scientific Computing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62562212,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "418cc44768ff9d0ed8cf4cef79869f90ab672f7b",
            "isKey": false,
            "numCitedBy": 215,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-new-view-of-the-EM-algorithm-that-justifies-and-Neal",
            "title": {
                "fragments": [],
                "text": "A new view of the EM algorithm that justifies incremental and other variants"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2271961"
                        ],
                        "name": "V. Chandru",
                        "slug": "V.-Chandru",
                        "structuredName": {
                            "firstName": "Vijay",
                            "lastName": "Chandru",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Chandru"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144635404"
                        ],
                        "name": "M. Rao",
                        "slug": "M.-Rao",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Rao",
                            "middleNames": [
                                "Rammohan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Rao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1558541,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "df65c38f24b8010ddb030d4d88ec4d7bbe8d36cd",
            "isKey": false,
            "numCitedBy": 1688,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Combinatorial-Optimization-Chandru-Rao",
            "title": {
                "fragments": [],
                "text": "Combinatorial Optimization"
            },
            "venue": {
                "fragments": [],
                "text": "The Computer Science and Engineering Handbook"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Multivariate adaptive regression splines (with discussion)"
            },
            "venue": {
                "fragments": [],
                "text": "Annals of Statistics"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Smooth regression analysis. Sankhy\u00af a"
            },
            "venue": {
                "fragments": [],
                "text": "The Indian Journal of Statistics, series A"
            },
            "year": 1964
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 129
                            }
                        ],
                        "text": "In this context it has been shown how a reformulation of the vector quantization problem\n(Luttrell, 1990; Buhmann & Ku\u0308hnel 1993; Luttrell, 1994; Luttrell, 1995) can avoid many of the problems with the SOM procedure discussed earlier."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A B a yesian analysis of self-organizing maps"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Auto-association by m ultilayer perceptrons and singular value decomposition"
            },
            "venue": {
                "fragments": [],
                "text": "Biological Cybernetics"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 145
                            }
                        ],
                        "text": "In this context it has been shown how a reformulation of the vector quantization problem\n(Luttrell, 1990; Buhmann & Ku\u0308hnel 1993; Luttrell, 1994; Luttrell, 1995) can avoid many of the problems with the SOM procedure discussed earlier."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Using self-organizin g maps to classify radar range proles"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. 5th IEE Conf. on Articial Neural Networks"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 145
                            }
                        ],
                        "text": "In this context it has been shown how a reformulation of the vector quantization problem\n(Luttrell, 1990; Buhmann & Ku\u0308hnel 1993; Luttrell, 1994; Luttrell, 1995) can avoid many of the problems with the SOM procedure discussed earlier."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Using self-organizing maps to classify radar range prooles"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings IEE Fourth International Conference o n A rtiicial Neural Networks"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 85
                            }
                        ],
                        "text": "There are also similarities between GTM and principal curves and principal surfaces (Hastie & Stuetzle, 1989; LeBlanc & Tibshirani 1994), which again involve a two-stage algorithm consisting of projection followed by smoothing, although these are not generative models."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 31
                            }
                        ],
                        "text": "It is interesting to note that Hastie and Stuetzle (1989) propose reducing the spatial width of the smoothing function during learning, in a manner analogous to the shrinking of the neighborhood function in the SOM."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "1989).Principal curves"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of the American Statistical Association,"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Kohonen's self-organizing feature maps for exploratory data analysis"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. Intern. Neural Networks"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 90
                            }
                        ],
                        "text": "In this context it has been shown how a reformulation of the vector quantization problem\n(Luttrell, 1990; Buhmann & Ku\u0308hnel 1993; Luttrell, 1994; Luttrell, 1995) can avoid many of the problems with the SOM procedure discussed earlier."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Derivation of a classof trainingalgorithms"
            },
            "venue": {
                "fragments": [],
                "text": "IEEETransactions on Neural Networks"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Developments of the GTM Algorithm"
            },
            "venue": {
                "fragments": [],
                "text": "Developments of the GTM Algorithm"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The statistical analysis of nite mixture distributions"
            },
            "venue": {
                "fragments": [],
                "text": "The statistical analysis of nite mixture distributions"
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 209,
                                "start": 185
                            }
                        ],
                        "text": "The technique of parameterized self-organizing maps (PSOMs) involves rst tting a standard SOM model to a data set and then nding a manifold in data space that interpolates the reference vectors (Ritter, 1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A fast EM algorithm for latent variable density models"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing Systems"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An information maximisation approach to blind separation and blind deconvolution Neural Networks for Pattern Recognition GTM Through Time"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the IEE Fifth International Conference o n A rtiicial Neural Networks IEE. C. M. Bishop and G. D. James. Analysis of multiphase ows using dual-energy gamma densitometry and neural networks. Nuclear Instruments and Methods in Physics Research, A327:580{593"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Statistical analysis of nite mixture distributions"
            },
            "venue": {
                "fragments": [],
                "text": "Statistical analysis of nite mixture distributions"
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 145
                            }
                        ],
                        "text": "In this context it has been shown how a reformulation of the vector quantization problem\n(Luttrell, 1990; Buhmann & Ku\u0308hnel 1993; Luttrell, 1994; Luttrell, 1995) can avoid many of the problems with the SOM procedure discussed earlier."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 0
                            }
                        ],
                        "text": "(Luttrell, 1990; Buhmann & K\u00fchnel 1993; Luttrell, 1994; Luttrell, 1995) can avoid many of the problems with the SOM procedure discussed earlier."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Using self-organizing maps to classify radar range pro\u008eles"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. 5th IEE Conf. on Arti\u008ecial Neural Networks, 335\u2013340."
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Density n e t works Multivariate analysis"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of Meeting on Statistics and Neural Nets"
            },
            "year": 1979
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 200
                            }
                        ],
                        "text": "The technique of parameterized self-organizing maps (PSOMs) involves first fitting a standard SOM model to a data set and then finding a manifold in data space that interpolates the reference vectors (Ritter, 1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 207,
                                "start": 195
                            }
                        ],
                        "text": "The technique of parameterized self-organizing maps (PSOMs) involves rst tting a standard SOM model to a data set and then nding a manifold in data space that interpolates the reference vectors (Ritter, 1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Parameterized self-organizing maps"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings ICANN\u201993 International Conference on Artificial Neural Networks, Amsterdam (pp. 568\u2013575). Berlin: Springer-Verlag."
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 146
                            }
                        ],
                        "text": "Our second example arises from the problem of determining the fraction of oil in a multiphase pipeline carrying a mixture of oil, water, and gas (Bishop & James, 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Analysis of multiphase ows using dual-energy gamma densitometry and neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Nuclear Instruments and Methods in Physics Research A327"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Correlation Theory of Stationary and Related Random Functions Volume I:Basic Results"
            },
            "venue": {
                "fragments": [],
                "text": "Correlation Theory of Stationary and Related Random Functions Volume I:Basic Results"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Self-organizing networks for visualization and classiication"
            },
            "venue": {
                "fragments": [],
                "text": "Information and Classiication"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Statistical analysis of \u008enite mixture distributions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Nonlinear principal component analysis using autoassociative n e u r a l n e t works"
            },
            "venue": {
                "fragments": [],
                "text": "AIChe Journal"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "EEcient implementation of Gaussian processes. in preparation, available at"
            },
            "venue": {
                "fragments": [],
                "text": "EEcient implementation of Gaussian processes. in preparation, available at"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An approach to nonlinear principal components{analysis using radially symmetrical kernel functions"
            },
            "venue": {
                "fragments": [],
                "text": "Statistics and Computing"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 146
                            }
                        ],
                        "text": "Our second example arises from the problem of determining the fraction of oil in a multiphase pipeline carrying a mixture of oil, water, and gas (Bishop & James, 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Analysis of multiphase ows using dualenergy gamma densitometry and neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Nuclear Instruments and Methods in Physics Research"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 146
                            }
                        ],
                        "text": "Our second example arises from the problem of determining the fraction of oil in a multiphase pipeline carrying a mixture of oil, water, and gas (Bishop & James, 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Analysis of multiphase \u008fows using dualenergy gamma densitometry and neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Nuclear Instruments and Methods in Physics Research,"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "GTM: The Generative Topographic Mapping"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1998
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 18,
            "methodology": 20
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 132,
        "totalPages": 14
    },
    "page_url": "https://www.semanticscholar.org/paper/GTM:-The-Generative-Topographic-Mapping-Bishop-Svens\u00e9n/2639515c248f220c73d44688c0097a99b01e1474?sort=total-citations"
}