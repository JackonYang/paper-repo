{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2580777"
                        ],
                        "name": "David M. Magerman",
                        "slug": "David-M.-Magerman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Magerman",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David M. Magerman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 285,
                                "start": 270
                            }
                        ],
                        "text": "\u2026# correct constituents inP # constituents inP\nA constituent inP is \u201ccorrect\u201d if there exists a constituent inT of the same label that spans the same words.2 Table 5 shows results using these measures, as well as results using the slightly more forgiving measures used in Magerman (1995)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 72
                            }
                        ],
                        "text": "For example, the parsers of Black et al. (1993), Jelinek et al. (1994), Magerman (1995) use a general learning technique\u2014decision trees\u2014to learn parsing actions, and need to represent words as bitstrings derived from a statistical word clustering technique."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 73
                            }
                        ],
                        "text": "Other recent machine learning approaches to shift-reduce parsing include Magerman (1995) and Hermjakob and Mooney (1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 100
                            }
                        ],
                        "text": "Table 5 shows that the maximum entropy parser compares favorably to other state-of-the-art systems (Magerman, 1995; Collins, 1996; Goodman, 1997; Charniak, 1997; Collins, 1997) and shows that only the results of Collins (1997) are better in both precision and recall."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 90
                            }
                        ],
                        "text": "The additional information used in these approaches, as well as the word clusters used in Magerman (1995), could in theory be implemented as features in the maximum entropy parser."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 21
                            }
                        ],
                        "text": "Both parses are grammatical, in the sense that a typical context free grammar for English will allow both structures, but only one corresponds to the likely interpretation of the sentence."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 93
                            }
                        ],
                        "text": "Constituent head words are found, when necessary, with the algorithm in Black et al. (1993), Magerman (1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 23
                            }
                        ],
                        "text": "Jelinek et al. (1994), Magerman (1995) also train history-based decision tree models from a treebank for use in a parser, but do not require an explicit hand-written grammar."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 608,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f0a14be7e7f5614b91d0f648ae5f2baafc6d7036",
            "isKey": true,
            "numCitedBy": 717,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Syntactic natural language parsers have shown themselves to be inadequate for processing highly-ambiguous large-vocabulary text, as is evidenced by their poor performance on domains like the Wall Street Journal, and by the movement away from parsing-based approaches to text-processing in general. In this paper, I describe SPATTER, a statistical parser based on decision-tree learning techniques which constructs a complete parse for every sentence and achieves accuracy rates far better than any published result. This work is based on the following premises: (1) grammars are too complex and detailed to develop manually for most interesting domains; (2) parsing models must rely heavily on lexical and contextual information to analyze sentences accurately; and (3) existing n-gram modeling techniques are inadequate for parsing models. In experiments comparing SPATTER with IBM's computer manuals parser, SPATTER significantly outperforms the grammar-based parser. Evaluating SPATTER against the Penn Treebank Wall Street Journal corpus using the PARSEVAL measures, SPATTER achieves 86% precision, 86% recall, and 1.3 crossing brackets per sentence for sentences of 40 words or less, and 91% precision, 90% recall, and 0.5 crossing brackets for sentences between 10 and 20 words in length."
            },
            "slug": "Statistical-Decision-Tree-Models-for-Parsing-Magerman",
            "title": {
                "fragments": [],
                "text": "Statistical Decision-Tree Models for Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "SPATTER is described, a statistical parser based on decision-tree learning techniques which constructs a complete parse for every sentence and achieves accuracy rates far better than any published result."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1818123"
                        ],
                        "name": "E. Black",
                        "slug": "E.-Black",
                        "structuredName": {
                            "firstName": "Ezra",
                            "lastName": "Black",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Black"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472759"
                        ],
                        "name": "F. Jelinek",
                        "slug": "F.-Jelinek",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Jelinek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jelinek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2580777"
                        ],
                        "name": "David M. Magerman",
                        "slug": "David-M.-Magerman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Magerman",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David M. Magerman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2474650"
                        ],
                        "name": "R. Mercer",
                        "slug": "R.-Mercer",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Mercer",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mercer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781292"
                        ],
                        "name": "S. Roukos",
                        "slug": "S.-Roukos",
                        "structuredName": {
                            "firstName": "Salim",
                            "lastName": "Roukos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roukos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 28
                            }
                        ],
                        "text": "For example, the parsers of Black et al. (1993), Jelinek et al. (1994), Magerman (1995) use a general learning technique\u2014decision trees\u2014to learn parsing actions, and need to represent words as bitstrings derived from a statistical word clustering technique."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 40
                            }
                        ],
                        "text": "The parser uses ahistory-basedapproach (Black et al., 1993), in which a probability pX(a | b) is used to score an actiona of procedureX \u2208 {TAG,CHUNK, BUILD,CHECK}, depending on the partial derivationb (also called acontextor history) that is available at the time of the decision."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 72
                            }
                        ],
                        "text": "Constituent head words are found, when necessary, with the algorithm in Black et al. (1993), Magerman (1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 0
                            }
                        ],
                        "text": "Black et al. (1993) introduceshistory-basedparsing, in which decision tree probability models, trained from a treebank, are used to score the different derivations of sentences produced by a hand-written grammar."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 59
                            }
                        ],
                        "text": "Accurate parsing is difficult because subtle aspects of word meaning\u2014from the parser\u2019s view\u2014dramatically affect the interpretation of the sentence."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5598810,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d0ccae6c9f33e41de9c00053aac0bc6c615c7b4a",
            "isKey": true,
            "numCitedBy": 282,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a generative probabilistic model of natural language, which we call HBG, that takes advantage of detailed linguistic information to resolve ambiguity. HBG incorporates lexical, syntactic, semantic, and structural information from the parse tree into the disambiguation process in a novel way. We use a corpus of bracketed sentences, called a Treebank, in combination with decision tree building to tease out the relevant aspects of a parse tree that will determine the correct parse of a sentence. This stands in contrast to the usual approach of further grammar tailoring via the usual linguistic introspection in the hope of generating the correct parse. In head-to-head tests against one of the best existing robust probabilistic parsing models, which we call P-CFG, the HBG model significantly outperforms P-CFG, increasing the parsing accuracy rate from 60% to 75%, a 37% reduction in error."
            },
            "slug": "Towards-History-based-Grammars:-Using-Richer-Models-Black-Jelinek",
            "title": {
                "fragments": [],
                "text": "Towards History-based Grammars: Using Richer Models for Probabilistic Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "HBG incorporates lexical, syntactic, semantic, and structural information from the parse tree into the disambiguation process in a novel way and significantly outperforms P-CFG, increasing the parsing accuracy rate from 60% to 75%, a 37% reduction in error."
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145693410"
                        ],
                        "name": "Ted Briscoe",
                        "slug": "Ted-Briscoe",
                        "structuredName": {
                            "firstName": "Ted",
                            "lastName": "Briscoe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ted Briscoe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144708726"
                        ],
                        "name": "John A. Carroll",
                        "slug": "John-A.-Carroll",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Carroll",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John A. Carroll"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 130
                            }
                        ],
                        "text": "It performs slightly better than or equal to most of the other systems compared in Table 5, and performs only slightly worse than Collins (1997). However, the differences in accuracy are fairly small, and it is unclear if the differences will matter to the performance of applications that require parsed input."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 2220955,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8ad8e98574a275930bf04a477ce3532fd13c503c",
            "isKey": false,
            "numCitedBy": 283,
            "numCiting": 106,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe work toward the construction of a very wide-coverage probabilistic parsing system for natural language (NL), based on LR parsing techniques. The system is intended to rank the large number of syntactic analyses produced by NL grammars according to the frequency of occurrence of the individual rules deployed in each analysis. We discuss a fully automatic procedure for constructing an LR parse table from a unification-based grammar formalism, and consider the suitability of alternative LALR(1) parse table construction methods for large grammars. The parse table is used as the basis for two parsers; a user-driven interactive system that provides a computationally tractable and labor-efficient method of supervised training of the statistical information required to drive the probabilistic parser. The latter is constructed by associating probabilities with the LR parse table directly. This technique is superior to parsers based on probabilistic lexical tagging or probabilistic context-free grammar because it allows for a more context-dependent probabilistic language model, as well as use of a more linguistically adequate grammar formalism. We compare the performance of an optimized variant of Tomita's (1987) generalized LR parsing algorithm to an (efficiently indexed and optimized) chart parser. We report promising results of a pilot study training on 150 noun definitions from the Longman Dictionary of Contemporary English (LDOCE) and retesting on these plus a further 55 definitions. Finally, we discuss limitations of the current system and possible extensions to deal with lexical (syntactic and semantic) frequency of occurrence."
            },
            "slug": "Generalized-Probabilistic-LR-Parsing-of-Natural-Briscoe-Carroll",
            "title": {
                "fragments": [],
                "text": "Generalized Probabilistic LR Parsing of Natural Language (Corpora) with Unification-Based Grammars"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "The construction of a very wide-coverage probabilistic parsing system for natural language (NL) based on LR parsing techniques, intended to rank the large number of syntactic analyses produced by NL grammars according to the frequency of occurrence of the individual rules deployed in each analysis."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Linguistics"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791311"
                        ],
                        "name": "U. Hermjakob",
                        "slug": "U.-Hermjakob",
                        "structuredName": {
                            "firstName": "Ulf",
                            "lastName": "Hermjakob",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Hermjakob"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797655"
                        ],
                        "name": "R. Mooney",
                        "slug": "R.-Mooney",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Mooney",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mooney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 14
                            }
                        ],
                        "text": "The parser of Hermjakob and Mooney (1997) also performs well (90% labelled precision and recall) on the Wall St. Journal domain, but uses a test set comprised of sentences with only frequent words and recovers a different form of annotation, and is therefore not comparable to the parsers in Table 5."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 93
                            }
                        ],
                        "text": "Other recent machine learning approaches to shift-reduce parsing include Magerman (1995) and Hermjakob and Mooney (1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 6
                            }
                        ],
                        "text": "Also, Hermjakob and Mooney (1997) uses a hand-constructed knowledge base and subcategorization table and report 90% labelled precision and recall, using a different test set and evaluation method."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 13
                            }
                        ],
                        "text": "In contrast, Hermjakob and Mooney (1997) use a rich semantic representation when training decision tree and decision list techniques to drive parser actions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10017527,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8f8e36b40ea49eddaa7e45193ec9758e185fd686",
            "isKey": true,
            "numCitedBy": 90,
            "numCiting": 588,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a knowledge and context-based system for parsing and translating natural language and evaluate it on sentences from the Wall Street Journal. Applying machine learning techniques, the system uses parse action examples acquired under supervision to generate a deterministic shift-reduce parser in the form of a decision structure. It relies heavily on context, as encoded in features which describe the morphological, syntactic, semantic and other aspects of a given parse state."
            },
            "slug": "Learning-Parse-and-Translation-Decisions-from-with-Hermjakob-Mooney",
            "title": {
                "fragments": [],
                "text": "Learning Parse and Translation Decisions from Examples with Rich Context"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "A knowledge and context-based system for parsing and translating natural language and evaluate it on sentences from the Wall Street Journal, which relies heavily on context, as encoded in features which describe the morphological, syntactic, semantic and other aspects of a given parse state."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732071"
                        ],
                        "name": "R. Weischedel",
                        "slug": "R.-Weischedel",
                        "structuredName": {
                            "firstName": "Ralph",
                            "lastName": "Weischedel",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Weischedel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3227843"
                        ],
                        "name": "M. Meteer",
                        "slug": "M.-Meteer",
                        "structuredName": {
                            "firstName": "Marie",
                            "lastName": "Meteer",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Meteer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35442155"
                        ],
                        "name": "R. Schwartz",
                        "slug": "R.-Schwartz",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Schwartz",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schwartz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744313"
                        ],
                        "name": "L. Ramshaw",
                        "slug": "L.-Ramshaw",
                        "structuredName": {
                            "firstName": "Lance",
                            "lastName": "Ramshaw",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Ramshaw"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2452408"
                        ],
                        "name": "Jeff Palmucci",
                        "slug": "Jeff-Palmucci",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Palmucci",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeff Palmucci"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6838726,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6a94da952fb8ffc77881028081e90efb494f1c5d",
            "isKey": false,
            "numCitedBy": 329,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "From spring 1990 through fall 1991, we performed a battery of small experiments to test the effectiveness of supplementing knowledge-based techniques with probabilistic models. This paper reports our experiments in predicting parts of speech of highly ambiguous words, predicting the intended interpretation of an utterance when more than one interpretation satisfies all known syntactic and semantic constraints, and learning caseframe informationfor verbsfrom example uses.From these experiments, we are convinced that probabilistic models based on annotated corpora can effectively reduce the ambiguity in processing text and can be used to acquire lexical informationfrom a corpus, by supplementing knowledge-based techniques.Based on the results of those experiments, we have constructed a new natural language system (PLUM) for extracting data from text, e.g., newswire text."
            },
            "slug": "Coping-with-Ambiguity-and-Unknown-Words-through-Weischedel-Meteer",
            "title": {
                "fragments": [],
                "text": "Coping with Ambiguity and Unknown Words through Probabilistic Models"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "A new natural language system (PLUM) is constructed for extracting data from text, e.g., newswire text, based on results of experiments in predicting parts of speech of highly ambiguous words, predicting the intended interpretation of an utterance when more than one interpretation satisfies all known syntactic and semantic constraints."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143707112"
                        ],
                        "name": "M. Collins",
                        "slug": "M.-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 116
                            }
                        ],
                        "text": "Table 5 shows that the maximum entropy parser compares favorably to other state-of-the-art systems (Magerman, 1995; Collins, 1996; Goodman, 1997; Charniak, 1997; Collins, 1997) and shows that only the results of Collins (1997) are better in both precision and recall."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 168
                            }
                        ],
                        "text": "Recent corpus-based parsers differ in the simplicity of their representation and the degree of supervision necessary, but agree in that they resolve parse structure ambiguities by looking at certain cooccurrences of constituenthead wordsin the ambiguous parse."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 0
                            }
                        ],
                        "text": "Collins (1996), Goodman (1997), Charniak (1997), Collins (1997) do not use general machine learning algorithms, but instead develop specialized statistical estimation techniques for their respective parsing tasks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 29
                            }
                        ],
                        "text": "Other parsers, like those of Collins (1996), Goodman (1997), Charniak (1997), Collins (1997) use natural linguistic representations of words and constituents, but do not use general machine learning techniques."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 212,
                                "start": 199
                            }
                        ],
                        "text": "\u2026show superior accuracies on freely occurring text are all classified as tatisticalor corpus-based, since they automatically learn to approximate syntactic and semantic knowledge for parsing from a large corpus of text, called atr ebank,\nthat has been manually annotated with syntactic information."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "Collins (1996, 1997) uses chart-parsing techniques with head word bigram statistics derived from a treebank."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12615602,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3764baa7465201f054083d02b58fa75f883c4461",
            "isKey": true,
            "numCitedBy": 736,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a new statistical parser which is based on probabilities of dependencies between head-words in the parse tree. Standard bigram probability estimation techniques are extended to calculate probabilities of dependencies between pairs of words. Tests using Wall Street Journal data show that the method performs at least as well as SPATTER (Magerman 95; Jelinek et al. 94), which has the best published results for a statistical parser on this task. The simplicity of the approach means the model trains on 40,000 sentences in under 15 minutes. With a beam search strategy parsing speed can be improved to over 200 sentences a minute with negligible loss in accuracy."
            },
            "slug": "A-New-Statistical-Parser-Based-on-Bigram-Lexical-Collins",
            "title": {
                "fragments": [],
                "text": "A New Statistical Parser Based on Bigram Lexical Dependencies"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "A new statistical parser which is based on probabilities of dependencies between head-words in the parse tree, which trains on 40,000 sentences in under 15 minutes and can be improved to over 200 sentences a minute with negligible loss in accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749837"
                        ],
                        "name": "Eugene Charniak",
                        "slug": "Eugene-Charniak",
                        "structuredName": {
                            "firstName": "Eugene",
                            "lastName": "Charniak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eugene Charniak"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 9
                            }
                        ],
                        "text": "In order to evaluate the accuracy of a statistical parser, we first train it on a subset of the treebank, test it on another non-overlapping subset, and then compare the labelled syntactic constituents it proposes with the labelled syntactic constituents in the annotation of the treebank."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 146
                            }
                        ],
                        "text": "Table 5 shows that the maximum entropy parser compares favorably to other state-of-the-art systems (Magerman, 1995; Collins, 1996; Goodman, 1997; Charniak, 1997; Collins, 1997) and shows that only the results of Collins (1997) are better in both precision and recall."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 215,
                                "start": 200
                            }
                        ],
                        "text": "Recent corpus-based parsers differ in the simplicity of their representation and the degree of supervision necessary, but agree in that they resolve parse structure ambiguities by looking at certain cooccurrences of constituenthead wordsin the ambiguous parse."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 32
                            }
                        ],
                        "text": "Collins (1996), Goodman (1997), Charniak (1997), Collins (1997) do not use general machine learning algorithms, but instead develop specialized statistical estimation techniques for their respective parsing tasks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 61
                            }
                        ],
                        "text": "Other parsers, like those of Collins (1996), Goodman (1997), Charniak (1997), Collins (1997) use natural linguistic representations of words and constituents, but do not use general machine learning techniques."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 0
                            }
                        ],
                        "text": "Charniak (1997)\nuses head word bigram statistics with a probabilistic context free grammar, while Goodman (1997) uses head word bigram statistics with a probabilistic feature grammar."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9880507,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2a5e619f2c5f4220438b1357e596db5b1578398d",
            "isKey": true,
            "numCitedBy": 643,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a parsing system based upon a language model for English that is, in turn, based upon assigning probabilities to possible parses for a sentence. This model is used in a parsing system by finding the parse for the sentence with the highest probability. This system outperforms previous schemes. As this is the third in a series of parsers by different authors that are similar enough to invite detailed comparisons but different enough to give rise to different levels of performance, we also report on some experiments designed to identify what aspects of these systems best explain their relative performance."
            },
            "slug": "Statistical-Parsing-with-a-Context-Free-Grammar-and-Charniak",
            "title": {
                "fragments": [],
                "text": "Statistical Parsing with a Context-Free Grammar and Word Statistics"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "A parsing system based upon a language model for English that is, in turn, based upon assigning probabilities to possible parses for a sentence that outperforms previous schemes is described."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI/IAAI"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714612"
                        ],
                        "name": "S. Sekine",
                        "slug": "S.-Sekine",
                        "structuredName": {
                            "firstName": "Satoshi",
                            "lastName": "Sekine",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sekine"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Portability across domains is an important concern, since corpus-based methods will suffer in accuracy if they are tested in a domain that is unrelated to the one in which they are trained (e.g., see  Sekine (1997) )."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 213,
                                "start": 200
                            }
                        ],
                        "text": "Portability across domains is an important concern, since corpus-based methods will suffer in accuracy if they are tested in a domain that is unrelated to the one in which they are trained (e.g., see Sekine (1997))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 470934,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f75ff8694578b82308c397e5f18252a5b3ed39a9",
            "isKey": false,
            "numCitedBy": 100,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "A major concern in corpus based approaches is that the applicability of the acquired knowledge may be limited by some feature of the corpus, in particular, the notion of text 'domain'. In order to examine the domain dependence of parsing, in this paper, we report 1) Comparison of structure distributions across domains; 2) Examples of domain specific structures; and 3) Parsing experiment using some domain dependent grammars. The observations using the Brown corpus demonstrate domain dependence and idiosyncrasy of syntactic structure. The parsing results show that the best accuracy is obtained using the grammar acquired from the same domain or the same class (fiction or nonfiction). We will also discuss the relationship between parsing accuracy and the size of training corpus."
            },
            "slug": "The-Domain-Dependence-of-Parsing-Sekine",
            "title": {
                "fragments": [],
                "text": "The Domain Dependence of Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Comparison of structure distributions across domains; examples of domain specific structures; and Parsing experiment using some domain dependent grammars demonstrate domain dependence and idiosyncrasy of syntactic structure."
            },
            "venue": {
                "fragments": [],
                "text": "ANLP"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472759"
                        ],
                        "name": "F. Jelinek",
                        "slug": "F.-Jelinek",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Jelinek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jelinek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2580777"
                        ],
                        "name": "David M. Magerman",
                        "slug": "David-M.-Magerman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Magerman",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David M. Magerman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2474650"
                        ],
                        "name": "R. Mercer",
                        "slug": "R.-Mercer",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Mercer",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mercer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793475"
                        ],
                        "name": "A. Ratnaparkhi",
                        "slug": "A.-Ratnaparkhi",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ratnaparkhi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ratnaparkhi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781292"
                        ],
                        "name": "S. Roukos",
                        "slug": "S.-Roukos",
                        "structuredName": {
                            "firstName": "Salim",
                            "lastName": "Roukos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roukos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 49
                            }
                        ],
                        "text": "For example, the parsers of Black et al. (1993), Jelinek et al. (1994), Magerman (1995) use a general learning technique\u2014decision trees\u2014to learn parsing actions, and need to represent words as bitstrings derived from a statistical word clustering technique."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 0
                            }
                        ],
                        "text": "Jelinek et al. (1994), Magerman (1995) also train history-based decision tree models from a treebank for use in a parser, but do not require an explicit hand-written grammar."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 936966,
            "fieldsOfStudy": [
                "Linguistics",
                "Computer Science"
            ],
            "id": "5bfa91e7ec19c6401a763c73f2a2007c04836609",
            "isKey": false,
            "numCitedBy": 110,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Parser development is generally viewed as a primarily linguistic enterprise. A grammarian examines sentences, skillfully extracts the linguistic generalizations evident in the data, and writes grammar rules which cover the language. The grammarian then evaluates the performance of the grammar, and upon analysis of the errors made by the grammar-based parser, carefully refines the rules, repeating this process, typically over a period of several years."
            },
            "slug": "Decision-Tree-Parsing-using-a-Hidden-Derivation-Jelinek-Lafferty",
            "title": {
                "fragments": [],
                "text": "Decision Tree Parsing using a Hidden Derivation Model"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The grammarian then evaluates the performance of the grammar, and upon analysis of the errors made by the grammar-based parser, carefully refines the rules, repeating this process, typically over a period of several years."
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145903504"
                        ],
                        "name": "R. Rosenfeld",
                        "slug": "R.-Rosenfeld",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Rosenfeld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rosenfeld"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "While the exact nature of the evidence is task dependent, the framework itself is independent of the parsing task and can be used for many other problems, like language modeling for speech recognition (Lau, Rosenfeld, & Roukos, 1993;  Rosenfeld, 1996 ) and machine translation (Berger et al., 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 249,
                                "start": 234
                            }
                        ],
                        "text": "While the exact nature of the evidence is task dependent, the framework itself is independent of the parsing task and can be used for many other problems, like language modeling for speech recognition (Lau, Rosenfeld, & Roukos, 1993; Rosenfeld, 1996) and machine translation (Berger et al., 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13110923,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "076fa8d095c37c657f2aff39cf90bc2ea883b7cb",
            "isKey": false,
            "numCitedBy": 608,
            "numCiting": 94,
            "paperAbstract": {
                "fragments": [],
                "text": "An adaptive statistical language model is described, which successfully integrates long distance linguistic information with other knowledge sources. Most existing statistical language models exploit only the immediate history of a text. To extract information from further back in the document's history, we propose and usetrigger pairsas the basic information bearing elements. This allows the model to adapt its expectations to the topic of discourse. Next, statistical evidence from multiple sources must be combined. Traditionally, linear interpolation and its variants have been used, but these are shown here to be seriously deficient. Instead, we apply the principle of Maximum Entropy (ME). Each information source gives rise to a set of constraints, to be imposed on the combined estimate. The intersection of these constraints is the set of probability functions which are consistent with all the information sources. The function with the highest entropy within that set is the ME solution. Given consistent statistical evidence, a unique ME solution is guaranteed to exist, and an iterative algorithm exists which is guaranteed to converge to it. The ME framework is extremely general: any phenomenon that can be described in terms of statistics of the text can be readily incorporated. An adaptive language model based on the ME approach was trained on theWall Street Journalcorpus, and showed a 32\u201339% perplexity reduction over the baseline. When interfaced to SPHINX-II, Carnegie Mellon's speech recognizer, it reduced its error rate by 10\u201314%. This thus illustrates the feasibility of incorporating many diverse knowledge sources in a single, unified statistical framework."
            },
            "slug": "A-maximum-entropy-approach-to-adaptive-statistical-Rosenfeld",
            "title": {
                "fragments": [],
                "text": "A maximum entropy approach to adaptive statistical language modelling"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "An adaptive statistical language model is described, which successfully integrates long distance linguistic information with other knowledge sources, and shows the feasibility of incorporating many diverse knowledge sources in a single, unified statistical framework."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Speech Lang."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744313"
                        ],
                        "name": "L. Ramshaw",
                        "slug": "L.-Ramshaw",
                        "structuredName": {
                            "firstName": "Lance",
                            "lastName": "Ramshaw",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Ramshaw"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734174"
                        ],
                        "name": "M. Marcus",
                        "slug": "M.-Marcus",
                        "structuredName": {
                            "firstName": "Mitchell",
                            "lastName": "Marcus",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Marcus"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Section 3.1 describes the actions of the tree-building procedures, Section 3.2 describe the maximum entropy probability models, and Section 3.3 describes the algorithm that searches for the highest scoring parse tree."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 79
                            }
                        ],
                        "text": "The chunking in our second pass differs from other chunkers in the literature (Ramshaw & Marcus, 1995; Church, 1988) in that it finds chunks of all constituent labels, and not just noun phrase chunks."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 725590,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d9c71db75046473f0e3d3229950d7c84c09afd5e",
            "isKey": true,
            "numCitedBy": 1530,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Transformation-based learning, a technique introduced by Eric Brill (1993b), has been shown to do part-of-speech tagging with fairly high accuracy. This same method can be applied at a higher level of textual interpretation for locating chunks in the tagged text, including non-recursive \u201cbaseNP\u201d chunks. For this purpose, it is convenient to view chunking as a tagging problem by encoding the chunk structure in new tags attached to each word. In automatic tests using Treebank-derived data, this technique achieved recall and precision rates of roughly 93% for baseNP chunks (trained on 950K words) and 88% for somewhat more complex chunks that partition the sentence (trained on 200K words). Working in this new application and with larger template and training sets has also required some interesting adaptations to the transformation-based learning approach."
            },
            "slug": "Text-Chunking-using-Transformation-Based-Learning-Ramshaw-Marcus",
            "title": {
                "fragments": [],
                "text": "Text Chunking using Transformation-Based Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work has shown that the transformation-based learning approach can be applied at a higher level of textual interpretation for locating chunks in the tagged text, including non-recursive \u201cbaseNP\u201d chunks."
            },
            "venue": {
                "fragments": [],
                "text": "VLC@ACL"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "17446277"
                        ],
                        "name": "J. Goodman",
                        "slug": "J.-Goodman",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Goodman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Goodman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 131
                            }
                        ],
                        "text": "Table 5 shows that the maximum entropy parser compares favorably to other state-of-the-art systems (Magerman, 1995; Collins, 1996; Goodman, 1997; Charniak, 1997; Collins, 1997) and shows that only the results of Collins (1997) are better in both precision and recall."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 184
                            }
                        ],
                        "text": "Recent corpus-based parsers differ in the simplicity of their representation and the degree of supervision necessary, but agree in that they resolve parse structure ambiguities by looking at certain cooccurrences of constituenthead wordsin the ambiguous parse."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 16
                            }
                        ],
                        "text": "Collins (1996), Goodman (1997), Charniak (1997), Collins (1997) do not use general machine learning algorithms, but instead develop specialized statistical estimation techniques for their respective parsing tasks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 45
                            }
                        ],
                        "text": "Other parsers, like those of Collins (1996), Goodman (1997), Charniak (1997), Collins (1997) use natural linguistic representations of words and constituents, but do not use general machine learning techniques."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 98
                            }
                        ],
                        "text": "Charniak (1997)\nuses head word bigram statistics with a probabilistic context free grammar, while Goodman (1997) uses head word bigram statistics with a probabilistic feature grammar."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18774233,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2588593c42126e059fb8aad7673fa1736755f1e1",
            "isKey": true,
            "numCitedBy": 61,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new formalism, probabilistic feature grammar (PFG). PFGs combine most of the best properties of several other formalisms, including those of Collins, Magerman, and Charniak, and in experiments have comparable or better performance. PFGs generate features one at a time, probabilistically, conditioning the probabilities of each feature on other features in a local context. Because the conditioning is local, efficient polynomial time parsing algorithms exist for computing inside, outside, and Viterbi parses. PFGs can produce probabilities of strings, making them potentially useful for language modeling. Precision and recall results are comparable to the state of the art with words, and the best reported without words."
            },
            "slug": "Probabilistic-Feature-Grammars-Goodman",
            "title": {
                "fragments": [],
                "text": "Probabilistic Feature Grammars"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "Probabilistic feature grammar combines most of the best properties of several other formalisms, including those of Collins, Magerman, and Charniak, and in experiments have comparable or better performance."
            },
            "venue": {
                "fragments": [],
                "text": "IWPT"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2244184"
                        ],
                        "name": "Kenneth Ward Church",
                        "slug": "Kenneth-Ward-Church",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Church",
                            "middleNames": [
                                "Ward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenneth Ward Church"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 15
                            }
                        ],
                        "text": "This implies that it is feasible to use maximum entropy models and other general learning techniques to drive the actions of other kinds of parsers trained from more linguistically sophisticated treebanks."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 103
                            }
                        ],
                        "text": "The chunking in our second pass differs from other chunkers in the literature (Ramshaw & Marcus, 1995; Church, 1988) in that it finds chunks of all constituent labels, and not just noun phrase chunks."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 3166885,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a7e084fe51a40eeaaf79bf0b78e837d5bc4a8e10",
            "isKey": false,
            "numCitedBy": 1058,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "A program that tags each word in an input sentence with the most likely part of speech has been written. The program uses a linear-time dynamic programming algorithm to find an assignment of parts of speech to words that optimizes the product of (a) lexical probabilities (probability of observing part of speech i given word i) and (b) contextual probabilities (probability of observing part of speech i given n following parts of speech). Program performance is encouraging; a 400-word sample is presented and is judged to be 99.5% correct.<<ETX>>"
            },
            "slug": "A-Stochastic-Parts-Program-and-Noun-Phrase-Parser-Church",
            "title": {
                "fragments": [],
                "text": "A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "A program that tags each word in an input sentence with the most likely part of speech has been written and performance is encouraging; a 400-word sample is presented and is judged to be 99.5% correct."
            },
            "venue": {
                "fragments": [],
                "text": "ANLP"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35371521"
                        ],
                        "name": "Kristina Toutanvoa",
                        "slug": "Kristina-Toutanvoa",
                        "structuredName": {
                            "firstName": "Kristina",
                            "lastName": "Toutanvoa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kristina Toutanvoa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10807721,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1504a9d5829033a8cb4cf37b8bb13dfd4baddc7b",
            "isKey": false,
            "numCitedBy": 1574,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents results for a maximum-entropy-based part of speech tagger, which achieves superior performance principally by enriching the information sources used for tagging. In particular, we get improved results by incorporating these features: (i) more extensive treatment of capitalization for unknown words; (ii) features for the disambiguation of the tense forms of verbs; (iii) features for disambiguating particles from prepositions and adverbs. The best resulting accuracy for the tagger on the Penn Treebank is 96.86% overall, and 86.91% on previously unseen words."
            },
            "slug": "Enriching-the-Knowledge-Sources-Used-in-a-Maximum-Toutanvoa-Manning",
            "title": {
                "fragments": [],
                "text": "Enriching the Knowledge Sources Used in a Maximum Entropy Part-of-Speech Tagger"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "This paper presents results for a maximum-entropy-based part of speech tagger, which achieves superior performance principally by enriching the information sources used for tagging by incorporating these features: more extensive treatment of capitalization for unknown words, and features for the disambiguation of the tense forms of verbs."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734174"
                        ],
                        "name": "M. Marcus",
                        "slug": "M.-Marcus",
                        "structuredName": {
                            "firstName": "Mitchell",
                            "lastName": "Marcus",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Marcus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2424234"
                        ],
                        "name": "Beatrice Santorini",
                        "slug": "Beatrice-Santorini",
                        "structuredName": {
                            "firstName": "Beatrice",
                            "lastName": "Santorini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Beatrice Santorini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2063206"
                        ],
                        "name": "Mary Ann Marcinkiewicz",
                        "slug": "Mary-Ann-Marcinkiewicz",
                        "structuredName": {
                            "firstName": "Mary",
                            "lastName": "Marcinkiewicz",
                            "middleNames": [
                                "Ann"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mary Ann Marcinkiewicz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 138
                            }
                        ],
                        "text": "When parsing a sentence, the parser uses a search procedure that efficiently explores the space of possible parse trees, and attempts to find the highest scoring parse tree."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 252796,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0b44fcbeea9415d400c5f5789d6b892b6f98daff",
            "isKey": false,
            "numCitedBy": 8177,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : As a result of this grant, the researchers have now published oil CDROM a corpus of over 4 million words of running text annotated with part-of- speech (POS) tags, with over 3 million words of that material assigned skeletal grammatical structure. This material now includes a fully hand-parsed version of the classic Brown corpus. About one half of the papers at the ACL Workshop on Using Large Text Corpora this past summer were based on the materials generated by this grant."
            },
            "slug": "Building-a-Large-Annotated-Corpus-of-English:-The-Marcus-Santorini",
            "title": {
                "fragments": [],
                "text": "Building a Large Annotated Corpus of English: The Penn Treebank"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "As a result of this grant, the researchers have now published on CDROM a corpus of over 4 million words of running text annotated with part-of- speech (POS) tags, which includes a fully hand-parsed version of the classic Brown corpus."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710580"
                        ],
                        "name": "A. Berger",
                        "slug": "A.-Berger",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Berger",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Berger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2714577"
                        ],
                        "name": "S. D. Pietra",
                        "slug": "S.-D.-Pietra",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Pietra",
                            "middleNames": [
                                "Della"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. D. Pietra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39944066"
                        ],
                        "name": "V. D. Pietra",
                        "slug": "V.-D.-Pietra",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Pietra",
                            "middleNames": [
                                "J.",
                                "Della"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. D. Pietra"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 65
                            }
                        ],
                        "text": "We follow the conditional maximum entropy framework described in Berger et al. (1996), which chooses p\u2217 such that"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 296,
                                "start": 275
                            }
                        ],
                        "text": "While the exact nature of the evidence is task dependent, the framework itself is independent of the parsing task and can be used for many other problems, like language modeling for speech recognition (Lau, Rosenfeld, & Roukos, 1993; Rosenfeld, 1996) and machine translation (Berger et al., 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1085832,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fb486e03369a64de2d5b0df86ec0a7b55d3907db",
            "isKey": false,
            "numCitedBy": 3452,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "The concept of maximum entropy can be traced back along multiple threads to Biblical times. Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition. In this paper, we describe a method for statistical modeling based on maximum entropy. We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing."
            },
            "slug": "A-Maximum-Entropy-Approach-to-Natural-Language-Berger-Pietra",
            "title": {
                "fragments": [],
                "text": "A Maximum Entropy Approach to Natural Language Processing"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A maximum-likelihood approach for automatically constructing maximum entropy models is presented and how to implement this approach efficiently is described, using as examples several problems in natural language processing."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Linguistics"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143707112"
                        ],
                        "name": "M. Collins",
                        "slug": "M.-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 162
                            }
                        ],
                        "text": "Table 5 shows that the maximum entropy parser compares favorably to other state-of-the-art systems (Magerman, 1995; Collins, 1996; Goodman, 1997; Charniak, 1997; Collins, 1997) and shows that only the results of Collins (1997) are better in both precision and recall."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 230,
                                "start": 217
                            }
                        ],
                        "text": "Recent corpus-based parsers differ in the simplicity of their representation and the degree of supervision necessary, but agree in that they resolve parse structure ambiguities by looking at certain cooccurrences of constituenthead wordsin the ambiguous parse."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 49
                            }
                        ],
                        "text": "Collins (1996), Goodman (1997), Charniak (1997), Collins (1997) do not use general machine learning algorithms, but instead develop specialized statistical estimation techniques for their respective parsing tasks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 130
                            }
                        ],
                        "text": "It performs slightly better than or equal to most of the other systems compared in Table 5, and performs only slightly worse than Collins (1997)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 78
                            }
                        ],
                        "text": "Other parsers, like those of Collins (1996), Goodman (1997), Charniak (1997), Collins (1997) use natural linguistic representations of words and constituents, but do not use general machine learning techniques."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 13
                            }
                        ],
                        "text": "For example, Collins (1997) uses the semantic tags in the Penn treebank while the other, slightly less accurate parsers in Table 5 discard this information."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "Collins (1996, 1997) uses chart-parsing techniques with head word bigram statistics derived from a treebank."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1345,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0ffa423a5283396c88ff3d4033d541796bd039cc",
            "isKey": true,
            "numCitedBy": 873,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we first propose a new statistical parsing model, which is a generative model of lexicalised context-free grammar. We then extend the model to include a probabilistic treatment of both subcategorisation and wh-movement. Results on Wall Street Journal text show that the parser performs at 88.1/87.5% constituent precision/recall, an average improvement of 2.3% over (Collins 96)."
            },
            "slug": "Three-Generative,-Lexicalised-Models-for-Parsing-Collins",
            "title": {
                "fragments": [],
                "text": "Three Generative, Lexicalised Models for Statistical Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "A new statistical parsing model is proposed, which is a generative model of lexicalised context-free grammar and extended to include a probabilistic treatment of both subcategorisation and wh-movement."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1818123"
                        ],
                        "name": "E. Black",
                        "slug": "E.-Black",
                        "structuredName": {
                            "firstName": "Ezra",
                            "lastName": "Black",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Black"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35551590"
                        ],
                        "name": "Steven P. Abney",
                        "slug": "Steven-P.-Abney",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Abney",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steven P. Abney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3277328"
                        ],
                        "name": "D. Flickenger",
                        "slug": "D.-Flickenger",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Flickenger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Flickenger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1929125"
                        ],
                        "name": "C. Gdaniec",
                        "slug": "C.-Gdaniec",
                        "structuredName": {
                            "firstName": "Claudia",
                            "lastName": "Gdaniec",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Gdaniec"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1788050"
                        ],
                        "name": "R. Grishman",
                        "slug": "R.-Grishman",
                        "structuredName": {
                            "firstName": "Ralph",
                            "lastName": "Grishman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Grishman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143878270"
                        ],
                        "name": "P. Harrison",
                        "slug": "P.-Harrison",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Harrison",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Harrison"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "21169546"
                        ],
                        "name": "Donald Hindle",
                        "slug": "Donald-Hindle",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Hindle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Donald Hindle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2319612"
                        ],
                        "name": "R. Ingria",
                        "slug": "R.-Ingria",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Ingria",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Ingria"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472759"
                        ],
                        "name": "F. Jelinek",
                        "slug": "F.-Jelinek",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Jelinek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jelinek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761739"
                        ],
                        "name": "Judith L. Klavans",
                        "slug": "Judith-L.-Klavans",
                        "structuredName": {
                            "firstName": "Judith",
                            "lastName": "Klavans",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Judith L. Klavans"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144173823"
                        ],
                        "name": "M. Liberman",
                        "slug": "M.-Liberman",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Liberman",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Liberman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734174"
                        ],
                        "name": "M. Marcus",
                        "slug": "M.-Marcus",
                        "structuredName": {
                            "firstName": "Mitchell",
                            "lastName": "Marcus",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Marcus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781292"
                        ],
                        "name": "S. Roukos",
                        "slug": "S.-Roukos",
                        "structuredName": {
                            "firstName": "Salim",
                            "lastName": "Roukos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roukos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2424234"
                        ],
                        "name": "Beatrice Santorini",
                        "slug": "Beatrice-Santorini",
                        "structuredName": {
                            "firstName": "Beatrice",
                            "lastName": "Santorini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Beatrice Santorini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791072"
                        ],
                        "name": "T. Strzalkowski",
                        "slug": "T.-Strzalkowski",
                        "structuredName": {
                            "firstName": "Tomek",
                            "lastName": "Strzalkowski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Strzalkowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17643319,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "7689778171dc100bb636fc0e4e2ce4063967d3c9",
            "isKey": false,
            "numCitedBy": 554,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of quantitatively comparing the performance of different broad-coverage grammars of English has to date resisted solution. Prima facie, known English grammars appear to disagree strongly with each other as to the elements of even the simplest sentences. For instance, the grammars of Steve Abney (Bellcore), Ezra Black (IBM), Dan Flickinger (Hewlett Packard), Claudia Gdaniec (Logos), Ralph Grishman and Tomek Strzalkowski (NYU), Phil Harrison (Boeing), Don Hindle (AT&T), Bob Ingria (BBN), and Mitch Marcus (U. of Pennsylvania) recognize in common only the following constituents, when each grammarian provides the single parse which he/she would ideally want his/her grammar to specify for three sample Brown Corpus sentences:The famed Yankee Clipper, now retired, has been assisting (as (a batting coach)).One of those capital-gains ventures, in fact, has saddled him (with Gore Court).He said this constituted a (very serious) misuse (of the (Criminal court) processes)."
            },
            "slug": "A-Procedure-for-Quantitatively-Comparing-the-of-Black-Abney",
            "title": {
                "fragments": [],
                "text": "A Procedure for Quantitatively Comparing the Syntactic Coverage of English Grammars"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The problem of quantitatively comparing the performance of different broad-coverage grammars of English has to date resisted solution."
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054787633"
                        ],
                        "name": "Raymond Lau",
                        "slug": "Raymond-Lau",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Lau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Raymond Lau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145903504"
                        ],
                        "name": "R. Rosenfeld",
                        "slug": "R.-Rosenfeld",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Rosenfeld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rosenfeld"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781292"
                        ],
                        "name": "S. Roukos",
                        "slug": "S.-Roukos",
                        "structuredName": {
                            "firstName": "Salim",
                            "lastName": "Roukos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roukos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 73
                            }
                        ],
                        "text": "Other recent machine learning approaches to shift-reduce parsing include Magerman (1995) and Hermjakob and Mooney (1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7536384,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "49b2862ab73be40bf69ac3f457039f18d12df0ae",
            "isKey": false,
            "numCitedBy": 74,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe our ongoing efforts at adaptive statistical language modeling. Central to our approach is the Maximum Entropy (ME) Principle, allowing us to combine evidence from multiple sources, such as long-distance triggers and conventional short-distance trigrams. Given consistent statistical evidence, a unique ME solution is guaranteed to exist, and an iterative algorithm exists which is guaranteed to converge to it. Among the advantages of this approach are its simplicity, its generality, and its incremental nature. Among its disadvantages are its computational requirements. We describe a succession of ME models, culminating in our current Maximum Likelihood/Maximum Entropy (ML/ME) model. Preliminary results with the latter show a 27% perplexity reduction as compared to a conventional trigram model."
            },
            "slug": "Adaptive-Language-Modeling-Using-the-Maximum-Lau-Rosenfeld",
            "title": {
                "fragments": [],
                "text": "Adaptive Language Modeling Using the Maximum Entropy Principle"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A succession of ME models are described, culminating in the current Maximum Likelihood/Maximum Entropy (ML/ME) model, which shows a 27% perplexity reduction as compared to a conventional trigram model."
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793475"
                        ],
                        "name": "A. Ratnaparkhi",
                        "slug": "A.-Ratnaparkhi",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ratnaparkhi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ratnaparkhi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 55
                            }
                        ],
                        "text": "The contextual predicates forTAG, discussed elsewhere (Ratnaparkhi, 1996), look at the previous 2 words and tags, the current word, and the following 2 words."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 51
                            }
                        ],
                        "text": "The search also uses atag dictionary, described in Ratnaparkhi (1996), that is constructed from training data and reduces the number of actions explored by the tagging model."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5914287,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a574e320d899e7e82e341eb64baef7dfe8a24642",
            "isKey": false,
            "numCitedBy": 1545,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a statistical model which trains from a corpus annotated with Part Of Speech tags and assigns them to previously unseen text with state of the art accuracy The model can be classi ed as a Maximum Entropy model and simultaneously uses many contextual features to predict the POS tag Furthermore this paper demonstrates the use of specialized fea tures to model di cult tagging decisions discusses the corpus consistency problems discovered during the implementation of these features and proposes a training strategy that mitigates these problems"
            },
            "slug": "A-Maximum-Entropy-Model-for-Part-Of-Speech-Tagging-Ratnaparkhi",
            "title": {
                "fragments": [],
                "text": "A Maximum Entropy Model for Part-Of-Speech Tagging"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A statistical model which trains from a corpus annotated with Part Of Speech tags and assigns them to previously unseen text with state of the art accuracy and discusses the corpus consistency problems discovered during the implementation of these features."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35551590"
                        ],
                        "name": "Steven P. Abney",
                        "slug": "Steven-P.-Abney",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Abney",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steven P. Abney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 68
                            }
                        ],
                        "text": "Our multi-pass approach is similar to the approach of the parser in Abney (1991), which also first finds chunks in one pass, and then attaches them together in the next pass."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9716882,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "56d7826f3afaa374077f87ca3529709b1ca7e044",
            "isKey": false,
            "numCitedBy": 992,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "I begin with an intuition: when I read a sentence, I read it a chunk at a time. For example, the previous sentence breaks up something like this: \n \n(1) \n \n[I begin] [with an intuition]: [when I read] [a sentence], [I read it] [a chunk] [at a time] \n \n \n \n \n \n \nThese chunks correspond in some way to prosodic patterns. It appears, for instance, that the strongest stresses in the sentence fall one to a chunk, and pauses are most likely to fall between chunks. Chunks also represent a grammatical watershed of sorts. The typical chunk consists of a single content word surrounded by a constellation of function words, matching a fixed template. A simple context-free grammar is quite adequate to describe the structure of chunks. By contrast, the relationships between chunks are mediated more by lexical selection than by rigid templates. Co-occurrence of chunks is determined not just by their syntactic categories, but is sensitive to the precise words that head them; and the order in which chunks occur is much more flexible than the order of words within chunks."
            },
            "slug": "Parsing-By-Chunks-Abney",
            "title": {
                "fragments": [],
                "text": "Parsing By Chunks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The typical chunk consists of a single content word surrounded by a constellation of function words, matching a fixed template, and the relationships between chunks are mediated more by lexical selection than by rigid templates."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2714577"
                        ],
                        "name": "S. D. Pietra",
                        "slug": "S.-D.-Pietra",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Pietra",
                            "middleNames": [
                                "Della"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. D. Pietra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39944066"
                        ],
                        "name": "V. D. Pietra",
                        "slug": "V.-D.-Pietra",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Pietra",
                            "middleNames": [
                                "J.",
                                "Della"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. D. Pietra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Other recent machine learning approaches to shift-reduce parsing include Magerman (1995) and Hermjakob and Mooney (1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 982,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b951b9f78b98a186ba259027996a48e4189d37e5",
            "isKey": false,
            "numCitedBy": 1305,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a technique for constructing random fields from a set of training samples. The learning paradigm builds increasingly complex fields by allowing potential functions, or features, that are supported by increasingly large subgraphs. Each feature has a weight that is trained by minimizing the Kullback-Leibler divergence between the model and the empirical distribution of the training data. A greedy algorithm determines how features are incrementally added to the field and an iterative scaling algorithm is used to estimate the optimal values of the weights. The random field models and techniques introduced in this paper differ from those common to much of the computer vision literature in that the underlying random fields are non-Markovian and have a large number of parameters that must be estimated. Relations to other learning approaches, including decision trees, are given. As a demonstration of the method, we describe its application to the problem of automatic word classification in natural language processing."
            },
            "slug": "Inducing-Features-of-Random-Fields-Pietra-Pietra",
            "title": {
                "fragments": [],
                "text": "Inducing Features of Random Fields"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The random field models and techniques introduced in this paper differ from those common to much of the computer vision literature in that the underlying random fields are non-Markovian and have a large number of parameters that must be estimated."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734174"
                        ],
                        "name": "M. Marcus",
                        "slug": "M.-Marcus",
                        "structuredName": {
                            "firstName": "Mitchell",
                            "lastName": "Marcus",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Marcus"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Aho, Sethi, and Ullman (1988) describe shift-reduce parsing techniques (for programming languages) in detail,  Marcus (1980)  uses shift-reduce parsing techniques for natural language, and Briscoe and Carroll (1993) describe probabilistic approaches to LR parsing, a type of shift-reduce parsing."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 110
                            }
                        ],
                        "text": "Aho, Sethi, and Ullman (1988) describe shift-reduce parsing techniques (for programming languages) in detail, Marcus (1980) uses shift-reduce parsing techniques for natural language, and Briscoe and Carroll (1993) describe probabilistic approaches to LR parsing, a type of shift-reduce parsing."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6616065,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bd459cc59b09e612eeec5327d0690d1508ffe362",
            "isKey": false,
            "numCitedBy": 876,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : Assume that the syntax of natural language can be parsed by a left-to-right deterministic mechanism without facilities for parallelism or backup. It will be shown that this 'determinism' hypothesis, explored within the context of the grammar of English, leads to a simple mechanism, a grammar interpreter. (Author)"
            },
            "slug": "A-theory-of-syntactic-recognition-for-natural-Marcus",
            "title": {
                "fragments": [],
                "text": "A theory of syntactic recognition for natural language"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It will be shown that this 'determinism' hypothesis, explored within the context of the grammar of English, leads to a simple mechanism, a grammar interpreter."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715952"
                        ],
                        "name": "A. Aho",
                        "slug": "A.-Aho",
                        "structuredName": {
                            "firstName": "Alfred",
                            "lastName": "Aho",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Aho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144281449"
                        ],
                        "name": "R. Sethi",
                        "slug": "R.-Sethi",
                        "structuredName": {
                            "firstName": "Ravi",
                            "lastName": "Sethi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sethi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742391"
                        ],
                        "name": "J. Ullman",
                        "slug": "J.-Ullman",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Ullman",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ullman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 116
                            }
                        ],
                        "text": "Examples of constituents that are marked as flat chunks in the Wall St. Journal domain of the Penn treebank include noun phrases such asnonexecutive director, adjective phrases such as 61years old, and quantifier phrases such asbout$370million."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 0
                            }
                        ],
                        "text": "Aho, Sethi, and Ullman (1988) describe shift-reduce parsing techniques (for programming languages) in detail, Marcus (1980) uses shift-reduce parsing techniques for natural language, and Briscoe and Carroll (1993) describe probabilistic approaches to LR parsing, a type of shift-reduce parsing."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 42981739,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b7f33d55d94e75a554251fe7dc07f1d7b4db8e1a",
            "isKey": false,
            "numCitedBy": 9130,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "1 Introduction 1.1 Language Processors 1.2 The Structure of a Compiler 1.3 The Evolution of Programming Languages 1.4 The Science of Building a Compiler 1.5 Applications of Compiler Technology 1.6 Programming Language Basics 1.7 Summary of Chapter 1 1.8 References for Chapter 1 2 A Simple Syntax-Directed Translator 2.1 Introduction 2.2 Syntax Definition 2.3 Syntax-Directed Translation 2.4 Parsing 2.5 A Translator for Simple Expressions 2.6 Lexical Analysis 2.7 Symbol Tables 2.8 Intermediate Code Generation 2.9 Summary of Chapter 2 3 Lexical Analysis 3.1 The Role of the Lexical Analyzer 3.2 Input Buffering 3.3 Specification of Tokens 3.4 Recognition of Tokens 3.5 The Lexical-Analyzer Generator Lex 3.6 Finite Automata 3.7 From Regular Expressions to Automata 3.8 Design of a Lexical-Analyzer Generator 3.9 Optimization of DFA-Based Pattern Matchers 3.10 Summary of Chapter 3 3.11 References for Chapter 3 4 Syntax Analysis 4.1 Introduction 4.2 Context-Free Grammars 4.3 Writing a Grammar 4.4 Top-Down Parsing 4.5 Bottom-Up Parsing 4.6 Introduction to LR Parsing: Simple LR 4.7 More Powerful LR Parsers 4.8 Using Ambiguous Grammars 4.9 Parser Generators 4.10 Summary of Chapter 4 4.11 References for Chapter 4 5 Syntax-Directed Translation 5.1 Syntax-Directed Definitions 5.2 Evaluation Orders for SDD's 5.3 Applications of Syntax-Directed Translation 5.4 Syntax-Directed Translation Schemes 5.5 Implementing L-Attributed SDD's 5.6 Summary of Chapter 5 5.7 References for Chapter 5 6 Intermediate-Code Generation 6.1 Variants of Syntax Trees 6.2 Three-Address Code 6.3 Types and Declarations 6.4 Translation of Expressions 6.5 Type Checking 6.6 Control Flow 6.7 Backpatching 6.8 Switch-Statements 6.9 Intermediate Code for Procedures 6.10 Summary of Chapter 6 6.11 References for Chapter 6 7 Run-Time Environments 7.1 Storage Organization 7.2 Stack Allocation of Space 7.3 Access to Nonlocal Data on the Stack 7.4 Heap Management 7.5 Introduction to Garbage Collection 7.6 Introduction to Trace-Based Collection 7.7 Short-Pause Garbage Collection 7.8 Advanced Topics in Garbage Collection 7.9 Summary of Chapter 7 7.10 References for Chapter 7 8 Code Generation 8.1 Issues in the Design of a Code Generator 8.2 The Target Language 8.3 Addresses in the Target Code 8.4 Basic Blocks and Flow Graphs 8.5 Optimization of Basic Blocks 8.6 A Simple Code Generator 8.7 Peephole Optimization 8.8 Register Allocation and Assignment 8.9 Instruction Selection by Tree Rewriting 8.10 Optimal Code Generation for Expressions 8.11 Dynamic Programming Code-Generation 8.12 Summary of Chapter 8 8.13 References for Chapter 8 9 Machine-Independent Optimizations 9.1 The Principal Sources of Optimization 9.2 Introduction to Data-Flow Analysis 9.3 Foundations of Data-Flow Analysis 9.4 Constant Propagation 9.5 Partial-Redundancy Elimination 9.6 Loops in Flow Graphs 9.7 Region-Based Analysis 9.8 Symbolic Analysis 9.9 Summary of Chapter 9 9.10 References for Chapter 9 10 Instruction-Level Parallelism 10.1 Processor Architectures 10.2 Code-Scheduling Constraints 10.3 Basic-Block Scheduling 10.4 Global Code Scheduling 10.5 Software Pipelining 10.6 Summary of Chapter 10 10.7 References for Chapter 10 11 Optimizing for Parallelism and Locality 11.1 Basic Concepts 11.2 Matrix Multiply: An In-Depth Example 11.3 Iteration Spaces 11.4 Affine Array Indexes 11.5 Data Reuse 11.6 Array Data-Dependence Analysis 11.7 Finding Synchronization-Free Parallelism 11.8 Synchronization Between Parallel Loops 11.9 Pipelining 11.10 Locality Optimizations 11.11 Other Uses of Affine Transforms 11.12 Summary of Chapter 11 11.13 References for Chapter 11 12 Interprocedural Analysis 12.1 Basic Concepts 12.2 Why Interprocedural Analysis? 12.3 A Logical Representation of Data Flow 12.4 A Simple Pointer-Analysis Algorithm 12.5 Context-Insensitive Interprocedural Analysis 12.6 Context-Sensitive Pointer Analysis 12.7 Datalog Implementation by BDD's 12.8 Summary of Chapter 12 12.9 References for Chapter 12 A A Complete Front End A.1 The Source Language A.2 Main A.3 Lexical Analyzer A.4 Symbol Tables and Types A.5 Intermediate Code for Expressions A.6 Jumping Code for Boolean Expressions A.7 Intermediate Code for Statements A.8 Parser A.9 Creating the Front End B Finding Linearly Independent Solutions Index"
            },
            "slug": "Compilers:-Principles,-Techniques,-and-Tools-Aho-Sethi",
            "title": {
                "fragments": [],
                "text": "Compilers: Principles, Techniques, and Tools"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "This book discusses the design of a Code Generator, the role of the Lexical Analyzer, and other topics related to code generation and optimization."
            },
            "venue": {
                "fragments": [],
                "text": "Addison-Wesley series in computer science / World student series edition"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35206065"
                        ],
                        "name": "E. Jaynes",
                        "slug": "E.-Jaynes",
                        "structuredName": {
                            "firstName": "Edwin",
                            "lastName": "Jaynes",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Jaynes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 33
                            }
                        ],
                        "text": "The Principle of Maximum Entropy (Jaynes, 1957) recommends that we choose the model p\u2217 with the highest entropy over the set of those models that are consistent with the observed expectations, i."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 34
                            }
                        ],
                        "text": "The Principle of Maximum Entropy (Jaynes, 1957) recommends that we choose the modelp\u2217 with the highest entropy over the set of those models that are consistent with the observed expectations, i.e., the model is that is maximally noncomittal beyond meeting the observed evidence."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17870175,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "08b67692bc037eada8d3d7ce76cc70994e7c8116",
            "isKey": false,
            "numCitedBy": 10877,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Treatment of the predictive aspect of statistical mechanics as a form of statistical inference is extended to the density-matrix formalism and applied to a discussion of the relation between irreversibility and information loss. A principle of \"statistical complementarity\" is pointed out, according to which the empirically verifiable probabilities of statistical mechanics necessarily correspond to incomplete predictions. A preliminary discussion is given of the second law of thermodynamics and of a certain class of irreversible processes, in an approximation equivalent to that of the semiclassical theory of radiation."
            },
            "slug": "Information-Theory-and-Statistical-Mechanics-Jaynes",
            "title": {
                "fragments": [],
                "text": "Information Theory and Statistical Mechanics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1957
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49223598"
                        ],
                        "name": "J. Darroch",
                        "slug": "J.-Darroch",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Darroch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Darroch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "12360582"
                        ],
                        "name": "D. Ratcliff",
                        "slug": "D.-Ratcliff",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Ratcliff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ratcliff"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 115
                            }
                        ],
                        "text": "The parameters{\u03b11, . . . , \u03b1k} of each model are found automatically with theG neralized Iterative Scalingalgorithm (Darroch & Ratcliff, 1972), which is summarized below:\n1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Iterative Scaling algorithm ( Darroch & Ratcliff, 1972 ), which is summarized below:"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 120862597,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "37c931cbaa9217b829596dd196520a838562a109",
            "isKey": false,
            "numCitedBy": 1329,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Generalized-Iterative-Scaling-for-Log-Linear-Models-Darroch-Ratcliff",
            "title": {
                "fragments": [],
                "text": "Generalized Iterative Scaling for Log-Linear Models"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1972
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35097577"
                        ],
                        "name": "W. Francis",
                        "slug": "W.-Francis",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Francis",
                            "middleNames": [
                                "Nelson"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Francis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "16318570"
                        ],
                        "name": "H. Kucera",
                        "slug": "H.-Kucera",
                        "structuredName": {
                            "firstName": "Henry",
                            "lastName": "Kucera",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Kucera"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053677731"
                        ],
                        "name": "Andrew Mackie",
                        "slug": "Andrew-Mackie",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Mackie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Mackie"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The new domains, namely \u201cMagazine & Journal Articles\u201d, \u201cGeneral Fiction\u201d, and \u201cAdventure Fiction\u201d, are from the Brown corpus ( Francis & Kucera, 1982 ), a collection of English text from Brown University that represents a wide variety of different domains."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60972899,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "729316fbded86763104f3412cadf98f00a9a3993",
            "isKey": false,
            "numCitedBy": 2159,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "FREQUENCY-ANALYSIS-OF-ENGLISH-USAGE:-LEXICON-AND-Francis-Kucera",
            "title": {
                "fragments": [],
                "text": "FREQUENCY ANALYSIS OF ENGLISH USAGE: LEXICON AND GRAMMAR"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145844737"
                        ],
                        "name": "James F. Allen",
                        "slug": "James-F.-Allen",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Allen",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James F. Allen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 63
                            }
                        ],
                        "text": "(Many other parsing techniques exist for natural language, see Allen (1995) for an introduction.)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 59902526,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7e2fcb5d6f3727ad76ae7e2edc3d978e5900ebd8",
            "isKey": false,
            "numCitedBy": 390,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Natural-language-understanding-(2nd-ed.)-Allen",
            "title": {
                "fragments": [],
                "text": "Natural language understanding (2nd ed.)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2244184"
                        ],
                        "name": "Kenneth Ward Church",
                        "slug": "Kenneth-Ward-Church",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Church",
                            "middleNames": [
                                "Ward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenneth Ward Church"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The chunking in our second pass differs from other chunkers in the literature (Ramshaw & Marcus, 1995;  Church, 1988 ) in that it finds chunks of all constituent labels, and not just noun phrase chunks."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 103
                            }
                        ],
                        "text": "The chunking in our second pass differs from other chunkers in the literature (Ramshaw & Marcus, 1995; Church, 1988) in that it finds chunks of all constituent labels, and not just noun phrase chunks."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 62735070,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bedfbe4395ead517cfa6e369a3564c8fb6876c04",
            "isKey": false,
            "numCitedBy": 349,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Stochastic-Parts-Program-and-Noun-Phrase-Parser-Church",
            "title": {
                "fragments": [],
                "text": "A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text"
            },
            "venue": {
                "fragments": [],
                "text": "ANLP"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 250,
                                "start": 201
                            }
                        ],
                        "text": "While the exact nature of the evidence is task dependent, the framework itself is independent of the parsing task and can be used for many other problems, like language modeling for speech recognition (Lau, Rosenfeld, & Roukos, 1993; Rosenfeld, 1996) and machine translation (Berger et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 249,
                                "start": 234
                            }
                        ],
                        "text": "While the exact nature of the evidence is task dependent, the framework itself is independent of the parsing task and can be used for many other problems, like language modeling for speech recognition (Lau, Rosenfeld, & Roukos, 1993; Rosenfeld, 1996) and machine translation (Berger et al., 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A maximum entropy approach to adaptive statistical language modeling"
            },
            "venue": {
                "fragments": [],
                "text": "Computer, Speech, and Language"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 68
                            }
                        ],
                        "text": "Our multi-pass approach is similar to the approach of the parser in Abney (1991), which also first finds chunks in one pass, and then attaches them together in the next pass."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Parsing by Chunks Principle-based parsing"
            },
            "venue": {
                "fragments": [],
                "text": "Parsing by Chunks Principle-based parsing"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Building a large annotated corpus"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 63
                            }
                        ],
                        "text": "(Many other parsing techniques exist for natural language, see Allen (1995) for an introduction."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 63
                            }
                        ],
                        "text": "(Many other parsing techniques exist for natural language, see Allen (1995) for an introduction.)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "1995).Natural language understanding"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 18,
            "methodology": 18,
            "result": 6
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 33,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/Learning-to-Parse-Natural-Language-with-Maximum-Ratnaparkhi/e6140a793a4554806eb39d15c018d8f782d2ac1e?sort=total-citations"
}