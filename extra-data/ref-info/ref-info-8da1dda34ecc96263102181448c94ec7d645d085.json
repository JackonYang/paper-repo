{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145491571"
                        ],
                        "name": "L. Jones",
                        "slug": "L.-Jones",
                        "structuredName": {
                            "firstName": "Lee",
                            "lastName": "Jones",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Jones"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122808966,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "fc91f9756da56e3ea7f1f18ca565606b96652a0c",
            "isKey": false,
            "numCitedBy": 153,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "A constructive algorithm for uniformly approximating real continuous mappings by linear combinations of bounded sigmoidal functions is given. G. Cybenko (1989) has demonstrated the existence of uniform approximations to any continuous f provided that sigma is continuous; the proof is nonconstructive, relying on the Hahn-Branch theorem and the dual characterization of C(I/sup n/). Cybenko's result is extended to include any bounded sigmoidal (even nonmeasurable ones). The approximating functions are explicitly constructed. The number of terms in the linear combination is minimal for first-order terms. >"
            },
            "slug": "Constructive-approximations-for-neural-networks-by-Jones",
            "title": {
                "fragments": [],
                "text": "Constructive approximations for neural networks by sigmoidal functions"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "G. Cybenko (1989) has demonstrated the existence of uniform approximations to any continuous f provided that sigma is continuous, relying on the Hahn-Branch theorem and the dual characterization of C(I/sup n/)."
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IEEE"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1899177"
                        ],
                        "name": "Ken-ichi Funahashi",
                        "slug": "Ken-ichi-Funahashi",
                        "structuredName": {
                            "firstName": "Ken-ichi",
                            "lastName": "Funahashi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ken-ichi Funahashi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10203109,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "386cbc45ceb59a7abb844b5078e5c944f17723b4",
            "isKey": false,
            "numCitedBy": 4188,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-the-approximate-realization-of-continuous-by-Funahashi",
            "title": {
                "fragments": [],
                "text": "On the approximate realization of continuous mappings by neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764952"
                        ],
                        "name": "K. Hornik",
                        "slug": "K.-Hornik",
                        "structuredName": {
                            "firstName": "Kurt",
                            "lastName": "Hornik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Hornik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2964655"
                        ],
                        "name": "M. Stinchcombe",
                        "slug": "M.-Stinchcombe",
                        "structuredName": {
                            "firstName": "Maxwell",
                            "lastName": "Stinchcombe",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Stinchcombe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2149702798"
                        ],
                        "name": "H. White",
                        "slug": "H.-White",
                        "structuredName": {
                            "firstName": "Halbert",
                            "lastName": "White",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. White"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2757547,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "f22f6972e66bdd2e769fa64b0df0a13063c0c101",
            "isKey": false,
            "numCitedBy": 17352,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Multilayer-feedforward-networks-are-universal-Hornik-Stinchcombe",
            "title": {
                "fragments": [],
                "text": "Multilayer feedforward networks are universal approximators"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144371774"
                        ],
                        "name": "S. M. Carroll",
                        "slug": "S.-M.-Carroll",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Carroll",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. M. Carroll"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2203239"
                        ],
                        "name": "B. Dickinson",
                        "slug": "B.-Dickinson",
                        "structuredName": {
                            "firstName": "Bradley",
                            "lastName": "Dickinson",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Dickinson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 18058503,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8e4bd5422c82009290a5cd71457388f0780530d6",
            "isKey": false,
            "numCitedBy": 173,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors present a method for constructing a feedforward neural net implementing an arbitrarily good approximation to any L/sub 2/ function over (-1, 1)/sup n/. The net uses n input nodes, a single hidden layer whose width is determined by the function to be implemented and the allowable mean square error, and a linear output neuron. Error bounds and an example are given for the method.<<ETX>>"
            },
            "slug": "Construction-of-neural-nets-using-the-radon-Carroll-Dickinson",
            "title": {
                "fragments": [],
                "text": "Construction of neural nets using the radon transform"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The authors present a method for constructing a feedforward neural net implementing an arbitrarily good approximation to any L/sub 2/ function over (-1, 1)/sup n/."
            },
            "venue": {
                "fragments": [],
                "text": "International 1989 Joint Conference on Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2796350"
                        ],
                        "name": "P. Diaconis",
                        "slug": "P.-Diaconis",
                        "structuredName": {
                            "firstName": "Persi",
                            "lastName": "Diaconis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Diaconis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144665070"
                        ],
                        "name": "M. Shahshahani",
                        "slug": "M.-Shahshahani",
                        "structuredName": {
                            "firstName": "Mehrdad",
                            "lastName": "Shahshahani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Shahshahani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 53656111,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "b0f09280ba01ab2e2c60e9450bef332d183ba2f3",
            "isKey": false,
            "numCitedBy": 162,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Projection pursuit algorithms approximate a function of p variables by a sum of nonlinear functions of linear combinations: \\[ (1)\\qquad f\\left( {x_1 , \\cdots ,x_p } \\right) \\doteq \\sum_{i = 1}^n {g_i \\left( {a_{i1} x_1 + \\cdots + a_{ip} x_p } \\right)} . \\] We develop some approximation theory, give a necessary and sufficient condition for equality in (1), and discuss nonuniqueness of the representation."
            },
            "slug": "On-Nonlinear-Functions-of-Linear-Combinations-Diaconis-Shahshahani",
            "title": {
                "fragments": [],
                "text": "On Nonlinear Functions of Linear Combinations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10080270"
                        ],
                        "name": "J. Makhoul",
                        "slug": "J.-Makhoul",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Makhoul",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Makhoul"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152901373"
                        ],
                        "name": "R. Schwartz",
                        "slug": "R.-Schwartz",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Schwartz",
                            "middleNames": [
                                "Evan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schwartz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1402969335"
                        ],
                        "name": "A. El-Jaroudi",
                        "slug": "A.-El-Jaroudi",
                        "structuredName": {
                            "firstName": "Amro",
                            "lastName": "El-Jaroudi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. El-Jaroudi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60660856,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c42c1305ce33c628ffc5401d5de2b0347f50ac78",
            "isKey": false,
            "numCitedBy": 35,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors consider the classification capabilities of feedforward two-layer neural nets with a single hidden layer and having threshold units only; that is they consider the type of decision regions that two-layer nets are capable of forming in the input space. It had been asserted previously that such nets are capable of forming only convex decision regions or nonconvex but connected regions. The authors show that two-layer nets are capable of forming disconnected decision regions as well. In addition to giving examples of the phenomena, they explain why and how disconnected decision regions are formed. They also derive an expression for the number of cells in the input space that are to be grouped together to form the decision regions. This expression can be useful in deciding how many nodes to have in the first layer. The results have bearing on neural networks where the nonlinear elements are smooth (sigmoid) functions rather than threshold functions.<<ETX>>"
            },
            "slug": "Classification-capabilities-of-two-layer-neural-Makhoul-Schwartz",
            "title": {
                "fragments": [],
                "text": "Classification capabilities of two-layer neural nets"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "The authors show that two-layer nets are capable of forming disconnected decision regions as well and derive an expression for the number of cells in the input space that are to be grouped together to form the decision regions."
            },
            "venue": {
                "fragments": [],
                "text": "International Conference on Acoustics, Speech, and Signal Processing,"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2403454"
                        ],
                        "name": "E. Baum",
                        "slug": "E.-Baum",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Baum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Baum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15659829,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "25406e6733a698bfc4ac836f8e74f458e75dad4f",
            "isKey": false,
            "numCitedBy": 1696,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the question of when a network can be expected to generalize from m random training examples chosen from some arbitrary probability distribution, assuming that future test examples are drawn from the same distribution. Among our results are the following bounds on appropriate sample vs. network size. Assume 0 < \u220a 1/8. We show that if m O(W/\u220a log N/\u220a) random examples can be loaded on a feedforward network of linear threshold functions with N nodes and W weights, so that at least a fraction 1 \u220a/2 of the examples are correctly classified, then one has confidence approaching certainty that the network will correctly classify a fraction 1 \u220a of future test examples drawn from the same distribution. Conversely, for fully-connected feedforward nets with one hidden layer, any learning algorithm using fewer than (W/\u220a) random training examples will, for some distributions of examples consistent with an appropriate weight choice, fail at least some fixed fraction of the time to find a weight choice that will correctly classify more than a 1 \u220a fraction of the future test examples."
            },
            "slug": "What-Size-Net-Gives-Valid-Generalization-Baum-Haussler",
            "title": {
                "fragments": [],
                "text": "What Size Net Gives Valid Generalization?"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that if m O(W/ \u220a log N/\u220a) random examples can be loaded on a feedforward network of linear threshold functions with N nodes and W weights, so that at least a fraction 1 \u220a/2 of the examples are correctly classified, then one has confidence approaching certainty that the network will correctly classify a fraction 2 \u220a of future test examples drawn from the same distribution."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144990248"
                        ],
                        "name": "R. Lippmann",
                        "slug": "R.-Lippmann",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Lippmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lippmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8275028,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b8778bb692cf105254fe767ef11a3a8afac4a068",
            "isKey": false,
            "numCitedBy": 3816,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "Artificial neural net models have been studied for many years in the hope of achieving human-like performance in the fields of speech and image recognition. These models are composed of many nonlinear computational elements operating in parallel and arranged in patterns reminiscent of biological neural nets. Computational elements or nodes are connected via weights that are typically adapted during use to improve performance. There has been a recent resurgence in the field of artificial neural nets caused by new net topologies and algorithms, analog VLSI implementation techniques, and the belief that massive parallelism is essential for high performance speech and image recognition. This paper provides an introduction to the field of artificial neural nets by reviewing six important neural net models that can be used for pattern classification. These nets are highly parallel building blocks that illustrate neural net components and design principles and can be used to construct more complex systems. In addition to describing these nets, a major emphasis is placed on exploring how some existing classification and clustering algorithms can be performed using simple neuron-like components. Single-layer nets can implement algorithms required by Gaussian maximum-likelihood classifiers and optimum minimum-error classifiers for binary patterns corrupted by noise. More generally, the decision regions required by any classification algorithm can be generated in a straightforward manner by three-layer feed-forward nets."
            },
            "slug": "An-introduction-to-computing-with-neural-nets-Lippmann",
            "title": {
                "fragments": [],
                "text": "An introduction to computing with neural nets"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper provides an introduction to the field of artificial neural nets by reviewing six important neural net models that can be used for pattern classification and exploring how some existing classification and clustering algorithms can be performed using simple neuron-like components."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE ASSP Magazine"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112105858"
                        ],
                        "name": "William Y. Huang",
                        "slug": "William-Y.-Huang",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Huang",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "William Y. Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144990248"
                        ],
                        "name": "R. Lippmann",
                        "slug": "R.-Lippmann",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Lippmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lippmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11607279,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d1382a29539b3de419d567f679b5f28cee459a49",
            "isKey": false,
            "numCitedBy": 179,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Previous work on nets with continuous-valued inputs led to generative procedures to construct convex decision regions with two-layer perceptrons (one hidden layer) and arbitrary decision regions with three-layer perceptrons (two hidden layers). Here we demonstrate that two-layer perceptron classifiers trained with back propagation can form both convex and disjoint decision regions. Such classifiers are robust, train rapidly, and provide good performance with simple decision regions. When complex decision regions are required, however, convergence time can be excessively long and performance is often no better than that of k-nearest neighbor classifiers. Three neural net classifiers are presented that provide more rapid training under such situations. Two use fixed weights in the first one or two layers and are similar to classifiers that estimate probability density functions using histograms. A third \"feature map classifier\" uses both unsupervised and supervised training. It provides good performance with little supervised training in situations such as speech recognition where much unlabeled training data is available. The architecture of this classifier can be used to implement a neural net k-nearest neighbor classifier."
            },
            "slug": "Neural-Net-and-Traditional-Classifiers-Huang-Lippmann",
            "title": {
                "fragments": [],
                "text": "Neural Net and Traditional Classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is demonstrated that two-layer perceptron classifiers trained with back propagation can form both convex and disjoint decision regions."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "100859010"
                        ],
                        "name": "V. Tikhomirov",
                        "slug": "V.-Tikhomirov",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Tikhomirov",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Tikhomirov"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 116968444,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "300328d09233d3ada652d6aace66353c3bdb5762",
            "isKey": false,
            "numCitedBy": 410,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The aim of this paper is to present a brief proof of the following theorem: Theorem. For any integer n \u2265 2 there are continuous real functions \u03c8 p q (x) on the closed unit interval E 1 = [0;1] such that each continuous real function f(x 1 ,\u2026,x n ) on the n-dimensional unit cube E n is representable as \n \n$$f\\left( {{{x}_{1}}, \\ldots ,{{x}_{n}}} \\right) = \\sum\\limits_{{q = 1}}^{{q = 2n + 1}} {Xq\\left[ {\\sum\\limits_{{p = 1}}^{n} {{{\\psi }^{{pq}}}\\left( {{{x}_{p}}} \\right)} } \\right]} ,$$ \n \n(1) \n \nwhere x q (y) are continuous real functions."
            },
            "slug": "On-the-Representation-of-Continuous-Functions-of-as-Tikhomirov",
            "title": {
                "fragments": [],
                "text": "On the Representation of Continuous Functions of Several Variables as Superpositions of Continuous Functions of one Variable and Addition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50012345"
                        ],
                        "name": "W. Rudin",
                        "slug": "W.-Rudin",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Rudin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Rudin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 119027153,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "f20b8e819512d17f59435043a18573127fea7d82",
            "isKey": false,
            "numCitedBy": 6995,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface Prologue: The Exponential Function Chapter 1: Abstract Integration Set-theoretic notations and terminology The concept of measurability Simple functions Elementary properties of measures Arithmetic in [0, ] Integration of positive functions Integration of complex functions The role played by sets of measure zero Exercises Chapter 2: Positive Borel Measures Vector spaces Topological preliminaries The Riesz representation theorem Regularity properties of Borel measures Lebesgue measure Continuity properties of measurable functions Exercises Chapter 3: Lp-Spaces Convex functions and inequalities The Lp-spaces Approximation by continuous functions Exercises Chapter 4: Elementary Hilbert Space Theory Inner products and linear functionals Orthonormal sets Trigonometric series Exercises Chapter 5: Examples of Banach Space Techniques Banach spaces Consequences of Baire's theorem Fourier series of continuous functions Fourier coefficients of L1-functions The Hahn-Banach theorem An abstract approach to the Poisson integral Exercises Chapter 6: Complex Measures Total variation Absolute continuity Consequences of the Radon-Nikodym theorem Bounded linear functionals on Lp The Riesz representation theorem Exercises Chapter 7: Differentiation Derivatives of measures The fundamental theorem of Calculus Differentiable transformations Exercises Chapter 8: Integration on Product Spaces Measurability on cartesian products Product measures The Fubini theorem Completion of product measures Convolutions Distribution functions Exercises Chapter 9: Fourier Transforms Formal properties The inversion theorem The Plancherel theorem The Banach algebra L1 Exercises Chapter 10: Elementary Properties of Holomorphic Functions Complex differentiation Integration over paths The local Cauchy theorem The power series representation The open mapping theorem The global Cauchy theorem The calculus of residues Exercises Chapter 11: Harmonic Functions The Cauchy-Riemann equations The Poisson integral The mean value property Boundary behavior of Poisson integrals Representation theorems Exercises Chapter 12: The Maximum Modulus Principle Introduction The Schwarz lemma The Phragmen-Lindelof method An interpolation theorem A converse of the maximum modulus theorem Exercises Chapter 13: Approximation by Rational Functions Preparation Runge's theorem The Mittag-Leffler theorem Simply connected regions Exercises Chapter 14: Conformal Mapping Preservation of angles Linear fractional transformations Normal families The Riemann mapping theorem The class L Continuity at the boundary Conformal mapping of an annulus Exercises Chapter 15: Zeros of Holomorphic Functions Infinite Products The Weierstrass factorization theorem An interpolation problem Jensen's formula Blaschke products The Muntz-Szas theorem Exercises Chapter 16: Analytic Continuation Regular points and singular points Continuation along curves The monodromy theorem Construction of a modular function The Picard theorem Exercises Chapter 17: Hp-Spaces Subharmonic functions The spaces Hp and N The theorem of F. and M. Riesz Factorization theorems The shift operator Conjugate functions Exercises Chapter 18: Elementary Theory of Banach Algebras Introduction The invertible elements Ideals and homomorphisms Applications Exercises Chapter 19: Holomorphic Fourier Transforms Introduction Two theorems of Paley and Wiener Quasi-analytic classes The Denjoy-Carleman theorem Exercises Chapter 20: Uniform Approximation by Polynomials Introduction Some lemmas Mergelyan's theorem Exercises Appendix: Hausdorff's Maximality Theorem Notes and Comments Bibliography List of Special Symbols Index"
            },
            "slug": "Real-and-complex-analysis-Rudin",
            "title": {
                "fragments": [],
                "text": "Real and complex analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1966
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2294422"
                        ],
                        "name": "B. Bavarian",
                        "slug": "B.-Bavarian",
                        "structuredName": {
                            "firstName": "Behnam",
                            "lastName": "Bavarian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Bavarian"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7028967,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "62e2b0d49f850f9a6d02f2e9e20b521aa23f2bb0",
            "isKey": false,
            "numCitedBy": 141,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Neural network architecture is presented as one approach to the design and implementation of intelligent control systems. Neural networks can be considered as massively parallel distributed processing systems with the potential for ever-improving performance through dynamical learning. The nomenclature and characteristics of neural networks are outlined. Two simple examples are presented to illustrate applications to control systems: one is fault isolation mapping, and the other involves optimization of a Hopfield network that defines a clockless analog-to-digital conversion.<<ETX>>"
            },
            "slug": "Introduction-to-neural-networks-for-intelligent-Bavarian",
            "title": {
                "fragments": [],
                "text": "Introduction to neural networks for intelligent control"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Two simple examples are presented to illustrate applications to control systems: one is fault isolation mapping, and the other involves optimization of a Hopfield network that defines a clockless analog-to-digital conversion."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Control Systems Magazine"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1881615"
                        ],
                        "name": "R. Dudley",
                        "slug": "R.-Dudley",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Dudley",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Dudley"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 116062098,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "6c06333850b96721709b3162c6332939a2fdce31",
            "isKey": false,
            "numCitedBy": 2181,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "1. Foundations: set theory 2. General topology 3. Measures 4. Integration 5. Lp spaces: introduction to functional analysis 6. Convex sets and duality of normed spaces 7. Measure, topology, and differentiation 8. Introduction to probability theory 9. Convergence of laws and central limit theorems 10. Conditional expectations and martingales 11. Convergence of laws on separable metric spaces 12. Stochastic processes 13. Measurability: Borel isomorphism and analytic sets Appendixes: A. Axiomatic set theory B. Complex numbers, vector spaces, and Taylor's theorem with remainder C. The problem of measure D. Rearranging sums of nonnegative terms E. Pathologies of compact nonmetric spaces Indices."
            },
            "slug": "Real-Analysis-and-Probability-Dudley",
            "title": {
                "fragments": [],
                "text": "Real Analysis and Probability"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741124"
                        ],
                        "name": "L. Valiant",
                        "slug": "L.-Valiant",
                        "structuredName": {
                            "firstName": "Leslie",
                            "lastName": "Valiant",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Valiant"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 59712,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "10ddb646feddc12337b5a755c72e153e37088c02",
            "isKey": false,
            "numCitedBy": 4189,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Humans appear to be able to learn new concepts without needing to be programmed explicitly in any conventional sense. In this paper we regard learning as the phenomenon of knowledge acquisition in the absence of explicit programming. We give a precise methodology for studying this phenomenon from a computational viewpoint. It consists of choosing an appropriate information gathering mechanism, the learning protocol, and exploring the class of concepts that can be learnt using it in a reasonable (polynomial) number of steps. We find that inherent algorithmic complexity appears to set serious limits to the range of concepts that can be so learnt. The methodology and results suggest concrete principles for designing realistic learning systems."
            },
            "slug": "A-theory-of-the-learnable-Valiant",
            "title": {
                "fragments": [],
                "text": "A theory of the learnable"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper regards learning as the phenomenon of knowledge acquisition in the absence of explicit programming, and gives a precise methodology for studying this phenomenon from a computational viewpoint."
            },
            "venue": {
                "fragments": [],
                "text": "STOC '84"
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701656"
                        ],
                        "name": "James L. McClelland",
                        "slug": "James-L.-McClelland",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "McClelland",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James L. McClelland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15291527,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ff2c2e3e83d1e8828695484728393c76ee07a101",
            "isKey": false,
            "numCitedBy": 15710,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The fundamental principles, basic mechanisms, and formal analyses involved in the development of parallel distributed processing (PDP) systems are presented in individual chapters contributed by leading experts. Topics examined include distributed representations, PDP models and general issues in cognitive science, feature discovery by competitive learning, the foundations of harmony theory, learning and relearning in Boltzmann machines, and learning internal representations by error propagation. Consideration is given to linear algebra in PDP, the logic of additive functions, resource requirements of standard and programmable nets, and the P3 parallel-network simulating system."
            },
            "slug": "Parallel-distributed-processing:-explorations-in-of-Rumelhart-McClelland",
            "title": {
                "fragments": [],
                "text": "Parallel distributed processing: explorations in the microstructure of cognition, vol. 1: foundations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2647026"
                        ],
                        "name": "A. Blumer",
                        "slug": "A.-Blumer",
                        "structuredName": {
                            "firstName": "Anselm",
                            "lastName": "Blumer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Blumer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1683946"
                        ],
                        "name": "A. Ehrenfeucht",
                        "slug": "A.-Ehrenfeucht",
                        "structuredName": {
                            "firstName": "Andrzej",
                            "lastName": "Ehrenfeucht",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ehrenfeucht"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794034"
                        ],
                        "name": "Manfred K. Warmuth",
                        "slug": "Manfred-K.-Warmuth",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Warmuth",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manfred K. Warmuth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5225434,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "29b6251c84def0cbd35397c71fada0d22cd9409c",
            "isKey": false,
            "numCitedBy": 200,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "We extend Valiant's learnability model to learning classes of concepts defined by regions in Euclidean space E\". Our methods lead to a unified treatment of some of Valiant's results, along with previous results of Pearl and Devroye and Wagner on distribution-free convergence of certain pattern recognition algorithms. We show that the essential condition for distribution-free learnability is finiteness of the Vapnik-Chervonenkis dimension, a simple combinatorial parameter of the class of concepts to be learned. Using this parameter, we analyze the complexity and closure properties of learnable classes. Authors A. Blumer and D. Haussler gratefully acknowledge the support of NSF grant IST-8317918, author A. Ehrenfeucht the support of NSF grant MCS-8305245, and author M. Warmuth the support of the Faculty Research Committee of the University of California at Santa Cruz. Part of this work was done while A. Blumer was visiting the University of California at Santa Cruz and M. Warmuth the Univer-"
            },
            "slug": "Classifying-learnable-geometric-concepts-with-the-Blumer-Ehrenfeucht",
            "title": {
                "fragments": [],
                "text": "Classifying learnable geometric concepts with the Vapnik-Chervonenkis dimension"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown that the essential condition for distribution-free learnability is finiteness of the Vapnik-Chervonenkis dimension, a simple combinatorial parameter of the class of concepts to be learned."
            },
            "venue": {
                "fragments": [],
                "text": "STOC '86"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "115005840"
                        ],
                        "name": "L. Brown",
                        "slug": "L.-Brown",
                        "structuredName": {
                            "firstName": "Lloyd",
                            "lastName": "Brown",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Brown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145779308"
                        ],
                        "name": "B. Schreiber",
                        "slug": "B.-Schreiber",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Schreiber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Schreiber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2069536428"
                        ],
                        "name": "B. A. Taylor",
                        "slug": "B.-A.-Taylor",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Taylor",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. A. Taylor"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 56168397,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "b420570ea7c45db77256519c77c4737fed6f5a97",
            "isKey": false,
            "numCitedBy": 115,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "\u00a9 Annales de l\u2019institut Fourier, 1973, tous droits r\u00e9serv\u00e9s. L\u2019acc\u00e8s aux archives de la revue \u00ab Annales de l\u2019institut Fourier \u00bb (http://annalif.ujf-grenoble.fr/) implique l\u2019accord avec les conditions g\u00e9n\u00e9rales d\u2019utilisation (http://www.numdam.org/legal.php). Toute utilisation commerciale ou impression syst\u00e9matique est constitutive d\u2019une infraction p\u00e9nale. Toute copie ou impression de ce fichier doit contenir la pr\u00e9sente mention de copyright."
            },
            "slug": "Spectral-synthesis-and-the-Pompeiu-problem-Brown-Schreiber",
            "title": {
                "fragments": [],
                "text": "Spectral synthesis and the Pompeiu problem"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701656"
                        ],
                        "name": "James L. McClelland",
                        "slug": "James-L.-McClelland",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "McClelland",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James L. McClelland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 820779,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "e03d550b23e45d1c5da474574914d91bee77101f",
            "isKey": false,
            "numCitedBy": 839,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter contains sections titled: Classes of PDP Models, Specific Versions of the General Parallel Activation Model, Sigma-Pi Units, Conclusion, Acknowledgments"
            },
            "slug": "A-general-framework-for-parallel-distributed-Rumelhart-Hinton",
            "title": {
                "fragments": [],
                "text": "A general framework for parallel distributed processing"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This chapter contains sections titled: Classes of PDP Models, Specific Versions of the General Parallel Activation Model, Sigma-Pi Units, Conclusion, Acknowledgments."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2443807"
                        ],
                        "name": "W. Dahmen",
                        "slug": "W.-Dahmen",
                        "structuredName": {
                            "firstName": "Wolfgang",
                            "lastName": "Dahmen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Dahmen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708279"
                        ],
                        "name": "C. Micchelli",
                        "slug": "C.-Micchelli",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Micchelli",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Micchelli"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 126346357,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "6b4ce7548944f496f8952432dba1849ebb239ec8",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Some-remarks-on-ridge-functions-Dahmen-Micchelli",
            "title": {
                "fragments": [],
                "text": "Some remarks on ridge functions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49182227"
                        ],
                        "name": "R. Ash",
                        "slug": "R.-Ash",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Ash",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Ash"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3504753"
                        ],
                        "name": "E. Luk\u00e1cs",
                        "slug": "E.-Luk\u00e1cs",
                        "structuredName": {
                            "firstName": "Eugene",
                            "lastName": "Luk\u00e1cs",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Luk\u00e1cs"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51395558"
                        ],
                        "name": "Z. Birnbaum",
                        "slug": "Z.-Birnbaum",
                        "structuredName": {
                            "firstName": "Zygmunt",
                            "lastName": "Birnbaum",
                            "middleNames": [
                                "William"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Birnbaum"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122527150,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "f1ba588d48a7447c3785aac4a37bf762427a055f",
            "isKey": false,
            "numCitedBy": 685,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Real-analysis-and-probability-Ash-Luk\u00e1cs",
            "title": {
                "fragments": [],
                "text": "Real analysis and probability"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1975
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "corpusId": 221992329,
            "fieldsOfStudy": [],
            "id": "66e5d3ba917f0d692d2130294db5bc0bb0d3d150",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Real Analysis and Probability"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Geometric analysis of neural network capabilities"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE First International Conference on Neural Networks"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Special section on neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans . Acoust . Speech Signal Process ."
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Continuous Valued Neural Networks with Two Hidden Layers are Sufficient"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Valiant , A theory of the learnable"
            },
            "venue": {
                "fragments": [],
                "text": "Comm . ACM"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning Machines, McGraw-Hill"
            },
            "venue": {
                "fragments": [],
                "text": "New York,"
            },
            "year": 1969
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The 13th problem of Hilbert"
            },
            "venue": {
                "fragments": [],
                "text": "Mathematical Developments Arising from Hilbert's Problems"
            },
            "year": 1976
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {},
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 27,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Approximation-by-superpositions-of-a-sigmoidal-Cybenko/8da1dda34ecc96263102181448c94ec7d645d085?sort=total-citations"
}