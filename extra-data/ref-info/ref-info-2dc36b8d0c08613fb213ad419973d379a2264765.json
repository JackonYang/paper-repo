{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35153517"
                        ],
                        "name": "D. Lewis",
                        "slug": "D.-Lewis",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Lewis",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lewis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34938639"
                        ],
                        "name": "W. Gale",
                        "slug": "W.-Gale",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Gale",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Gale"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 915058,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5194b668c67aa83c037e71599a087f63c98eb713",
            "isKey": false,
            "numCitedBy": 2404,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "The ability to cheaply train text classifiers is critical to their use in information retrieval, content analysis, natural language processing, and other tasks involving data which is partly or fully textual. An algorithm for sequential sampling during machine learning of statistical classifiers was developed and tested on a newswire text categorization task. This method, which we call uncertainty sampling, reduced by as much as 500-fold the amount of training data that would have to be manually classified to achieve a given level of effectiveness."
            },
            "slug": "A-sequential-algorithm-for-training-text-Lewis-Gale",
            "title": {
                "fragments": [],
                "text": "A sequential algorithm for training text classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "An algorithm for sequential sampling during machine learning of statistical classifiers was developed and tested on a newswire text categorization task and reduced by as much as 500-fold the amount of training data that would have to be manually classified to achieve a given level of effectiveness."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '94"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50056360"
                        ],
                        "name": "William W. Cohen",
                        "slug": "William-W.-Cohen",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Cohen",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "William W. Cohen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5327274,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ce2d6de9cec4a6d135c32bb8d2d02bba09928b33",
            "isKey": false,
            "numCitedBy": 572,
            "numCiting": 80,
            "paperAbstract": {
                "fragments": [],
                "text": "Two recently implemented machine-learning algorithms, RIPPER and sleeping-experts for phrases, are evaluated on a number of large text categorization problems. These algorithms both construct classifiers that allow the \u201ccontext\u201d of a word w to affect how (or even whether) the presence or absence of w will contribute to a classification. However, RIPPER and sleeping-experts differ radically in many other respects: differences include different notions as to what constitutes a context, different ways of combining contexts to construct a classifier, different methods to search for a combination of contexts, and different criteria as to what contexts should be included in such a combination. In spite of these differences, both RIPPER and sleeping-experts perform extremely well across a wide variety of categorization problems, generally outperforming previously applied learning methods. We view this result as a confirmation of the usefulness of classifiers that represent contextual information."
            },
            "slug": "Context-sensitive-learning-methods-for-text-Cohen-Singer",
            "title": {
                "fragments": [],
                "text": "Context-sensitive learning methods for text categorization"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "RIPPER and sleeping-experts perform extremely well across a wide variety of categorization problems, generally outperforming previously applied learning methods and are viewed as a confirmation of the usefulness of classifiers that represent contextual information."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '96"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35153517"
                        ],
                        "name": "D. Lewis",
                        "slug": "D.-Lewis",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Lewis",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lewis"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 17260485,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "91cbbe24c807473b7b935d39b63df5b15da9bb32",
            "isKey": false,
            "numCitedBy": 392,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Text retrieval systems typically produce a ranking of documents and let a user decide how far down that ranking to go. In contrast, programs that filter text streams, software that categorizes documents, agents which alert users, and many other IR systems must make decisions without human input or supervision. It is important to define what constitutes good effectiveness for these autonomous systems, tune the systems to achieve the highest possible effectiveness, and estimate how the effectiveness changes as new data is processed. We show how to do this for binary text classification systems, emphasizing that different goals for the system le ad to different optimal behaviors. Optimizing and estimating effectiveness is greatly aided if classifiers that explicitly estimate the probability of class membership are used. Ranked retrieval is the information retrieval (IR) researc her\u2019s favorite tool for dealing with information overload. Ranked retrieval systems display documents in order of probability of releva nce or some similar measure. Users see the best documents first, anddecide how far down the ranking to go in examining the available information. The central role played by ranking in this appr oach has led researchers to evaluate IR systems primarily, often exclusively, on the quality of their rankings. (See, for instance , the TREC evaluations [1].) In some IR applications, however, ranking is not enough: A company provides an SDI (selective dissemination of information) service which filters newswire feeds. Relevant articles are faxed each morning to clients. Interaction between customer and system takes place infrequently. The cost of resources (tying up phone lines, fax machine paper, etc.) is a factor to consider in operating the system. A text categorization system assigns controlled vocabulary categories to incoming documents as they are stored in a text database. Cost cutting has eliminated manual checking of category assignments."
            },
            "slug": "Evaluating-and-optimizing-autonomous-text-systems-Lewis",
            "title": {
                "fragments": [],
                "text": "Evaluating and optimizing autonomous text classification systems"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This work shows how to define what constitutes good effectiveness for binary text classification systems, tune the systems to achieve the highest possible effectiveness, and estimate how the effectiveness changes as new data is processed."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '95"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35153517"
                        ],
                        "name": "D. Lewis",
                        "slug": "D.-Lewis",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Lewis",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lewis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 15704538,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "33c1f29dbc73cc4d0f787d6b048b465649b2d92b",
            "isKey": false,
            "numCitedBy": 99,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "At ACM SIGIR '94, I compared the effectiveness of uncertainty sampling with that of random sampling and relevance sampling in choosing training data for a text categorization data set [1]. (Relevance sampling is the application of relevance feedback [3] to producing a training sample.) I have discovered a bug in my experimental software which caused the relevance sampling results reported in the SIGIR '94 paper to be incorrect. (The uncertainty sampling and random sampling results in that paper were correct.) I have since fixed the bug and rerun the experiments. This note presents the corrected results, along with additional data supporting the original claim that uncertainty sampling has an advantage over relevance sampling in most training situations."
            },
            "slug": "A-sequential-algorithm-for-training-text-and-data-Lewis",
            "title": {
                "fragments": [],
                "text": "A sequential algorithm for training text classifiers: corrigendum and additional data"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A bug in my experimental software caused the relevance sampling results reported in the SIGIR '94 paper to be incorrect, and this note presents the corrected results, along with additional data supporting the original claim that uncertainty sampling has an advantage over relevance sampling in most training situations."
            },
            "venue": {
                "fragments": [],
                "text": "SIGF"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35729970"
                        ],
                        "name": "Yiming Yang",
                        "slug": "Yiming-Yang",
                        "structuredName": {
                            "firstName": "Yiming",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yiming Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792682"
                        ],
                        "name": "C. Chute",
                        "slug": "C.-Chute",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Chute",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Chute"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 148
                            }
                        ],
                        "text": "This may require using a batch mode version of EG, which \nwe in any case wish to compare with other batch mode error minimization procedures (Yang &#38; Chute, \n1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16063479,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f926a0022e794485ec469124894aaaf29b087d70",
            "isKey": false,
            "numCitedBy": 489,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "A unified model for text categorization and text retrieval is introduced. We use a training set of manually categorized documents to learn word-category associations, and use these associations to predict the categories of arbitrary documents. Similarly, we use a training set of queries and their related documents to obtain empirical associations between query words and indexing terms of documents, and use these associations to predict the related documents of arbitrary queries. A Linear Least Squares Fit (LLSF) technique is employed to estimate the likelihood of these associations. Document collections from the MEDLINE database and Mayo patient records are used for studies on the effectiveness of our approach, and on how much the effectiveness depends on the choices of training data, indexing language, word-weighting scheme, and morphological canonicalization. Alternative methods are also tested on these data collections for comparison. It is evident that the LLSF approach uses the relevance information effectively within human decisions of categorization and retrieval, and achieves a semantic mapping of free texts to their representations in an indexing language. Such a semantic mapping lead to a significant improvement in categorization and retrieval, compared to alternative approaches."
            },
            "slug": "An-example-based-mapping-method-for-text-and-Yang-Chute",
            "title": {
                "fragments": [],
                "text": "An example-based mapping method for text categorization and retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is evident that the LLSF approach uses the relevance information effectively within human decisions of categorization and retrieval, and achieves a semantic mapping of free texts to their representations in an indexing language."
            },
            "venue": {
                "fragments": [],
                "text": "TOIS"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50056360"
                        ],
                        "name": "William W. Cohen",
                        "slug": "William-W.-Cohen",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Cohen",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "William W. Cohen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 47270497,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "90848c88f56fcd421ac3cfd2c87d3e61211103ea",
            "isKey": false,
            "numCitedBy": 107,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Text-Categorization-and-Relational-Learning-Cohen",
            "title": {
                "fragments": [],
                "text": "Text Categorization and Relational Learning"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797808"
                        ],
                        "name": "G. Salton",
                        "slug": "G.-Salton",
                        "structuredName": {
                            "firstName": "Gerard",
                            "lastName": "Salton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Salton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144009691"
                        ],
                        "name": "C. Buckley",
                        "slug": "C.-Buckley",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Buckley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Buckley"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 102
                            }
                        ],
                        "text": "Both binary feature values and cosine-normalized t~ x id~ feature values (SMART tfc weights (Salton &#38; \nBuckley, 1988)) were used for Rocchio and WH, with idj estimated on the training set for that run."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7725217,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e50a316f97c9a405aa000d883a633bd5707f1a34",
            "isKey": false,
            "numCitedBy": 9463,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Term-Weighting-Approaches-in-Automatic-Text-Salton-Buckley",
            "title": {
                "fragments": [],
                "text": "Term-Weighting Approaches in Automatic Text Retrieval"
            },
            "venue": {
                "fragments": [],
                "text": "Inf. Process. Manag."
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144987107"
                        ],
                        "name": "Jamie Callan",
                        "slug": "Jamie-Callan",
                        "structuredName": {
                            "firstName": "Jamie",
                            "lastName": "Callan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jamie Callan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144456145"
                        ],
                        "name": "W. Bruce Croft",
                        "slug": "W.-Bruce-Croft",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Croft",
                            "middleNames": [
                                "Bruce"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Bruce Croft"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2533154"
                        ],
                        "name": "J. Broglio",
                        "slug": "J.-Broglio",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Broglio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Broglio"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 46
                            }
                        ],
                        "text": "The \nbasic INQUERY weight\u00ading formula was used (Callan et al., 1995), which has a min\u00adimum feature value of \n0.4 and weights that tend to be in the range 0.4 to 0.5."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5779688,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ac1410ee22f8163cf0513b02a5d43054eb037424",
            "isKey": false,
            "numCitedBy": 175,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "TREC-and-Tipster-Experiments-with-Inquery-Callan-Croft",
            "title": {
                "fragments": [],
                "text": "TREC and Tipster Experiments with Inquery"
            },
            "venue": {
                "fragments": [],
                "text": "Inf. Process. Manag."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2076886337"
                        ],
                        "name": "D. K. Harmon",
                        "slug": "D.-K.-Harmon",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Harmon",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. K. Harmon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 69704062,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1acfa730407aae835418c9381a3fb31f23fae494",
            "isKey": false,
            "numCitedBy": 275,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nHeld in Gaithersburg, MD, August November 2-4, 1994. The conference was co-sponsored by the National Inst. of Standards & Technology (NIST) & the Advanced Research Projects Agency (ARPA) & was attended by 150 people involved in the 32 participating groups. Evaluates new technologies in text retrieval. Includes 34 papers: indexing structures, fragmentation schemes, probabilistic retrieval, latent semantic indexing, interactive document retrieval, & much more. Numerous graphs, tables & charts."
            },
            "slug": "Overview-of-the-Third-Text-Retrieval-Conference-Harmon",
            "title": {
                "fragments": [],
                "text": "Overview of the Third Text Retrieval Conference (TREC-3)"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Evaluates new technologies in text retrieval with 34 papers: indexing structures, fragmentation schemes, probabilistic retrieval, latent semantic indexing, interactive document retrieval, & much more."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144890574"
                        ],
                        "name": "James Allan",
                        "slug": "James-Allan",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Allan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Allan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145978046"
                        ],
                        "name": "Lisa Ballesteros",
                        "slug": "Lisa-Ballesteros",
                        "structuredName": {
                            "firstName": "Lisa",
                            "lastName": "Ballesteros",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lisa Ballesteros"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144987107"
                        ],
                        "name": "Jamie Callan",
                        "slug": "Jamie-Callan",
                        "structuredName": {
                            "firstName": "Jamie",
                            "lastName": "Callan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jamie Callan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144456145"
                        ],
                        "name": "W. Bruce Croft",
                        "slug": "W.-Bruce-Croft",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Croft",
                            "middleNames": [
                                "Bruce"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Bruce Croft"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110326971"
                        ],
                        "name": "Zhihong Lu",
                        "slug": "Zhihong-Lu",
                        "structuredName": {
                            "firstName": "Zhihong",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhihong Lu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13284661,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "df17b6307d0f4dd25aa5387efd326633ab7a927e",
            "isKey": false,
            "numCitedBy": 83,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : Past TREC experiments by the University of Massachusetts have focused primarily on ad hoc query creation. Substantial effort was directed towards automatically translating TREC topics into queries using a set of simple heuristics and query expansion. Less emphasis was placed on the routing task although results were generally good. The Spanish experiments in TREC-3 concentrated on simple indexing sophisticated stemming and simple methods of creating queries. The TREC-4 experiments were a departure from the past. The ad hoc experiments involved \"fine tuning\" existing approaches and modifications to the INQUERY term weighting algorithm. However, much of the research focus in TREC-4 was on the routing, Spanish, and collection merging experiments. These tracks more closely match our broader research interests in document routing document filtering distributed IR, and multilingual retrieval. The University of Massachusetts experiments were conducted with version 3.0 of the INQUERY information retrieval system. INQUERY is based on the Bayesian inference network retrieval model. It is described elsewhere [7, 5, 12, 11], so this paper focuses on relevant differences to the previously published algorithms."
            },
            "slug": "Recent-Experiments-with-INQUERY-Allan-Ballesteros",
            "title": {
                "fragments": [],
                "text": "Recent Experiments with INQUERY"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The University of Massachusetts experiments were conducted with version 3.0 of the INQUERY information retrieval system, based on the Bayesian inference network retrieval model, so this paper focuses on relevant differences to the previously published algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "TREC"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144430625"
                        ],
                        "name": "S. Robertson",
                        "slug": "S.-Robertson",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Robertson",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Robertson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145848824"
                        ],
                        "name": "Karen Sp\u00e4rck Jones",
                        "slug": "Karen-Sp\u00e4rck-Jones",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Sp\u00e4rck Jones",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karen Sp\u00e4rck Jones"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "where p; and qi are probabilities to be estimated based on training data ( Robertson & Sparck Jones, 1976 ) or the text of a user request (Croft & Harper, 1979), and the X3\u2019s are binary (1 if a word is present in a document, O otherwise)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 45186038,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f6e3e57567e9803718623ec088cd7fea65cfbc9d",
            "isKey": false,
            "numCitedBy": 2372,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper examines statistical techniques for exploiting relevance information to weight search terms. These techniques are presented as a natural extension of weighting methods using information about the distribution of index terms in documents in general. A series of relevance weighting functions is derived and is justified by theoretical considerations. In particular, it is shown that specific weighted search methods are implied by a general probabilistic theory of retrieval. Different applications of relevance weighting are illustrated by experimental results for test collections."
            },
            "slug": "Relevance-weighting-of-search-terms-Robertson-Jones",
            "title": {
                "fragments": [],
                "text": "Relevance weighting of search terms"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "This paper examines statistical techniques for exploiting relevance information to weight search terms using information about the distribution of index terms in documents in general and shows that specific weighted search methods are implied by a general probabilistic theory of retrieval."
            },
            "venue": {
                "fragments": [],
                "text": "J. Am. Soc. Inf. Sci."
            },
            "year": 1976
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145163573"
                        ],
                        "name": "A. Singhal",
                        "slug": "A.-Singhal",
                        "structuredName": {
                            "firstName": "Amit",
                            "lastName": "Singhal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Singhal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144009691"
                        ],
                        "name": "C. Buckley",
                        "slug": "C.-Buckley",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Buckley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Buckley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51933055"
                        ],
                        "name": "M. Mitra",
                        "slug": "M.-Mitra",
                        "structuredName": {
                            "firstName": "Manclar",
                            "lastName": "Mitra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mitra"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13184498,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "173443511c450bd8f61e3d1122982f74c94147ae",
            "isKey": false,
            "numCitedBy": 587,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Automatic information retrieval systems have to deal with documents of varying lengths in a text collection. Document length normalization is used to fairly retrieve documents of all lengths. In this study, we ohserve that a normalization scheme that retrieves documents of all lengths with similar chances as their likelihood of relevance will outperform another scheme which retrieves documents with chances very different from their likelihood of relevance. We show that the retrievaf probabilities for a particular normalization method deviate systematically from the relevance probabilities across different collections. We present pivoted normalization, a technique that can be used to modify any normalization function thereby reducing the gap between the relevance and the retrieval probabilities. Training pivoted normalization on one collection, we can successfully use it on other (new) text collections, yielding a robust, collectzorz independent normalization technique. We use the idea of pivoting with the well known cosine normalization function. We point out some shortcomings of the cosine function andpresent two new normalization functions\u2013-pivoted unique normalization and piuotert byte size nornaahzation."
            },
            "slug": "Pivoted-document-length-normalization-Singhal-Buckley",
            "title": {
                "fragments": [],
                "text": "Pivoted document length normalization"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Pivoted normalization is presented, a technique that can be used to modify any normalization function thereby reducing the gap between the relevance and the retrieval probabilities, and two new normalization functions are presented\u2013-pivoted unique normalization and piuotert byte size nornaahzation."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '96"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144169170"
                        ],
                        "name": "D. Harman",
                        "slug": "D.-Harman",
                        "structuredName": {
                            "firstName": "Donna",
                            "lastName": "Harman",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Harman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 201,
                                "start": 188
                            }
                        ],
                        "text": "For top\u00adics 51-100, a mean \nof 1,784 training documents (328 relevant and 1,456 nonrelevant ) and 2,340 test documents (220 rele\u00advant \nand 2,121 nonrelevant ), selected by a pooling strategy (Harman, 1995a), have been judged for relevance."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 188,
                                "start": 175
                            }
                        ],
                        "text": "We measure how close to perfect \nranking the classifier came using simple average precision (SAP), which is the mean of precision measured \nat each class member in the ranking (Harman, 1995b, p. A-9)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 212,
                                "start": 199
                            }
                        ],
                        "text": "\u2026(Buckley &#38; Salton, 1995) m- Table 3: Mean R-precision across routing topics \nfor various training procedur~s. R-precision is pre~sion at a number of documents equal to the number \nof relevant documents (Harman, 1995b, p. A-10). w indicates that expansion terms are words, p indicates \nphrases."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 97
                            }
                        ],
                        "text": "7.1 A TREC Routing Data \nSet Our routing experiments used data developed in the TREC evaluations (Harman, 1995a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 4
                            }
                        ],
                        "text": "In (Harman, \n1995b)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1208194,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3d49df3eb2daf21fc508d82b1d96e3fdb1d29a75",
            "isKey": true,
            "numCitedBy": 305,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "In November of 1992 the first Text REtrieval Conference (TREC-1) was held at NIST [Harman 1993]. The conference, co-sponsored by ARPA and NIST, brought together information retrieval researchers to discuss their system results on a new large test collection (the TIPSTER collection). This conference became the first in a series of ongoing conferences dedicated to encouraging research in retrieval from large-scale test collections, and to encouraging increased interaction among research groups in industry and academia. From the beginning there has been an almost equal number of universities and companies participating, with an emphasis on exploring many different types of approaches to the text retrieval problem."
            },
            "slug": "Overview-of-the-Third-Text-REtrieval-Conference-Harman",
            "title": {
                "fragments": [],
                "text": "Overview of the Third Text REtrieval Conference (TREC-3)"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This conference became the first in a series of ongoing conferences dedicated to encouraging research in retrieval from large-scale test collections, and to encouraging increased interaction among research groups in industry and academia."
            },
            "venue": {
                "fragments": [],
                "text": "TREC"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688124"
                        ],
                        "name": "W. Hersh",
                        "slug": "W.-Hersh",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Hersh",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Hersh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144009691"
                        ],
                        "name": "C. Buckley",
                        "slug": "C.-Buckley",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Buckley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Buckley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2089238953"
                        ],
                        "name": "T. J. Leone",
                        "slug": "T.-J.-Leone",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Leone",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. J. Leone"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760869"
                        ],
                        "name": "D. Hickam",
                        "slug": "D.-Hickam",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Hickam",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Hickam"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 215,
                                "start": 197
                            }
                        ],
                        "text": "6.1 The OHSUMED Text Categorization \nTest Col\u00adlection The first collection consists of Medline records from the years 1987 to 1991, distributed \nas part of the OHSUMED text retrieval test collection (Hersh et al., 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15094383,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "e91fc6cba8b23688d02b0dc3ead69ed05210bf33",
            "isKey": true,
            "numCitedBy": 900,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "A series of information retrieval experiments was carried out with a computer installed in a medical practice setting for relatively inexperienced physician end-users. Using a commercial MEDLINE product based on the vector space model, these physicians searched just as effectively as more experienced searchers using Boolean searching. The results of this experiment were subsequently used to create a new large medical test collection, which was used in experiments with the SMART retrieval system to obtain baseline performance data as well as compare SMART with the other searchers."
            },
            "slug": "OHSUMED:-an-interactive-retrieval-evaluation-and-Hersh-Buckley",
            "title": {
                "fragments": [],
                "text": "OHSUMED: an interactive retrieval evaluation and new large test collection for research"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "A series of information retrieval experiments was carried out with a computer installed in a medical practice setting for relatively inexperienced physician end-users using a commercial MEDLINE product based on the vector space model, finding that these physicians searched just as effectively as more experienced searchers using Boolean searching."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '94"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690967"
                        ],
                        "name": "A. Blum",
                        "slug": "A.-Blum",
                        "structuredName": {
                            "firstName": "Avrim",
                            "lastName": "Blum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Blum"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 23
                            }
                        ],
                        "text": "Blum s recent success (Blum, 1995) with a related multiplicative update algorithm on \na learn\u00ading problem with some textual features also encouraged us to try EG."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 164
                            }
                        ],
                        "text": "Maintaining and updating very large weight vectors may take too much space or time, so methods \nfor pruning weight vectors while maintaining theoretical guarantees (Blum, 1995) are also worth examining."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 195325954,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1f23d52268e53f9ea81cc6b367eac55f38090257",
            "isKey": false,
            "numCitedBy": 97,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Empirical-Support-for-Winnow-and-Weighted-Majority-Blum",
            "title": {
                "fragments": [],
                "text": "Empirical Support for Winnow and Weighted-Majority Based Algorithms: Results on a Calendar Scheduling Domain"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797808"
                        ],
                        "name": "G. Salton",
                        "slug": "G.-Salton",
                        "structuredName": {
                            "firstName": "Gerard",
                            "lastName": "Salton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Salton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144009691"
                        ],
                        "name": "C. Buckley",
                        "slug": "C.-Buckley",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Buckley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Buckley"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17637032,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "2ebb3dd597bbd7028d8c68bcf509e5bb09ea1e78",
            "isKey": false,
            "numCitedBy": 1442,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Relevance feedback is an automatic process, introduced over 20 years ago, designed to produce query formulations following an initial retrieval operation. The principal relevance feedback methods described over the years are examined briefly, and evaluation data are included to demonstrate the effectiveness of the various methods. Prescriptions are given for conducting text retrieval operations iteratively using relevance feedback."
            },
            "slug": "Improving-retrieval-performance-by-relevance-Salton-Buckley",
            "title": {
                "fragments": [],
                "text": "Improving retrieval performance by relevance feedback"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "Prescriptions are given for conducting text retrieval operations iteratively using relevance feedback, and evaluation data are included to demonstrate the effectiveness of the various methods."
            },
            "venue": {
                "fragments": [],
                "text": "J. Am. Soc. Inf. Sci."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797808"
                        ],
                        "name": "G. Salton",
                        "slug": "G.-Salton",
                        "structuredName": {
                            "firstName": "Gerard",
                            "lastName": "Salton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Salton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144321599"
                        ],
                        "name": "M. McGill",
                        "slug": "M.-McGill",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "McGill",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. McGill"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 43685115,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "49af3e80343eb80c61e727ae0c27541628c7c5e2",
            "isKey": false,
            "numCitedBy": 12605,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Some people may be laughing when looking at you reading in your spare time. Some may be admired of you. And some may want be like you who have reading hobby. What about your own feel? Have you felt right? Reading is a need and a hobby at once. This condition is the on that will make you feel that you must read. If you know are looking for the book enPDFd introduction to modern information retrieval as the choice of reading, you can find here."
            },
            "slug": "Introduction-to-Modern-Information-Retrieval-Salton-McGill",
            "title": {
                "fragments": [],
                "text": "Introduction to Modern Information Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Reading is a need and a hobby at once and this condition is the on that will make you feel that you must read."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35106875"
                        ],
                        "name": "R. Duda",
                        "slug": "R.-Duda",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Duda",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Duda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3108177"
                        ],
                        "name": "P. Hart",
                        "slug": "P.-Hart",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Hart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hart"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12946615,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b07ce649d6f6eb636872527104b0209d3edc8188",
            "isKey": false,
            "numCitedBy": 16927,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Provides a unified, comprehensive and up-to-date treatment of both statistical and descriptive methods for pattern recognition. The topics treated include Bayesian decision theory, supervised and unsupervised learning, nonparametric techniques, discriminant analysis, clustering, preprosessing of pictorial data, spatial filtering, shape description techniques, perspective transformations, projective invariants, linguistic procedures, and artificial intelligence techniques for scene analysis."
            },
            "slug": "Pattern-classification-and-scene-analysis-Duda-Hart",
            "title": {
                "fragments": [],
                "text": "Pattern classification and scene analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "The topics treated include Bayesian decision theory, supervised and unsupervised learning, nonparametric techniques, discriminant analysis, clustering, preprosessing of pictorial data, spatial filtering, shape description techniques, perspective transformations, projective invariants, linguistic procedures, and artificial intelligence techniques for scene analysis."
            },
            "venue": {
                "fragments": [],
                "text": "A Wiley-Interscience publication"
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144009691"
                        ],
                        "name": "C. Buckley",
                        "slug": "C.-Buckley",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Buckley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Buckley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797808"
                        ],
                        "name": "G. Salton",
                        "slug": "G.-Salton",
                        "structuredName": {
                            "firstName": "Gerard",
                            "lastName": "Salton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Salton"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15284993,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7cb2c518415b16bbeba176bc0202feebb5e7231a",
            "isKey": false,
            "numCitedBy": 251,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "I)epartment of ('omputm $icience <brnell University ]thaca. NY 14 K5;j-7501 christ)@cs.cornell. du, gS{@CS.CC)rIIell.P(i II N[etllocls forlearilillg weightsuf{errnh nsiiig r(,levance infer-mation from a learning set of doctlnlents have been stll{lie{l for decadesin information retrieval research, The approach used here, Dynamic Feedback C)ptimization. starts with a good weighting scheme based npoa Rocchi(r ferxlback. and then improves those weights in a dynamic fashion by t,rw,-iug possible changes of query weights on th~ learning set, documents. The resulting optimized query performs 1o-1.5% better thant heoriginal whcncvaluated on t,het,est set. \\Ve discuss the constant, tension het,ween describing what a relevant, document should contain, a,n(l ciescribing what the known relevant cfocnrnents do contain. 1 Irltrmdllction In the tvi>ical information retrieval environrnent, irrclrrd-ing both ad-hoc interactive retrieval and document, filtering based on long-term information needs, an original query is submitted to a system which then returns documents for inspection. (Isers then look at those retrieved documents and submit a new query based upon their original need and the returned documents. Relevance feedback is the process of automatically altering an existing query using information supplied by users abont tllerclcvance ofj)reviously retrieved documents. Relevance feedback has been an importalit research topic forwell over 15years[Ide71, Roc71, I{J76, S1390]. Irrcreaserf attention has been paid to relevance feedback in the past sever <al j-e,ars due to both t,fle in<le <ased acceptance of ~t,a-tistictrl inforrnatiou retrieval systems whith can easily use relevance feedback, and the effects of the TRf?(\" ! confert=nces [Har9:l, Har94, Har95]. Eval~~atiollo frelevancef eedback(,n t,he small frre-TEtfX; test collections was notoriously f.liffi-crrlt. \" ~he '1'REC routing environment offers a straip,htfor-warcl context in which relevance feedback can be evaluated and compared. \" This study was supported m part by the National Science Foun. datlon under grant Ittl 93-(101?1 Permission totnoke digit[ll/ll:~rci c{~pies () f:lll,)r l>:ll.t(~~tl~is matcri:~i without fee is granted provided [hot [he c(~pics arc m)l Ill:ldc or distributed for~>rofit c>rct)i~llncl.c i:ll aLivonL:lge, tIlc ACh4 ctJpyrigilt/ server notice, the title {)flllc pul>lic:lli{)n :Inci ils ci:]lc :Ippwr, :lnci notice is given that copyright is hy pemlls<ILIn of Lhc Ass(x_i:lli(ln I'or Computing Machinery, Inc. (ACM) T[lc[~l>y t,t\\lcrwise, L[]rci>Ll(>lisll, to post on servers or to rcciislribulcto [is(s, requires sllcciiic permission andlor fee. Itecmrt work of our group at (70rn~ll and others has centered on expanding the original clnery by large rrun-i)ers of terms which occut in the relevant d&rrrreuts [BSA!L). We've shown that effectiveness contrnnes to increase even after adding hundreds of terms using \u2026"
            },
            "slug": "Optimization-of-relevance-feedback-weights-Buckley-Salton",
            "title": {
                "fragments": [],
                "text": "Optimization of relevance feedback weights"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The approach used here, Dynamic Feedback C)ptimization starts with a good weighting scheme based npoa Rocchi(r ferxlback) and improves those weights in a dynamic fashion by t,rw,iug possible changes of query weights on th~ learning set, documents, and the resulting optimized query performs 1o-1.5% better."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '95"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144009691"
                        ],
                        "name": "C. Buckley",
                        "slug": "C.-Buckley",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Buckley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Buckley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797808"
                        ],
                        "name": "G. Salton",
                        "slug": "G.-Salton",
                        "structuredName": {
                            "firstName": "Gerard",
                            "lastName": "Salton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Salton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144890574"
                        ],
                        "name": "James Allan",
                        "slug": "James-Allan",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Allan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Allan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 17522959,
            "fieldsOfStudy": [
                "Computer Science",
                "Business"
            ],
            "id": "c972982771aeaaafdbdfbbcc7fe205bdecb3cf24",
            "isKey": false,
            "numCitedBy": 372,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "The effects of adding information from relevant documents are examined in the TREC routing environment. A modified Rocchio relevance feedback approach is used, with a varying number of relevant documents retrieved by an initial SMART search, and a varying number of terms from those relevant documents used to expand the initial query. Recall-precision evaluation reveals that as the amount of expansion of the query due to adding terms from relevant documents increases, so does the effectiveness. There appears to be a linear relationship between the log of the number of terms added and the recall-precision effectiveness. There also appears to be a linear relationship between the log of the number of known relevant documents and the recall-precision effectiveness."
            },
            "slug": "The-effect-of-adding-relevance-information-in-a-Buckley-Salton",
            "title": {
                "fragments": [],
                "text": "The effect of adding relevance information in a relevance feedback environment"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Recall-precision evaluation reveals that as the amount of expansion of the query due to adding terms from relevant documents increases, so does the effectiveness, and there appears to be a linear relationship between the log of the number of terms added and the recall- Precision effectiveness."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '94"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700597"
                        ],
                        "name": "Jyrki Kivinen",
                        "slug": "Jyrki-Kivinen",
                        "structuredName": {
                            "firstName": "Jyrki",
                            "lastName": "Kivinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jyrki Kivinen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794034"
                        ],
                        "name": "Manfred K. Warmuth",
                        "slug": "Manfred-K.-Warmuth",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Warmuth",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manfred K. Warmuth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 0
                            }
                        ],
                        "text": "Kivinen and Warmuth focus on deriving upper bounds on the \nerror of WH and EG for various set\u00adtings of the learning rate q."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 274,
                                "start": 251
                            }
                        ],
                        "text": "\u2026&#38; Warmuth, 1994)) which suggest that a better choice is the average of the weight vectors \ncomputed along the way: w= (4) -+&#38; n+lt=l  3.3 Kivinen &#38; Warmuth s EG Algorithm The exponentiated-gradient \nor EG algorithm was introduced by Kivinen and Warmuth (Kivinen &#38; Warmuth, 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 0
                            }
                        ],
                        "text": "Kivinen and Warmuth give a detailed motivation for both EG and WH. Briefly, the \nnew weight vector Wi+l can be shown to minimize a formula which trades off the con\u00adflicting goals of \n(1) minimizing the 10SS(wi+l . xi y, )2 of the new vector W,+l on the current example xi, and (2) penalizing \nthe choice of a new vector Wi+l which is far from the old vector w,."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 51
                            }
                        ],
                        "text": "4 Error Bounds for WEI and EG Kivinen and Warmuth (Kivinen &#38; \nWarmuth, 1994) study in detail the theoretical behavior of EG and WH, build\u00ading on previous work (Cesa-Bianchi \net al., 1993; Widrow &#38; Stearns, 1985)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 0
                            }
                        ],
                        "text": "Kivinen and Warmuth prove bounds of a somewhat dif\u00adferent form for EG."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 48
                            }
                        ],
                        "text": "However, there are theoretical arguments (e.g. \n(Kivinen &#38; Warmuth, 1994)) which suggest that a better choice is the average of the weight vectors \ncomputed along the way: w= (4) -+&#38; n+lt=l  3.3 Kivinen &#38; Warmuth s EG Algorithm The exponentiated-gradient \nor EG algorithm was introduced\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 8
                            }
                        ],
                        "text": "In sum, Kivinen and Warmuth s results suggest that EG is likely to work \nwell on high dimensional problems."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 305,
                                "start": 286
                            }
                        ],
                        "text": "However, there are theoretical arguments (e.g. \n(Kivinen &#38; Warmuth, 1994)) which suggest that a better choice is the average of the weight vectors \ncomputed along the way: w= (4) -+&#38; n+lt=l  3.3 Kivinen &#38; Warmuth s EG Algorithm The exponentiated-gradient \nor EG algorithm was introduced by Kivinen and Warmuth (Kivinen &#38; Warmuth, 1994)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6130401,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "98eed3f082351c4821d1edb315846207a8fefbe9",
            "isKey": true,
            "numCitedBy": 911,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider two algorithm for on-line prediction based on a linear model. The algorithms are the well-known Gradient Descent (GD) algorithm and a new algorithm, which we call EG(+/-). They both maintain a weight vector using simple updates. For the GD algorithm, the update is based on subtracting the gradient of the squared error made on a prediction. The EG(+/-) algorithm uses the components of the gradient in the exponents of factors that are used in updating the weight vector multiplicatively. We present worst-case loss bounds for EG(+/-) and compare them to previously known bounds for the GD algorithm. The bounds suggest that the losses of the algorithms are in general incomparable, but EG(+/-) has a much smaller loss if only a few components of the input are relevant for the predictions. We have performed experiments, which show that our worst-case upper bounds are quite tight already on simple artificial data."
            },
            "slug": "Exponentiated-Gradient-Versus-Gradient-Descent-for-Kivinen-Warmuth",
            "title": {
                "fragments": [],
                "text": "Exponentiated Gradient Versus Gradient Descent for Linear Predictors"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The bounds suggest that the losses of the algorithms are in general incomparable, but EG(+/-) has a much smaller loss if only a few components of the input are relevant for the predictions, which is quite tight already on simple artificial data."
            },
            "venue": {
                "fragments": [],
                "text": "Inf. Comput."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1772099"
                        ],
                        "name": "D. Helmbold",
                        "slug": "D.-Helmbold",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Helmbold",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Helmbold"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700597"
                        ],
                        "name": "Jyrki Kivinen",
                        "slug": "Jyrki-Kivinen",
                        "structuredName": {
                            "firstName": "Jyrki",
                            "lastName": "Kivinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jyrki Kivinen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794034"
                        ],
                        "name": "Manfred K. Warmuth",
                        "slug": "Manfred-K.-Warmuth",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Warmuth",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manfred K. Warmuth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18269579,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c07d63295b084af6904a4055e59774640643841f",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "We analyze and compare the well-known Gradient Descent algorithm and a new algorithm, called the Exponentiated Gradient algorithm, for training a single neuron with an arbitrary transfer function. Both algorithms are easily generalized to larger neural networks, and the generalization of Gradient Descent is the standard back-propagation algorithm. In this paper we prove worst-case loss bounds for both algorithms in the single neuron case. Since local minima make it difficult to prove worst-case bounds for gradient-based algorithms, we must use a loss function that prevents the formation of spurious local minima. We define such a matching loss function for any strictly increasing differentiable transfer function and prove worst-case loss bound for any such transfer function and its corresponding matching loss. For example, the matching loss for the identity function is the square loss and the matching loss for the logistic sigmoid is the entropic loss. The different structure of the bounds for the two algorithms indicates that the new algorithm out-performs Gradient Descent when the inputs contain a large number of irrelevant components."
            },
            "slug": "Worst-case-Loss-Bounds-for-Single-Neurons-Helmbold-Kivinen",
            "title": {
                "fragments": [],
                "text": "Worst-case Loss Bounds for Single Neurons"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "This paper analyzes and compares the well-known Gradient Descent algorithm and a new algorithm, called the Exponentiated Gradient algorithm, for training a single neuron with an arbitrary transfer function and proves worst-case loss bounds for both algorithms in the single neuron case."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "134211067"
                        ],
                        "name": "J. Rocchio",
                        "slug": "J.-Rocchio",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Rocchio",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Rocchio"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61859400,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4083ad1066cfa2ff0d65866ef4b011399d6873d1",
            "isKey": false,
            "numCitedBy": 3242,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Relevance-feedback-in-information-retrieval-Rocchio",
            "title": {
                "fragments": [],
                "text": "Relevance feedback in information retrieval"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1971
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1388387856"
                        ],
                        "name": "N. Cesa-Bianchi",
                        "slug": "N.-Cesa-Bianchi",
                        "structuredName": {
                            "firstName": "Nicol\u00f2",
                            "lastName": "Cesa-Bianchi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cesa-Bianchi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144007105"
                        ],
                        "name": "Philip M. Long",
                        "slug": "Philip-M.-Long",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Long",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philip M. Long"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794034"
                        ],
                        "name": "Manfred K. Warmuth",
                        "slug": "Manfred-K.-Warmuth",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Warmuth",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manfred K. Warmuth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 158
                            }
                        ],
                        "text": "4 Error Bounds for WEI and EG Kivinen and Warmuth (Kivinen &#38; \nWarmuth, 1994) study in detail the theoretical behavior of EG and WH, build\u00ading on previous work (Cesa-Bianchi \net al., 1993; Widrow &#38; Stearns, 1985)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12379829,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "985abb957e4b3a153e6c8c66853713684ab7bed4",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We prove worst-case bounds on the sum of squared errors incurred by a generalization of the classical Widrow-Hoff algorithm to inner product spaces. We describe applications of this result to obtain worst-case agnostic learning results for classes of smooth functions and of linear functions."
            },
            "slug": "Worst-case-quadratic-loss-bounds-for-a-of-the-rule-Cesa-Bianchi-Long",
            "title": {
                "fragments": [],
                "text": "Worst-case quadratic loss bounds for a generalization of the Widrow-Hoff rule"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "It is proved that worst-case bounds on the sum of squared errors incurred by a generalization of the classical Widrow-Hoff algorithm to inner product spaces are correct."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '93"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152822011"
                        ],
                        "name": "H. Lowe",
                        "slug": "H.-Lowe",
                        "structuredName": {
                            "firstName": "H",
                            "lastName": "Lowe",
                            "middleNames": [
                                "J"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Lowe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145819899"
                        ],
                        "name": "G. Barnett",
                        "slug": "G.-Barnett",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Barnett",
                            "middleNames": [
                                "Octo"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Barnett"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 39513997,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "ebaba3a69f74ff4a36c7d445ac9cd8f77401bde6",
            "isKey": false,
            "numCitedBy": 422,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "The United States National Library of Medicine's (NLM) MEDLINE database is the largest and most widely used medical bibliographic database. MEDLINE is manually indexed with NLM's Medical Subject Headings (MeSH) vocabulary. Using MeSH, a searcher can potentially create powerful and unambiguous MEDLINE queries. This article reviews the structure and use of MeSH, directed toward the nonexpert, and outlines how MeSH may help resolve a number of common difficulties encountered when searching MEDLINE. The increasing importance of the MEDLINE database as an information resource and the trend toward individuals performing their own bibliographic searches makes it crucial that health care professionals become familiar with MeSH."
            },
            "slug": "Understanding-and-using-the-medical-subject-(MeSH)-Lowe-Barnett",
            "title": {
                "fragments": [],
                "text": "Understanding and using the medical subject headings (MeSH) vocabulary to perform literature searches."
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The structure and use of MeSH are reviewed, directed toward the nonexpert, and how MeSH may help resolve a number of common difficulties encountered when searching MEDLINE are outlined."
            },
            "venue": {
                "fragments": [],
                "text": "JAMA"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "97301357"
                        ],
                        "name": "F. David",
                        "slug": "F.-David",
                        "structuredName": {
                            "firstName": "F.",
                            "lastName": "David",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. David"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "92448292"
                        ],
                        "name": "S. Siegel",
                        "slug": "S.-Siegel",
                        "structuredName": {
                            "firstName": "Sidney",
                            "lastName": "Siegel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Siegel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 124165178,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "53b9b242f8cb2007e8e3dd9db5cd11b88fa6c4a7",
            "isKey": false,
            "numCitedBy": 33564,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This is the revision of the classic text in the field, adding two new chapters and thoroughly updating all others. The original structure is retained, and the book continues to serve as a combined text/reference."
            },
            "slug": "Nonparametric-Statistics-for-the-Behavioral-David-Siegel",
            "title": {
                "fragments": [],
                "text": "Nonparametric Statistics for the Behavioral Sciences."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1956
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 64
                            }
                        ],
                        "text": "Rocchio used ~ = 16 and ~ = 4, as suggested by Buckley, et al. (Buckley \net al., 1994), but with cr = O since no query was used."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 315,
                                "start": 310
                            }
                        ],
                        "text": "The features used were the content \nwords occurring in the textual description of the topic (on average 7.92 words/topic for topics 51-100 \nand 8.76 words/topic for topics 101-150), and either 50 or 1000 additional words cho\u00adsen by a query expansion \nprocess similar to that used in the U Mass TREC-4 experiments (Allan et al., 1996)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 143
                            }
                        ],
                        "text": "\u2026Q+50w / Rocchio .326 .341 Q+50w / WH .361 .288 Q+50w / EG .415 .403 Q+1OOOW / Rocchio \n.203 .190 Q+1OOOW / WH .216 .192 Q+1OOOW / EG .404 .295 (Buckley et al., 1994) Q+50w / Rocchio .3829 \nQ+500w / Rocchio .4068 (Buckley &#38; Salton, 1995) m- Table 3: Mean R-precision across routing topics\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 28
                            }
                        ],
                        "text": "Buck\u00adley, Salton, and Allan (Buckley et al., 1994) found Rocchio better \nsuited to large feature sets on topics 51-100 than we did, probably due to differences in document length \nnormal\u00adization."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 27
                            }
                        ],
                        "text": "Buckley, C., \nSalton, G., &#38; Allan, J. (1994)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 11
                            }
                        ],
                        "text": "References Allan, J., Ballesteros, L., Callan, J. P., Croft, W. B., &#38; \nLu., Z. (1996)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The e ect of adding relevance information in arelevance feedback environment"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 32
                            }
                        ],
                        "text": "Helmbold, \nD. P., Kivinen, J., &#38; Warmuth, M. K. (1996)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 33
                            }
                        ],
                        "text": "Cesa-Bianchi, \nN., Long, P. M., &#38; Warmuth, M. K. (1993)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 97
                            }
                        ],
                        "text": "Acknowledgments Thanks \nto William Cohen, Isabelle Moulinier, Amit Sing\u00adhal, Yoram Singer, Manfred Warmuth, and Yiming Yang for \nhelpful comments on this work."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 27
                            }
                        ],
                        "text": "Kivinen, J., &#38; Warmuth, M. K. (1994)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 61
                            }
                        ],
                        "text": "4 Error Bounds for WEI and EG Kivinen and Warmuth (Kivinen &#38; \nWarmuth, 1994) study in detail the theoretical behavior of EG and WH, build\u00ading on previous work (Cesa-Bianchi \net al., 1993; Widrow &#38; Stearns, 1985)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 58
                            }
                        ],
                        "text": "However, there are theoretical arguments (e.g. \n(Kivinen &#38; Warmuth, 1994)) which suggest that a better choice is the average of the weight vectors \ncomputed along the way: w= (4) -+&#38; n+lt=l  3.3 Kivinen &#38; Warmuth s EG Algorithm The exponentiated-gradient \nor EG algorithm was introduced by Kivinen and Warmuth (Kivinen &#38; Warmuth, 1994)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Exponentiated gradient versus gradient"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144169170"
                        ],
                        "name": "D. Harman",
                        "slug": "D.-Harman",
                        "structuredName": {
                            "firstName": "Donna",
                            "lastName": "Harman",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Harman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 71
                            }
                        ],
                        "text": "The classical vector space model (Salton &#38; McGill, 1983, pp. 120 123) (Harman, 1992a), ranks documents \nusing a nonlinear similarity measure called the cosine correlation: SIM(q, X) = ~ 114111+1 where IIxII \n= @~=l x~2, and q is a query vector with the same features as x."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 69
                            }
                        ],
                        "text": "3.1 The Rocchio Algorithm The Rocchio algorithm (Rocchio, Jr., 1971; Harman, 1992b) is a batch \nalgorithm."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 9
                            }
                        ],
                        "text": "120\u2013123) (Harman, 1992a), ranks documents using a nonlinear similarity measure called the cosine correlation:"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 286,
                                "start": 273
                            }
                        ],
                        "text": "For instance, qj might be 1 if a word \nappeared in a textual user request and O otherwise, while XJ is the number of times the word occurs in \nthe document of interest, times its inverse document frequency, i.e., a form of tf x idf weighting (Salton \n&#38; McGill, 1983, p. 63), (Harman, 1992a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 31569086,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c48f63010bfdda717cf1f96e24c49e49efcaa05a",
            "isKey": true,
            "numCitedBy": 232,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Ranking-Algorithms-Harman",
            "title": {
                "fragments": [],
                "text": "Ranking Algorithms"
            },
            "venue": {
                "fragments": [],
                "text": "Information Retrieval: Data Structures & Algorithms"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 54
                            }
                        ],
                        "text": "Understanding and using the medical \nsubject headings (MeSH) vocabulary to perform literature searches."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 129
                            }
                        ],
                        "text": "For text categorization experiments, \nwe ignore the queries and rel\u00ad evance judgments in the collection, and make use of the MeSH (Lowe &#38; \nBarnett, 1994) controlled vocabulary terms assigned to the records by National Library of Medicine in\u00ad \ndexers."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 8
                            }
                        ],
                        "text": "The 119 MeSH Heart Disease categories was extracted \nby Yim\u00ading Yang from the April 1994 (5th Ed.)"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "MeSH terms consist of a mam heading optionally flagged with \nsubheadings and importance markers."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 48
                            }
                        ],
                        "text": "Of the 348,566 OHSUMED records, all but 23 have MeSH categories assigned."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 191,
                                "start": 171
                            }
                        ],
                        "text": "In text categorization research with OHSUMED we have fo\u00adcused on the set of 119 MeSH categories in the \nHeart Dis\u00adease subtree of the Cardiovascular Diseases tree structure (Lowe &#38; Barnett, 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Understanding and using the medical subject headings ( MeSH ) vocabulary to perform literaturesearches"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of the American Medical Association"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 71
                            }
                        ],
                        "text": "The classical vector space model (Salton &#38; McGill, 1983, pp. 120 123) (Harman, 1992a), ranks documents \nusing a nonlinear similarity measure called the cosine correlation: SIM(q, X) = ~ 114111+1 where IIxII \n= @~=l x~2, and q is a query vector with the same features as x."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 69
                            }
                        ],
                        "text": "3.1 The Rocchio Algorithm The Rocchio algorithm (Rocchio, Jr., 1971; Harman, 1992b) is a batch \nalgorithm."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 286,
                                "start": 273
                            }
                        ],
                        "text": "For instance, qj might be 1 if a word \nappeared in a textual user request and O otherwise, while XJ is the number of times the word occurs in \nthe document of interest, times its inverse document frequency, i.e., a form of tf x idf weighting (Salton \n&#38; McGill, 1983, p. 63), (Harman, 1992a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Relevance feedback and other query"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 118
                            }
                        ],
                        "text": "WH and EG counts are significantly higher (p < 0.05) than \nthe corresponding Rocchio counts by a one-tailed sign test (Siegel, 1956, Ch. 5) unless a ? is shown."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Nonparametric Statistics for the Behav"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1956
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 16
                            }
                        ],
                        "text": "Lewis, D. D., &#38; Gale, W. A. (1994)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 142
                            }
                        ],
                        "text": "Several previous text categorization studies with a \npro\u00adprietary AP collection have used two sets of 10 categories: Set 1 (Lewis &#38; Gale, 1994; Cohen, \n1995; Cohen &#38; Singer, 1996) and Set 2 (Lewis, 1995b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 31
                            }
                        ],
                        "text": "We used the F-measure (Lewis &#38; Gale, 1994) (see also \n(van Rijsbergen, 1979, pp. 173-176)), a weighted combina\u00adtion of recall and precision that can be defined \nin terms of the contingency table values: Fe= (B2 + 1)~~ = (/32 + l)a ~ P+R (p + l)a + b+ p c We use \nFB with /3= 1, i.e., F1 = 2a/(2a + b+ c)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 121
                            }
                        ],
                        "text": "Several previous text categorization studies with a proprietary AP collection have used two sets of 10 categories: Set 1 (Lewis & Gale, 1994; Cohen, 1995; Cohen & Singer, 1996) and Set 2 (Lewis, 1995b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Text categorization and relational"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144323963"
                        ],
                        "name": "S. Alexander",
                        "slug": "S.-Alexander",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Alexander",
                            "middleNames": [
                                "Thomas"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Alexander"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 12374910,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a7d78b005150b873a1b72423cdc045267e03daa7",
            "isKey": false,
            "numCitedBy": 3372,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Adaptive-Signal-Processing-Alexander",
            "title": {
                "fragments": [],
                "text": "Adaptive Signal Processing"
            },
            "venue": {
                "fragments": [],
                "text": "Texts and Monographs in Computer Science"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144169170"
                        ],
                        "name": "D. Harman",
                        "slug": "D.-Harman",
                        "structuredName": {
                            "firstName": "Donna",
                            "lastName": "Harman",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Harman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 46426807,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5fa83eb09ab77bbfbb9543790c2bc7557bea717a",
            "isKey": false,
            "numCitedBy": 202,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Relevance-Feedback-and-Other-Query-Modification-Harman",
            "title": {
                "fragments": [],
                "text": "Relevance Feedback and Other Query Modification Techniques"
            },
            "venue": {
                "fragments": [],
                "text": "Information Retrieval: Data Structures & Algorithms"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "TREC andTIPSTER experiments with INQUERY"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 54
                            }
                        ],
                        "text": "We used the F-measure (Lewis &#38; Gale, 1994) (see also \n(van Rijsbergen, 1979, pp. 173-176)), a weighted combina\u00adtion of recall and precision that can be defined \nin terms of the contingency table values: Fe= (B2 + 1)~~ = (/32 + l)a ~ P+R (p + l)a + b+ p c We use \nFB with /3= 1, i.e., F1 = 2a/(2a\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Information Retrieval. Butterworths"
            },
            "venue": {
                "fragments": [],
                "text": "Information Retrieval. Butterworths"
            },
            "year": 1979
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 55
                            }
                        ],
                        "text": "The nal classi er was selected by a pocketing strategy (Gallant, 1986)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 59
                            }
                        ],
                        "text": "The final classifier was selected by a pocketing \nstrategy (Gallant, 1986)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Optimal linear discriminants"
            },
            "venue": {
                "fragments": [],
                "text": "Inter- national Conference on Pattern Recognition"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Using probabilisticmodels of document retrieval without relevance feedback"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Documentation"
            },
            "year": 1979
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 58
                            }
                        ],
                        "text": "We used the F-measure (Lewis &#38; Gale, 1994) (see also \n(van Rijsbergen, 1979, pp. 173-176)), a weighted combina\u00adtion of recall and precision that can be defined \nin terms of the contingency table values: Fe= (B2 + 1)~~ = (/32 + l)a ~ P+R (p + l)a + b+ p c We use \nFB with /3= 1, i.e., F1 = 2a/(2a\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Information Retrieval. Butterworths, London, second edition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1979
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Using probabilistic models of document retrieval without relevance feedback"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Documentation"
            },
            "year": 1979
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 98
                            }
                        ],
                        "text": "%) log W(I (1 pj)qj j=l where p; and qi are probabilities to \nbe estimated based on training data (Robertson &#38; Sparck Jones, 1976) or the text of a user request \n(Croft &#38; Harper, 1979), and the X3 s are binary (1 if a word is present in a document, O otherwise)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Relevance weighting of searchterms"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of the American Society for InformationScience"
            },
            "year": 1976
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 122
                            }
                        ],
                        "text": "Several previous text categorization studies with a \npro\u00adprietary AP collection have used two sets of 10 categories: Set 1 (Lewis &#38; Gale, 1994; Cohen, \n1995; Cohen &#38; Singer, 1996) and Set 2 (Lewis, 1995b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 12
                            }
                        ],
                        "text": "We used the F-measure (Lewis &#38; Gale, 1994) (see also \n(van Rijsbergen, 1979, pp. 173-176)), a weighted combina\u00adtion of recall and precision that can be defined \nin terms of the contingency table values: Fe= (B2 + 1)~~ = (/32 + l)a ~ P+R (p + l)a + b+ p c We use \nFB with /3= 1, i.e., F1 = 2a/(2a + b+ c)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 23
                            }
                        ],
                        "text": "We used the F-measure (Lewis &#38; Gale, 1994) (see also \n(van Rijsbergen, 1979, pp. 173-176)), a weighted combina\u00adtion of recall and precision that can be defined \nin terms of the contingency table values: Fe= (B2 + 1)~~ = (/32 + l)a ~ P+R (p + l)a + b+ p c We use \nFB with /3= 1, i.e., F1 = 2a/(2a\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A sequential algorithm"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 294,
                                "start": 282
                            }
                        ],
                        "text": "\u2026to define a threshold t,and assign a document x to \nthe class if w . x > t.The threshold is chosen so as to optimize the desired effectiveness measure on \nthe training set, with the hope that effectiveness on the test set will also be optimized, though this \napproach has weaknesses (Lewis, 1995a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 134
                            }
                        ],
                        "text": "One alternative \nwould be to apply EG to sigmoidal units (Helmbold et al., 1996), which produce probabilities usable for \noptimization (Lewis, 1995a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 200,
                                "start": 188
                            }
                        ],
                        "text": "Several previous text categorization studies with a \npro\u00adprietary AP collection have used two sets of 10 categories: Set 1 (Lewis &#38; Gale, 1994; Cohen, \n1995; Cohen &#38; Singer, 1996) and Set 2 (Lewis, 1995b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A sequential algorithm for training"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 161
                            }
                        ],
                        "text": "%) log W(I (1 pj)qj j=l where p; and qi are probabilities to \nbe estimated based on training data (Robertson &#38; Sparck Jones, 1976) or the text of a user request \n(Croft &#38; Harper, 1979), and the X3 s are binary (1 if a word is present in a document, O otherwise)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Using probabilistic models of document retrievalwithout relevance feedback"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Documentation"
            },
            "year": 1979
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "OHSUMED: an interactive retrieval"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 44
                            }
                        ],
                        "text": "The use of general optimization procedures (Buckley &#38; Salton, 1995) \nis one answer to this problem, but one that sacrifices efficiency and theoretical guarantees."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 107
                            }
                        ],
                        "text": "For \nQ+50 features and Rocchio starting weights on topics 101-150, EG does as well as Buckley and Salton s \n(Buckley &#38; Salton, 1995) computationally intensive Dynamic Feed\u00adback Optimization."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 148
                            }
                        ],
                        "text": "\u2026.403 Q+1OOOW / Rocchio \n.203 .190 Q+1OOOW / WH .216 .192 Q+1OOOW / EG .404 .295 (Buckley et al., 1994) Q+50w / Rocchio .3829 \nQ+500w / Rocchio .4068 (Buckley &#38; Salton, 1995) m- Table 3: Mean R-precision across routing topics \nfor various training procedur~s. R-precision is pre~sion at a\u2026"
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Optimization of relevance"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 117
                            }
                        ],
                        "text": "%) log W(I (1 pj)qj j=l where p; and qi are probabilities to \nbe estimated based on training data (Robertson &#38; Sparck Jones, 1976) or the text of a user request \n(Croft &#38; Harper, 1979), and the X3 s are binary (1 if a word is present in a document, O otherwise)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Relevance weighting of search"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1976
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Relevance feedback in information"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1971
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 44
                            }
                        ],
                        "text": "The use of general optimization procedures (Buckley &#38; Salton, 1995) \nis one answer to this problem, but one that sacrifices efficiency and theoretical guarantees."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 107
                            }
                        ],
                        "text": "For \nQ+50 features and Rocchio starting weights on topics 101-150, EG does as well as Buckley and Salton s \n(Buckley &#38; Salton, 1995) computationally intensive Dynamic Feed\u00adback Optimization."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 148
                            }
                        ],
                        "text": "\u2026.403 Q+1OOOW / Rocchio \n.203 .190 Q+1OOOW / WH .216 .192 Q+1OOOW / EG .404 .295 (Buckley et al., 1994) Q+50w / Rocchio .3829 \nQ+500w / Rocchio .4068 (Buckley &#38; Salton, 1995) m- Table 3: Mean R-precision across routing topics \nfor various training procedur~s. R-precision is pre~sion at a\u2026"
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Optimization of relevancefeedback weights"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 99
                            }
                        ],
                        "text": "Rocchio performed par\u00adticularly poorly on a binary representation, \nas has previously been observed (Salton &#38; Buckley, 1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Improving retrieval"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A sequential algorithmfor training text classi ers : Corrigendum and additionaldata"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 22
                            }
                        ],
                        "text": "Blum's recent success (Blum, 1995) with a related multiplicative update algorithm on a learning problem with some textual features also encouraged us to try EG."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 23
                            }
                        ],
                        "text": "Blum s recent success (Blum, 1995) with a related multiplicative update algorithm on \na learn\u00ading problem with some textual features also encouraged us to try EG."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 164
                            }
                        ],
                        "text": "Maintaining and updating very large weight vectors may take too much space or time, so methods \nfor pruning weight vectors while maintaining theoretical guarantees (Blum, 1995) are also worth examining."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Emprical support for Winnow"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 44
                            }
                        ],
                        "text": "The use of general optimization procedures (Buckley &#38; Salton, 1995) \nis one answer to this problem, but one that sacrifices efficiency and theoretical guarantees."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 107
                            }
                        ],
                        "text": "For \nQ+50 features and Rocchio starting weights on topics 101-150, EG does as well as Buckley and Salton s \n(Buckley &#38; Salton, 1995) computationally intensive Dynamic Feed\u00adback Optimization."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 148
                            }
                        ],
                        "text": "\u2026.403 Q+1OOOW / Rocchio \n.203 .190 Q+1OOOW / WH .216 .192 Q+1OOOW / EG .404 .295 (Buckley et al., 1994) Q+50w / Rocchio .3829 \nQ+500w / Rocchio .4068 (Buckley &#38; Salton, 1995) m- Table 3: Mean R-precision across routing topics \nfor various training procedur~s. R-precision is pre~sion at a\u2026"
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Optimization of relevancefeedback weights The e ect ofadding relevance information in a relevance feedbackenvironment"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Worst-case quadratic loss"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Ranking algorithms Information Re- trieval: Data Structures and Algorithms"
            },
            "venue": {
                "fragments": [],
                "text": "Ranking algorithms Information Re- trieval: Data Structures and Algorithms"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 294,
                                "start": 282
                            }
                        ],
                        "text": "\u2026to define a threshold t,and assign a document x to \nthe class if w . x > t.The threshold is chosen so as to optimize the desired effectiveness measure on \nthe training set, with the hope that effectiveness on the test set will also be optimized, though this \napproach has weaknesses (Lewis, 1995a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 134
                            }
                        ],
                        "text": "One alternative \nwould be to apply EG to sigmoidal units (Helmbold et al., 1996), which produce probabilities usable for \noptimization (Lewis, 1995a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 200,
                                "start": 188
                            }
                        ],
                        "text": "Several previous text categorization studies with a \npro\u00adprietary AP collection have used two sets of 10 categories: Set 1 (Lewis &#38; Gale, 1994; Cohen, \n1995; Cohen &#38; Singer, 1996) and Set 2 (Lewis, 1995b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Evaluating and optimizing au"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 14,
            "methodology": 22
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 57,
        "totalPages": 6
    },
    "page_url": "https://www.semanticscholar.org/paper/Training-algorithms-for-linear-text-classifiers-Lewis-Schapire/2dc36b8d0c08613fb213ad419973d379a2264765?sort=total-citations"
}