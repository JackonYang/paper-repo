{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145157221"
                        ],
                        "name": "E. T. K. Sang",
                        "slug": "E.-T.-K.-Sang",
                        "structuredName": {
                            "firstName": "Erik",
                            "lastName": "Sang",
                            "middleNames": [
                                "Tjong",
                                "Kim"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. T. K. Sang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105414"
                        ],
                        "name": "F. D. Meulder",
                        "slug": "F.-D.-Meulder",
                        "structuredName": {
                            "firstName": "Fien",
                            "lastName": "Meulder",
                            "middleNames": [
                                "De"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. D. Meulder"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2470716,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "10f97f1fb4f5c2c8e6c44d4a33da46d331dd4aeb",
            "isKey": false,
            "numCitedBy": 2959,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe the CoNLL-2003 shared task: language-independent named entity recognition. We give background information on the data sets (English and German) and the evaluation method, present a general overview of the systems that have taken part in the task and discuss their performance."
            },
            "slug": "Introduction-to-the-CoNLL-2003-Shared-Task:-Named-Sang-Meulder",
            "title": {
                "fragments": [],
                "text": "Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "The CoNLL-2003 shared task: language-independent named entity recognition is described and a general overview of the systems that have taken part in the task and discuss their performance is presented."
            },
            "venue": {
                "fragments": [],
                "text": "CoNLL"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145157221"
                        ],
                        "name": "E. T. K. Sang",
                        "slug": "E.-T.-K.-Sang",
                        "structuredName": {
                            "firstName": "Erik",
                            "lastName": "Sang",
                            "middleNames": [
                                "Tjong",
                                "Kim"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. T. K. Sang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3262157,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "502858989368adae91e0f4a643ebc2aa6efd5738",
            "isKey": false,
            "numCitedBy": 660,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe the CoNLL-2002 shared task: language-independent named entity recognition. We give background information on the data sets and the evaluation method, present a general overview of the systems that have taken part in the task and discuss their performance."
            },
            "slug": "Introduction-to-the-CoNLL-2002-Shared-Task:-Named-Sang",
            "title": {
                "fragments": [],
                "text": "Introduction to the CoNLL-2002 Shared Task: Language-Independent Named Entity Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "The CoNLL-2002 shared task: language-independent named entity recognition is described and a general overview of the systems that have taken part in the task and discuss their performance is presented."
            },
            "venue": {
                "fragments": [],
                "text": "CoNLL"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "73040249"
                        ],
                        "name": "Silviu Cucerzan",
                        "slug": "Silviu-Cucerzan",
                        "structuredName": {
                            "firstName": "Silviu",
                            "lastName": "Cucerzan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Silviu Cucerzan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693517"
                        ],
                        "name": "David Yarowsky",
                        "slug": "David-Yarowsky",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Yarowsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Yarowsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1066131,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "fa9e372d55a9f19b25fe078161a7ae059befee7d",
            "isKey": false,
            "numCitedBy": 44,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper investigates the use of a language independent model for named entity recognition based on iterative learning in a co-training fashion, using word-internal and contextual information as independent evidence sources. Its bootstrapping process begins with only seed entities and seed contexts extracted from the provided annotated corpus. F-measure exceeds 77 in Spanish and 72 in Dutch."
            },
            "slug": "Language-Independent-NER-using-a-Unified-Model-of-Cucerzan-Yarowsky",
            "title": {
                "fragments": [],
                "text": "Language Independent NER using a Unified Model of Internal and Contextual Evidence"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This paper investigates the use of a language independent model for named entity recognition based on iterative learning in a co-training fashion, using word-internal and contextual information as independent evidence sources."
            },
            "venue": {
                "fragments": [],
                "text": "CoNLL"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "73040249"
                        ],
                        "name": "Silviu Cucerzan",
                        "slug": "Silviu-Cucerzan",
                        "structuredName": {
                            "firstName": "Silviu",
                            "lastName": "Cucerzan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Silviu Cucerzan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693517"
                        ],
                        "name": "David Yarowsky",
                        "slug": "David-Yarowsky",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Yarowsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Yarowsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11312031,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5ee96af7e166dd833fcfc43dec290115fac4e7d2",
            "isKey": false,
            "numCitedBy": 267,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Identifying and classifying personal, geographic, institutional or other names in a text is an important task for numerous applications. This paper describes and evaluates a language-independent bootstrapping algorithm based on iterative learning and re-estimation of contextual and morphological patterns captured in hierarchically smoothed trie models. The algorithm learns from unannotated text and achieves competitive performance when trained on a very short labelled name list with no other required language-speci c information, tokenizers or tools."
            },
            "slug": "Language-Independent-Named-Entity-Recognition-and-Cucerzan-Yarowsky",
            "title": {
                "fragments": [],
                "text": "Language Independent Named Entity Recognition Combining Morphological and Contextual Evidence"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A language-independent bootstrapping algorithm based on iterative learning and re-estimation of contextual and morphological patterns captured in hierarchically smoothed trie models is described and evaluated."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2939803"
                        ],
                        "name": "Ronan Collobert",
                        "slug": "Ronan-Collobert",
                        "structuredName": {
                            "firstName": "Ronan",
                            "lastName": "Collobert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronan Collobert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "21432929"
                        ],
                        "name": "Michael Karlen",
                        "slug": "Michael-Karlen",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Karlen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Karlen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2645384"
                        ],
                        "name": "K. Kavukcuoglu",
                        "slug": "K.-Kavukcuoglu",
                        "structuredName": {
                            "firstName": "Koray",
                            "lastName": "Kavukcuoglu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kavukcuoglu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46283650"
                        ],
                        "name": "P. Kuksa",
                        "slug": "P.-Kuksa",
                        "structuredName": {
                            "firstName": "Pavel",
                            "lastName": "Kuksa",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Kuksa"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 14
                            }
                        ],
                        "text": "For instance, Collobert et al. (2011) uses a CNN over a sequence of word embeddings with a CRF layer on top."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 54
                            }
                        ],
                        "text": "This architecture is similar to the ones presented by Collobert et al. (2011) and Huang et al."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 77
                            }
                        ],
                        "text": "However, even systems that have relied extensively on unsupervised features (Collobert et al., 2011; Turian et al., 2010; Lin and Wu, 2009; Ando and Zhang, 2005b, inter alia) have used these to augment, rather than replace, hand-engineered features (e.g., knowledge about capitalization patterns and\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 54
                            }
                        ],
                        "text": "This architecture is similar to the ones presented by Collobert et al. (2011) and Huang et al. (2015)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 6
                            }
                        ],
                        "text": "As in Collobert et al. (2011), we use pretrained word embeddings to initialize our lookup table."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 351666,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bc1022b031dc6c7019696492e8116598097a8c12",
            "isKey": true,
            "numCitedBy": 6657,
            "numCiting": 108,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements."
            },
            "slug": "Natural-Language-Processing-(Almost)-from-Scratch-Collobert-Weston",
            "title": {
                "fragments": [],
                "text": "Natural Language Processing (Almost) from Scratch"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "A unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144720379"
                        ],
                        "name": "Alexandre Passos",
                        "slug": "Alexandre-Passos",
                        "structuredName": {
                            "firstName": "Alexandre",
                            "lastName": "Passos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexandre Passos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112232561"
                        ],
                        "name": "Vineet Kumar",
                        "slug": "Vineet-Kumar",
                        "structuredName": {
                            "firstName": "Vineet",
                            "lastName": "Kumar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vineet Kumar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "Passos et al. (2014) also used a linear chain CRF with spelling features and gazetteers."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9345583,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d53d878cf1a3f0bed5d9c68c925994cb72f47304",
            "isKey": false,
            "numCitedBy": 281,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Most state-of-the-art approaches for named-entity recognition (NER) use semi supervised information in the form of word clusters and lexicons. Recently neural network-based language models have been explored, as they as a byproduct generate highly informative vector representations for words, known as word embeddings. In this paper we present two contributions: a new form of learning word embeddings that can leverage information from relevant lexicons to improve the representations, and the first system to use neural word embeddings to achieve state-of-the-art results on named-entity recognition in both CoNLL and Ontonotes NER. Our system achieves an F1 score of 90.90 on the test set for CoNLL 2003---significantly better than any previous system trained on public data, and matching a system employing massive private industrial query-log data."
            },
            "slug": "Lexicon-Infused-Phrase-Embeddings-for-Named-Entity-Passos-Kumar",
            "title": {
                "fragments": [],
                "text": "Lexicon Infused Phrase Embeddings for Named Entity Resolution"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A new form of learning word embeddings that can leverage information from relevant lexicons to improve the representations, and the first system to use neural word embedDings to achieve state-of-the-art results on named-entity recognition in both CoNLL and Ontonotes NER are presented."
            },
            "venue": {
                "fragments": [],
                "text": "CoNLL"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790831"
                        ],
                        "name": "C. D. Santos",
                        "slug": "C.-D.-Santos",
                        "structuredName": {
                            "firstName": "C\u00edcero",
                            "lastName": "Santos",
                            "middleNames": [
                                "Nogueira",
                                "dos"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. D. Santos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2070318426"
                        ],
                        "name": "Victor Guimar\u00e3es",
                        "slug": "Victor-Guimar\u00e3es",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Guimar\u00e3es",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Victor Guimar\u00e3es"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9150889,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "91a93f751f912b2f96d6771018d8f06c41e11152",
            "isKey": false,
            "numCitedBy": 297,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Most state-of-the-art named entity recognition (NER) systems rely on handcrafted features and on the output of other NLP tasks such as part-of-speech (POS) tagging and text chunking. In this work we propose a language-independent NER system that uses automatically learned features only. Our approach is based on the CharWNN deep neural network, which uses word-level and character-level representations (embeddings) to perform sequential classification. We perform an extensive number of experiments using two annotated corpora in two different languages: HAREM I corpus, which contains texts in Portuguese; and the SPA CoNLL-2002 corpus, which contains texts in Spanish. Our experimental results shade light on the contribution of neural character embeddings for NER. Moreover, we demonstrate that the same neural network which has been successfully applied to POS tagging can also achieve state-of-the-art results for language-independet NER, using the same hyperparameters, and without any handcrafted features. For the HAREM I corpus, CharWNN outperforms the state-of-the-art system by 7.9 points in the F1-score for the total scenario (ten NE classes), and by 7.2 points in the F1 for the selective scenario (five NE classes)."
            },
            "slug": "Boosting-Named-Entity-Recognition-with-Neural-Santos-Guimar\u00e3es",
            "title": {
                "fragments": [],
                "text": "Boosting Named Entity Recognition with Neural Character Embeddings"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This work proposes a language-independent NER system that uses automatically learned features only and demonstrates that the same neural network which has been successfully applied to POS tagging can also achieve state-of-the-art results for language-independet NER, using the same hyperparameters, and without any handcrafted features."
            },
            "venue": {
                "fragments": [],
                "text": "NEWS@ACL"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2335225"
                        ],
                        "name": "Lev-Arie Ratinov",
                        "slug": "Lev-Arie-Ratinov",
                        "structuredName": {
                            "firstName": "Lev-Arie",
                            "lastName": "Ratinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lev-Arie Ratinov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144590225"
                        ],
                        "name": "D. Roth",
                        "slug": "D.-Roth",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Roth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Roth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1859014,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aa9efc8b2737eac0675ba5abb5feab8305482c12",
            "isKey": false,
            "numCitedBy": 1397,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "We analyze some of the fundamental design challenges and misconceptions that underlie the development of an efficient and robust NER system. In particular, we address issues such as the representation of text chunks, the inference approach needed to combine local NER decisions, the sources of prior knowledge and how to use them within an NER system. In the process of comparing several solutions to these challenges we reach some surprising conclusions, as well as develop an NER system that achieves 90.8 F1 score on the CoNLL-2003 NER shared task, the best reported result for this dataset."
            },
            "slug": "Design-Challenges-and-Misconceptions-in-Named-Ratinov-Roth",
            "title": {
                "fragments": [],
                "text": "Design Challenges and Misconceptions in Named Entity Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "Some of the fundamental design challenges and misconceptions that underlie the development of an efficient and robust NER system are analyzed, and several solutions to these challenges are developed."
            },
            "venue": {
                "fragments": [],
                "text": "CoNLL"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707117"
                        ],
                        "name": "Radu Florian",
                        "slug": "Radu-Florian",
                        "structuredName": {
                            "firstName": "Radu",
                            "lastName": "Florian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radu Florian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1683685"
                        ],
                        "name": "Abraham Ittycheriah",
                        "slug": "Abraham-Ittycheriah",
                        "structuredName": {
                            "firstName": "Abraham",
                            "lastName": "Ittycheriah",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Abraham Ittycheriah"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40544823"
                        ],
                        "name": "Hongyan Jing",
                        "slug": "Hongyan-Jing",
                        "structuredName": {
                            "firstName": "Hongyan",
                            "lastName": "Jing",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hongyan Jing"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117881943"
                        ],
                        "name": "Tong Zhang",
                        "slug": "Tong-Zhang",
                        "structuredName": {
                            "firstName": "Tong",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tong Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 41
                            }
                        ],
                        "text": "Next year, in the CoNLL2003 Shared Task, Florian et al. (2003) obtained the best score on German by combining the output of four diverse classifiers."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 0
                            }
                        ],
                        "text": "Gillick et al. (2015) model the task of sequencelabeling as a sequence to sequence learning problem and incorporate character-based representations into their encoder model."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10606201,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d1ba322f795adbdc706651dbc76ad53b9c5f9468",
            "isKey": false,
            "numCitedBy": 477,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a classifier-combination experimental framework for named entity recognition in which four diverse classifiers (robust linear classifier, maximum entropy, transformation-based learning, and hidden Markov model) are combined under different conditions. When no gazetteer or other additional training resources are used, the combined system attains a performance of 91.6F on the English development data; integrating name, location and person gazetteers, and named entity systems trained on additional, more general, data reduces the F-measure error by a factor of 15 to 21% on the English data."
            },
            "slug": "Named-Entity-Recognition-through-Classifier-Florian-Ittycheriah",
            "title": {
                "fragments": [],
                "text": "Named Entity Recognition through Classifier Combination"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "A classifier-combination experimental framework for named entity recognition in which four diverse classifiers (robust linear classifier, maximum entropy, transformation-based learning, and hidden Markov model) are combined under different conditions is presented."
            },
            "venue": {
                "fragments": [],
                "text": "CoNLL"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052473561"
                        ],
                        "name": "Jason P. C. Chiu",
                        "slug": "Jason-P.-C.-Chiu",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Chiu",
                            "middleNames": [
                                "P.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jason P. C. Chiu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143662759"
                        ],
                        "name": "Eric Nichols",
                        "slug": "Eric-Nichols",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Nichols",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eric Nichols"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 132
                            }
                        ],
                        "text": "Our StackLSTM model also outperforms all previous models that do not incorporate external features, apart from the one presented by Chiu and Nichols (2015)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 0
                            }
                        ],
                        "text": "Chiu and Nichols (2015) employ an architecture similar to ours, but instead use CNNs to learn character-level features, in a way similar to the work by Santos and Guimar\u00e3es (2015)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 6300165,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "10a4db59e81d26b2e0e896d3186ef81b4458b93f",
            "isKey": false,
            "numCitedBy": 1335,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "Named entity recognition is a challenging task that has traditionally required large amounts of knowledge in the form of feature engineering and lexicons to achieve high performance. In this paper, we present a novel neural network architecture that automatically detects word- and character-level features using a hybrid bidirectional LSTM and CNN architecture, eliminating the need for most feature engineering. We also propose a novel method of encoding partial lexicon matches in neural networks and compare it to existing approaches. Extensive evaluation shows that, given only tokenized text and publicly available word embeddings, our system is competitive on the CoNLL-2003 dataset and surpasses the previously reported state of the art performance on the OntoNotes 5.0 dataset by 2.13 F1 points. By using two lexicons constructed from publicly-available sources, we establish new state of the art performance with an F1 score of 91.62 on CoNLL-2003 and 86.28 on OntoNotes, surpassing systems that employ heavy feature engineering, proprietary lexicons, and rich entity linking information."
            },
            "slug": "Named-Entity-Recognition-with-Bidirectional-Chiu-Nichols",
            "title": {
                "fragments": [],
                "text": "Named Entity Recognition with Bidirectional LSTM-CNNs"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A novel neural network architecture is presented that automatically detects word- and character-level features using a hybrid bidirectional LSTM and CNN architecture, eliminating the need for most feature engineering."
            },
            "venue": {
                "fragments": [],
                "text": "TACL"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46447747"
                        ],
                        "name": "X. Zhang",
                        "slug": "X.-Zhang",
                        "structuredName": {
                            "firstName": "Xiang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7818229"
                        ],
                        "name": "J. Zhao",
                        "slug": "J.-Zhao",
                        "structuredName": {
                            "firstName": "Junbo",
                            "lastName": "Zhao",
                            "middleNames": [
                                "Jake"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Zhao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 139
                            }
                        ],
                        "text": "Alternative approaches\u2014 most notably like convolutional networks\u2014have been proposed to learn representations of words from their characters (Zhang et al., 2015; Kim et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 368182,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "51a55df1f023571a7e07e338ee45a3e3d66ef73e",
            "isKey": false,
            "numCitedBy": 3477,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classification. We constructed several large-scale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag of words, n-grams and their TFIDF variants, and deep learning models such as word-based ConvNets and recurrent neural networks."
            },
            "slug": "Character-level-Convolutional-Networks-for-Text-Zhang-Zhao",
            "title": {
                "fragments": [],
                "text": "Character-level Convolutional Networks for Text Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "This article constructed several large-scale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results in text classification."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791105"
                        ],
                        "name": "Yanjun Qi",
                        "slug": "Yanjun-Qi",
                        "structuredName": {
                            "firstName": "Yanjun",
                            "lastName": "Qi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yanjun Qi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2939803"
                        ],
                        "name": "Ronan Collobert",
                        "slug": "Ronan-Collobert",
                        "structuredName": {
                            "firstName": "Ronan",
                            "lastName": "Collobert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronan Collobert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46283650"
                        ],
                        "name": "P. Kuksa",
                        "slug": "P.-Kuksa",
                        "structuredName": {
                            "firstName": "Pavel",
                            "lastName": "Kuksa",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Kuksa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2645384"
                        ],
                        "name": "K. Kavukcuoglu",
                        "slug": "K.-Kavukcuoglu",
                        "structuredName": {
                            "firstName": "Koray",
                            "lastName": "Kavukcuoglu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kavukcuoglu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 0
                            }
                        ],
                        "text": "Qi et al. (2009) later improved on this with a neural network by doing unsupervised learning on a massive unlabeled corpus."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14966541,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e1366a1fe99c51ac2a060250af522f387f2c07e1",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a novel simple and highly scalable semi-supervised method called Word-Class Distribution Learning (WCDL), and apply it task of information extraction (IE) by utilizing unlabeled sentences to improve supervised classification methods. WCDL iteratively builds class label distributions for each word in the dictionary by averaging predicted labels over all cases in the unlabeled corpus, and re-training a base classifier adding these distributions as word features. In contrast, traditional self-training or co-training methods self-labeled examples (rather than features) which can degrade performance due to incestuous learning bias. WCDL exhibits robust behavior, and has no difficult parameters to tune. We applied our method on German and English name entity recognition (NER) tasks. WCDL shows improvements over self-training, multi-task semi-supervision or supervision alone, in particular yielding a state-of-the art 75.72 F1 score on the German NER task."
            },
            "slug": "Combining-labeled-and-unlabeled-data-with-learning-Qi-Collobert",
            "title": {
                "fragments": [],
                "text": "Combining labeled and unlabeled data with word-class distribution learning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Word-Class Distribution Learning shows improvements over self-training, multi-task semi-supervision or supervision alone, in particular yielding a state-of-the art 75.72 F1 score on the German NER task."
            },
            "venue": {
                "fragments": [],
                "text": "CIKM"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056099729"
                        ],
                        "name": "Gang Luo",
                        "slug": "Gang-Luo",
                        "structuredName": {
                            "firstName": "Gang",
                            "lastName": "Luo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gang Luo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794577"
                        ],
                        "name": "Xiaojiang Huang",
                        "slug": "Xiaojiang-Huang",
                        "structuredName": {
                            "firstName": "Xiaojiang",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaojiang Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781574"
                        ],
                        "name": "Chin-Yew Lin",
                        "slug": "Chin-Yew-Lin",
                        "structuredName": {
                            "firstName": "Chin-Yew",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chin-Yew Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38301933"
                        ],
                        "name": "Zaiqing Nie",
                        "slug": "Zaiqing-Nie",
                        "structuredName": {
                            "firstName": "Zaiqing",
                            "lastName": "Nie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zaiqing Nie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 43
                            }
                        ],
                        "text": "The best score reported on this task is by Luo et al. (2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12016540,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7fe65a296757d03241370a21d08fe0d9ae2b381e",
            "isKey": false,
            "numCitedBy": 70,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Extracting named entities in text and linking extracted names to a given knowledge base are fundamental tasks in applications for text understanding. Existing systems typically run a named entity recognition (NER) model to extract entity names first, then run an entity linking model to link extracted names to a knowledge base. NER and linking models are usually trained separately, and the mutual dependency between the two tasks is ignored. We propose JERL, Joint Entity Recognition and Linking, to jointly model NER and linking tasks and capture the mutual dependency between them. It allows the information from each task to improve the performance of the other. To the best of our knowledge, JERL is the first model to jointly optimize NER and linking tasks together completely. In experiments on the CoNLL\u201903/AIDA data set, JERL outperforms state-of-art NER and linking systems, and we find improvements of 0.4% absolute F1 for NER on CoNLL\u201903, and 0.36% absolute precision@1 for linking on AIDA."
            },
            "slug": "Joint-Named-Entity-Recognition-and-Disambiguation-Luo-Huang",
            "title": {
                "fragments": [],
                "text": "Joint Named Entity Recognition and Disambiguation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "JERL, Joint Entity Recognition and Linking, is the first model to jointly optimize NER and linking tasks together completely, and in experiments on the CoNLL\u201903/AIDA data set, JERL outperforms state-of-art N ER and linking systems."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49178343"
                        ],
                        "name": "Jie Zhou",
                        "slug": "Jie-Zhou",
                        "structuredName": {
                            "firstName": "Jie",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jie Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145738410"
                        ],
                        "name": "W. Xu",
                        "slug": "W.-Xu",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Xu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12688069,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c34e41312b47f60986458759d5cc546c2b53f748",
            "isKey": false,
            "numCitedBy": 308,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "Semantic role labeling (SRL) is one of the basic natural language processing (NLP) problems. To this date, most of the successful SRL systems were built on top of some form of parsing results (Koomen et al., 2005; Palmer et al., 2010; Pradhan et al., 2013), where pre-defined feature templates over the syntactic structure are used. The attempts of building an end-to-end SRL learning system without using parsing were less successful (Collobert et al., 2011). In this work, we propose to use deep bi-directional recurrent network as an end-to-end system for SRL. We take only original text information as input feature, without using any syntactic knowledge. The proposed algorithm for semantic role labeling was mainly evaluated on CoNLL-2005 shared task and achieved F1 score of 81.07. This result outperforms the previous state-of-the-art system from the combination of different parsing trees or models. We also obtained the same conclusion with F1 = 81.27 on CoNLL2012 shared task. As a result of simplicity, our model is also computationally efficient that the parsing speed is 6.7k tokens per second. Our analysis shows that our model is better at handling longer sentences than traditional models. And the latent variables of our model implicitly capture the syntactic structure of a sentence."
            },
            "slug": "End-to-end-learning-of-semantic-role-labeling-using-Zhou-Xu",
            "title": {
                "fragments": [],
                "text": "End-to-end learning of semantic role labeling using recurrent neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This work proposes to use deep bi-directional recurrent network as an end-to-end system for SRL, which takes only original text information as input feature, without using any syntactic knowledge."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690956"
                        ],
                        "name": "Dekang Lin",
                        "slug": "Dekang-Lin",
                        "structuredName": {
                            "firstName": "Dekang",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dekang Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39096001"
                        ],
                        "name": "Xiaoyun Wu",
                        "slug": "Xiaoyun-Wu",
                        "structuredName": {
                            "firstName": "Xiaoyun",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaoyun Wu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8148140,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b5e12de7d3f2a66bfcf10a510f9ac80afb3541c1",
            "isKey": false,
            "numCitedBy": 244,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a simple and scalable algorithm for clustering tens of millions of phrases and use the resulting clusters as features in discriminative classifiers. To demonstrate the power and generality of this approach, we apply the method in two very different applications: named entity recognition and query classification. Our results show that phrase clusters offer significant improvements over word clusters. Our NER system achieves the best current result on the widely used CoNLL benchmark. Our query classifier is on par with the best system in KDDCUP 2005 without resorting to labor intensive knowledge engineering efforts."
            },
            "slug": "Phrase-Clustering-for-Discriminative-Learning-Lin-Wu",
            "title": {
                "fragments": [],
                "text": "Phrase Clustering for Discriminative Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "A simple and scalable algorithm for clustering tens of millions of phrases and using the resulting clusters as features in discriminative classifiers to demonstrate the power and generality of this approach."
            },
            "venue": {
                "fragments": [],
                "text": "ACL/IJCNLP"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3083916"
                        ],
                        "name": "J. Nothman",
                        "slug": "J.-Nothman",
                        "structuredName": {
                            "firstName": "Joel",
                            "lastName": "Nothman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Nothman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2937117"
                        ],
                        "name": "Nicky Ringland",
                        "slug": "Nicky-Ringland",
                        "structuredName": {
                            "firstName": "Nicky",
                            "lastName": "Ringland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nicky Ringland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145122113"
                        ],
                        "name": "Will Radford",
                        "slug": "Will-Radford",
                        "structuredName": {
                            "firstName": "Will",
                            "lastName": "Radford",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Will Radford"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2061521923"
                        ],
                        "name": "T. Murphy",
                        "slug": "T.-Murphy",
                        "structuredName": {
                            "firstName": "Tara",
                            "lastName": "Murphy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Murphy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733593"
                        ],
                        "name": "J. Curran",
                        "slug": "J.-Curran",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Curran",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Curran"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3721623,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "696b505083d34c6f995aef88d0352d70d7f7e8c8",
            "isKey": false,
            "numCitedBy": 294,
            "numCiting": 109,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-multilingual-named-entity-recognition-from-Nothman-Ringland",
            "title": {
                "fragments": [],
                "text": "Learning multilingual named entity recognition from Wikipedia"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2047446108"
                        ],
                        "name": "Tomas Mikolov",
                        "slug": "Tomas-Mikolov",
                        "structuredName": {
                            "firstName": "Tomas",
                            "lastName": "Mikolov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tomas Mikolov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118440152"
                        ],
                        "name": "Kai Chen",
                        "slug": "Kai-Chen",
                        "structuredName": {
                            "firstName": "Kai",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32131713"
                        ],
                        "name": "G. Corrado",
                        "slug": "G.-Corrado",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Corrado",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Corrado"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49959210"
                        ],
                        "name": "J. Dean",
                        "slug": "J.-Dean",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Dean",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Dean"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 91
                            }
                        ],
                        "text": "Embeddings are pretrained using skip-n-gram (Ling et al., 2015a), a variation of word2vec (Mikolov et al., 2013a) that accounts for word order."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 237,
                                "start": 216
                            }
                        ],
                        "text": "To capture orthographic sensitivity, we use character-based word representation model (Ling et al., 2015b) to capture distributional sensitivity, we combine these representations with distributional representations (Mikolov et al., 2013b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 34
                            }
                        ],
                        "text": ", 2015a), a variation of word2vec (Mikolov et al., 2013a) that accounts for word order."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5959482,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "330da625c15427c6e42ccfa3b747fb29e5835bf0",
            "isKey": false,
            "numCitedBy": 21881,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose two novel model architectures for computing continuous vector\nrepresentations of words from very large data sets. The quality of these\nrepresentations is measured in a word similarity task, and the results are\ncompared to the previously best performing techniques based on different types\nof neural networks. We observe large improvements in accuracy at much lower\ncomputational cost, i.e. it takes less than a day to learn high quality word\nvectors from a 1.6 billion words data set. Furthermore, we show that these\nvectors provide state-of-the-art performance on our test set for measuring\nsyntactic and semantic word similarities."
            },
            "slug": "Efficient-Estimation-of-Word-Representations-in-Mikolov-Chen",
            "title": {
                "fragments": [],
                "text": "Efficient Estimation of Word Representations in Vector Space"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "Two novel model architectures for computing continuous vector representations of words from very large data sets are proposed and it is shown that these vectors provide state-of-the-art performance on the authors' test set for measuring syntactic and semantic word similarities."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38367242"
                        ],
                        "name": "Yoon Kim",
                        "slug": "Yoon-Kim",
                        "structuredName": {
                            "firstName": "Yoon",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoon Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2262249"
                        ],
                        "name": "Yacine Jernite",
                        "slug": "Yacine-Jernite",
                        "structuredName": {
                            "firstName": "Yacine",
                            "lastName": "Jernite",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yacine Jernite"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746662"
                        ],
                        "name": "D. Sontag",
                        "slug": "D.-Sontag",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Sontag",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Sontag"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2531268"
                        ],
                        "name": "Alexander M. Rush",
                        "slug": "Alexander-M.-Rush",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Rush",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander M. Rush"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 159
                            }
                        ],
                        "text": "Alternative approaches\u2014 most notably like convolutional networks\u2014have been proposed to learn representations of words from their characters (Zhang et al., 2015; Kim et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 686481,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "891ce1687e2befddd19f54e4eef1d3f39c8dbaf7",
            "isKey": false,
            "numCitedBy": 1428,
            "numCiting": 83,
            "paperAbstract": {
                "fragments": [],
                "text": "\n \n We describe a simple neural language model that relies only on character-level inputs. Predictions are still made at the word-level. Our model employs a convolutional neural network (CNN) and a highway net work over characters, whose output is given to a long short-term memory (LSTM) recurrent neural network language model (RNN-LM). On the English Penn Treebank the model is on par with the existing state-of-the-art despite having 60% fewer parameters. On languages with rich morphology (Arabic, Czech, French, German, Spanish, Russian), the model outperforms word-level/morpheme-level LSTM baselines, again with fewer parameters. The results suggest that on many languages, character inputs are sufficient for language modeling. Analysis of word representations obtained from the character composition part of the model reveals that the model is able to encode, from characters only, both semantic and orthographic information.\n \n"
            },
            "slug": "Character-Aware-Neural-Language-Models-Kim-Jernite",
            "title": {
                "fragments": [],
                "text": "Character-Aware Neural Language Models"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "A simple neural language model that relies only on character-level inputs that is able to encode, from characters only, both semantic and orthographic information and suggests that on many languages, character inputs are sufficient for language modeling."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153160559"
                        ],
                        "name": "Joseph P. Turian",
                        "slug": "Joseph-P.-Turian",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Turian",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joseph P. Turian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2335225"
                        ],
                        "name": "Lev-Arie Ratinov",
                        "slug": "Lev-Arie-Ratinov",
                        "structuredName": {
                            "firstName": "Lev-Arie",
                            "lastName": "Ratinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lev-Arie Ratinov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 101
                            }
                        ],
                        "text": "However, even systems that have relied extensively on unsupervised features (Collobert et al., 2011; Turian et al., 2010; Lin and Wu, 2009; Ando and Zhang, 2005b, inter alia) have used these to augment, rather than replace, hand-engineered features (e.g., knowledge about capitalization patterns and\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 629094,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dac72f2c509aee67524d3321f77e97e8eff51de6",
            "isKey": false,
            "numCitedBy": 2163,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": "If we take an existing supervised NLP system, a simple and general way to improve accuracy is to use unsupervised word representations as extra word features. We evaluate Brown clusters, Collobert and Weston (2008) embeddings, and HLBL (Mnih & Hinton, 2009) embeddings of words on both NER and chunking. We use near state-of-the-art supervised baselines, and find that each of the three word representations improves the accuracy of these baselines. We find further improvements by combining different word representations. You can download our word features, for off-the-shelf use in existing NLP systems, as well as our code, here: http://metaoptimize.com/projects/wordreprs/"
            },
            "slug": "Word-Representations:-A-Simple-and-General-Method-Turian-Ratinov",
            "title": {
                "fragments": [],
                "text": "Word Representations: A Simple and General Method for Semi-Supervised Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work evaluates Brown clusters, Collobert and Weston (2008) embeddings, and HLBL (Mnih & Hinton, 2009) embeds of words on both NER and chunking, and finds that each of the three word representations improves the accuracy of these baselines."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144154709"
                        ],
                        "name": "Jacob Eisenstein",
                        "slug": "Jacob-Eisenstein",
                        "structuredName": {
                            "firstName": "Jacob",
                            "lastName": "Eisenstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jacob Eisenstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40599545"
                        ],
                        "name": "T. Yano",
                        "slug": "T.-Yano",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Yano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Yano"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50056360"
                        ],
                        "name": "William W. Cohen",
                        "slug": "William-W.-Cohen",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Cohen",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "William W. Cohen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144365875"
                        ],
                        "name": "Noah A. Smith",
                        "slug": "Noah-A.-Smith",
                        "structuredName": {
                            "firstName": "Noah",
                            "lastName": "Smith",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noah A. Smith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143977260"
                        ],
                        "name": "E. Xing",
                        "slug": "E.-Xing",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Xing",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Xing"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 0
                            }
                        ],
                        "text": "Eisenstein et al. (2011) use Bayesian nonparametrics to construct a database of named entities in an almost unsupervised setting. Ratinov and Roth (2009) quantitatively compare several approaches for NER and build their own supervised model using a regularized average perceptron and aggregating context information."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 0
                            }
                        ],
                        "text": "Eisenstein et al. (2011) use Bayesian nonparametrics to construct a database of named entities in an almost unsupervised setting."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5749835,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "02aebef93baeef3396f3cb4468a7054067f190c6",
            "isKey": false,
            "numCitedBy": 6,
            "numCiting": 232,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a nonparametric Bayesian approach to extract a structured database of entities from text. Neither the number of entities nor the fields that characterize each entity are provided in advance; the only supervision is a set of five prototype examples. Our method jointly accomplishes three tasks: (i) identifying a set of canonical entities, (ii) inferring a schema for the fields that describe each entity, and (iii) matching entities to their references in raw text. Empirical evaluation shows that the approach learns an accurate database of entities and a sensible model of name structure."
            },
            "slug": "Structured-Databases-of-Named-Entities-from-Eisenstein-Yano",
            "title": {
                "fragments": [],
                "text": "Structured Databases of Named Entities from Bayesian Nonparametrics"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "Empirical evaluation shows that the nonparametric Bayesian approach to extract a structured database of entities from text learns an accurate database of entity and a sensible model of name structure."
            },
            "venue": {
                "fragments": [],
                "text": "ULNLP@EMNLP"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1727527"
                        ],
                        "name": "Johannes Hoffart",
                        "slug": "Johannes-Hoffart",
                        "structuredName": {
                            "firstName": "Johannes",
                            "lastName": "Hoffart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Johannes Hoffart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34796954"
                        ],
                        "name": "M. Yosef",
                        "slug": "M.-Yosef",
                        "structuredName": {
                            "firstName": "Mohamed",
                            "lastName": "Yosef",
                            "middleNames": [
                                "Amir"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Yosef"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3033855"
                        ],
                        "name": "Ilaria Bordino",
                        "slug": "Ilaria-Bordino",
                        "structuredName": {
                            "firstName": "Ilaria",
                            "lastName": "Bordino",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilaria Bordino"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1683588"
                        ],
                        "name": "Hagen F\u00fcrstenau",
                        "slug": "Hagen-F\u00fcrstenau",
                        "structuredName": {
                            "firstName": "Hagen",
                            "lastName": "F\u00fcrstenau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hagen F\u00fcrstenau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717560"
                        ],
                        "name": "Manfred Pinkal",
                        "slug": "Manfred-Pinkal",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Pinkal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manfred Pinkal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3115338"
                        ],
                        "name": "M. Spaniol",
                        "slug": "M.-Spaniol",
                        "structuredName": {
                            "firstName": "Marc",
                            "lastName": "Spaniol",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Spaniol"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3326792"
                        ],
                        "name": "Bilyana Taneva",
                        "slug": "Bilyana-Taneva",
                        "structuredName": {
                            "firstName": "Bilyana",
                            "lastName": "Taneva",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bilyana Taneva"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1727272"
                        ],
                        "name": "Stefan Thater",
                        "slug": "Stefan-Thater",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Thater",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stefan Thater"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751591"
                        ],
                        "name": "G. Weikum",
                        "slug": "G.-Weikum",
                        "structuredName": {
                            "firstName": "Gerhard",
                            "lastName": "Weikum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Weikum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 55
                            }
                        ],
                        "text": "2 by jointly modeling the NER and entity linking tasks (Hoffart et al., 2011)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 81
                            }
                        ],
                        "text": "They obtained a F1 of 91.2 by jointly modeling the NER and entity linking tasks (Hoffart et al., 2011)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6216506,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d95738f38d97a030d98508357e4d5c78a4a208ba",
            "isKey": true,
            "numCitedBy": 948,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Disambiguating named entities in natural-language text maps mentions of ambiguous names onto canonical entities like people or places, registered in a knowledge base such as DBpedia or YAGO. This paper presents a robust method for collective disambiguation, by harnessing context from knowledge bases and using a new form of coherence graph. It unifies prior approaches into a comprehensive framework that combines three measures: the prior probability of an entity being mentioned, the similarity between the contexts of a mention and a candidate entity, as well as the coherence among candidate entities for all mentions together. The method builds a weighted graph of mentions and candidate entities, and computes a dense subgraph that approximates the best joint mention-entity mapping. Experiments show that the new method significantly outperforms prior methods in terms of accuracy, with robust behavior across a variety of inputs."
            },
            "slug": "Robust-Disambiguation-of-Named-Entities-in-Text-Hoffart-Yosef",
            "title": {
                "fragments": [],
                "text": "Robust Disambiguation of Named Entities in Text"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A robust method for collective disambiguation is presented, by harnessing context from knowledge bases and using a new form of coherence graph that significantly outperforms prior methods in terms of accuracy, with robust behavior across a variety of inputs."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745899"
                        ],
                        "name": "Chris Dyer",
                        "slug": "Chris-Dyer",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Dyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris Dyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143668305"
                        ],
                        "name": "Miguel Ballesteros",
                        "slug": "Miguel-Ballesteros",
                        "structuredName": {
                            "firstName": "Miguel",
                            "lastName": "Ballesteros",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Miguel Ballesteros"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1379953252"
                        ],
                        "name": "Wang Ling",
                        "slug": "Wang-Ling",
                        "structuredName": {
                            "firstName": "Wang",
                            "lastName": "Ling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wang Ling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144633696"
                        ],
                        "name": "Austin Matthews",
                        "slug": "Austin-Matthews",
                        "structuredName": {
                            "firstName": "Austin",
                            "lastName": "Matthews",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Austin Matthews"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144365875"
                        ],
                        "name": "Noah A. Smith",
                        "slug": "Noah-A.-Smith",
                        "structuredName": {
                            "firstName": "Noah",
                            "lastName": "Smith",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noah A. Smith"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 60
                            }
                        ],
                        "text": "Finally, we refer interested readers to the original paper (Dyer et al., 2015) for details about the StackLSTM model since in this paper we merely use the same architecture through a new transition-based algorithm presented in the following Section."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 10
                            }
                        ],
                        "text": "Following Dyer et al. (2015), we use stack LSTMs to compute a fixed dimensional embedding of each of these, and take a concatenation of these to obtain the full algorithm state."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 115
                            }
                        ],
                        "text": "To obtain representations of this stack used for predicting subsequent actions, we use the Stack-LSTM presented by Dyer et al. (2015), in which the LSTM is augmented with a \u201cstack pointer.\u201d"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6278207,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "b36b7f7c68923d14ba2859b5d28a1124616a8c89",
            "isKey": false,
            "numCitedBy": 708,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "This work was sponsored in part by the U. S. Army Research Laboratory and the U. S. Army Research Office/nunder contract/grant number W911NF-10-1-0533, and in part by NSF CAREER grant IIS-1054319./nMiguel Ballesteros is supported by the European Commission under the contract numbers FP7-ICT-610411 (project MULTISENSOR) and H2020-RIA-645012 (project KRISTINA)."
            },
            "slug": "Transition-Based-Dependency-Parsing-with-Stack-Long-Dyer-Ballesteros",
            "title": {
                "fragments": [],
                "text": "Transition-Based Dependency Parsing with Stack Long Short-Term Memory"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "This work was sponsored in part by the U. S. Army Research Laboratory and the NSF CAREER grant IIS-1054319 and the European Commission."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2378954"
                        ],
                        "name": "Xuezhe Ma",
                        "slug": "Xuezhe-Ma",
                        "structuredName": {
                            "firstName": "Xuezhe",
                            "lastName": "Ma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xuezhe Ma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144547315"
                        ],
                        "name": "E. Hovy",
                        "slug": "E.-Hovy",
                        "structuredName": {
                            "firstName": "Eduard",
                            "lastName": "Hovy",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Hovy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10489017,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8dd6aae51e31a72752c4be5cddbdd76dfdc6cda4",
            "isKey": false,
            "numCitedBy": 1994,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "State-of-the-art sequence labeling systems traditionally require large amounts of task-specific knowledge in the form of hand-crafted features and data pre-processing. In this paper, we introduce a novel neutral network architecture that benefits from both word- and character-level representations automatically, by using combination of bidirectional LSTM, CNN and CRF. Our system is truly end-to-end, requiring no feature engineering or data pre-processing, thus making it applicable to a wide range of sequence labeling tasks. We evaluate our system on two data sets for two sequence labeling tasks --- Penn Treebank WSJ corpus for part-of-speech (POS) tagging and CoNLL 2003 corpus for named entity recognition (NER). We obtain state-of-the-art performance on both the two data --- 97.55\\% accuracy for POS tagging and 91.21\\% F1 for NER."
            },
            "slug": "End-to-end-Sequence-Labeling-via-Bi-directional-Ma-Hovy",
            "title": {
                "fragments": [],
                "text": "End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A novel neutral network architecture is introduced that benefits from both word- and character-level representations automatically, by using combination of bidirectional LSTM, CNN and CRF, thus making it applicable to a wide range of sequence labeling tasks."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143668305"
                        ],
                        "name": "Miguel Ballesteros",
                        "slug": "Miguel-Ballesteros",
                        "structuredName": {
                            "firstName": "Miguel",
                            "lastName": "Ballesteros",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Miguel Ballesteros"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2089067"
                        ],
                        "name": "Yoav Goldberg",
                        "slug": "Yoav-Goldberg",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Goldberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoav Goldberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745899"
                        ],
                        "name": "Chris Dyer",
                        "slug": "Chris-Dyer",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Dyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris Dyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144365875"
                        ],
                        "name": "Noah A. Smith",
                        "slug": "Noah-A.-Smith",
                        "structuredName": {
                            "firstName": "Noah",
                            "lastName": "Smith",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noah A. Smith"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 290,
                                "start": 266
                            }
                        ],
                        "text": "\u2026the scores using the best dropout rate for each language.3 It is a greedy model that apply locally optimal actions until the entire sentence is processed, further improvements might be obtained with beam search (Zhang and Clark, 2011) or training with exploration (Ballesteros et al., 2016)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 233,
                                "start": 207
                            }
                        ],
                        "text": "3 It is a greedy model that apply locally optimal actions until the entire sentence is processed, further improvements might be obtained with beam search (Zhang and Clark, 2011) or training with exploration (Ballesteros et al., 2016)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9716222,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6789e0dbd294cccb3b7dd4e001c9e8ba4813f334",
            "isKey": false,
            "numCitedBy": 74,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "We adapt the greedy Stack-LSTM dependency parser of Dyer et al. (2015) to support a training-with-exploration procedure using dynamic oracles(Goldberg and Nivre, 2013) instead of cross-entropy minimization. This form of training, which accounts for model predictions at training time rather than assuming an error-free action history, improves parsing accuracies for both English and Chinese, obtaining very strong results for both languages. We discuss some modifications needed in order to get training with exploration to work well for a probabilistic neural-network."
            },
            "slug": "Training-with-Exploration-Improves-a-Greedy-Stack-Ballesteros-Goldberg",
            "title": {
                "fragments": [],
                "text": "Training with Exploration Improves a Greedy Stack LSTM Parser"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "This work adapts the greedy Stack-LSTM dependency parser to support a training-with-exploration procedure using dynamic oracles instead of cross-entropy minimization, which improves parsing accuracies for both English and Chinese."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2047446108"
                        ],
                        "name": "Tomas Mikolov",
                        "slug": "Tomas-Mikolov",
                        "structuredName": {
                            "firstName": "Tomas",
                            "lastName": "Mikolov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tomas Mikolov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701686"
                        ],
                        "name": "Ilya Sutskever",
                        "slug": "Ilya-Sutskever",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Sutskever",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilya Sutskever"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118440152"
                        ],
                        "name": "Kai Chen",
                        "slug": "Kai-Chen",
                        "structuredName": {
                            "firstName": "Kai",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32131713"
                        ],
                        "name": "G. Corrado",
                        "slug": "G.-Corrado",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Corrado",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Corrado"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49959210"
                        ],
                        "name": "J. Dean",
                        "slug": "J.-Dean",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Dean",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Dean"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 91
                            }
                        ],
                        "text": "Embeddings are pretrained using skip-n-gram (Ling et al., 2015a), a variation of word2vec (Mikolov et al., 2013a) that accounts for word order."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 237,
                                "start": 216
                            }
                        ],
                        "text": "To capture orthographic sensitivity, we use character-based word representation model (Ling et al., 2015b) to capture distributional sensitivity, we combine these representations with distributional representations (Mikolov et al., 2013b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 117
                            }
                        ],
                        "text": ", 2015b) to capture distributional sensitivity, we combine these representations with distributional representations (Mikolov et al., 2013b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16447573,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "87f40e6f3022adbc1f1905e3e506abad05a9964f",
            "isKey": false,
            "numCitedBy": 26053,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. \n \nAn inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible."
            },
            "slug": "Distributed-Representations-of-Words-and-Phrases-Mikolov-Sutskever",
            "title": {
                "fragments": [],
                "text": "Distributed Representations of Words and Phrases and their Compositionality"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper presents a simple method for finding phrases in text, and shows that learning good vector representations for millions of phrases is possible and describes a simple alternative to the hierarchical softmax called negative sampling."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1379953252"
                        ],
                        "name": "Wang Ling",
                        "slug": "Wang-Ling",
                        "structuredName": {
                            "firstName": "Wang",
                            "lastName": "Ling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wang Ling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145317727"
                        ],
                        "name": "Yulia Tsvetkov",
                        "slug": "Yulia-Tsvetkov",
                        "structuredName": {
                            "firstName": "Yulia",
                            "lastName": "Tsvetkov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yulia Tsvetkov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1840645"
                        ],
                        "name": "Silvio Amir",
                        "slug": "Silvio-Amir",
                        "structuredName": {
                            "firstName": "Silvio",
                            "lastName": "Amir",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Silvio Amir"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3394760"
                        ],
                        "name": "Ram\u00f3n Fern\u00e1ndez Astudillo",
                        "slug": "Ram\u00f3n-Fern\u00e1ndez-Astudillo",
                        "structuredName": {
                            "firstName": "Ram\u00f3n",
                            "lastName": "Astudillo",
                            "middleNames": [
                                "Fern\u00e1ndez"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ram\u00f3n Fern\u00e1ndez Astudillo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745899"
                        ],
                        "name": "Chris Dyer",
                        "slug": "Chris-Dyer",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Dyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris Dyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690706"
                        ],
                        "name": "A. Black",
                        "slug": "A.-Black",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Black",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Black"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691021"
                        ],
                        "name": "I. Trancoso",
                        "slug": "I.-Trancoso",
                        "structuredName": {
                            "firstName": "Isabel",
                            "lastName": "Trancoso",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Trancoso"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7528635"
                        ],
                        "name": "Chu-Cheng Lin",
                        "slug": "Chu-Cheng-Lin",
                        "structuredName": {
                            "firstName": "Chu-Cheng",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chu-Cheng Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 167
                            }
                        ],
                        "text": "They have been found useful for morphologically rich languages and to handle the outof-vocabulary problem for tasks like part-of-speech tagging and language modeling (Ling et al., 2015b) or dependency parsing (Ballesteros et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 44
                            }
                        ],
                        "text": "Embeddings are pretrained using skip-n-gram (Ling et al., 2015a), a variation of word2vec (Mikolov et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 45
                            }
                        ],
                        "text": "Embeddings are pretrained using skip-n-gram (Ling et al., 2015a), a variation of word2vec (Mikolov et al., 2013a) that accounts for word order."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 218,
                                "start": 199
                            }
                        ],
                        "text": "\u2026decision for each token (i.e., the Pi,y\u2019s) are defined to be the dot product between the embedding of a wordin-context computed with a bidirectional LSTM\u2014 exactly the same as the POS tagging model of Ling et al. (2015b) and these are combined with bigram compatibility scores (i.e., the Ay,y\u2032\u2019s)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 191,
                                "start": 171
                            }
                        ],
                        "text": ", the Pi,y\u2019s) are defined to be the dot product between the embedding of a wordin-context computed with a bidirectional LSTM\u2014 exactly the same as the POS tagging model of Ling et al. (2015b) and these are combined with bigram compatibility scores (i."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 87
                            }
                        ],
                        "text": "To capture orthographic sensitivity, we use character-based word representation model (Ling et al., 2015b) to capture distributional sensitivity, we combine these representations with distributional representations (Mikolov et al., 2013b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 141
                            }
                        ],
                        "text": "A very simple\u2014but surprisingly effective\u2014tagging model is to use the ht\u2019s as features to make independent tagging decisions for each output yt (Ling et al., 2015b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 43
                            }
                        ],
                        "text": "The best score reported on this task is by Luo et al. (2015). They obtained a F1 of 91."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1144632,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1347bd4f826f72ff561b70e665477edadb2a72be",
            "isKey": false,
            "numCitedBy": 125,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce an extension to the bag-ofwords model for learning words representations that take into account both syntactic and semantic properties within language. This is done by employing an attention model that finds within the contextual words, the words that are relevant for each prediction. The general intuition of our model is that some words are only relevant for predicting local context (e.g. function words), while other words are more suited for determining global context, such as the topic of the document. Experiments performed on both semantically and syntactically oriented tasks show gains using our model over the existing bag of words model. Furthermore, compared to other more sophisticated models, our model scales better as we increase the size of the context of the model."
            },
            "slug": "Not-All-Contexts-Are-Created-Equal:-Better-Word-Ling-Tsvetkov",
            "title": {
                "fragments": [],
                "text": "Not All Contexts Are Created Equal: Better Word Representations with Variable Attention"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "An extension to the bag-ofwords model for learning words representations that take into account both syntactic and semantic properties within language is introduced by employing an attention model that finds within the contextual words, the words that are relevant for each prediction."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2396669"
                        ],
                        "name": "D. Gillick",
                        "slug": "D.-Gillick",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Gillick",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Gillick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3202382"
                        ],
                        "name": "Clifford Brunk",
                        "slug": "Clifford-Brunk",
                        "structuredName": {
                            "firstName": "Clifford",
                            "lastName": "Brunk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Clifford Brunk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689108"
                        ],
                        "name": "Oriol Vinyals",
                        "slug": "Oriol-Vinyals",
                        "structuredName": {
                            "firstName": "Oriol",
                            "lastName": "Vinyals",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oriol Vinyals"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726492"
                        ],
                        "name": "A. Subramanya",
                        "slug": "A.-Subramanya",
                        "structuredName": {
                            "firstName": "Amarnag",
                            "lastName": "Subramanya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Subramanya"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 0
                            }
                        ],
                        "text": "Gillick et al. (2015) model the task of sequencelabeling as a sequence to sequence learning problem and incorporate character-based representations into their encoder model."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 48
                            }
                        ],
                        "text": "The only exception is Dutch, where the model of Gillick et al. (2015) can perform better by leveraging the information from other NER datasets."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 384520,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4dabd6182ce2681c758f654561d351739e8df7bf",
            "isKey": false,
            "numCitedBy": 189,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe an LSTM-based model which we call Byte-to-Span (BTS) that reads text as bytes and outputs span annotations of the form [start, length, label] where start positions, lengths, and labels are separate entries in our vocabulary. Because we operate directly on unicode bytes rather than language-specific words or characters, we can analyze text in many languages with a single model. Due to the small vocabulary size, these multilingual models are very compact, but produce results similar to or better than the state-of- the-art in Part-of-Speech tagging and Named Entity Recognition that use only the provided training datasets (no external data sources). Our models are learning \"from scratch\" in that they do not rely on any elements of the standard pipeline in Natural Language Processing (including tokenization), and thus can run in standalone fashion on raw text."
            },
            "slug": "Multilingual-Language-Processing-From-Bytes-Gillick-Brunk",
            "title": {
                "fragments": [],
                "text": "Multilingual Language Processing From Bytes"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "An LSTM-based model that reads text as bytes and outputs span annotations of the form [start, length, label] where start positions, lengths, and labels are separate entries in the authors' vocabulary is described."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1379953252"
                        ],
                        "name": "Wang Ling",
                        "slug": "Wang-Ling",
                        "structuredName": {
                            "firstName": "Wang",
                            "lastName": "Ling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wang Ling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745899"
                        ],
                        "name": "Chris Dyer",
                        "slug": "Chris-Dyer",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Dyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris Dyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690706"
                        ],
                        "name": "A. Black",
                        "slug": "A.-Black",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Black",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Black"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691021"
                        ],
                        "name": "I. Trancoso",
                        "slug": "I.-Trancoso",
                        "structuredName": {
                            "firstName": "Isabel",
                            "lastName": "Trancoso",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Trancoso"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3394760"
                        ],
                        "name": "Ram\u00f3n Fern\u00e1ndez Astudillo",
                        "slug": "Ram\u00f3n-Fern\u00e1ndez-Astudillo",
                        "structuredName": {
                            "firstName": "Ram\u00f3n",
                            "lastName": "Astudillo",
                            "middleNames": [
                                "Fern\u00e1ndez"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ram\u00f3n Fern\u00e1ndez Astudillo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1840645"
                        ],
                        "name": "Silvio Amir",
                        "slug": "Silvio-Amir",
                        "structuredName": {
                            "firstName": "Silvio",
                            "lastName": "Amir",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Silvio Amir"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728026"
                        ],
                        "name": "Lu\u00eds Marujo",
                        "slug": "Lu\u00eds-Marujo",
                        "structuredName": {
                            "firstName": "Lu\u00eds",
                            "lastName": "Marujo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lu\u00eds Marujo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2979183"
                        ],
                        "name": "T. Lu\u00eds",
                        "slug": "T.-Lu\u00eds",
                        "structuredName": {
                            "firstName": "Tiago",
                            "lastName": "Lu\u00eds",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Lu\u00eds"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 167
                            }
                        ],
                        "text": "They have been found useful for morphologically rich languages and to handle the outof-vocabulary problem for tasks like part-of-speech tagging and language modeling (Ling et al., 2015b) or dependency parsing (Ballesteros et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 45
                            }
                        ],
                        "text": "Embeddings are pretrained using skip-n-gram (Ling et al., 2015a), a variation of word2vec (Mikolov et al., 2013a) that accounts for word order."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 218,
                                "start": 199
                            }
                        ],
                        "text": "\u2026decision for each token (i.e., the Pi,y\u2019s) are defined to be the dot product between the embedding of a wordin-context computed with a bidirectional LSTM\u2014 exactly the same as the POS tagging model of Ling et al. (2015b) and these are combined with bigram compatibility scores (i.e., the Ay,y\u2032\u2019s)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 87
                            }
                        ],
                        "text": "To capture orthographic sensitivity, we use character-based word representation model (Ling et al., 2015b) to capture distributional sensitivity, we combine these representations with distributional representations (Mikolov et al., 2013b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 141
                            }
                        ],
                        "text": "A very simple\u2014but surprisingly effective\u2014tagging model is to use the ht\u2019s as features to make independent tagging decisions for each output yt (Ling et al., 2015b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1689426,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6dab1c6491929d396e9e5463bc2e87af88602aa2",
            "isKey": false,
            "numCitedBy": 580,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a model for constructing vector representations of words by composing characters using bidirectional LSTMs. Relative to traditional word representation models that have independent vectors for each word type, our model requires only a single vector per character type and a fixed set of parameters for the compositional model. Despite the compactness of this model and, more importantly, the arbitrary nature of the form\u2010function relationship in language, our \u201ccomposed\u201d word representations yield state-of-the-art results in language modeling and part-of-speech tagging. Benefits over traditional baselines are particularly pronounced in morphologically rich languages (e.g., Turkish)."
            },
            "slug": "Finding-Function-in-Form:-Compositional-Character-Ling-Dyer",
            "title": {
                "fragments": [],
                "text": "Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A model for constructing vector representations of words by composing characters using bidirectional LSTMs that requires only a single vector per character type and a fixed set of parameters for the compositional model, which yields state- of-the-art results in language modeling and part-of-speech tagging."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763608"
                        ],
                        "name": "Chris Callison-Burch",
                        "slug": "Chris-Callison-Burch",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Callison-Burch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris Callison-Burch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1755162"
                        ],
                        "name": "Philipp Koehn",
                        "slug": "Philipp-Koehn",
                        "structuredName": {
                            "firstName": "Philipp",
                            "lastName": "Koehn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philipp Koehn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696402"
                        ],
                        "name": "Christof Monz",
                        "slug": "Christof-Monz",
                        "structuredName": {
                            "firstName": "Christof",
                            "lastName": "Monz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christof Monz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31276899"
                        ],
                        "name": "Kay Peterson",
                        "slug": "Kay-Peterson",
                        "structuredName": {
                            "firstName": "Kay",
                            "lastName": "Peterson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kay Peterson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2282719"
                        ],
                        "name": "Mark A. Przybocki",
                        "slug": "Mark-A.-Przybocki",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Przybocki",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark A. Przybocki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1936277"
                        ],
                        "name": "Omar Zaidan",
                        "slug": "Omar-Zaidan",
                        "structuredName": {
                            "firstName": "Omar",
                            "lastName": "Zaidan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Omar Zaidan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 222,
                                "start": 195
                            }
                        ],
                        "text": "\u2026networks using the back-propagation algorithm updating our parameters on every training example, one at a time, using stochastic gradient descent (SGD) with\n2(Graff, 2011; Biemann et al., 2007; Callison-Burch et al., 2010; Parker et al., 2009)\na learning rate of 0.01 and a gradient clipping of 5.0."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 44699483,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "da6918ed87095d1313bd20606a934f899d4084b0",
            "isKey": true,
            "numCitedBy": 229,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents the results of the WMT10 and MetricsMATR10 shared tasks, which included a translation task, a system combination task, and an evaluation task. We conducted a large-scale manual evaluation of 104 machine translation systems and 41 system combination entries. We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality for 26 metrics. This year we also investigated increasing the number of human judgments by hiring non-expert annotators through Amazon's Mechanical Turk."
            },
            "slug": "Findings-of-the-2010-Joint-Workshop-on-Statistical-Callison-Burch-Koehn",
            "title": {
                "fragments": [],
                "text": "Findings of the 2010 Joint Workshop on Statistical Machine Translation and Metrics for Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A large-scale manual evaluation of 104 machine translation systems and 41 system combination entries was conducted, which used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality for 26 metrics."
            },
            "venue": {
                "fragments": [],
                "text": "WMT@ACL"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39939186"
                        ],
                        "name": "Yue Zhang",
                        "slug": "Yue-Zhang",
                        "structuredName": {
                            "firstName": "Yue",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yue Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144523372"
                        ],
                        "name": "S. Clark",
                        "slug": "S.-Clark",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Clark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Clark"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 154
                            }
                        ],
                        "text": "3 It is a greedy model that apply locally optimal actions until the entire sentence is processed, further improvements might be obtained with beam search (Zhang and Clark, 2011) or training with exploration (Ballesteros et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7245369,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f24fb33e739b0181fc171b515f957190e3bb6430",
            "isKey": false,
            "numCitedBy": 219,
            "numCiting": 67,
            "paperAbstract": {
                "fragments": [],
                "text": "We study a range of syntactic processing tasks using a general statistical framework that consists of a global linear model, trained by the generalized perceptron together with a generic beam-search decoder. We apply the framework to word segmentation, joint segmentation and POS-tagging, dependency parsing, and phrase-structure parsing. Both components of the framework are conceptually and computationally very simple. The beam-search decoder only requires the syntactic processing task to be broken into a sequence of decisions, such that, at each stage in the process, the decoder is able to consider the top-n candidates and generate all possibilities for the next stage. Once the decoder has been defined, it is applied to the training data, using trivial updates according to the generalized perceptron to induce a model. This simple framework performs surprisingly well, giving accuracy results competitive with the state-of-the-art on all the tasks we consider. The computational simplicity of the decoder and training algorithm leads to significantly higher test speeds and lower training times than their main alternatives, including log-linear and large-margin training algorithms and dynamic-programming for decoding. Moreover, the framework offers the freedom to define arbitrary features which can make alternative training and decoding algorithms prohibitively slow. We discuss how the general framework is applied to each of the problems studied in this article, making comparisons with alternative learning and decoding algorithms. We also show how the comparability of candidates considered by the beam is an important factor in the performance. We argue that the conceptual and computational simplicity of the framework, together with its language-independent nature, make it a competitive choice for a range of syntactic processing tasks and one that should be considered for comparison by developers of alternative approaches."
            },
            "slug": "Syntactic-Processing-Using-the-Generalized-and-Beam-Zhang-Clark",
            "title": {
                "fragments": [],
                "text": "Syntactic Processing Using the Generalized Perceptron and Beam Search"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is argued that the conceptual and computational simplicity of the framework, together with its language-independent nature, make it a competitive choice for a range of syntactic processing tasks and one that should be considered for comparison by developers of alternative approaches."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2812486"
                        ],
                        "name": "P. Simard",
                        "slug": "P.-Simard",
                        "structuredName": {
                            "firstName": "Patrice",
                            "lastName": "Simard",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Simard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688235"
                        ],
                        "name": "P. Frasconi",
                        "slug": "P.-Frasconi",
                        "structuredName": {
                            "firstName": "Paolo",
                            "lastName": "Frasconi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Frasconi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 158
                            }
                        ],
                        "text": "Although RNNs can, in theory, learn long dependencies, in practice they fail to do so and tend to be biased towards their most recent inputs in the sequence (Bengio et al., 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 206457500,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d0be39ee052d246ae99c082a565aba25b811be2d",
            "isKey": false,
            "numCitedBy": 6144,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "Recurrent neural networks can be used to map input sequences to output sequences, such as for recognition, production or prediction problems. However, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases. These results expose a trade-off between efficient learning by gradient descent and latching on information for long periods. Based on an understanding of this problem, alternatives to standard gradient descent are considered."
            },
            "slug": "Learning-long-term-dependencies-with-gradient-is-Bengio-Simard",
            "title": {
                "fragments": [],
                "text": "Learning long-term dependencies with gradient descent is difficult"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work shows why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases, and exposes a trade-off between efficient learning by gradient descent and latching on information for long periods."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38070424"
                        ],
                        "name": "R. Ando",
                        "slug": "R.-Ando",
                        "structuredName": {
                            "firstName": "Rie",
                            "lastName": "Ando",
                            "middleNames": [
                                "Kubota"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Ando"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117881943"
                        ],
                        "name": "Tong Zhang",
                        "slug": "Tong-Zhang",
                        "structuredName": {
                            "firstName": "Tong",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tong Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13650160,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "944e1a7b2c5c62e952418d7684e3cade89c76f87",
            "isKey": false,
            "numCitedBy": 1414,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the most important issues in machine learning is whether one can improve the performance of a supervised learning algorithm by including unlabeled data. Methods that use both labeled and unlabeled data are generally referred to as semi-supervised learning. Although a number of such methods are proposed, at the current stage, we still don't have a complete understanding of their effectiveness. This paper investigates a closely related problem, which leads to a novel approach to semi-supervised learning. Specifically we consider learning predictive structures on hypothesis spaces (that is, what kind of classifiers have good predictive power) from multiple learning tasks. We present a general framework in which the structural learning problem can be formulated and analyzed theoretically, and relate it to learning with unlabeled data. Under this framework, algorithms for structural learning will be proposed, and computational issues will be investigated. Experiments will be given to demonstrate the effectiveness of the proposed algorithms in the semi-supervised learning setting."
            },
            "slug": "A-Framework-for-Learning-Predictive-Structures-from-Ando-Zhang",
            "title": {
                "fragments": [],
                "text": "A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper presents a general framework in which the structural learning problem can be formulated and analyzed theoretically, and relate it to learning with unlabeled data, and algorithms for structural learning will be proposed, and computational issues will be investigated."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2897313"
                        ],
                        "name": "Nitish Srivastava",
                        "slug": "Nitish-Srivastava",
                        "structuredName": {
                            "firstName": "Nitish",
                            "lastName": "Srivastava",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nitish Srivastava"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064160"
                        ],
                        "name": "A. Krizhevsky",
                        "slug": "A.-Krizhevsky",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Krizhevsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Krizhevsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701686"
                        ],
                        "name": "Ilya Sutskever",
                        "slug": "Ilya-Sutskever",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Sutskever",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilya Sutskever"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 83
                            }
                        ],
                        "text": "To encourage the model to depend on both representations, we use dropout training (Hinton et al., 2012), applying a dropout mask to the final embedding layer just before the input to the bidirectional LSTM in Figure 1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14832074,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1366de5bb112746a555e9c0cd00de3ad8628aea8",
            "isKey": false,
            "numCitedBy": 6191,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This \"overfitting\" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random \"dropout\" gives big improvements on many benchmark tasks and sets new records for speech and object recognition."
            },
            "slug": "Improving-neural-networks-by-preventing-of-feature-Hinton-Srivastava",
            "title": {
                "fragments": [],
                "text": "Improving neural networks by preventing co-adaptation of feature detectors"
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48799969"
                        ],
                        "name": "Matthew D. Zeiler",
                        "slug": "Matthew-D.-Zeiler",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Zeiler",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew D. Zeiler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 87
                            }
                        ],
                        "text": "Several methods have been proposed to enhance the performance of SGD, such as Adadelta (Zeiler, 2012) or Adam (Kingma and Ba, 2014)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7365802,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8729441d734782c3ed532a7d2d9611b438c0a09a",
            "isKey": false,
            "numCitedBy": 5464,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment."
            },
            "slug": "ADADELTA:-An-Adaptive-Learning-Rate-Method-Zeiler",
            "title": {
                "fragments": [],
                "text": "ADADELTA: An Adaptive Learning Rate Method"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A novel per-dimension learning rate method for gradient descent called ADADELTA that dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent is presented."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753223"
                        ],
                        "name": "A. Graves",
                        "slug": "A.-Graves",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Graves",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Graves"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055652031"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "Juergen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 75
                            }
                        ],
                        "text": "This forward and backward LSTM pair is referred to as a bidirectional LSTM (Graves and Schmidhuber, 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9594328,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "685d42a668413422615519a52ac75d66fded4611",
            "isKey": false,
            "numCitedBy": 378,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we apply bidirectional training to a long short term memory (LSTM) network for the first time. We also present a modified, full gradient version of the LSTM learning algorithm. We discuss the significance of framewise phoneme classification to continuous speech recognition, and the validity of using bidirectional networks for online causal tasks. On the TIMIT speech database, we measure the framewise phoneme classification scores of bidirectional and unidirectional variants of both LSTM and conventional recurrent neural networks (RNNs). We find that bidirectional LSTM outperforms both RNNs and unidirectional LSTM."
            },
            "slug": "Framewise-phoneme-classification-with-bidirectional-Graves-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Framewise phoneme classification with bidirectional LSTM networks"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "It is found that bidirectional LSTM outperforms both RNNs and unidirectionalLSTM, and the significance of framewise phoneme classification to continuous speech recognition and the validity of usingbidirectional networks for online causal tasks is discussed."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720988"
                        ],
                        "name": "Joakim Nivre",
                        "slug": "Joakim-Nivre",
                        "structuredName": {
                            "firstName": "Joakim",
                            "lastName": "Nivre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joakim Nivre"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 149
                            }
                        ],
                        "text": "We designed a transition inventory which is given in Figure 2 that is inspired by transition-based parsers, in particular the arc-standard parser of Nivre (2004). In this algorithm, we make use of two stacks (designated output and stack representing, respectively, completed chunks and scratch space) and a buffer that contains the words that have yet to be processed."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14358598,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "032906f0d86fd6bbf7512d3fffd06fcb83593012",
            "isKey": false,
            "numCitedBy": 299,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Deterministic dependency parsing is a robust and efficient approach to syntactic parsing of unrestricted natural language text. In this paper, we analyze its potential for incremental processing and conclude that strict incrementality is not achievable within this framework. However, we also show that it is possible to minimize the number of structures that require non-incremental processing by choosing an optimal parsing algorithm. This claim is substantiated with experimental evidence showing that the algorithm achieves incremental parsing for 68.9% of the input when tested on a random sample of Swedish text. When restricted to sentences that are accepted by the parser, the degree of incrementality increases to 87.9%."
            },
            "slug": "Incrementality-in-Deterministic-Dependency-Parsing-Nivre",
            "title": {
                "fragments": [],
                "text": "Incrementality in Deterministic Dependency Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is concluded that strict incrementality is not achievable within this framework and it is shown that it is possible to minimize the number of structures that require non-incremental processing by choosing an optimal parsing algorithm."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3109481"
                        ],
                        "name": "Zhiheng Huang",
                        "slug": "Zhiheng-Huang",
                        "structuredName": {
                            "firstName": "Zhiheng",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhiheng Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145738410"
                        ],
                        "name": "W. Xu",
                        "slug": "W.-Xu",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2150332252"
                        ],
                        "name": "Kai Yu",
                        "slug": "Kai-Yu",
                        "structuredName": {
                            "firstName": "Kai",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai Yu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 15
                            }
                        ],
                        "text": "More recently, Huang et al. (2015) presented a model similar to our LSTM-CRF, but using hand-crafted spelling features."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 82
                            }
                        ],
                        "text": "This architecture is similar to the ones presented by Collobert et al. (2011) and Huang et al. (2015)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 12740621,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "af88ce6116c2cd2927a4198745e99e5465173783",
            "isKey": false,
            "numCitedBy": 2424,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose a variety of Long Short-Term Memory (LSTM) based models for sequence tagging. These models include LSTM networks, bidirectional LSTM (BI-LSTM) networks, LSTM with a Conditional Random Field (CRF) layer (LSTM-CRF) and bidirectional LSTM with a CRF layer (BI-LSTM-CRF). Our work is the first to apply a bidirectional LSTM CRF (denoted as BI-LSTM-CRF) model to NLP benchmark sequence tagging data sets. We show that the BI-LSTM-CRF model can efficiently use both past and future input features thanks to a bidirectional LSTM component. It can also use sentence level tag information thanks to a CRF layer. The BI-LSTM-CRF model can produce state of the art (or close to) accuracy on POS, chunking and NER data sets. In addition, it is robust and has less dependence on word embedding as compared to previous observations."
            },
            "slug": "Bidirectional-LSTM-CRF-Models-for-Sequence-Tagging-Huang-Xu",
            "title": {
                "fragments": [],
                "text": "Bidirectional LSTM-CRF Models for Sequence Tagging"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work is the first to apply a bidirectional LSTM CRF model to NLP benchmark sequence tagging data sets and it is shown that the BI-LSTM-CRF model can efficiently use both past and future input features thanks to a biddirectional L STM component."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691366"
                        ],
                        "name": "Hong-Jie Dai",
                        "slug": "Hong-Jie-Dai",
                        "structuredName": {
                            "firstName": "Hong-Jie",
                            "lastName": "Dai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hong-Jie Dai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50436451"
                        ],
                        "name": "Po-Ting Lai",
                        "slug": "Po-Ting-Lai",
                        "structuredName": {
                            "firstName": "Po-Ting",
                            "lastName": "Lai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Po-Ting Lai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706956"
                        ],
                        "name": "Yung-Chun Chang",
                        "slug": "Yung-Chun-Chang",
                        "structuredName": {
                            "firstName": "Yung-Chun",
                            "lastName": "Chang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yung-Chun Chang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724351"
                        ],
                        "name": "Richard Tzong-Han Tsai",
                        "slug": "Richard-Tzong-Han-Tsai",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Tsai",
                            "middleNames": [
                                "Tzong-Han"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Richard Tzong-Han Tsai"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 28
                            }
                        ],
                        "text": "Ratinov and Roth (2009) and Dai et al. (2015) showed that using a more expressive tagging scheme like IOBES improves model performance marginally."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 218912,
            "fieldsOfStudy": [
                "Chemistry"
            ],
            "id": "db194322e7a1a0c4a8f8d1e73177f6cc436cadac",
            "isKey": false,
            "numCitedBy": 65,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "BackgroundThe functions of chemical compounds and drugs that affect biological processes and their particular effect on the onset and treatment of diseases have attracted increasing interest with the advancement of research in the life sciences. To extract knowledge from the extensive literatures on such compounds and drugs, the organizers of BioCreative IV administered the CHEMical Compound and Drug Named Entity Recognition (CHEMDNER) task to establish a standard dataset for evaluating state-of-the-art chemical entity recognition methods.MethodsThis study introduces the approach of our CHEMDNER system. Instead of emphasizing the development of novel feature sets for machine learning, this study investigates the effect of various tag schemes on the recognition of the names of chemicals and drugs by using conditional random fields. Experiments were conducted using combinations of different tokenization strategies and tag schemes to investigate the effects of tag set selection and tokenization method on the CHEMDNER task.ResultsThis study presents the performance of CHEMDNER of three more representative tag schemes-IOBE, IOBES, and IOB12E-when applied to a widely utilized IOB tag set and combined with the coarse-/fine-grained tokenization methods. The experimental results thus reveal that the fine-grained tokenization strategy performance best in terms of precision, recall and F-scores when the IOBES tag set was utilized. The IOBES model with fine-grained tokenization yielded the best-F-scores in the six chemical entity categories other than the \"Multiple\" entity category. Nonetheless, no significant improvement was observed when a more representative tag schemes was used with the coarse or fine-grained tokenization rules. The best F-scores that were achieved using the developed system on the test dataset of the CHEMDNER task were 0.833 and 0.815 for the chemical documents indexing and the chemical entity mention recognition tasks, respectively.ConclusionsThe results herein highlight the importance of tag set selection and the use of different tokenization strategies. Fine-grained tokenization combined with the tag set IOBES most effectively recognizes chemical and drug names. To the best of the authors' knowledge, this investigation is the first comprehensive investigation use of various tag set schemes combined with different tokenization strategies for the recognition of chemical entities."
            },
            "slug": "Enhancing-of-chemical-compound-and-drug-name-using-Dai-Lai",
            "title": {
                "fragments": [],
                "text": "Enhancing of chemical compound and drug name recognition using representative tag scheme and fine-grained tokenization"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This investigation is the first comprehensive investigation use of various tag set schemes combined with different tokenization strategies for the recognition of chemical entities and reveals that fine-grained tokenization strategy performance best in terms of precision, recall and F-scores when the IOBES tag set was utilized."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Cheminformatics"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726807"
                        ],
                        "name": "Diederik P. Kingma",
                        "slug": "Diederik-P.-Kingma",
                        "structuredName": {
                            "firstName": "Diederik",
                            "lastName": "Kingma",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Diederik P. Kingma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2503659"
                        ],
                        "name": "Jimmy Ba",
                        "slug": "Jimmy-Ba",
                        "structuredName": {
                            "firstName": "Jimmy",
                            "lastName": "Ba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jimmy Ba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 110
                            }
                        ],
                        "text": "Several methods have been proposed to enhance the performance of SGD, such as Adadelta (Zeiler, 2012) or Adam (Kingma and Ba, 2014)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6628106,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
            "isKey": false,
            "numCitedBy": 90052,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm."
            },
            "slug": "Adam:-A-Method-for-Stochastic-Optimization-Kingma-Ba",
            "title": {
                "fragments": [],
                "text": "Adam: A Method for Stochastic Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "This work introduces Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments, and provides a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3308557"
                        ],
                        "name": "S. Hochreiter",
                        "slug": "S.-Hochreiter",
                        "structuredName": {
                            "firstName": "Sepp",
                            "lastName": "Hochreiter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Hochreiter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 187,
                                "start": 153
                            }
                        ],
                        "text": "They do so using several gates that control the proportion of the input to give to the memory cell, and the proportion from the previous state to forget (Hochreiter and Schmidhuber, 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1915014,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "44d2abe2175df8153f465f6c39b68b76a0d40ab9",
            "isKey": false,
            "numCitedBy": 51691,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms."
            },
            "slug": "Long-Short-Term-Memory-Hochreiter-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Long Short-Term Memory"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A novel, efficient, gradient based method called long short-term memory (LSTM) is introduced, which can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "113414328"
                        ],
                        "name": "Fernando Pereira",
                        "slug": "Fernando-Pereira",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pereira",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fernando Pereira"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 120
                            }
                        ],
                        "text": "Therefore, instead of modeling tagging decisions independently, we model them jointly using a conditional random field (Lafferty et al., 2001)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 219683473,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f4ba954b0412773d047dc41231c733de0c1f4926",
            "isKey": false,
            "numCitedBy": 13409,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "We present conditional random fields , a framework for building probabilistic models to segment and label sequence data. Conditional random fields offer several advantages over hidden Markov models and stochastic grammars for such tasks, including the ability to relax strong independence assumptions made in those models. Conditional random fields also avoid a fundamental limitation of maximum entropy Markov models (MEMMs) and other discriminative Markov models based on directed graphical models, which can be biased towards states with few successor states. We present iterative parameter estimation algorithms for conditional random fields and compare the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data."
            },
            "slug": "Conditional-Random-Fields:-Probabilistic-Models-for-Lafferty-McCallum",
            "title": {
                "fragments": [],
                "text": "Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work presents iterative parameter estimation algorithms for conditional random fields and compares the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 992,
                                "start": 31
                            }
                        ],
                        "text": "In the CoNLL-2002 shared task, Carreras et al. (2002) obtained among the best results on both Dutch and Spanish by combining several small fixed-depth decision trees. Next year, in the CoNLL2003 Shared Task, Florian et al. (2003) obtained the best score on German by combining the output of four diverse classifiers. Qi et al. (2009) later improved on this with a neural network by doing unsupervised learning on a massive unlabeled corpus. Several other neural architectures have previously been proposed for NER. For instance, Collobert et al. (2011) uses a CNN over a sequence of word embeddings with a CRF layer on top. This can be thought of as our first model without character-level embeddings and with the bidirectional LSTM being replaced by a CNN. More recently, Huang et al. (2015) presented a model similar to our LSTM-CRF, but using hand-crafted spelling features. Zhou and Xu (2015) also used a similar model and adapted it to the semantic role labeling task. Lin and Wu (2009) used a linear chain CRF with L2 regularization, they added phrase cluster features extracted from the web data and spelling features."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 31
                            }
                        ],
                        "text": "In the CoNLL-2002 shared task, Carreras et al. (2002) obtained among the best results on both Dutch and Spanish by combining several small fixed-depth decision trees."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 897,
                                "start": 31
                            }
                        ],
                        "text": "In the CoNLL-2002 shared task, Carreras et al. (2002) obtained among the best results on both Dutch and Spanish by combining several small fixed-depth decision trees. Next year, in the CoNLL2003 Shared Task, Florian et al. (2003) obtained the best score on German by combining the output of four diverse classifiers. Qi et al. (2009) later improved on this with a neural network by doing unsupervised learning on a massive unlabeled corpus. Several other neural architectures have previously been proposed for NER. For instance, Collobert et al. (2011) uses a CNN over a sequence of word embeddings with a CRF layer on top. This can be thought of as our first model without character-level embeddings and with the bidirectional LSTM being replaced by a CNN. More recently, Huang et al. (2015) presented a model similar to our LSTM-CRF, but using hand-crafted spelling features. Zhou and Xu (2015) also used a similar model and adapted it to the semantic role labeling task."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 230,
                                "start": 31
                            }
                        ],
                        "text": "In the CoNLL-2002 shared task, Carreras et al. (2002) obtained among the best results on both Dutch and Spanish by combining several small fixed-depth decision trees. Next year, in the CoNLL2003 Shared Task, Florian et al. (2003) obtained the best score on German by combining the output of four diverse classifiers."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 793,
                                "start": 31
                            }
                        ],
                        "text": "In the CoNLL-2002 shared task, Carreras et al. (2002) obtained among the best results on both Dutch and Spanish by combining several small fixed-depth decision trees. Next year, in the CoNLL2003 Shared Task, Florian et al. (2003) obtained the best score on German by combining the output of four diverse classifiers. Qi et al. (2009) later improved on this with a neural network by doing unsupervised learning on a massive unlabeled corpus. Several other neural architectures have previously been proposed for NER. For instance, Collobert et al. (2011) uses a CNN over a sequence of word embeddings with a CRF layer on top. This can be thought of as our first model without character-level embeddings and with the bidirectional LSTM being replaced by a CNN. More recently, Huang et al. (2015) presented a model similar to our LSTM-CRF, but using hand-crafted spelling features."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1147,
                                "start": 31
                            }
                        ],
                        "text": "In the CoNLL-2002 shared task, Carreras et al. (2002) obtained among the best results on both Dutch and Spanish by combining several small fixed-depth decision trees. Next year, in the CoNLL2003 Shared Task, Florian et al. (2003) obtained the best score on German by combining the output of four diverse classifiers. Qi et al. (2009) later improved on this with a neural network by doing unsupervised learning on a massive unlabeled corpus. Several other neural architectures have previously been proposed for NER. For instance, Collobert et al. (2011) uses a CNN over a sequence of word embeddings with a CRF layer on top. This can be thought of as our first model without character-level embeddings and with the bidirectional LSTM being replaced by a CNN. More recently, Huang et al. (2015) presented a model similar to our LSTM-CRF, but using hand-crafted spelling features. Zhou and Xu (2015) also used a similar model and adapted it to the semantic role labeling task. Lin and Wu (2009) used a linear chain CRF with L2 regularization, they added phrase cluster features extracted from the web data and spelling features. Passos et al. (2014) also used a linear chain CRF with spelling features and gazetteers."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 334,
                                "start": 31
                            }
                        ],
                        "text": "In the CoNLL-2002 shared task, Carreras et al. (2002) obtained among the best results on both Dutch and Spanish by combining several small fixed-depth decision trees. Next year, in the CoNLL2003 Shared Task, Florian et al. (2003) obtained the best score on German by combining the output of four diverse classifiers. Qi et al. (2009) later improved on this with a neural network by doing unsupervised learning on a massive unlabeled corpus."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 553,
                                "start": 31
                            }
                        ],
                        "text": "In the CoNLL-2002 shared task, Carreras et al. (2002) obtained among the best results on both Dutch and Spanish by combining several small fixed-depth decision trees. Next year, in the CoNLL2003 Shared Task, Florian et al. (2003) obtained the best score on German by combining the output of four diverse classifiers. Qi et al. (2009) later improved on this with a neural network by doing unsupervised learning on a massive unlabeled corpus. Several other neural architectures have previously been proposed for NER. For instance, Collobert et al. (2011) uses a CNN over a sequence of word embeddings with a CRF layer on top."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Named entity extraction using adaboost, proceedings of the 6th conference on natural language learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 160
                            }
                        ],
                        "text": "\u2026networks using the back-propagation algorithm updating our parameters on every training example, one at a time, using stochastic gradient descent (SGD) with\n2(Graff, 2011; Biemann et al., 2007; Callison-Burch et al., 2010; Parker et al., 2009)\na learning rate of 0.01 and a gradient clipping of 5.0."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Spanish gigaword third edition (ldc2011t12). Linguistic Data Consortium"
            },
            "venue": {
                "fragments": [],
                "text": "Spanish gigaword third edition (ldc2011t12). Linguistic Data Consortium"
            },
            "year": 2011
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 160
                            }
                        ],
                        "text": "\u2026networks using the back-propagation algorithm updating our parameters on every training example, one at a time, using stochastic gradient descent (SGD) with\n2(Graff, 2011; Biemann et al., 2007; Callison-Burch et al., 2010; Parker et al., 2009)\na learning rate of 0.01 and a gradient clipping of 5.0."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Spanish gigaword third edition (ldc2011t12). Linguistic Data Consortium, Univer-sity of Pennsylvania"
            },
            "venue": {
                "fragments": [],
                "text": "Spanish gigaword third edition (ldc2011t12). Linguistic Data Consortium, Univer-sity of Pennsylvania"
            },
            "year": 2011
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1051,
                                "start": 31
                            }
                        ],
                        "text": "In the CoNLL-2002 shared task, Carreras et al. (2002) obtained among the best results on both Dutch and Spanish by combining several small fixed-depth decision trees. Next year, in the CoNLL2003 Shared Task, Florian et al. (2003) obtained the best score on German by combining the output of four diverse classifiers. Qi et al. (2009) later improved on this with a neural network by doing unsupervised learning on a massive unlabeled corpus. Several other neural architectures have previously been proposed for NER. For instance, Collobert et al. (2011) uses a CNN over a sequence of word embeddings with a CRF layer on top. This can be thought of as our first model without character-level embeddings and with the bidirectional LSTM being replaced by a CNN. More recently, Huang et al. (2015) presented a model similar to our LSTM-CRF, but using hand-crafted spelling features. Lin and Wu (2009) used a linear chain CRF with L2 regularization, they added phrase cluster features extracted from the web data and spelling features. Passos et al. (2014) also used a linear chain CRF with spelling features and gazetteers."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 896,
                                "start": 31
                            }
                        ],
                        "text": "In the CoNLL-2002 shared task, Carreras et al. (2002) obtained among the best results on both Dutch and Spanish by combining several small fixed-depth decision trees. Next year, in the CoNLL2003 Shared Task, Florian et al. (2003) obtained the best score on German by combining the output of four diverse classifiers. Qi et al. (2009) later improved on this with a neural network by doing unsupervised learning on a massive unlabeled corpus. Several other neural architectures have previously been proposed for NER. For instance, Collobert et al. (2011) uses a CNN over a sequence of word embeddings with a CRF layer on top. This can be thought of as our first model without character-level embeddings and with the bidirectional LSTM being replaced by a CNN. More recently, Huang et al. (2015) presented a model similar to our LSTM-CRF, but using hand-crafted spelling features. Lin and Wu (2009) used a linear chain CRF with L2 regularization, they added phrase cluster features extracted from the web data and spelling features."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 31
                            }
                        ],
                        "text": "In the CoNLL-2002 shared task, Carreras et al. (2002) obtained among the best results on both Dutch and Spanish by combining several small fixed-depth decision trees."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 230,
                                "start": 31
                            }
                        ],
                        "text": "In the CoNLL-2002 shared task, Carreras et al. (2002) obtained among the best results on both Dutch and Spanish by combining several small fixed-depth decision trees. Next year, in the CoNLL2003 Shared Task, Florian et al. (2003) obtained the best score on German by combining the output of four diverse classifiers."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 793,
                                "start": 31
                            }
                        ],
                        "text": "In the CoNLL-2002 shared task, Carreras et al. (2002) obtained among the best results on both Dutch and Spanish by combining several small fixed-depth decision trees. Next year, in the CoNLL2003 Shared Task, Florian et al. (2003) obtained the best score on German by combining the output of four diverse classifiers. Qi et al. (2009) later improved on this with a neural network by doing unsupervised learning on a massive unlabeled corpus. Several other neural architectures have previously been proposed for NER. For instance, Collobert et al. (2011) uses a CNN over a sequence of word embeddings with a CRF layer on top. This can be thought of as our first model without character-level embeddings and with the bidirectional LSTM being replaced by a CNN. More recently, Huang et al. (2015) presented a model similar to our LSTM-CRF, but using hand-crafted spelling features."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 334,
                                "start": 31
                            }
                        ],
                        "text": "In the CoNLL-2002 shared task, Carreras et al. (2002) obtained among the best results on both Dutch and Spanish by combining several small fixed-depth decision trees. Next year, in the CoNLL2003 Shared Task, Florian et al. (2003) obtained the best score on German by combining the output of four diverse classifiers. Qi et al. (2009) later improved on this with a neural network by doing unsupervised learning on a massive unlabeled corpus."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 553,
                                "start": 31
                            }
                        ],
                        "text": "In the CoNLL-2002 shared task, Carreras et al. (2002) obtained among the best results on both Dutch and Spanish by combining several small fixed-depth decision trees. Next year, in the CoNLL2003 Shared Task, Florian et al. (2003) obtained the best score on German by combining the output of four diverse classifiers. Qi et al. (2009) later improved on this with a neural network by doing unsupervised learning on a massive unlabeled corpus. Several other neural architectures have previously been proposed for NER. For instance, Collobert et al. (2011) uses a CNN over a sequence of word embeddings with a CRF layer on top."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Named entity extraction using adaboost, proceedings of the 6th conference on natural language learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 60
                            }
                        ],
                        "text": "Finally, we refer interested readers to the original paper (Dyer et al., 2015) for details about the StackLSTM model since in this paper we merely use the same architecture through a new transition-based algorithm presented in the following Section."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 115
                            }
                        ],
                        "text": "To obtain representations of this stack used for predicting subsequent actions, we use the Stack-LSTM presented by Dyer et al. (2015), in which the LSTM is augmented with a \u201cstack pointer."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 10
                            }
                        ],
                        "text": "Following Dyer et al. (2015), we use stack LSTMs to compute a fixed dimensional embedding of each of these, and take a concatenation of these to obtain the full algorithm state."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 115
                            }
                        ],
                        "text": "To obtain representations of this stack used for predicting subsequent actions, we use the Stack-LSTM presented by Dyer et al. (2015), in which the LSTM is augmented with a \u201cstack pointer.\u201d"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Transitionbased dependency parsing with stack long short-term memory"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. ACL."
            },
            "year": 2015
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 193,
                                "start": 173
                            }
                        ],
                        "text": "\u2026networks using the back-propagation algorithm updating our parameters on every training example, one at a time, using stochastic gradient descent (SGD) with\n2(Graff, 2011; Biemann et al., 2007; Callison-Burch et al., 2010; Parker et al., 2009)\na learning rate of 0.01 and a gradient clipping of 5.0."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The leipzig corpora collection-monolingual corpora of standard size"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of Corpus Linguistic."
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 243,
                                "start": 224
                            }
                        ],
                        "text": "\u2026networks using the back-propagation algorithm updating our parameters on every training example, one at a time, using stochastic gradient descent (SGD) with\n2(Graff, 2011; Biemann et al., 2007; Callison-Burch et al., 2010; Parker et al., 2009)\na learning rate of 0.01 and a gradient clipping of 5.0."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "English gigaword fourth edition (ldc2009t13)"
            },
            "venue": {
                "fragments": [],
                "text": "Linguistic Data Consortium, Univer-sity of Pennsylvania, Philadelphia, PA."
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Improved transition-based dependency parsing by modeling characters instead of words with LSTMs"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of EMNLP"
            },
            "year": 2015
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "English gigaword fourth edition (ldc2009t13). Linguistic Data Consortium, Univer-sity of Pennsylvania"
            },
            "venue": {
                "fragments": [],
                "text": "English gigaword fourth edition (ldc2009t13). Linguistic Data Consortium, Univer-sity of Pennsylvania"
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Hochreiter and Schmidhuber1997] Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long short-term memory"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "Passos et al. (2014) also used a linear chain CRF with spelling features and gazetteers."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Lexicon infused phrase"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2014
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Joint SIGDAT Conference on EMNLP and VLC"
            },
            "venue": {
                "fragments": [],
                "text": "Joint SIGDAT Conference on EMNLP and VLC"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Xavier Carreras, Llu\u00eds M` arquez, and Llu\u00eds Padr\u00f3 Named entity extraction using adaboost , proceedings of the 6th conference on natural language learning"
            },
            "venue": {
                "fragments": [],
                "text": "Xavier Carreras, Llu\u00eds M` arquez, and Llu\u00eds Padr\u00f3 Named entity extraction using adaboost , proceedings of the 6th conference on natural language learning"
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 159
                            }
                        ],
                        "text": "Alternative approaches\u2014 most notably like convolutional networks\u2014have been proposed to learn representations of words from their characters (Zhang et al., 2015; Kim et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Character-aware neural language models. CoRR, abs/1508.06615"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2015
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning predictive structures"
            },
            "venue": {
                "fragments": [],
                "text": "Learning predictive structures"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 143
                            }
                        ],
                        "text": "To demonstrate our model\u2019s ability to generalize to different languages, we present results on the CoNLL-2002 and CoNLL2003 datasets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) that contain independent named entity labels for English, Spanish, German and Dutch."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 159
                            }
                        ],
                        "text": "Alternative approaches\u2014 most notably like convolutional networks\u2014have been proposed to learn representations of words from their characters (Zhang et al., 2015; Kim et al., 2015)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Tjong Kim Sang and Fien De Meulder . 2003 . Introduction to the conll - 2003 shared task : Language - independent named entity recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Proc . CoNLL ."
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "End-toend learning of semantic role labeling using recurrent neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Annual Meeting of the Association for Computational Linguistics"
            },
            "year": 2015
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 159
                            }
                        ],
                        "text": "Alternative approaches\u2014 most notably like convolutional networks\u2014have been proposed to learn representations of words from their characters (Zhang et al., 2015; Kim et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Character-aware neural language models. CoRR, abs/1508"
            },
            "venue": {
                "fragments": [],
                "text": "Character-aware neural language models. CoRR, abs/1508"
            },
            "year": 2015
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Introduction to the conll2002 shared task: Language-independent named entity recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. CoNLL."
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Oriol Vinyals, and Amarnag Subramanya. 2015. Multilingual language processing from bytes"
            },
            "venue": {
                "fragments": [],
                "text": "Oriol Vinyals, and Amarnag Subramanya. 2015. Multilingual language processing from bytes"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The leipzig corpora collection-monolingual corpora of standard size"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of Corpus Linguistic"
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Spanish gigaword third edition (ldc2011t12)"
            },
            "venue": {
                "fragments": [],
                "text": "Linguistic Data Consortium, University of Pennsylvania, Philadelphia, PA."
            },
            "year": 2011
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 234,
                                "start": 210
                            }
                        ],
                        "text": "They have been found useful for morphologically rich languages and to handle the outof-vocabulary problem for tasks like part-of-speech tagging and language modeling (Ling et al., 2015b) or dependency parsing (Ballesteros et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Improved transition-based dependency parsing by modeling characters instead of words with LSTMs"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of EMNLP."
            },
            "year": 2015
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning predictive structures"
            },
            "venue": {
                "fragments": [],
                "text": "JMLR, 6:1817\u20131853."
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "End-toend learning of semantic role labeling using recurrent"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2015
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 20,
            "methodology": 29,
            "result": 6
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 66,
        "totalPages": 7
    },
    "page_url": "https://www.semanticscholar.org/paper/Neural-Architectures-for-Named-Entity-Recognition-Lample-Ballesteros/24158c9fc293c8a998ac552b1188404a877da292?sort=total-citations"
}