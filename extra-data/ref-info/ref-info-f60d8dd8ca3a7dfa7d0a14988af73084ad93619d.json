{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710450"
                        ],
                        "name": "J. Zelle",
                        "slug": "J.-Zelle",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Zelle",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Zelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797655"
                        ],
                        "name": "R. Mooney",
                        "slug": "R.-Mooney",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Mooney",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mooney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 315,
                                "start": 312
                            }
                        ],
                        "text": "The only exceptions of which we are aware are a statistical approach to mapping airline-information queries into SQL presented in [10], a probabilistic decision-tree method for the same task described in [11], and an approach using inductive logic programming to learn a logic-based semantic parser described in [1]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 145
                            }
                        ],
                        "text": "Chillin [3] was the rst ILP algorithm that has been applied to the task of learning control rules for a semantic parser in a system called Chill [1]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 95
                            }
                        ],
                        "text": "For instance, in learning control rules for semantic parsing using inductive logic programming [1], one needs to specialize a parser by inducing control rules for a large set of parsing operators."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 54
                            }
                        ],
                        "text": "Geography database (Geoquery) as a sample application [1] which is available on the Web at http://www."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 263135,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b7c0e47f8b768258b7d536c21b218e6c46ab8791",
            "isKey": false,
            "numCitedBy": 642,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents recent work using the CHILL parser acquisition system to automate the construction of a natural-language interface for database queries. CHILL treats parser acquisition as the learning of search-control rules within a logic program representing a shift-reduce parser and uses techniques from Inductive Logic Programming to learn relational control knowledge. Starting with a general framework for constructing a suitable logical form, CHILL is able to train on a corpus comprising sentences paired with database queries and induce parsers that map subsequent sentences directly into executable queries. Experimental results with a complete database-query application for U.S. geography show that CHILL is able to learn parsers that outperform a preexisting, hand-crafted counterpart. These results demonstrate the ability of a corpus-based system to produce more than purely syntactic representations. They also provide direct evidence of the utility of an empirical approach at the level of a complete natural language application."
            },
            "slug": "Learning-to-Parse-Database-Queries-Using-Inductive-Zelle-Mooney",
            "title": {
                "fragments": [],
                "text": "Learning to Parse Database Queries Using Inductive Logic Programming"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Experimental results with a complete database-query application for U.S. geography show that CHILL is able to learn parsers that outperform a preexisting, hand-crafted counterpart, and provide direct evidence of the utility of an empirical approach at the level of a complete natural language application."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI/IAAI, Vol. 2"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145147566"
                        ],
                        "name": "S. Muggleton",
                        "slug": "S.-Muggleton",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Muggleton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Muggleton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70219052"
                        ],
                        "name": "Wray L. Buntine",
                        "slug": "Wray-L.-Buntine",
                        "structuredName": {
                            "firstName": "Wray",
                            "lastName": "Buntine",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wray L. Buntine"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 66
                            }
                        ],
                        "text": "A very simple measure is employed here as our complexity estimate [8]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6159430,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e340a9b752f9b358cda4dc7a1c6e3c6280867158",
            "isKey": false,
            "numCitedBy": 599,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Machine-Invention-of-First-Order-Predicates-by-Muggleton-Buntine",
            "title": {
                "fragments": [],
                "text": "Machine Invention of First Order Predicates by Inverting Resolution"
            },
            "venue": {
                "fragments": [],
                "text": "ML"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1683540"
                        ],
                        "name": "B. Kijsirikul",
                        "slug": "B.-Kijsirikul",
                        "structuredName": {
                            "firstName": "Boonserm",
                            "lastName": "Kijsirikul",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Kijsirikul"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9136228"
                        ],
                        "name": "M. Numao",
                        "slug": "M.-Numao",
                        "structuredName": {
                            "firstName": "Masayuki",
                            "lastName": "Numao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Numao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1845202"
                        ],
                        "name": "M. Shimura",
                        "slug": "M.-Shimura",
                        "structuredName": {
                            "firstName": "Masamichi",
                            "lastName": "Shimura",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Shimura"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 144
                            }
                        ],
                        "text": "If a clause cannot be re ned using the given set of background knowledge, it will attempt to invent a predicate using a method similar to Champ [4]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 42792546,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "24d2e1afbaca7a54c19ebf339d78a532db611220",
            "isKey": false,
            "numCitedBy": 71,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a new approach to constructive induction, Discrimination-Based Constructive induction(DBC), which invents useful predicates in learning relations. Triggered by failure of selective induction, DBC finds a minimal set of variables forming a new predicate that discriminates between positive and negative examples, and induces a definition of the invented predicate. If necessary, it also induces subpredicates for the definition. Experimental results show that DBC learns meaningful predicates without any interactive guidance."
            },
            "slug": "Discrimination-Based-Constructive-Induction-of-Kijsirikul-Numao",
            "title": {
                "fragments": [],
                "text": "Discrimination-Based Constructive Induction of Logic Programs"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Experimental results show that DBC learns meaningful predicates without any interactive guidance."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710450"
                        ],
                        "name": "J. Zelle",
                        "slug": "J.-Zelle",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Zelle",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Zelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797655"
                        ],
                        "name": "R. Mooney",
                        "slug": "R.-Mooney",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Mooney",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mooney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2834139"
                        ],
                        "name": "Joshua B. Konvisser",
                        "slug": "Joshua-B.-Konvisser",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Konvisser",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joshua B. Konvisser"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5406859,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f5bd46825dc9dae3b4ecaa063ad62a772e99dc8f",
            "isKey": false,
            "numCitedBy": 68,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Combining-Top-down-and-Bottom-up-Techniques-in-Zelle-Mooney",
            "title": {
                "fragments": [],
                "text": "Combining Top-down and Bottom-up Techniques in Inductive Logic Programming"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1967815"
                        ],
                        "name": "M. E. Califf",
                        "slug": "M.-E.-Califf",
                        "structuredName": {
                            "firstName": "Mary",
                            "lastName": "Califf",
                            "middleNames": [
                                "Elaine"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. E. Califf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797655"
                        ],
                        "name": "R. Mooney",
                        "slug": "R.-Mooney",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Mooney",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mooney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 306,
                                "start": 302
                            }
                        ],
                        "text": "Information from these job postings are extracted to create a database which contains the following types of information: 1) the job title, 2) the company, 3) the recruiter, 4) the location, 5) the salary, 6) the languages and platforms used, and 7) required or desired years of experience and degrees [13]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 489775,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "16bd1fbe3694173eda4ad4338a85f8288d19bf02",
            "isKey": false,
            "numCitedBy": 700,
            "numCiting": 77,
            "paperAbstract": {
                "fragments": [],
                "text": "Information extraction is a form of shallow text processing that locates a specified set of relevant items in a natural-language document. Systems for this task require significant domain-specific knowledge and are time-consuming and difficult to build by hand, making them a good application for machine learning. We present a system, RAPIER, that uses pairs of sample documents and filled templates to induce pattern-match rules that directly extract fillers for the slots in the template. RAPIER employs a bottom-up learning algorithm which incorporates techniques from several inductive logic programming systems and acquires unbounded patterns that include constraints on the words, part-of-speech tags, and semantic classes present in the filler and the surrounding text. We present encouraging experimental results on two domains."
            },
            "slug": "Relational-Learning-of-Pattern-Match-Rules-for-Califf-Mooney",
            "title": {
                "fragments": [],
                "text": "Relational Learning of Pattern-Match Rules for Information Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "RAPIER employs a bottom-up learning algorithm which incorporates techniques from several inductive logic programming systems and acquires unbounded patterns that include constraints on the words, part-of-speech tags, and semantic classes present in the filler and the surrounding text."
            },
            "venue": {
                "fragments": [],
                "text": "CoNLL"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3255957"
                        ],
                        "name": "Cynthia A. Thompson",
                        "slug": "Cynthia-A.-Thompson",
                        "structuredName": {
                            "firstName": "Cynthia",
                            "lastName": "Thompson",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cynthia A. Thompson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797655"
                        ],
                        "name": "R. Mooney",
                        "slug": "R.-Mooney",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Mooney",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mooney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8502079,
            "fieldsOfStudy": [
                "Computer Science",
                "Linguistics"
            ],
            "id": "744285041424022aaf3e38ba9bd662fb8134e64c",
            "isKey": false,
            "numCitedBy": 44,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a system, WOLFIE (WOrd Learning From Interpreted Examples), that acquires a semantic lexicon from a corpus of sentences paired with semantic representations. The lexicon learned consists of words paired with meaning representations. WOLFIE is part of an integrated system that learns to parse novel sentences into semantic representations, such as logical database queries. Experimental results are presented demonstrating WOLFIE's ability to learn useful lexicons for a database interface in four different natural languages. The lexicons learned by WOLFIE are compared to those acquired by a similar system developed by Siskind (1996)."
            },
            "slug": "Automatic-Construction-of-Semantic-Lexicons-for-Thompson-Mooney",
            "title": {
                "fragments": [],
                "text": "Automatic Construction of Semantic Lexicons for Learning Natural Language Interfaces"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Experimental results are presented demonstrating WOLFIE's ability to learn useful lexicons for a database interface in four different natural languages."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI/IAAI"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143780768"
                        ],
                        "name": "A. Srinivasan",
                        "slug": "A.-Srinivasan",
                        "structuredName": {
                            "firstName": "Ashwin",
                            "lastName": "Srinivasan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Srinivasan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145147566"
                        ],
                        "name": "S. Muggleton",
                        "slug": "S.-Muggleton",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Muggleton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Muggleton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144041292"
                        ],
                        "name": "Michael Bain",
                        "slug": "Michael-Bain",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Bain",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Bain"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 1113379,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "77e31808c5ab75d482daf09eb15d8a90387fd8d4",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "Non-demonstrative or inductive reasoning is a crucial component in the skills of a learner. A leading candidate for this form of reasoning involves the automatic formation of hypotheses. Initial successes in the construction of propositional theories have now been followed by algorithms that attempt to generalise sentences in the predicate calculus. An important defect in these new-generation systems is the lack of a clear model for theory justiication. In this paper we describe a method of evaluating the signiicance of a hypothesis based on the degree to which it allows compression of the observed data with respect to prior knowledge. This can be measured by comparing the lengths of the input and output tapes of a reference Turing machine which will generate the examples from the hypothesis and a set of derivational proofs. The model extends an earlier approach of Muggleton by allowing for noise. The truth values of noisy instances are switched by making use of correction codes. The utility of compression as a signiicance measure is evaluated empirically in three independent domains. In particular, the results show that the existence of compression distinguishes a larger number of signiicant clauses than other signiicance tests. The method also appears to distinguish noise as incompressible data."
            },
            "slug": "The-Justification-of-Logical-Theories-based-on-Data-Srinivasan-Muggleton",
            "title": {
                "fragments": [],
                "text": "The Justification of Logical Theories based on Data Compression"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A method of evaluating the signiicance of a hypothesis based on the degree to which it allows compression of the observed data with respect to prior knowledge, which appears to distinguish noise as incompressible data."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Intelligence 13"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143937779"
                        ],
                        "name": "R. Kuhn",
                        "slug": "R.-Kuhn",
                        "structuredName": {
                            "firstName": "Roland",
                            "lastName": "Kuhn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kuhn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714393"
                        ],
                        "name": "R. Mori",
                        "slug": "R.-Mori",
                        "structuredName": {
                            "firstName": "Renato",
                            "lastName": "Mori",
                            "middleNames": [
                                "De"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mori"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 204
                            }
                        ],
                        "text": "The only exceptions of which we are aware are a statistical approach to mapping airline-information queries into SQL presented in [10], a probabilistic decision-tree method for the same task described in [11], and an approach using inductive logic programming to learn a logic-based semantic parser described in [1]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6213072,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f29d9348ce38198ad270d440ba6a5e9f3fc4afb6",
            "isKey": false,
            "numCitedBy": 158,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "This article describes a new method for building a natural language understanding (NLU) system, in which the system's rules are learnt automatically from training data. The method has been applied to design of a speech understanding (SU) system. Designers of such systems rely increasingly on robust matchers to perform the task of extracting meaning from one or several word sequence hypotheses generated by a speech recognizer. We describe a new data structure, the semantic classification tree (SCT), that learns semantic rules from training data and can be a building block for robust matchers for NLU tasks. By reducing the need for handcoding and debugging a large number of rules, this approach facilitates rapid construction of an NLU system. In the case of an SU system, the rules learned by an SCT are highly resistant to errors by the speaker or by the speech recognizer because they depend on a small number of words in each utterance. Our work shows that semantic rules can be learned automatically from training data, yielding successful NLU for a realistic application. >"
            },
            "slug": "The-Application-of-Semantic-Classification-Trees-to-Kuhn-Mori",
            "title": {
                "fragments": [],
                "text": "The Application of Semantic Classification Trees to Natural Language Understanding"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A new data structure is described, the semantic classification tree (SCT), that learns semantic rules from training data and can be a building block for robust matchers for NLU tasks."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145147566"
                        ],
                        "name": "S. Muggleton",
                        "slug": "S.-Muggleton",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Muggleton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Muggleton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143780768"
                        ],
                        "name": "A. Srinivasan",
                        "slug": "A.-Srinivasan",
                        "structuredName": {
                            "firstName": "Ashwin",
                            "lastName": "Srinivasan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Srinivasan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144041292"
                        ],
                        "name": "Michael Bain",
                        "slug": "Michael-Bain",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Bain",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Bain"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 100
                            }
                        ],
                        "text": "The metric is purely syntactic; it does not take into account the complexity of proving an instance [14]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 119
                            }
                        ],
                        "text": "Apparently, this is due to the fact that some compressive clauses were wrongly rejected by a statistical based measure [14]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 46385792,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d4b0c4b006a7de5869f047c6151893fb6ab38927",
            "isKey": false,
            "numCitedBy": 73,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Compression,-Significance,-and-Accuracy-Muggleton-Srinivasan",
            "title": {
                "fragments": [],
                "text": "Compression, Significance, and Accuracy"
            },
            "venue": {
                "fragments": [],
                "text": "ML"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110271578"
                        ],
                        "name": "Scott Miller",
                        "slug": "Scott-Miller",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Miller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Scott Miller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145482266"
                        ],
                        "name": "D. Stallard",
                        "slug": "D.-Stallard",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Stallard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Stallard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2189985"
                        ],
                        "name": "R. Bobrow",
                        "slug": "R.-Bobrow",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Bobrow",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Bobrow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35442155"
                        ],
                        "name": "R. Schwartz",
                        "slug": "R.-Schwartz",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Schwartz",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schwartz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 130
                            }
                        ],
                        "text": "The only exceptions of which we are aware are a statistical approach to mapping airline-information queries into SQL presented in [10], a probabilistic decision-tree method for the same task described in [11], and an approach using inductive logic programming to learn a logic-based semantic parser described in [1]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10983275,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e9c1f510bcf5933d3cf8ec8108a04a9ba601a843",
            "isKey": false,
            "numCitedBy": 158,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a natural language interface system which is based entirely on trained statistical models. The system consists of three stages of processing: parsing, semantic interpretation, and discourse. Each of these stages is modeled as a statistical process. The models are fully integrated, resulting in an end-to-end system that maps input utterances into meaning representation frames."
            },
            "slug": "A-Fully-Statistical-Approach-to-Natural-Language-Miller-Stallard",
            "title": {
                "fragments": [],
                "text": "A Fully Statistical Approach to Natural Language Interfaces"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "This work presents a natural language interface system which is based entirely on trained statistical models, resulting in an end-to-end system that maps input utterances into meaning representation frames."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804633"
                        ],
                        "name": "G. Hendrix",
                        "slug": "G.-Hendrix",
                        "structuredName": {
                            "firstName": "Gary",
                            "lastName": "Hendrix",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Hendrix"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1767123"
                        ],
                        "name": "E. Sacerdoti",
                        "slug": "E.-Sacerdoti",
                        "structuredName": {
                            "firstName": "Earl",
                            "lastName": "Sacerdoti",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Sacerdoti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1795468"
                        ],
                        "name": "Daniel Sagalowicz",
                        "slug": "Daniel-Sagalowicz",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Sagalowicz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Sagalowicz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763912"
                        ],
                        "name": "J. Slocum",
                        "slug": "J.-Slocum",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Slocum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Slocum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 136
                            }
                        ],
                        "text": "Traditional (rationalist) approaches to constructing database interfaces require an expert to hand-craft an appropriate semantic parser [9]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15391397,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6f0aa57820d5f1700461b317faabad9b98d0f70d",
            "isKey": false,
            "numCitedBy": 580,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "Aspects of an intelligent interface that provides natural language access to a large body of data distributed over a computer network are described. The overall system architecture is presented, showing how a user is buffered from the actual database management systems (DBMSs) by three layers of insulating components. These layers operate in series to convert natural language queries into calls to DBMSs at remote sites. Attention is then focused on the first of the insulating components, the natural language system. A pragmatic approach to language access that has proved useful for building interfaces to databases is described and illustrated by examples. Special language features that increase system usability, such as spelling correction, processing of incomplete inputs, and run-time system personalization, are also discussed. The language system is contrasted with other work in applied natural language processing, and the system's limitations are analyzed."
            },
            "slug": "Developing-a-natural-language-interface-to-complex-Hendrix-Sacerdoti",
            "title": {
                "fragments": [],
                "text": "Developing a natural language interface to complex data"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "The overall system architecture is presented, showing how a user is buffered from the actual database management systems (DBMSs) by three layers of insulating components."
            },
            "venue": {
                "fragments": [],
                "text": "TODS"
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2516166"
                        ],
                        "name": "J. Rissanen",
                        "slug": "J.-Rissanen",
                        "structuredName": {
                            "firstName": "Jorma",
                            "lastName": "Rissanen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Rissanen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 73
                            }
                        ],
                        "text": "It has been formulated in the Minimum Description Length (MDL) principle [6] that the most probable hypothesis H given the evidence (training data) D is the one that minimizes the complexity of H given D which is de ned as"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 30140639,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d382b9c11e5c6a8e173fbeb442545e3be8d3e3a5",
            "isKey": false,
            "numCitedBy": 6261,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Modeling-By-Shortest-Data-Description*-Rissanen",
            "title": {
                "fragments": [],
                "text": "Modeling By Shortest Data Description*"
            },
            "venue": {
                "fragments": [],
                "text": "Autom."
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2476128"
                        ],
                        "name": "B. Cestnik",
                        "slug": "B.-Cestnik",
                        "structuredName": {
                            "firstName": "Bojan",
                            "lastName": "Cestnik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Cestnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 65
                            }
                        ],
                        "text": "However, it uses a more direct accuracy estimate, the m-estimate [5], to measure the expected accuracy of a clause which is de ned as"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 20779819,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ad58594194155af8b48eaf3ab9525a1fafd24e58",
            "isKey": false,
            "numCitedBy": 580,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Estimating-Probabilities:-A-Crucial-Task-in-Machine-Cestnik",
            "title": {
                "fragments": [],
                "text": "Estimating Probabilities: A Crucial Task in Machine Learning"
            },
            "venue": {
                "fragments": [],
                "text": "ECAI"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 8
                            }
                        ],
                        "text": "Chillin [3] was the rst ILP algorithm that has been applied to the task of learning control rules for a semantic parser in a system called Chill [1]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Combining Top-Down and Bottom-Up Methods in Inductive Logic Programming"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Eleventh International Conference on Machine Learning"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 46
                            }
                        ],
                        "text": "For example, to construct a clause using Foil [2] given an existing partial theory Tp (which is initially empty) and a set of training examples [ (positive and negative), one uses all the positive examples not covered by Tp to learn a single clause C."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning Logical De nitions from Relations"
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning 5"
            },
            "year": 1990
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 3,
            "methodology": 10
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 15,
        "totalPages": 2
    },
    "page_url": "https://www.semanticscholar.org/paper/Using-Multiple-Clause-Constructors-in-Inductive-for-Tang-Mooney/f60d8dd8ca3a7dfa7d0a14988af73084ad93619d?sort=total-citations"
}