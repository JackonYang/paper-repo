{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1765114"
                        ],
                        "name": "Ulf Brefeld",
                        "slug": "Ulf-Brefeld",
                        "structuredName": {
                            "firstName": "Ulf",
                            "lastName": "Brefeld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ulf Brefeld"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751348"
                        ],
                        "name": "T. Scheffer",
                        "slug": "T.-Scheffer",
                        "structuredName": {
                            "firstName": "Tobias",
                            "lastName": "Scheffer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Scheffer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13189641,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "75e41094babcabcdb95757b30c732b03836e1b7c",
            "isKey": false,
            "numCitedBy": 73,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of learning a mapping between input and structured, interdependent output variables covers sequential, spatial, and relational learning as well as predicting recursive structures. Joint feature representations of the input and output variables have paved the way to leveraging discriminative learners such as SVMs to this class of problems. We address the problem of semi-supervised learning in joint input output spaces. The co-training approach is based on the principle of maximizing the consensus among multiple independent hypotheses; we develop this principle into a semi-supervised support vector learning algorithm for joint input output spaces and arbitrary loss functions. Experiments investigate the benefit of semi-supervised structured models in terms of accuracy and F1 score."
            },
            "slug": "Semi-supervised-learning-for-structured-output-Brefeld-Scheffer",
            "title": {
                "fragments": [],
                "text": "Semi-supervised learning for structured output variables"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The co-training approach is based on the principle of maximizing the consensus among multiple independent hypotheses and developed into a semi-supervised support vector learning algorithm for joint input output spaces and arbitrary loss functions."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1808676"
                        ],
                        "name": "V. Sindhwani",
                        "slug": "V.-Sindhwani",
                        "structuredName": {
                            "firstName": "Vikas",
                            "lastName": "Sindhwani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Sindhwani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770745"
                        ],
                        "name": "P. Niyogi",
                        "slug": "P.-Niyogi",
                        "structuredName": {
                            "firstName": "Partha",
                            "lastName": "Niyogi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Niyogi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145520115"
                        ],
                        "name": "Mikhail Belkin",
                        "slug": "Mikhail-Belkin",
                        "structuredName": {
                            "firstName": "Mikhail",
                            "lastName": "Belkin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mikhail Belkin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "It has been suggested that temporal correlation serves as the glue, as summarized by (Sinha et al., 2006) (Result 14). It seems when we observe an object with changing angles, we link the images as \u2018containing the same object\u2019 by the virtue that the images are close in time. Wallis and B\u00fclthoff (2001) created artificial image sequences where a frontal face is morphed into the profile face of a different person."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6035769,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ec7c68427a26f812532b1c913c68fcf84b7de58e",
            "isKey": false,
            "numCitedBy": 474,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Due to its occurrence in engineering domains and implications for natural learning, the problem of utilizing unlabeled data is attracting increasing attention in machine learning. A large body of recent literature has focussed on the transductive setting where labels of unlabeled examples are estimated by learning a function defined only over the point cloud data. In a truly semi-supervised setting however, a learning machine has access to labeled and unlabeled examples and must make predictions on data points never encountered before. In this paper, we show how to turn transductive and standard supervised learning algorithms into semi-supervised learners. We construct a family of data-dependent norms on Reproducing Kernel Hilbert Spaces (RKHS). These norms allow us to warp the structure of the RKHS to reflect the underlying geometry of the data. We derive explicit formulas for the corresponding new kernels. Our approach demonstrates state of the art performance on a variety of classification tasks."
            },
            "slug": "Beyond-the-point-cloud:-from-transductive-to-Sindhwani-Niyogi",
            "title": {
                "fragments": [],
                "text": "Beyond the point cloud: from transductive to semi-supervised learning"
            },
            "tldr": {
                "abstractSimilarityScore": 36,
                "text": "This paper constructs a family of data-dependent norms on Reproducing Kernel Hilbert Spaces (RKHS) that allow the structure of the RKHS to reflect the underlying geometry of the data."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144782042"
                        ],
                        "name": "Kai Yu",
                        "slug": "Kai-Yu",
                        "structuredName": {
                            "firstName": "Kai",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai Yu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14313123,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "36f7805632939e610f9474a7d5868f503de7197e",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Considerable progress was recently made on semi-supervised learning, which differs from the traditional supervised learning by additionally exploring the information of the unlabeled examples. However, a disadvantage of many existing methods is that it does not generalize to unseen inputs. This paper suggests a space of basis functions to perform semi-supervised inductive learning. As a nice property, the proposed method allows efficient training and can easily handle new test points. We validate the method based on both toy data and real world data sets."
            },
            "slug": "Semi-supervised-Induction-with-Basis-Functions-Yu",
            "title": {
                "fragments": [],
                "text": "Semi-supervised Induction with Basis Functions"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A space of basis functions to perform semi-supervised inductive learning is suggested and the proposed method allows efficient training and can easily handle new test points."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35043531"
                        ],
                        "name": "A. Dempster",
                        "slug": "A.-Dempster",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "Dempster",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dempster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7890796"
                        ],
                        "name": "N. Laird",
                        "slug": "N.-Laird",
                        "structuredName": {
                            "firstName": "Nan",
                            "lastName": "Laird",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Laird"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2235217"
                        ],
                        "name": "D. Rubin",
                        "slug": "D.-Rubin",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Rubin",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rubin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 143
                            }
                        ],
                        "text": "Even if the mixture model assumption is correct, in practice mixture components are identified by the Expectation-Maximization (EM) algorithm (Dempster et al., 1977)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 160
                            }
                        ],
                        "text": "3 EM Local Maxima Even if the mixture model assumption is correct, in practice mixture components are identified by the Expectation-Maximization (EM) algorithm (Dempster et al., 1977)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 4193919,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "d36efb9ad91e00faa334b549ce989bfae7e2907a",
            "isKey": false,
            "numCitedBy": 48406,
            "numCiting": 134,
            "paperAbstract": {
                "fragments": [],
                "text": "Vibratory power unit for vibrating conveyers and screens comprising an asynchronous polyphase motor, at least one pair of associated unbalanced masses disposed on the shaft of said motor, with the first mass of a pair of said unbalanced masses being rigidly fastened to said shaft and with said second mass of said pair being movably arranged relative to said first mass, means for controlling and regulating the conveying rate during conveyer operation by varying the rotational speed of said motor between predetermined minimum and maximum values, said second mass being movably outwardly by centrifugal force against the pressure of spring means, said spring means being prestressed in such a manner that said second mass is, at rotational motor speeds lower than said minimum speed, held in its initial position, and at motor speeds between said lower and upper values in positions which are radially offset with respect to the axis of said motor to an extent depending on the value of said rotational motor speed."
            },
            "slug": "Maximum-likelihood-from-incomplete-data-via-the-EM-Dempster-Laird",
            "title": {
                "fragments": [],
                "text": "Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "17027818"
                        ],
                        "name": "C. Rosenberg",
                        "slug": "C.-Rosenberg",
                        "structuredName": {
                            "firstName": "Chuck",
                            "lastName": "Rosenberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Rosenberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145670946"
                        ],
                        "name": "M. Hebert",
                        "slug": "M.-Hebert",
                        "structuredName": {
                            "firstName": "Martial",
                            "lastName": "Hebert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hebert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2085375589"
                        ],
                        "name": "H. Schneiderman",
                        "slug": "H.-Schneiderman",
                        "structuredName": {
                            "firstName": "Henry",
                            "lastName": "Schneiderman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Schneiderman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7648360,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "59dc87ac9fd2c39d7b9f8ab1a2bf43ac53891e96",
            "isKey": false,
            "numCitedBy": 685,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "The construction of appearance-based object detection systems is time-consuming and difficult because a large number of training examples must be collected and manually labeled in order to capture variations in object appearance. Semi-supervised training is a means for reducing the effort needed to prepare the training set by training the model with a small number of fully labeled examples and an additional set of unlabeled or weakly labeled examples. In this work we present a semi-supervised approach to training object detection systems based on self-training. We implement our approach as a wrapper around the training process of an existing object detector and present empirical results. The key contributions of this empirical study is to demonstrate that a model trained in this manner can achieve results comparable to a model trained in the traditional manner using a much larger set of fully labeled data, and that a training data selection metric that is defined independently of the detector greatly outperforms a selection metric based on the detection confidence generated by the detector."
            },
            "slug": "Semi-Supervised-Self-Training-of-Object-Detection-Rosenberg-Hebert",
            "title": {
                "fragments": [],
                "text": "Semi-Supervised Self-Training of Object Detection Models"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The key contributions of this empirical study are to demonstrate that a model trained in this manner can achieve results comparable to a modeltrained in the traditional manner using a much larger set of fully labeled data, and that a training data selection metric that is defined independently of the detector greatly outperforms a selection metric based on the detection confidence generated by the detector."
            },
            "venue": {
                "fragments": [],
                "text": "2005 Seventh IEEE Workshops on Applications of Computer Vision (WACV/MOTION'05) - Volume 1"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3021403"
                        ],
                        "name": "Maryam Mahdaviani",
                        "slug": "Maryam-Mahdaviani",
                        "structuredName": {
                            "firstName": "Maryam",
                            "lastName": "Mahdaviani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Maryam Mahdaviani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737568"
                        ],
                        "name": "N. D. Freitas",
                        "slug": "N.-D.-Freitas",
                        "structuredName": {
                            "firstName": "Nando",
                            "lastName": "Freitas",
                            "middleNames": [
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. D. Freitas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "122698698"
                        ],
                        "name": "R. Fraser",
                        "slug": "R.-Fraser",
                        "structuredName": {
                            "firstName": "R.J.C.",
                            "lastName": "Fraser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fraser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1811875"
                        ],
                        "name": "F. Hamze",
                        "slug": "F.-Hamze",
                        "structuredName": {
                            "firstName": "Firas",
                            "lastName": "Hamze",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Hamze"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "been applied to dense graphs in semi-supervised learning, reducing the computational cost from O(n) to O(n) (Mahdaviani et al., 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16107929,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f6ee548c2b9e6587e0fb2530c1d4cb361b489749",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes numerical algorithms for reducing the computational cost of semi-supervised and active learning procedures for visually guided mobile robots from O(M3to O(M), while reducing the storage requirements from M2to M . This reduction in cost is essential for real-time interaction with mobile robots. The considerable speed ups are achieved using Krylov subspace methods and the fast Gauss transform. Although these state-of-the-art numerical algorithms are known, their application to semi-supervised learning, active learning and mobile robotics is new and should be of interest and great value to the robotics community. We apply our fast algorithms to interactive object recognition on Sony\u2019s ERS-7 Aibo. We provide comparisons that clearly demonstrate remarkable improvements in computational speed."
            },
            "slug": "Fast-Computational-Methods-for-Visually-Guided-Mahdaviani-Freitas",
            "title": {
                "fragments": [],
                "text": "Fast Computational Methods for Visually Guided Robots"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "Numerical algorithms for reducing the computational cost of semi-supervised and active learning procedures for visually guided mobile robots from O(M3to O( M), while reducing the storage requirements from M2to M are proposed."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2005 IEEE International Conference on Robotics and Automation"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725303"
                        ],
                        "name": "Y. Teh",
                        "slug": "Y.-Teh",
                        "structuredName": {
                            "firstName": "Yee",
                            "lastName": "Teh",
                            "middleNames": [
                                "Whye"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Teh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9330607"
                        ],
                        "name": "S. Roweis",
                        "slug": "S.-Roweis",
                        "structuredName": {
                            "firstName": "Sam",
                            "lastName": "Roweis",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roweis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 481910,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a0ac8c6271b4b10dcee2a0aa68f5284ee4b306df",
            "isKey": false,
            "numCitedBy": 190,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an automatic alignment procedure which maps the disparate internal representations learned by several local dimensionality reduction experts into a single, coherent global coordinate system for the original data space. Our algorithm can be applied to any set of experts, each of which produces a low-dimensional local representation of a high-dimensional input. Unlike recent efforts to coordinate such models by modifying their objective functions [1, 2], our algorithm is invoked after training and applies an efficient eigensolver to post-process the trained models. The post-processing has no local optima and the size of the system it must solve scales with the number of local models rather than the number of original data points, making it more efficient than model-free algorithms such as Isomap [3] or LLE [4]."
            },
            "slug": "Automatic-Alignment-of-Local-Representations-Teh-Roweis",
            "title": {
                "fragments": [],
                "text": "Automatic Alignment of Local Representations"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "An automatic alignment procedure which maps the disparate internal representations learned by several local dimensionality reduction experts into a single, coherent global coordinate system for the original data space is presented."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1807998"
                        ],
                        "name": "I. Tsang",
                        "slug": "I.-Tsang",
                        "structuredName": {
                            "firstName": "Ivor",
                            "lastName": "Tsang",
                            "middleNames": [
                                "Wai-Hung"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Tsang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145193332"
                        ],
                        "name": "J. Kwok",
                        "slug": "J.-Kwok",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Kwok",
                            "middleNames": [
                                "Tin-Yau"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kwok"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 583357,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2673d492667db10e828379b6d9cc607616ffa9d1",
            "isKey": false,
            "numCitedBy": 84,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Semi-supervised learning is more powerful than supervised learning by using both labeled and unlabeled data. In particular, the manifold regularization framework, together with kernel methods, leads to the Laplacian SVM (LapSVM) that has demonstrated state-of-the-art performance. However, the LapSVM solution typically involves kernel expansions of all the labeled and unlabeled examples, and is slow on testing. Moreover, existing semi-supervised learning methods, including the LapSVM, can only handle a small number of unlabeled examples. In this paper, we integrate manifold regularization with the core vector machine, which has been used for large-scale supervised and unsupervised learning. By using a sparsified manifold regularizer and formulating as a center-constrained minimum enclosing ball problem, the proposed method produces sparse solutions with low time and space complexities. Experimental results show that it is much faster than the LapSVM, and can handle a million unlabeled examples on a standard PC; while the LapSVM can only handle several thousand patterns."
            },
            "slug": "Large-Scale-Sparsified-Manifold-Regularization-Tsang-Kwok",
            "title": {
                "fragments": [],
                "text": "Large-Scale Sparsified Manifold Regularization"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper integrates manifold regularization with the core vector machine and produces sparse solutions with low time and space complexities by using a sparsified manifold regularizer and formulating as a center-constrained minimum enclosing ball problem."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2460212"
                        ],
                        "name": "Olivier Delalleau",
                        "slug": "Olivier-Delalleau",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Delalleau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Olivier Delalleau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7245737"
                        ],
                        "name": "Nicolas Le Roux",
                        "slug": "Nicolas-Le-Roux",
                        "structuredName": {
                            "firstName": "Nicolas",
                            "lastName": "Le Roux",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nicolas Le Roux"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7513025,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7b662bb37a4fc10bcaf0f2d6df1b0ccab5c9b6c7",
            "isKey": false,
            "numCitedBy": 199,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "There has been an increase of interest for semi-supervised learning recently, because of the many datasets with large amounts of unlabeled examples and only a few labeled ones. This paper follows up on proposed non-parametric algorithms which provide an estimated continuous label for the given unlabeled examples. It extends them to function induction algorithms that correspond to the minimization of a regularization criterion applied to an out-of-sample example, and happens to have the form of a Parzen windows regressor. The advantage of the extension is that it allows predicting the label for a new example without having to solve again a linear system of dimension 'n' (the number of unlabeled and labeled training examples), which can cost O(n^3). Experiments show that the extension works well, in the sense of predicting a label close to the one that would have been obtained if the test example had been included in the unlabeled set. This relatively efficient function induction procedure can also be used when 'n' is large to approximate the solution by writing it only in terms of a kernel expansion with 'm' Keywords: non-parametric models, classification, regression, semi-supervised learning, modeles non parametriques, classification, regression, apprentissage semi-supervise"
            },
            "slug": "Efficient-Non-Parametric-Function-Induction-in-Delalleau-Bengio",
            "title": {
                "fragments": [],
                "text": "Efficient Non-Parametric Function Induction in Semi-Supervised Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "Experiments show that the proposed non-parametric algorithms which provide an estimated continuous label for the given unlabeled examples are extended to function induction algorithms that correspond to the minimization of a regularization criterion applied to an out-of-sample example, and happens to have the form of a Parzen windows regressor."
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2482936"
                        ],
                        "name": "Gideon S. Mann",
                        "slug": "Gideon-S.-Mann",
                        "structuredName": {
                            "firstName": "Gideon",
                            "lastName": "Mann",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gideon S. Mann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5862158,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "619fd1de5aa0dc649841c8f7d5cb65965a3b7fc6",
            "isKey": false,
            "numCitedBy": 173,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "Although semi-supervised learning has been an active area of research, its use in deployed applications is still relatively rare because the methods are often difficult to implement, fragile in tuning, or lacking in scalability. This paper presents expectation regularization, a semi-supervised learning method for exponential family parametric models that augments the traditional conditional label-likelihood objective function with an additional term that encourages model predictions on unlabeled data to match certain expectations---such as label priors. The method is extremely easy to implement, scales as well as logistic regression, and can handle non-independent features. We present experiments on five different data sets, showing accuracy improvements over other semi-supervised methods."
            },
            "slug": "Simple,-robust,-scalable-semi-supervised-learning-Mann-McCallum",
            "title": {
                "fragments": [],
                "text": "Simple, robust, scalable semi-supervised learning via expectation regularization"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Expectation regularization is presented, a semi-supervised learning method for exponential family parametric models that augments the traditional conditional label-likelihood objective function with an additional term that encourages model predictions on unlabeled data to match certain expectations---such as label priors."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '07"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40400230"
                        ],
                        "name": "Wei Li",
                        "slug": "Wei-Li",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6407325,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b89926ec5f0046f3a5671d8e68c918ab9cac76fd",
            "isKey": false,
            "numCitedBy": 69,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Although there has been significant previous work on semi-supervised learning for classification, there has been relatively little in sequence modeling. This paper presents an approach that leverages recent work in manifold-learning on sequences to discover word clusters from language data, including both syntactic classes and semantic topics. From unlabeled data we form a smooth. low-dimensional feature space, where each word token is projected based on its underlying role as a function or content word. We then use this projection as additional input features to a linear-chain conditional random field trained on limited labeled training data. On standard part-of-speech tagging and Chinese word segmentation data sets we show as much as 14% error reduction due to the unlabeled data, and also statistically-significant improvements over a related semi-supervised sequence tagging method due to Miller et al."
            },
            "slug": "Semi-Supervised-Sequence-Modeling-with-Syntactic-Li-McCallum",
            "title": {
                "fragments": [],
                "text": "Semi-Supervised Sequence Modeling with Syntactic Topic Models"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This paper presents an approach that leverages recent work in manifold-learning on sequences to discover word clusters from language data, including both syntactic classes and semantic topics, with statistically-significant improvements over a related semi-supervised sequence tagging method."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1880237"
                        ],
                        "name": "S. Dasgupta",
                        "slug": "S.-Dasgupta",
                        "structuredName": {
                            "firstName": "Sanjoy",
                            "lastName": "Dasgupta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dasgupta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144885169"
                        ],
                        "name": "M. Littman",
                        "slug": "M.-Littman",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Littman",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Littman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145689002"
                        ],
                        "name": "David A. McAllester",
                        "slug": "David-A.-McAllester",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "McAllester",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David A. McAllester"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "(Dasgupta et al., 2001) provide a PAC-style theoretical analysis."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "De Bie and Cristianini (2006a) present an SDP relaxation of the normalized graph cut problem, including a series of relaxations between spectral relaxations above, and SDP relaxation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 280438,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a48abd949978432b8a9498f94b029e440bc1b4f3",
            "isKey": false,
            "numCitedBy": 312,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "The rule-based bootstrapping introduced by Yarowsky, and its co-training variant by Blum and Mitchell, have met with considerable empirical success. Earlier work on the theory of co-training has been only loosely related to empirically useful co-training algorithms. Here we give a new PAC-style bound on generalization error which justifies both the use of confidences \u2014 partial rules and partial labeling of the unlabeled data \u2014 and the use of an agreement-based objective function as suggested by Collins and Singer. Our bounds apply to the multiclass case, i.e., where instances are to be assigned one of labels for k \u2265 2."
            },
            "slug": "PAC-Generalization-Bounds-for-Co-training-Dasgupta-Littman",
            "title": {
                "fragments": [],
                "text": "PAC Generalization Bounds for Co-training"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A new PAC-style bound on generalization error is given which justifies both the use of confidences \u2014 partial rules and partial labeling of the unlabeled data \u2014 and theUse of an agreement-based objective function as suggested by Collins and Singer."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3276863"
                        ],
                        "name": "Ion Muslea",
                        "slug": "Ion-Muslea",
                        "structuredName": {
                            "firstName": "Ion",
                            "lastName": "Muslea",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ion Muslea"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "26602711"
                        ],
                        "name": "Steven N. Minton",
                        "slug": "Steven-N.-Minton",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Minton",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steven N. Minton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745117"
                        ],
                        "name": "Craig A. Knoblock",
                        "slug": "Craig-A.-Knoblock",
                        "structuredName": {
                            "firstName": "Craig",
                            "lastName": "Knoblock",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Craig A. Knoblock"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Nigam et al. (2000) apply the EM algorithm on mixture of multinomial for the task of text classification."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 367122,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c84b9806d3de2d3b2d45f8fcb2df6760e98b1a15",
            "isKey": false,
            "numCitedBy": 343,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "In a multi-view problem, the features of the domain can be partitioned into disjoint subsets (views) that are sufficient to learn the target concept. Semi-supervised, multi-view algorithms, which reduce the amount of labeled data required for learning, rely on the assumptions that the views are compatible and uncorrelated (i.e., every example is identically labeled by the target concepts in each view; and, given the label of any example, its descriptions in each view are independent). As these assumptions are unlikely to hold in practice, it is crucial to understand the behavior of multi-view algorithms on problems with incompatible, correlated views. We address this issue by studying several algorithms on a parameterized family of text classification problems in which we control both view correlation and incompatibility. We first show that existing semi-supervised algorithms are not robust over the whole spectrum of parameterized problems. Then we introduce a new multi-view algorithm, Co-EMT, which combines semi-supervised and active learning. Co-EMT outperforms the other algorithms both on the parameterized problems and on two additional real world domains. Our experiments suggest that Co-EMT\u2019s robustness comes from active learning compensating for the correlation of the views."
            },
            "slug": "Active-+-Semi-supervised-Learning-=-Robust-Learning-Muslea-Minton",
            "title": {
                "fragments": [],
                "text": "Active + Semi-supervised Learning = Robust Multi-View Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A new multi-view algorithm, Co-EMT, which combines semi-supervised and active learning is introduced, which outperforms the other algorithms both on the parameterized problems and on two additional real world domains."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780015"
                        ],
                        "name": "V. D. Sa",
                        "slug": "V.-D.-Sa",
                        "structuredName": {
                            "firstName": "Virginia",
                            "lastName": "Sa",
                            "middleNames": [
                                "R.",
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. D. Sa"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9890353,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5bef9f0d1e74409ca0e67f79f9547c5f4e4e4257",
            "isKey": false,
            "numCitedBy": 222,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the advantages of supervised learning is that the final error metric is available during training. For classifiers, the algorithm can directly reduce the number of misclassifications on the training set. Unfortunately, when modeling human learning or constructing classifiers for autonomous robots, supervisory labels are often not available or too expensive. In this paper we show that we can substitute for the labels by making use of structure between the pattern distributions to different sensory modalities. We show that minimizing the disagreement between the outputs of networks processing patterns from these different modalities is a sensible approximation to minimizing the number of misclassifications in each modality, and leads to similar results. Using the Peterson-Barney vowel dataset we show that the algorithm performs well in finding appropriate placement for the codebook vectors particularly when the confuseable classes are different for the two modalities."
            },
            "slug": "Learning-Classification-with-Unlabeled-Data-Sa",
            "title": {
                "fragments": [],
                "text": "Learning Classification with Unlabeled Data"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper shows that minimizing the disagreement between the outputs of networks processing patterns from these different modalities is a sensible approximation to minimizing the number of misclassifications in each modality, and leads to similar results."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110594"
                        ],
                        "name": "G. Getz",
                        "slug": "G.-Getz",
                        "structuredName": {
                            "firstName": "Gad",
                            "lastName": "Getz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Getz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1787804"
                        ],
                        "name": "N. Shental",
                        "slug": "N.-Shental",
                        "structuredName": {
                            "firstName": "Noam",
                            "lastName": "Shental",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Shental"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2426224"
                        ],
                        "name": "E. Domany",
                        "slug": "E.-Domany",
                        "structuredName": {
                            "firstName": "Eytan",
                            "lastName": "Domany",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Domany"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6987542,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6e144122da27b73a5ca1460e8b5fdf2a5edd50e3",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel approach to semisupervised learning which is based on statistical physics. Most of the former work in the field of semi-supervised learning classifies the points by minimizing a certain energy function, which corresponds to a minimal k-way cut solution. In contrast to these methods, we estimate the distribution of classifications, instead of the sole minimal k-way cut, which yields more accurate and robust results. Our approach may be applied to all energy functions used for semi-supervised learning. The method is based on sampling using a Multicanonical Markov chain Monte-Carlo algorithm, and has a straightforward probabilistic interpretation, which allows for soft assignments of points to classes, and also to cope with yet unseen class types. The suggested approach is demonstrated on a toy data set and on two real-life data sets of gene expression."
            },
            "slug": "Semi-Supervised-Learning-A-Statistical-Physics-Getz-Shental",
            "title": {
                "fragments": [],
                "text": "Semi-Supervised Learning -- A Statistical Physics Approach"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "A novel approach to semisupervised learning which is based on statistical physics, based on sampling using a Multicanonical Markov chain Monte-Carlo algorithm, and has a straightforward probabilistic interpretation, which allows for soft assignments of points to classes, and also to cope with yet unseen class types."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2939803"
                        ],
                        "name": "Ronan Collobert",
                        "slug": "Ronan-Collobert",
                        "structuredName": {
                            "firstName": "Ronan",
                            "lastName": "Collobert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronan Collobert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50095217"
                        ],
                        "name": "Fabian H Sinz",
                        "slug": "Fabian-H-Sinz",
                        "structuredName": {
                            "firstName": "Fabian",
                            "lastName": "Sinz",
                            "middleNames": [
                                "H"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fabian H Sinz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Yarowsky (1995) uses self-training for word sense disambiguation, e."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8291528,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6ce0ff082da032789b16a040c3137585556524c7",
            "isKey": false,
            "numCitedBy": 176,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we study a new framework introduced by Vapnik (1998) and Vapnik (2006) that is an alternative capacity concept to the large margin approach. In the particular case of binary classification, we are given a set of labeled examples, and a collection of \"non-examples\" that do not belong to either class of interest. This collection, called the Universum, allows one to encode prior knowledge by representing meaningful concepts in the same domain as the problem at hand. We describe an algorithm to leverage the Universum by maximizing the number of observed contradictions, and show experimentally that this approach delivers accuracy improvements over using labeled data alone."
            },
            "slug": "Inference-with-the-Universum-Weston-Collobert",
            "title": {
                "fragments": [],
                "text": "Inference with the Universum"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "An algorithm is described to leverage the Universum by maximizing the number of observed contradictions, and it is shown experimentally that this approach delivers accuracy improvements over using labeled data alone."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1808676"
                        ],
                        "name": "V. Sindhwani",
                        "slug": "V.-Sindhwani",
                        "structuredName": {
                            "firstName": "Vikas",
                            "lastName": "Sindhwani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Sindhwani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144106136"
                        ],
                        "name": "S. Keerthi",
                        "slug": "S.-Keerthi",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Keerthi",
                            "middleNames": [
                                "Sathiya"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Keerthi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730609"
                        ],
                        "name": "O. Chapelle",
                        "slug": "O.-Chapelle",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Chapelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Chapelle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2689304,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b7b9d64e05fff6e64b202f2b72859ebe4cf2723d",
            "isKey": false,
            "numCitedBy": 125,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "An intuitive approach to utilizing unlabeled data in kernel-based classification algorithms is to simply treat unknown labels as additional optimization variables. For margin-based loss functions, one can view this approach as attempting to learn low-density separators. However, this is a hard optimization problem to solve in typical semi-supervised settings where unlabeled data is abundant. The popular Transductive SVM algorithm is a label-switching-retraining procedure that is known to be susceptible to local minima. In this paper, we present a global optimization framework for semi-supervised Kernel machines where an easier problem is parametrically deformed to the original hard problem and minimizers are smoothly tracked. Our approach is motivated from deterministic annealing techniques and involves a sequence of convex optimization problems that are exactly and efficiently solved. We present empirical results on several synthetic and real world datasets that demonstrate the effectiveness of our approach."
            },
            "slug": "Deterministic-annealing-for-semi-supervised-kernel-Sindhwani-Keerthi",
            "title": {
                "fragments": [],
                "text": "Deterministic annealing for semi-supervised kernel machines"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This paper presents a global optimization framework for semi-supervised Kernel machines where an easier problem is parametrically deformed to the original hard problem and minimizers are smoothly tracked."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730609"
                        ],
                        "name": "O. Chapelle",
                        "slug": "O.-Chapelle",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Chapelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Chapelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66191239"
                        ],
                        "name": "Bernhard Schlkopf",
                        "slug": "Bernhard-Schlkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Schlkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bernhard Schlkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2281542"
                        ],
                        "name": "A. Zien",
                        "slug": "A.-Zien",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Zien",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Zien"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 60860751,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5ee8a371fc5adc5469435020a52fb815f3b57a71",
            "isKey": false,
            "numCitedBy": 2539,
            "numCiting": 473,
            "paperAbstract": {
                "fragments": [],
                "text": "In the field of machine learning, semi-supervised learning (SSL) occupies the middle ground, between supervised learning (in which all training examples are labeled) and unsupervised learning (in which no label data are given). Interest in SSL has increased in recent years, particularly because of application domains in which unlabeled data are plentiful, such as images, text, and bioinformatics. This first comprehensive overview of SSL presents state-of-the-art algorithms, a taxonomy of the field, selected applications, benchmark experiments, and perspectives on ongoing and future research. Semi-Supervised Learning first presents the key assumptions and ideas underlying the field: smoothness, cluster or low-density separation, manifold structure, and transduction. The core of the book is the presentation of SSL methods, organized according to algorithmic strategies. After an examination of generative models, the book describes algorithms that implement the low-density separation assumption, graph-based methods, and algorithms that perform two-step learning. The book then discusses SSL applications and offers guidelines for SSL practitioners by analyzing the results of extensive benchmark experiments. Finally, the book looks at interesting directions for SSL research. The book closes with a discussion of the relationship between semi-supervised learning and transduction. Adaptive Computation and Machine Learning series"
            },
            "slug": "Semi-Supervised-Learning-Chapelle-Schlkopf",
            "title": {
                "fragments": [],
                "text": "Semi-Supervised Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This first comprehensive overview of semi-supervised learning presents state-of-the-art algorithms, a taxonomy of the field, selected applications, benchmark experiments, and perspectives on ongoing and future research."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145624000"
                        ],
                        "name": "Zhi-Hua Zhou",
                        "slug": "Zhi-Hua-Zhou",
                        "structuredName": {
                            "firstName": "Zhi-Hua",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhi-Hua Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2150636778"
                        ],
                        "name": "Jun-Ming Xu",
                        "slug": "Jun-Ming-Xu",
                        "structuredName": {
                            "firstName": "Jun-Ming",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jun-Ming Xu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15076400,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "79a8334eb8393be100503b3d7b8f27dab2181528",
            "isKey": false,
            "numCitedBy": 170,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "Multi-instance learning and semi-supervised learning are different branches of machine learning. The former attempts to learn from a training set consists of labeled bags each containing many unlabeled instances; the latter tries to exploit abundant unlabeled instances when learning with a small number of labeled examples. In this paper, we establish a bridge between these two branches by showing that multi-instance learning can be viewed as a special case of semi-supervised learning. Based on this recognition, we propose the MissSVM algorithm which addresses multi-instance learning using a special semi-supervised support vector machine. Experiments show that solving multi-instance problems from the view of semi-supervised learning is feasible, and the MissSVM algorithm is competitive with state-of-the-art multi-instance learning algorithms."
            },
            "slug": "On-the-relation-between-multi-instance-learning-and-Zhou-Xu",
            "title": {
                "fragments": [],
                "text": "On the relation between multi-instance learning and semi-supervised learning"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The MissSVM algorithm is proposed which addresses multi- instance learning using a special semi-supervised support vector machine and is competitive with state-of-the-art multi-instance learning algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '07"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1808676"
                        ],
                        "name": "V. Sindhwani",
                        "slug": "V.-Sindhwani",
                        "structuredName": {
                            "firstName": "Vikas",
                            "lastName": "Sindhwani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Sindhwani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770745"
                        ],
                        "name": "P. Niyogi",
                        "slug": "P.-Niyogi",
                        "structuredName": {
                            "firstName": "Partha",
                            "lastName": "Niyogi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Niyogi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145520115"
                        ],
                        "name": "Mikhail Belkin",
                        "slug": "Mikhail-Belkin",
                        "structuredName": {
                            "firstName": "Mikhail",
                            "lastName": "Belkin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mikhail Belkin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10331842,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5098221cf78ba60b5afd26c171da50baf2670996",
            "isKey": false,
            "numCitedBy": 381,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "The Co-Training algorithm uses unlabeled examples in multiple views to bootstrap classifiers in each view, typically in a greedy manner, and operating under assumptions of view-independence and compatibility. In this paper, we propose a Co-Regularization framework where classifiers are learnt in each view through forms of multi-view regularization. We propose algorithms within this framework that are based on optimizing measures of agreement and smoothness over labeled and unlabeled examples. These algorithms naturally extend standard regularization methods like Support Vector Machines (SVM) and Regularized Least squares (RLS) for multi-view semi-supervised learning, and inherit their benefits and applicability to high-dimensional classification problems. An empirical investigation is presented that confirms the promise of this approach."
            },
            "slug": "A-Co-Regularization-Approach-to-Semi-supervised-Sindhwani-Niyogi",
            "title": {
                "fragments": [],
                "text": "A Co-Regularization Approach to Semi-supervised Learning with Multiple Views"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper proposes a Co-Regularization framework where classifiers are learnt in each view through forms of multi-view regularization, and proposes algorithms within this framework that are based on optimizing measures of agreement and smoothness over labeled and unlabeled examples."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745410"
                        ],
                        "name": "Maria-Florina Balcan",
                        "slug": "Maria-Florina-Balcan",
                        "structuredName": {
                            "firstName": "Maria-Florina",
                            "lastName": "Balcan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Maria-Florina Balcan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690967"
                        ],
                        "name": "A. Blum",
                        "slug": "A.-Blum",
                        "structuredName": {
                            "firstName": "Avrim",
                            "lastName": "Blum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Blum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143781496"
                        ],
                        "name": "Ke Yang",
                        "slug": "Ke-Yang",
                        "structuredName": {
                            "firstName": "Ke",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ke Yang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 895559,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0eaa981275145025c5064107c9ec7e58fec39f6b",
            "isKey": false,
            "numCitedBy": 279,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Co-training is a method for combining labeled and unlabeled data when examples can be thought of as containing two distinct sets of features. It has had a number of practical successes, yet previous theoretical analyses have needed very strong assumptions on the data that are unlikely to be satisfied in practice. \n \nIn this paper, we propose a much weaker \"expansion\" assumption on the underlying data distribution, that we prove is sufficient for iterative co-training to succeed given appropriately strong PAC-learning algorithms on each feature set, and that to some extent is necessary as well. This expansion assumption in fact motivates the iterative nature of the original co-training algorithm, unlike stronger assumptions (such as independence given the label) that allow a simpler one-shot co-training to succeed. We also heuristically analyze the effect on performance of noise in the data. Predicted behavior is qualitatively matched in synthetic experiments on expander graphs."
            },
            "slug": "Co-Training-and-Expansion:-Towards-Bridging-Theory-Balcan-Blum",
            "title": {
                "fragments": [],
                "text": "Co-Training and Expansion: Towards Bridging Theory and Practice"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A much weaker \"expansion\" assumption on the underlying data distribution is proposed, that is proved to be sufficient for iterative co-training to succeed given appropriately strong PAC-learning algorithms on each feature set, and that to some extent is necessary as well."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109465325"
                        ],
                        "name": "Chi-Hoon Lee",
                        "slug": "Chi-Hoon-Lee",
                        "structuredName": {
                            "firstName": "Chi-Hoon",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chi-Hoon Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50694888"
                        ],
                        "name": "Shaojun Wang",
                        "slug": "Shaojun-Wang",
                        "structuredName": {
                            "firstName": "Shaojun",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaojun Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145644673"
                        ],
                        "name": "Feng Jiao",
                        "slug": "Feng-Jiao",
                        "structuredName": {
                            "firstName": "Feng",
                            "lastName": "Jiao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Feng Jiao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714772"
                        ],
                        "name": "Dale Schuurmans",
                        "slug": "Dale-Schuurmans",
                        "structuredName": {
                            "firstName": "Dale",
                            "lastName": "Schuurmans",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dale Schuurmans"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143686063"
                        ],
                        "name": "R. Greiner",
                        "slug": "R.-Greiner",
                        "structuredName": {
                            "firstName": "Russell",
                            "lastName": "Greiner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Greiner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1446998,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a60bac0369473ac60411144ec74bb075fd8f346c",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel, semi-supervised approach to training discriminative random fields (DRFs) that efficiently exploits labeled and unlabeled training data to achieve improved accuracy in a variety of image processing tasks. We formulate DRF training as a form of MAP estimation that combines conditional loglikelihood on labeled data, given a data-dependent prior, with a conditional entropy regularizer defined on unlabeled data. Although the training objective is no longer concave, we develop an efficient local optimization procedure that produces classifiers that are more accurate than ones based on standard supervised DRF training. We then apply our semi-supervised approach to train DRFs to segment both synthetic and real data sets, and demonstrate significant improvements over supervised DRFs in each case."
            },
            "slug": "Learning-to-Model-Spatial-Dependency:-Random-Fields-Lee-Wang",
            "title": {
                "fragments": [],
                "text": "Learning to Model Spatial Dependency: Semi-Supervised Discriminative Random Fields"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work forms DRF training as a form of MAP estimation that combines conditional loglikelihood on labeled data, given a data-dependent prior, with a conditional entropy regularizer defined on unlabeled data, and develops an efficient local optimization procedure that produces classifiers that are more accurate than ones based on standard supervised DRFs."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51204489"
                        ],
                        "name": "T. D. Bie",
                        "slug": "T.-D.-Bie",
                        "structuredName": {
                            "firstName": "Tijl",
                            "lastName": "Bie",
                            "middleNames": [
                                "De"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. D. Bie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685083"
                        ],
                        "name": "N. Cristianini",
                        "slug": "N.-Cristianini",
                        "structuredName": {
                            "firstName": "Nello",
                            "lastName": "Cristianini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cristianini"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 629,
                                "start": 3
                            }
                        ],
                        "text": "De Bie and Cristianini (2006a) present an SDP relaxation of the normalized graph cut problem, including a series of relaxations between spectra l relaxations above, and SDP relaxation. The SDP formulation can easily include pa rtial label or constraint information, and therefore applicable for transducti ve lassification. The data points are mapped into a new space spanned by the first k eigenvectors of the normalized Laplacian in (Ng et al., 2001), with special normalization . Clustering is then performed with traditional methods (like k-means) in this new space. This is very similar to kernel PCA. Fowlkes et al. (2004) use the Nystr \u00f6m method to reduce the computation cost for large spectral clustering problems."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 475,
                                "start": 3
                            }
                        ],
                        "text": "De Bie and Cristianini (De Bie & Cristianini, 2004; De Bie & Cristianini, 2006b) relax the TSVM training problem, and transductive learning proble ms in general to semi-definite programming (SDP). The basic idea is to work with the binary label matrix of rank 1, and relax it by a positive semi-definite matrix witho ut the rank constraint. The paper also includes a speech up trick to solve media n-sized problems with around 1000 unlabeled points. Xu and Schuurmans (2005) p resent a similar multi-class version of SDP formulation, which results in multi-class SVM for semi-supervised learning."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 795,
                                "start": 3
                            }
                        ],
                        "text": "De Bie and Cristianini (2006a) present an SDP relaxation of the normalized graph cut problem, including a series of relaxations between spectra l relaxations above, and SDP relaxation. The SDP formulation can easily include pa rtial label or constraint information, and therefore applicable for transducti ve lassification. The data points are mapped into a new space spanned by the first k eigenvectors of the normalized Laplacian in (Ng et al., 2001), with special normalization . Clustering is then performed with traditional methods (like k-means) in this new space. This is very similar to kernel PCA. Fowlkes et al. (2004) use the Nystr \u00f6m method to reduce the computation cost for large spectral clustering problems. This is related to the method in (Zhu, 20 05) Chapter 10. Chung (1997) presents the mathematical details of spectral graph theory."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 3
                            }
                        ],
                        "text": "De Bie and Cristianini (2006a) present an SDP relaxation of the normalized graph cut problem, including a series of relaxations between spectra l relaxations above, and SDP relaxation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1208015,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "22a2a99643109ba39ad427a41deaa30fb36bcf12",
            "isKey": true,
            "numCitedBy": 121,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "The 2-class transduction problem, as formulated by Vapnik [1], involves finding a separating hyperplane for a labelled data set that is also maximally distant from a given set of unlabelled test points. In this form, the problem has exponential computational complexity in the size of the working set. So far it has been attacked by means of integer programming techniques [2] that do not scale to reasonable problem sizes, or by local search procedures [3]. In this paper we present a relaxation of this task based on semi-definite programming (SDP), resulting in a convex optimization problem that has polynomial complexity in the size of the data set. The results are very encouraging for mid sized data sets, however the cost is still too high for large scale problems, due to the high dimensional search space. To this end, we restrict the feasible region by introducing an approximation based on solving an eigenproblem. With this approximation, the computational cost of the algorithm is such that problems with more than 1000 points can be treated."
            },
            "slug": "Convex-Methods-for-Transduction-Bie-Cristianini",
            "title": {
                "fragments": [],
                "text": "Convex Methods for Transduction"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This paper presents a relaxation of the 2-class transduction problem based on semi-definite programming (SDP), resulting in a convex optimization problem that has polynomial complexity in the size of the data set."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712517"
                        ],
                        "name": "L. Rabiner",
                        "slug": "L.-Rabiner",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Rabiner",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Rabiner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 193,
                                "start": 178
                            }
                        ],
                        "text": "1 Generative Models One example of generative models for semi-supervised sequence learnin g is the Hidden Markov Model (HMM), in particular the Baum-Welsh HMM training algorithm (Rabiner, 1989)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13618539,
            "fieldsOfStudy": [
                "Geology"
            ],
            "id": "8fe2ea0a67954f1380b3387e3262f1cdb9f9b3e5",
            "isKey": false,
            "numCitedBy": 24804,
            "numCiting": 98,
            "paperAbstract": {
                "fragments": [],
                "text": "The fabric comprises a novel type of netting which will have particular utility in screening out mosquitoes and like insects and pests. The fabric is defined of voids having depth as well as width and length. The fabric is usable as a material from which to form clothing for wear, or bed coverings, or sleeping bags, etc., besides use simply as a netting."
            },
            "slug": "A-Tutorial-on-Hidden-Markov-Models-and-Selected-Rabiner",
            "title": {
                "fragments": [],
                "text": "A Tutorial on Hidden Markov Models and Selected Applications"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The fabric comprises a novel type of netting which will have particular utility in screening out mosquitoes and like insects and pests."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145624000"
                        ],
                        "name": "Zhi-Hua Zhou",
                        "slug": "Zhi-Hua-Zhou",
                        "structuredName": {
                            "firstName": "Zhi-Hua",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhi-Hua Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40655383"
                        ],
                        "name": "Ming Li",
                        "slug": "Ming-Li",
                        "structuredName": {
                            "firstName": "Ming",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming Li"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 0
                            }
                        ],
                        "text": "Zhu et al. (2003c) propose that new test point be classified by its neare st neighbor inL\u222aU ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8574005,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fa4fac1e42c402c093ddcabe9653adaaf2eb2449",
            "isKey": false,
            "numCitedBy": 59,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "The traditional setting of supervised learning requires a large amount of labeled training examples in order to achieve good generalization. However, in many practical applications, unlabeled training examples are readily available but labeled ones are fairly expensive to obtain. Therefore, semi-supervised learning has attracted much attention. Previous research on semi-supervised learning mainly focuses on semi-supervised classification. Although regression is almost as important as classification, semisupervised regression is largely understudied. In particular, although co-training is a main paradigm in semi-supervised learning, few works has been devoted to co-training style semi-supervised regression algorithms. In this paper, a co-training style semi-supervised regression algorithm, i.e. COREG, is proposed. This algorithm uses two regressors each labels the unlabeled data for the other regressor, where the confidence in labeling an unlabeled example is estimated through the amount of reduction in mean square error over the labeled neighborhood of that example. Analysis and experiments show that COREG can effectively exploit unlabeled data to improve regression estimates."
            },
            "slug": "Semi-Supervised-Regression-with-Co-Training-Style-Zhou-Li",
            "title": {
                "fragments": [],
                "text": "Semi-Supervised Regression with Co-Training Style Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "Analysis and experiments show that COREG can effectively exploit unlabeled data to improve regression estimates and is proposed as a co-training style semi-supervised regression algorithm."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2703148"
                        ],
                        "name": "Mingrui Wu",
                        "slug": "Mingrui-Wu",
                        "structuredName": {
                            "firstName": "Mingrui",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mingrui Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18501843,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6366a51c794b675976a6953421ebf908f45f1a02",
            "isKey": false,
            "numCitedBy": 145,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "The idea of local learning, classifying a particular point based on its neighbors, has been successfully applied to supervised learning problems. In this paper, we adapt it for Transductive Classification (TC) problems. Specifically, we formulate a Local Learning Regularizer (LL-Reg) which leads to a solution with the property that the label of each data point can be well predicted based on its neighbors and their labels. For model selection, an efficient way to compute the leave-one-out classification error is provided for the proposed and related algorithms. Experimental results using several benchmark datasets illustrate the effectiveness of the proposed approach."
            },
            "slug": "Transductive-Classification-via-Local-Learning-Wu-Sch\u00f6lkopf",
            "title": {
                "fragments": [],
                "text": "Transductive Classification via Local Learning Regularization"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A Local Learning Regularizer (LL-Reg) is formulated which leads to a solution with the property that the label of each data point can be well predicted based on its neighbors and their labels."
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1816048"
                        ],
                        "name": "S. Rosset",
                        "slug": "S.-Rosset",
                        "structuredName": {
                            "firstName": "Saharon",
                            "lastName": "Rosset",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Rosset"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "115816494"
                        ],
                        "name": "Ji Zhu",
                        "slug": "Ji-Zhu",
                        "structuredName": {
                            "firstName": "Ji",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ji Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48861660"
                        ],
                        "name": "H. Zou",
                        "slug": "H.-Zou",
                        "structuredName": {
                            "firstName": "Hui",
                            "lastName": "Zou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Zou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784682"
                        ],
                        "name": "T. Hastie",
                        "slug": "T.-Hastie",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Hastie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Hastie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1433153,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "07d5a672c7770341d6464e9203585e1023e4a351",
            "isKey": false,
            "numCitedBy": 46,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the situation in semi-supervised learning, where the \"label sampling\" mechanism stochastically depends on the true response (as well as potentially on the features). We suggest a method of moments for estimating this stochastic dependence using the unlabeled data. This is potentially useful for two distinct purposes: a. As an input to a supervised learning procedure which can be used to \"de-bias\" its results using labeled data only and b. As a potentially interesting learning task in itself. We present several examples to illustrate the practical usefulness of our method."
            },
            "slug": "A-Method-for-Inferring-Label-Sampling-Mechanisms-in-Rosset-Zhu",
            "title": {
                "fragments": [],
                "text": "A Method for Inferring Label Sampling Mechanisms in Semi-Supervised Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "A method of moments for estimating this stochastic dependence using the unlabeled data in semi-supervised learning, where the \"label sampling\" mechanism stochastically depends on the true response (as well as potentially on the features)."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145115014"
                        ],
                        "name": "Corinna Cortes",
                        "slug": "Corinna-Cortes",
                        "structuredName": {
                            "firstName": "Corinna",
                            "lastName": "Cortes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Corinna Cortes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "81080659"
                        ],
                        "name": "M. Mohri",
                        "slug": "M.-Mohri",
                        "structuredName": {
                            "firstName": "Mehryar",
                            "lastName": "Mohri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mohri"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Cozman et al. (2003) give a formal derivation on how this might happen."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6794505,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e75591901afd4a1868ff2e0a1e161592102ccff1",
            "isKey": false,
            "numCitedBy": 118,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "In many modern large-scale learning applications, the amount of unlabeled data far exceeds that of labeled data. A common instance of this problem is the transductive setting where the unlabeled test points are known to the learning algorithm. This paper presents a study of regression problems in that setting. It presents explicit VC-dimension error bounds for transductive regression that hold for all bounded loss functions and coincide with the tight classification bounds of Vapnik when applied to classification. It also presents a new transductive regression algorithm inspired by our bound that admits a primal and kernelized closed-form solution and deals efficiently with large amounts of unlabeled data. The algorithm exploits the position of unlabeled points to locally estimate their labels and then uses a global optimization to ensure robust predictions. Our study also includes the results of experiments with several publicly available regression data sets with up to 20,000 unlabeled examples. The comparison with other transductive regression algorithms shows that it performs well and that it can scale to large data sets."
            },
            "slug": "On-Transductive-Regression-Cortes-Mohri",
            "title": {
                "fragments": [],
                "text": "On Transductive Regression"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper presents explicit VC-dimension error bounds for transductive regression that hold for all bounded loss functions and coincide with the tight classification bounds of Vapnik when applied to classification."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2114075909"
                        ],
                        "name": "Kai Yu",
                        "slug": "Kai-Yu",
                        "structuredName": {
                            "firstName": "Kai",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai Yu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1589402,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "a601be73d1abff89ef86a40d01c380bc2f82770f",
            "isKey": false,
            "numCitedBy": 14,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we consider supervised learning on large-scale graphs, which is highly demanding in terms of time and memory costs. We demonstrate that, if a graph has a bipartite structure that contains a small set of nodes separating the remaining from each other, the inference can be equivalently done over an induced graph connecting only the separators. Since each separator influences a certain neighborhood, the method essentially explores the block structure of graphs to improve the scalability. In the next step, instead of identifying the bipartite structure in a given graph, which is often difficult, we propose to construct a set of separators via two methods, one is adjacency matrix factorization and the other is mixture models, which both naturally ends up with a bipartite graph and meanwhile preserves the original data structure. Finally we report results of experiments on a toy problem and an intrusion detection problem."
            },
            "slug": "Blockwise-Supervised-Inference-on-Large-Graphs-Yu",
            "title": {
                "fragments": [],
                "text": "Blockwise Supervised Inference on Large Graphs"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "It is demonstrated that, if a graph has a bipartite structure that contains a small set of nodes separating the remaining from each other, the inference can be equivalently done over an induced graph connecting only the separators."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144848317"
                        ],
                        "name": "Glenn Fung",
                        "slug": "Glenn-Fung",
                        "structuredName": {
                            "firstName": "Glenn",
                            "lastName": "Fung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Glenn Fung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747026"
                        ],
                        "name": "O. Mangasarian",
                        "slug": "O.-Mangasarian",
                        "structuredName": {
                            "firstName": "Olvi",
                            "lastName": "Mangasarian",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Mangasarian"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5320604,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fced0bfd90bd624876762dd6bfacb992e5ed3b27",
            "isKey": false,
            "numCitedBy": 211,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "A concave minimization approach is proposed for classifying unlabeled data based on the following ideas: (i) A small representative percentage (5% to 10%) of the unlabeled data is chosen by a clustering algorithm and given to an expert or oracle to label, (ii) A linear support vector machine is trained using the small labeled sample while simultaneously assigning the remaining bulk of the unlabeled dataset to one of two classes so as to maximize the margin (distance) between the two bounding planes that determine the separating plane midway between them. This latter problem is formulated as a concave minimization problem on a polyhedral set for which a stationary point is quickly obtained by solving a few (5 to 7) linear programs. Such stationary points turn out to be very effective as evidenced by our computational results which show that clustered concave minimization yields: (a) Test set improvement as high as 20.4% over a linear support vector machine trained on a correspondingly small but randomly chosen subset that is labeled by an expert. (b) Test set correctness averaged to within 5.1% when compared to that of a completely supervised linear support vector machine trained on the entire dataset which has been labeled by an expert."
            },
            "slug": "Semi-superyised-support-vector-machines-for-data-Fung-Mangasarian",
            "title": {
                "fragments": [],
                "text": "Semi-superyised support vector machines for unlabeled data classification"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Computational results show that clustered concave minimization yields test set improvement as high as 20.4% over a linear support vector machine trained on a correspondingly small but randomly chosen subset that is labeled by an expert."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48584242"
                        ],
                        "name": "M. K\u00e4\u00e4ri\u00e4inen",
                        "slug": "M.-K\u00e4\u00e4ri\u00e4inen",
                        "structuredName": {
                            "firstName": "Matti",
                            "lastName": "K\u00e4\u00e4ri\u00e4inen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. K\u00e4\u00e4ri\u00e4inen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Kaariainen (2005) presents another generalization error bound for semi-supervised learning."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14784619,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c1d94dff8762a0f4c26fd9f3b1f0debe53161995",
            "isKey": false,
            "numCitedBy": 59,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We present two new methods for obtaining generalization error bounds in a semi-supervised setting. Both methods are based on approximating the disagreement probability of pairs of classifiers using unlabeled data. The first method works in the realizable case. It suggests how the ERM principle can be refined using unlabeled data and has provable optimality guarantees when the number of unlabeled examples is large. Furthermore, the technique extends easily to cover active learning. A downside is that the method is of little use in practice due to its limitation to the realizable case. The idea in our second method is to use unlabeled data to transform bounds for randomized classifiers into bounds for simpler deterministic classifiers. As a concrete example of how the general method works in practice, we apply it to a bound based on cross-validation. The result is a semi-supervised bound for classifiers learned based on all the labeled data. The bound is easy to implement and apply and should be tight whenever cross-validation makes sense. Applying the bound to SVMs on the MNIST benchmark data set gives results that suggest that the bound may be tight enough to be useful in practice."
            },
            "slug": "Generalization-Error-Bounds-Using-Unlabeled-Data-K\u00e4\u00e4ri\u00e4inen",
            "title": {
                "fragments": [],
                "text": "Generalization Error Bounds Using Unlabeled Data"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "Two new methods for obtaining generalization error bounds in a semi-supervised setting based on approximating the disagreement probability of pairs of classifiers using unlabeled data are presented."
            },
            "venue": {
                "fragments": [],
                "text": "COLT"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145624000"
                        ],
                        "name": "Zhi-Hua Zhou",
                        "slug": "Zhi-Hua-Zhou",
                        "structuredName": {
                            "firstName": "Zhi-Hua",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhi-Hua Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721819"
                        ],
                        "name": "De-Chuan Zhan",
                        "slug": "De-Chuan-Zhan",
                        "structuredName": {
                            "firstName": "De-Chuan",
                            "lastName": "Zhan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "De-Chuan Zhan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152290618"
                        ],
                        "name": "Qiang Yang",
                        "slug": "Qiang-Yang",
                        "structuredName": {
                            "firstName": "Qiang",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qiang Yang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11692761,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0e937b39d9863b388bc89ac5a757d11808112646",
            "isKey": false,
            "numCitedBy": 144,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "In semi-supervised learning, a number of labeled examples are usually required for training an initial weakly useful predictor which is in turn used for exploiting the unlabeled examples. However, in many real-world applications there may exist very few labeled training examples, which makes the weakly useful predictor difficult to generate, and therefore these semisupervised learning methods cannot be applied. This paper proposes a method working under a two-view setting. By taking advantages of the correlations between the views using canonical component analysis, the proposed method can perform semi-supervised learning with only one labeled training example. Experiments and an application to content-based image retrieval validate the effectiveness of the proposed method."
            },
            "slug": "Semi-Supervised-Learning-with-Very-Few-Labeled-Zhou-Zhan",
            "title": {
                "fragments": [],
                "text": "Semi-Supervised Learning with Very Few Labeled Training Examples"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "By taking advantages of the correlations between the views using canonical component analysis, the proposed method can perform semi-supervised learning with only one labeled training example."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802711"
                        ],
                        "name": "Yves Grandvalet",
                        "slug": "Yves-Grandvalet",
                        "structuredName": {
                            "firstName": "Yves",
                            "lastName": "Grandvalet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yves Grandvalet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7890982,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ad67ccee45b801b0138016e2f44a566344e77320",
            "isKey": false,
            "numCitedBy": 1275,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the semi-supervised learning problem, where a decision rule is to be learned from labeled and unlabeled data. In this framework, we motivate minimum entropy regularization, which enables to incorporate unlabeled data in the standard supervised learning. Our approach includes other approaches to the semi-supervised problem as particular or limiting cases. A series of experiments illustrates that the proposed solution benefits from unlabeled data. The method challenges mixture models when the data are sampled from the distribution class spanned by the generative model. The performances are definitely in favor of minimum entropy regularization when generative models are misspecified, and the weighting of unlabeled data provides robustness to the violation of the \"cluster assumption\". Finally, we also illustrate that the method can also be far superior to manifold learning in high dimension spaces."
            },
            "slug": "Semi-supervised-Learning-by-Entropy-Minimization-Grandvalet-Bengio",
            "title": {
                "fragments": [],
                "text": "Semi-supervised Learning by Entropy Minimization"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This framework, which motivates minimum entropy regularization, enables to incorporate unlabeled data in the standard supervised learning, and includes other approaches to the semi-supervised problem as particular or limiting cases."
            },
            "venue": {
                "fragments": [],
                "text": "CAP"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690967"
                        ],
                        "name": "A. Blum",
                        "slug": "A.-Blum",
                        "structuredName": {
                            "firstName": "Avrim",
                            "lastName": "Blum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Blum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714122"
                        ],
                        "name": "M. Rwebangira",
                        "slug": "M.-Rwebangira",
                        "structuredName": {
                            "firstName": "Mugizi",
                            "lastName": "Rwebangira",
                            "middleNames": [
                                "Robert"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Rwebangira"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37341510"
                        ],
                        "name": "Rajashekar Reddy",
                        "slug": "Rajashekar-Reddy",
                        "structuredName": {
                            "firstName": "Rajashekar",
                            "lastName": "Reddy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rajashekar Reddy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 807019,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f4563e793ef639480e915e34b2f4788a27f9c344",
            "isKey": false,
            "numCitedBy": 272,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "In many application domains there is a large amount of unlabeled data but only a very limited amount of labeled training data. One general approach that has been explored for utilizing this unlabeled data is to construct a graph on all the data points based on distance relationships among examples, and then to use the known labels to perform some type of graph partitioning. One natural partitioning to use is the minimum cut that agrees with the labeled data (Blum & Chawla, 2001), which can be thought of as giving the most probable label assignment if one views labels as generated according to a Markov Random Field on the graph. Zhu et al. (2003) propose a cut based on a relaxation of this field, and Joachims (2003) gives an algorithm based on finding an approximate min-ratio cut.In this paper, we extend the mincut approach by adding randomness to the graph structure. The resulting algorithm addresses several short-comings of the basic mincut approach, and can be given theoretical justification from both a Markov random field perspective and from sample complexity considerations. In cases where the graph does not have small cuts for a given classification problem, randomization may not help. However, our experiments on several datasets show that when the structure of the graph supports small cuts, this can result in highly accurate classifiers with good accuracy/coverage tradeoffs. In addition, we are able to achieve good performance with a very simple graph-construction procedure."
            },
            "slug": "Semi-supervised-learning-using-randomized-mincuts-Blum-Lafferty",
            "title": {
                "fragments": [],
                "text": "Semi-supervised learning using randomized mincuts"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The experiments on several datasets show that when the structure of the graph supports small cuts, this can result in highly accurate classifiers with good accuracy/coverage tradeoffs, and can be given theoretical justification from both a Markov random field perspective and from sample complexity considerations."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745410"
                        ],
                        "name": "Maria-Florina Balcan",
                        "slug": "Maria-Florina-Balcan",
                        "structuredName": {
                            "firstName": "Maria-Florina",
                            "lastName": "Balcan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Maria-Florina Balcan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690967"
                        ],
                        "name": "A. Blum",
                        "slug": "A.-Blum",
                        "structuredName": {
                            "firstName": "Avrim",
                            "lastName": "Blum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Blum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 184,
                                "start": 0
                            }
                        ],
                        "text": "Balcan et al. (2005b) relax th e conditional independence assumption with a much weaker expansion condition, and justif y the iterative co-training procedure. Johnson and Zhang (2007) prop ose a two-view model that relaxes the conditional independence assumption."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 0
                            }
                        ],
                        "text": "Balcan et al. (2005a) build gra phs for video surveillance using strong domain knowledge, where the graph of we bcam images consists of time edges, color edges and face edges."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 651,
                                "start": 0
                            }
                        ],
                        "text": "Balcan et al. (2005a) build gra phs for video surveillance using strong domain knowledge, where the graph of we bcam images consists of time edges, color edges and face edges. Such graphs reflect a deep understanding of the problem structure and how unlabeled data is ex pect d to help. Carreira-Perpinan and Zemel (2005) build robust graphs from multiple minimum spanning trees by perturbation and edge removal. Wang and Zhang (2 006) perform an operation very similar to locally linear embedding (LLE) on the data points first, but constraining the LLE weights to be non-negative. These w ights are then used as graph weights. Hein and Maier (2006) propose an algorithm to de-noise points sampled from a manifold."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 17
                            }
                        ],
                        "text": "In another work, Balcan et al. (2005a) constru ct a graph on webcam images using temporal links (as well as color, face similarity links) for"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1189,
                                "start": 0
                            }
                        ],
                        "text": "Balcan et al. (2005a) build gra phs for video surveillance using strong domain knowledge, where the graph of we bcam images consists of time edges, color edges and face edges. Such graphs reflect a deep understanding of the problem structure and how unlabeled data is ex pect d to help. Carreira-Perpinan and Zemel (2005) build robust graphs from multiple minimum spanning trees by perturbation and edge removal. Wang and Zhang (2 006) perform an operation very similar to locally linear embedding (LLE) on the data points first, but constraining the LLE weights to be non-negative. These w ights are then used as graph weights. Hein and Maier (2006) propose an algorithm to de-noise points sampled from a manifold. That is, data points are assumed to be noisy samples of some unknown underlying manifold. They used the de noising algorithm as a prepro c ssing step for graph-based semi-supervised learning, so that the graph c an be constructed from better separated data points. Such preprocessing results in better semi-supervised classification accuracy. When using a Gaussian function as edge weights, the bandwidth of the Gaus sian needs to be carefully chosen. Zhang and Lee (2006) derive a cr oss validation approach to tune the bandwidth for each feature dimension, by minimizing the leave-one-out mean squared error of predictions and given labels on labeled points."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 0
                            }
                        ],
                        "text": "Balcan et al. (2005b) relax th e conditional independence assumption with a much weaker expansion condition, and justif y the iterative co-training procedure."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 322,
                                "start": 0
                            }
                        ],
                        "text": "Balcan et al. (2005a) build gra phs for video surveillance using strong domain knowledge, where the graph of we bcam images consists of time edges, color edges and face edges. Such graphs reflect a deep understanding of the problem structure and how unlabeled data is ex pect d to help. Carreira-Perpinan and Zemel (2005) build robust graphs from multiple minimum spanning trees by perturbation and edge removal."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5170310,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "59be86632a3bcc2d24c3bde1c0dfa5d325552c5c",
            "isKey": true,
            "numCitedBy": 113,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "There has been growing interest in practice in using unlabeled data together with labeled data in machine learning, and a number of different approaches have been developed. However, the assumptions these methods are based on are often quite distinct and not captured by standard theoretical models. In this paper we describe a PAC-style framework that can be used to model many of these assumptions, and analyze sample-complexity issues in this setting: that is, how much of each type of data one should expect to need in order to learn well, and what are the basic quantities that these numbers depend on. Our model can be viewed as an extension of the standard PAC model, where in addition to a concept class C, one also proposes a type of compatibility that one believes the target concept should have with the underlying distribution. In this view, unlabeled data can be helpful because it allows one to estimate compatibility over the space of hypotheses, and reduce the size of the search space to those that, according to one's assumptions, are a-priori reasonable with respect to the distribution. We discuss a number of technical issues that arise in this context, and provide sample-complexity bounds both for uniform convergence and e-cover based algorithms. We also consider algorithmic issues, and give an efficient algorithm for a special case of co-training."
            },
            "slug": "A-PAC-Style-Model-for-Learning-from-Labeled-and-Balcan-Blum",
            "title": {
                "fragments": [],
                "text": "A PAC-Style Model for Learning from Labeled and Unlabeled Data"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper describes a PAC-style framework that can be used to model many of these assumptions, and analyzes sample-complexity issues in this setting: that is, how much of each type of data one should expect to need in order to learn well, and what are the basic quantities that these numbers depend on."
            },
            "venue": {
                "fragments": [],
                "text": "COLT"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685978"
                        ],
                        "name": "B. Taskar",
                        "slug": "B.-Taskar",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Taskar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Taskar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730156"
                        ],
                        "name": "Carlos Guestrin",
                        "slug": "Carlos-Guestrin",
                        "structuredName": {
                            "firstName": "Carlos",
                            "lastName": "Guestrin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carlos Guestrin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736370"
                        ],
                        "name": "D. Koller",
                        "slug": "D.-Koller",
                        "structuredName": {
                            "firstName": "Daphne",
                            "lastName": "Koller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Koller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 201720,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0c450531e1121cfb657be5195e310217a4675397",
            "isKey": false,
            "numCitedBy": 1477,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "In typical classification tasks, we seek a function which assigns a label to a single object. Kernel-based approaches, such as support vector machines (SVMs), which maximize the margin of confidence of the classifier, are the method of choice for many such tasks. Their popularity stems both from the ability to use high-dimensional feature spaces, and from their strong theoretical guarantees. However, many real-world tasks involve sequential, spatial, or structured data, where multiple labels must be assigned. Existing kernel-based methods ignore structure in the problem, assigning labels independently to each object, losing much useful information. Conversely, probabilistic graphical models, such as Markov networks, can represent correlations between labels, by exploiting problem structure, but cannot handle high-dimensional feature spaces, and lack strong theoretical generalization guarantees. In this paper, we present a new framework that combines the advantages of both approaches: Maximum margin Markov (M3) networks incorporate both kernels, which efficiently deal with high-dimensional features, and the ability to capture correlations in structured data. We present an efficient algorithm for learning M3 networks based on a compact quadratic program formulation. We provide a new theoretical bound for generalization in structured domains. Experiments on the task of handwritten character recognition and collective hypertext classification demonstrate very significant gains over previous approaches."
            },
            "slug": "Max-Margin-Markov-Networks-Taskar-Guestrin",
            "title": {
                "fragments": [],
                "text": "Max-Margin Markov Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "Maximum margin Markov (M3) networks incorporate both kernels, which efficiently deal with high-dimensional features, and the ability to capture correlations in structured data, and a new theoretical bound for generalization in structured domains is provided."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145520115"
                        ],
                        "name": "Mikhail Belkin",
                        "slug": "Mikhail-Belkin",
                        "structuredName": {
                            "firstName": "Mikhail",
                            "lastName": "Belkin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mikhail Belkin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770745"
                        ],
                        "name": "P. Niyogi",
                        "slug": "P.-Niyogi",
                        "structuredName": {
                            "firstName": "Partha",
                            "lastName": "Niyogi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Niyogi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1808676"
                        ],
                        "name": "V. Sindhwani",
                        "slug": "V.-Sindhwani",
                        "structuredName": {
                            "firstName": "Vikas",
                            "lastName": "Sindhwani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Sindhwani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5896838,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e626de498fd5638456af1943443cb7ca5e12435d",
            "isKey": false,
            "numCitedBy": 312,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a family of learning algorithms based on a new form of regularization that allows us to exploit the geometry of the marginal distribution. We focus on a semi-supervised framework that incorporates labeled and unlabeled data in a general-purpose learner. Some transductive graph learning algorithms and standard methods including Support Vector Machines and Regularized Least Squares can be obtained as special cases. We utilize properties of Reproducing Kernel Hilbert spaces to prove new Representer theorems that provide theoretical basis for the algorithms. As a result (in contrast to purely graph based approaches) we obtain a natural out-of-sample extension to novel examples and so are able to handle both transductive and truly semi-supervised settings. We present experimental evidence suggesting that our semi-supervised algorithms are able to use unlabeled data effectively. Finally we have a brief discussion of unsupervised and fully supervised learning within our general framework."
            },
            "slug": "Manifold-Regularization-:-A-Geometric-Framework-for-Belkin-Niyogi",
            "title": {
                "fragments": [],
                "text": "Manifold Regularization : A Geometric Framework for Learning from Examples"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A semi-supervised framework that incorporates labeled and unlabeled data in a general-purpose learner is focused on and properties of Reproducing Kernel Hilbert spaces are utilized to prove new Representer theorems that provide theoretical basis for the algorithms."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38070424"
                        ],
                        "name": "R. Ando",
                        "slug": "R.-Ando",
                        "structuredName": {
                            "firstName": "Rie",
                            "lastName": "Ando",
                            "middleNames": [
                                "Kubota"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Ando"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117881943"
                        ],
                        "name": "Tong Zhang",
                        "slug": "Tong-Zhang",
                        "structuredName": {
                            "firstName": "Tong",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tong Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6008493,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6100169e81d2172375279dcc9700a9a4b186d481",
            "isKey": false,
            "numCitedBy": 101,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider a setting for discriminative semi-supervised learning where unlabeled data are used with a generative model to learn effective feature representations for discriminative training. Within this framework, we revisit the two-view feature generation model of co-training and prove that the optimum predictor can be expressed as a linear combination of a few features constructed from unlabeled data. From this analysis, we derive methods that employ two views but are very different from co-training. Experiments show that our approach is more robust than co-training and EM, under various data generation conditions."
            },
            "slug": "Two-view-feature-generation-model-for-learning-Ando-Zhang",
            "title": {
                "fragments": [],
                "text": "Two-view feature generation model for semi-supervised learning"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The two-view feature generation model of co-training is revisited and it is proved that the optimum predictor can be expressed as a linear combination of a few features constructed from unlabeled data."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '07"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1832364"
                        ],
                        "name": "Xiaojin Zhu",
                        "slug": "Xiaojin-Zhu",
                        "structuredName": {
                            "firstName": "Xiaojin",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaojin Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744700"
                        ],
                        "name": "Zoubin Ghahramani",
                        "slug": "Zoubin-Ghahramani",
                        "structuredName": {
                            "firstName": "Zoubin",
                            "lastName": "Ghahramani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zoubin Ghahramani"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 204018731,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8b4a99762d33927e4db312082f9552cce1df9182",
            "isKey": false,
            "numCitedBy": 544,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Active and semi-supervised learning are important techniques when labeled data are scarce. We combine the two under a Gaussian random field model. Labeled and unlabeled data are represented as vertices in a weighted graph, with edge weights encoding the similarity between instances. The semi-supervised learning problem is then formulated in terms of a Gaussian random field on this graph, the mean of which is characterized in terms of harmonic functions. Active learning is performed on top of the semisupervised learning scheme by greedily selecting queries from the unlabeled data to minimize the estimated expected classification error (risk); in the case of Gaussian fields the risk is efficiently computed using matrix methods. We present experimental results on synthetic data, handwritten digit recognition, and text classification tasks. The active learning scheme requires a much smaller number of queries to achieve high accuracy compared with random query selection."
            },
            "slug": "Combining-active-learning-and-semi-supervised-using-Zhu-Lafferty",
            "title": {
                "fragments": [],
                "text": "Combining active learning and semi-supervised learning using Gaussian fields and harmonic functions"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "This work combines active and semi-supervised learning techniques under a Gaussian random field model, which requires a much smaller number of queries to achieve high accuracy compared with random query selection."
            },
            "venue": {
                "fragments": [],
                "text": "ICML 2003"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38070424"
                        ],
                        "name": "R. Ando",
                        "slug": "R.-Ando",
                        "structuredName": {
                            "firstName": "Rie",
                            "lastName": "Ando",
                            "middleNames": [
                                "Kubota"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Ando"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117881943"
                        ],
                        "name": "Tong Zhang",
                        "slug": "Tong-Zhang",
                        "structuredName": {
                            "firstName": "Tong",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tong Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 101
                            }
                        ],
                        "text": "One can also perform PCA on the unlabeled data, and use the resulting low dimensional representation Ando and Zhang (2005); Johnson and Zhang (2007) build on a two-vie w feature generation framework, where the input features form two subsets w ith a feature split x = (z1, z2)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13650160,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "944e1a7b2c5c62e952418d7684e3cade89c76f87",
            "isKey": false,
            "numCitedBy": 1414,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the most important issues in machine learning is whether one can improve the performance of a supervised learning algorithm by including unlabeled data. Methods that use both labeled and unlabeled data are generally referred to as semi-supervised learning. Although a number of such methods are proposed, at the current stage, we still don't have a complete understanding of their effectiveness. This paper investigates a closely related problem, which leads to a novel approach to semi-supervised learning. Specifically we consider learning predictive structures on hypothesis spaces (that is, what kind of classifiers have good predictive power) from multiple learning tasks. We present a general framework in which the structural learning problem can be formulated and analyzed theoretically, and relate it to learning with unlabeled data. Under this framework, algorithms for structural learning will be proposed, and computational issues will be investigated. Experiments will be given to demonstrate the effectiveness of the proposed algorithms in the semi-supervised learning setting."
            },
            "slug": "A-Framework-for-Learning-Predictive-Structures-from-Ando-Zhang",
            "title": {
                "fragments": [],
                "text": "A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper presents a general framework in which the structural learning problem can be formulated and analyzed theoretically, and relate it to learning with unlabeled data, and algorithms for structural learning will be proposed, and computational issues will be investigated."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2979876"
                        ],
                        "name": "R. Raina",
                        "slug": "R.-Raina",
                        "structuredName": {
                            "firstName": "Rajat",
                            "lastName": "Raina",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Raina"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2078284037"
                        ],
                        "name": "Alexis Battle",
                        "slug": "Alexis-Battle",
                        "structuredName": {
                            "firstName": "Alexis",
                            "lastName": "Battle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexis Battle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697141"
                        ],
                        "name": "Honglak Lee",
                        "slug": "Honglak-Lee",
                        "structuredName": {
                            "firstName": "Honglak",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Honglak Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1409971380"
                        ],
                        "name": "Ben Packer",
                        "slug": "Ben-Packer",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Packer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ben Packer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6692382,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b3852f0113fcf8a3913c55ae92393ae6ccde347e",
            "isKey": false,
            "numCitedBy": 1611,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new machine learning framework called \"self-taught learning\" for using unlabeled data in supervised classification tasks. We do not assume that the unlabeled data follows the same class labels or generative distribution as the labeled data. Thus, we would like to use a large number of unlabeled images (or audio samples, or text documents) randomly downloaded from the Internet to improve performance on a given image (or audio, or text) classification task. Such unlabeled data is significantly easier to obtain than in typical semi-supervised or transfer learning settings, making self-taught learning widely applicable to many practical learning problems. We describe an approach to self-taught learning that uses sparse coding to construct higher-level features using the unlabeled data. These features form a succinct input representation and significantly improve classification performance. When using an SVM for classification, we further show how a Fisher kernel can be learned for this representation."
            },
            "slug": "Self-taught-learning:-transfer-learning-from-data-Raina-Battle",
            "title": {
                "fragments": [],
                "text": "Self-taught learning: transfer learning from unlabeled data"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "An approach to self-taught learning that uses sparse coding to construct higher-level features using the unlabeled data to form a succinct input representation and significantly improve classification performance."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '07"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2561045"
                        ],
                        "name": "Gholamreza Haffari",
                        "slug": "Gholamreza-Haffari",
                        "structuredName": {
                            "firstName": "Gholamreza",
                            "lastName": "Haffari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gholamreza Haffari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3028658"
                        ],
                        "name": "Anoop Sarkar",
                        "slug": "Anoop-Sarkar",
                        "structuredName": {
                            "firstName": "Anoop",
                            "lastName": "Sarkar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anoop Sarkar"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 904344,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9b6a78f2ab866c47d37dbaf1e2d0452438fd2cda",
            "isKey": false,
            "numCitedBy": 73,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "The Yarowsky algorithm is a rule-based semi-supervised learning algorithm that has been successfully applied to some problems in computational linguistics. The algorithm was not mathematically well understood until (Abney 2004) which analyzed some specific variants of the algorithm, and also proposed some new algorithms for bootstrapping. In this paper, we extend Abney's work and show that some of his proposed algorithms actually optimize (an upper-bound on) an objective function based on a new definition of cross-entropy which is based on a particular instantiation of the Bregman distance between probability distributions. Moreover, we suggest some new algorithms for rule-based semi-supervised learning and show connections with harmonic functions and minimum multi-way cuts in graph-based semi-supervised learning."
            },
            "slug": "Analysis-of-Semi-Supervised-Learning-with-the-Haffari-Sarkar",
            "title": {
                "fragments": [],
                "text": "Analysis of Semi-Supervised Learning with the Yarowsky Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper extends Abney's work and shows that some of his proposed algorithms actually optimize an objective function based on a new definition of cross-entropy which isbased on a particular instantiation of the Bregman distance between probability distributions."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783941"
                        ],
                        "name": "Y. Altun",
                        "slug": "Y.-Altun",
                        "structuredName": {
                            "firstName": "Yasemin",
                            "lastName": "Altun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Altun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145689002"
                        ],
                        "name": "David A. McAllester",
                        "slug": "David-A.-McAllester",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "McAllester",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David A. McAllester"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145520115"
                        ],
                        "name": "Mikhail Belkin",
                        "slug": "Mikhail-Belkin",
                        "structuredName": {
                            "firstName": "Mikhail",
                            "lastName": "Belkin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mikhail Belkin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17137268,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f83288f12b1ea85f3166be427d04a77c04de0330",
            "isKey": false,
            "numCitedBy": 116,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Many real-world classification problems involve the prediction of multiple inter-dependent variables forming some structural dependency. Recent progress in machine learning has mainly focused on supervised classification of such structured variables. In this paper, we investigate structured classification in a semi-supervised setting. We present a discriminative approach that utilizes the intrinsic geometry of input patterns revealed by unlabeled data points and we derive a maximum-margin formulation of semi-supervised learning for structured variables. Unlike transductive algorithms, our formulation naturally extends to new test points."
            },
            "slug": "Maximum-Margin-Semi-Supervised-Learning-for-Altun-McAllester",
            "title": {
                "fragments": [],
                "text": "Maximum Margin Semi-Supervised Learning for Structured Variables"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper presents a discriminative approach that utilizes the intrinsic geometry of input patterns revealed by unlabeled data points and derives a maximum-margin formulation of semi-supervised learning for structured variables."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS 2005"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796044"
                        ],
                        "name": "L. Saul",
                        "slug": "L.-Saul",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Saul",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Saul"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9330607"
                        ],
                        "name": "S. Roweis",
                        "slug": "S.-Roweis",
                        "structuredName": {
                            "firstName": "Sam",
                            "lastName": "Roweis",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roweis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2071866,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8db95dbd08e4ee64fb258e5380e78cfa507ed94d",
            "isKey": false,
            "numCitedBy": 1611,
            "numCiting": 85,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of dimensionality reduction arises in many fields of information processing, including machine learning, data compression, scientific visualization, pattern recognition, and neural computation. Here we describe locally linear embedding (LLE), an unsupervised learning algorithm that computes low dimensional, neighborhood preserving embeddings of high dimensional data. The data, assumed to be sampled from an underlying manifold, are mapped into a single global coordinate system of lower dimensionality. The mapping is derived from the symmetries of locally linear reconstructions, and the actual computation of the embedding reduces to a sparse eigenvalue problem. Notably, the optimizations in LLE---though capable of generating highly nonlinear embeddings---are simple to implement, and they do not involve local minima. In this paper, we describe the implementation of the algorithm in detail and discuss several extensions that enhance its performance. We present results of the algorithm applied to data sampled from known manifolds, as well as to collections of images of faces, lips, and handwritten digits. These examples are used to provide extensive illustrations of the algorithm's performance---both successes and failures---and to relate the algorithm to previous and ongoing work in nonlinear dimensionality reduction."
            },
            "slug": "Think-Globally,-Fit-Locally:-Unsupervised-Learning-Saul-Roweis",
            "title": {
                "fragments": [],
                "text": "Think Globally, Fit Locally: Unsupervised Learning of Low Dimensional Manifold"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Locally linear embedding (LLE), an unsupervised learning algorithm that computes low dimensional, neighborhood preserving embeddings of high dimensional data, is described and several extensions that enhance its performance are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1832364"
                        ],
                        "name": "Xiaojin Zhu",
                        "slug": "Xiaojin-Zhu",
                        "structuredName": {
                            "firstName": "Xiaojin",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaojin Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145903504"
                        ],
                        "name": "R. Rosenfeld",
                        "slug": "R.-Rosenfeld",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Rosenfeld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rosenfeld"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "A comparison of three iterative methods: label propagation, conjugate gradient and loopy belief propagation is presented in (Zhu, 2005) Appendix F."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "2 of (Zhu, 2005) uses entropy minimization."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The issue has been discussed in (Zhu, 2005) Chapter 3 and Chapter 7."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "author\u2019s doctoral thesis (Zhu, 2005)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This is related to the method in (Zhu, 2005) Chapter 10."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The mean is known as a harmonic function, which has many interesting properties (Zhu, 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60708159,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3bc4736f9b8512043ed47357a81f26b93a1204b6",
            "isKey": false,
            "numCitedBy": 606,
            "numCiting": 174,
            "paperAbstract": {
                "fragments": [],
                "text": "In traditional machine learning approaches to classification, one uses only a labeled set to train the classifier. Labeled instances however are often difficult, expensive, or time consuming to obtain, as they require the efforts of experienced human annotators. Meanwhile unlabeled data may be relatively easy to collect, but there has been few ways to use them. Semi-supervised learning addresses this problem by using large amount of unlabeled data, together with the labeled data, to build better classifiers. Because semi-supervised learning requires less human effort and gives higher accuracy, it is of great interest both in theory and in practice. \nWe present a series of novel semi-supervised learning approaches arising from a graph representation, where labeled and unlabeled instances are represented as vertices, and edges encode the similarity between instances. They address the following questions: How to use unlabeled data? (label propagation); What is the probabilistic interpretation? (Gaussian fields and harmonic functions); What if we can choose labeled data? (active learning); How to construct good graphs? (hyperparameter learning); How to work with kernel machines like SVM? (graph kernels); How to handle complex data like sequences? (kernel conditional random fields); How to handle scalability and induction? (harmonic mixtures). An extensive literature review is included at the end."
            },
            "slug": "Semi-supervised-learning-with-graphs-Zhu-Lafferty",
            "title": {
                "fragments": [],
                "text": "Semi-supervised learning with graphs"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "A series of novel semi-supervised learning approaches arising from a graph representation, where labeled and unlabeled instances are represented as vertices, and edges encode the similarity between instances are presented."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2279864"
                        ],
                        "name": "J. Garcke",
                        "slug": "J.-Garcke",
                        "structuredName": {
                            "firstName": "Jochen",
                            "lastName": "Garcke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Garcke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1803879"
                        ],
                        "name": "M. Griebel",
                        "slug": "M.-Griebel",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Griebel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Griebel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Goldman and Zhou (2000) use two learners of different type but both takes the whole feature set, and essentially use one learner\u2019s high confidence data points, identified with a set of statistical tests, in U to teach the other learning and vice versa."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15823535,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "595ef2328bb502adb1ec5ff8a36282570e48146e",
            "isKey": false,
            "numCitedBy": 44,
            "numCiting": 80,
            "paperAbstract": {
                "fragments": [],
                "text": "Sparse grids were recently introduced for classification and regression problems. In this article we apply the sparse grid approach to semi-supervised classification. We formulate the semi-supervised learning problem by a regularization approach. Here, besides a regression formulation for the labeled data, an additional term is involved which is based on the graph Laplacian for an adjacency graph of all, labeled and unlabeled data points. It reflects the intrinsic geometric structure of the data distribution. We discretize the resulting problem in function space by the sparse grid method and solve the arising equations using the so-called combination technique. In contrast to recently proposed kernel based methods which currently scale cubic in regard to the number of overall data, our method scales only linear, provided that a sparse graph Laplacian is used. This allows to deal with huge data sets which involve millions of points. We show experimental results with the new approach."
            },
            "slug": "Semi-supervised-learning-with-sparse-grids-Garcke-Griebel",
            "title": {
                "fragments": [],
                "text": "Semi-supervised learning with sparse grids"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This article formulate the semi-supervised learning problem by a regularization approach and discretize the resulting problem in function space by the sparse grid method and solve the arising equations using the so-called combination technique."
            },
            "venue": {
                "fragments": [],
                "text": "ICML 2005"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2672785"
                        ],
                        "name": "Nizar Grira",
                        "slug": "Nizar-Grira",
                        "structuredName": {
                            "firstName": "Nizar",
                            "lastName": "Grira",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nizar Grira"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719698"
                        ],
                        "name": "M. Crucianu",
                        "slug": "M.-Crucianu",
                        "structuredName": {
                            "firstName": "Michel",
                            "lastName": "Crucianu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Crucianu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7238091,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "acb799150e4cd3a75fe79ca262047612bd8e5194",
            "isKey": false,
            "numCitedBy": 271,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "Clustering (or cluster analysis) aims to organize a collection of data items into clusters, such that items within a cluster are more \u201csimilar\u201d to each other than they are to items in the other clusters. This notion of similarity can be expressed in very different ways, according to the purpose of the study, to domain-specific assumptions and to prior knowledge of the problem. Clustering is usually performed when no information is available concerning the membership of data items to predefined classes. For this reason, clustering is traditionally seen as part of unsupervised learning. We nevertheless speak here of unsupervised clustering to distinguish it from a more recent and less common approach that makes use of a small amount of supervision to \u201cguide\u201d or \u201cadjust\u201d clustering (see section 2). To support the extensive use of clustering in computer vision, pattern recognition, information retrieval, data mining, etc., very many different methods were developed in several communities. Detailed surveys of this domain can be found in [25], [27] or [26]. In the following, we attempt to briefly review a few core concepts of cluster analysis and describe categories of clustering methods that are best represented in the literature. We also take this opportunity to provide some pointers to more recent work on clustering."
            },
            "slug": "Unsupervised-and-Semi-supervised-Clustering-:-a-\u2217-Grira-Crucianu",
            "title": {
                "fragments": [],
                "text": "Unsupervised and Semi-supervised Clustering : a Brief Survey \u2217"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The word clustering is spoken here of unsupervised clustering to distinguish it from a more recent and less common approach that makes use of a small amount of supervision to \u201cguide\u201d or \u201cadjust\u201d clustering."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690967"
                        ],
                        "name": "A. Blum",
                        "slug": "A.-Blum",
                        "structuredName": {
                            "firstName": "Avrim",
                            "lastName": "Blum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Blum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144135485"
                        ],
                        "name": "Tom. Mitchell",
                        "slug": "Tom.-Mitchell",
                        "structuredName": {
                            "firstName": "Tom.",
                            "lastName": "Mitchell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tom. Mitchell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 207228399,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "278841ab0cb24c1abcb75e363aeed1fa741c8cc4",
            "isKey": false,
            "numCitedBy": 5471,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of using a large unlabeled sample to boost performance of a learning algorit,hrn when only a small set of labeled examples is available. In particular, we consider a problem setting motivated by the task of learning to classify web pages, in which the description of each example can be partitioned into two distinct views. For example, the description of a web page can be partitioned into the words occurring on that page, and the words occurring in hyperlinks t,hat point to that page. We assume that either view of the example would be sufficient for learning if we had enough labeled data, but our goal is to use both views together to allow inexpensive unlabeled data to augment, a much smaller set of labeled examples. Specifically, the presence of two distinct views of each example suggests strategies in which two learning algorithms are trained separately on each view, and then each algorithm\u2019s predictions on new unlabeled examples are used to enlarge the training set of the other. Our goal in this paper is to provide a PAC-style analysis for this setting, and, more broadly, a PAC-style framework for the general problem of learning from both labeled and unlabeled data. We also provide empirical results on real web-page data indicating that this use of unlabeled examples can lead to significant improvement of hypotheses in practice. *This research was supported in part by the DARPA HPKB program under contract F30602-97-1-0215 and by NSF National Young investigator grant CCR-9357793. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. TO copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. COLT 98 Madison WI USA Copyright ACM 1998 l-58113-057--0/98/ 7...%5.00 92 Tom Mitchell School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213-3891 mitchell+@cs.cmu.edu"
            },
            "slug": "Combining-labeled-and-unlabeled-data-with-Blum-Mitchell",
            "title": {
                "fragments": [],
                "text": "Combining labeled and unlabeled data with co-training"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A PAC-style analysis is provided for a problem setting motivated by the task of learning to classify web pages, in which the description of each example can be partitioned into two distinct views, to allow inexpensive unlabeled data to augment, a much smaller set of labeled examples."
            },
            "venue": {
                "fragments": [],
                "text": "COLT' 98"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145300792"
                        ],
                        "name": "Charles Kemp",
                        "slug": "Charles-Kemp",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Kemp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charles Kemp"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1799860"
                        ],
                        "name": "T. Griffiths",
                        "slug": "T.-Griffiths",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Griffiths",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Griffiths"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2602565"
                        ],
                        "name": "Sean Stromsten",
                        "slug": "Sean-Stromsten",
                        "structuredName": {
                            "firstName": "Sean",
                            "lastName": "Stromsten",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sean Stromsten"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763295"
                        ],
                        "name": "J. Tenenbaum",
                        "slug": "J.-Tenenbaum",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Tenenbaum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tenenbaum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1303107,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1a8bed1f13ff4b7b3ae4eedee25a17f7ad2583eb",
            "isKey": false,
            "numCitedBy": 71,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a nonparametric Bayesian approach to generalizing from few labeled examples, guided by a larger set of unlabeled objects and the assumption of a latent tree-structure to the domain. The tree (or a distribution over trees) may be inferred using the unlabeled data. A prior over concepts generated by a mutation process on the inferred tree(s) allows efficient computation of the optimal Bayesian classification function from the labeled examples. We test our approach on eight real-world datasets."
            },
            "slug": "Semi-Supervised-Learning-with-Trees-Kemp-Griffiths",
            "title": {
                "fragments": [],
                "text": "Semi-Supervised Learning with Trees"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A nonparametric Bayesian approach to generalizing from few labeled examples, guided by a larger set of unlabeled objects and the assumption of a latent tree-structure to the domain is described."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714772"
                        ],
                        "name": "Dale Schuurmans",
                        "slug": "Dale-Schuurmans",
                        "structuredName": {
                            "firstName": "Dale",
                            "lastName": "Schuurmans",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dale Schuurmans"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2716445"
                        ],
                        "name": "F. Southey",
                        "slug": "F.-Southey",
                        "structuredName": {
                            "firstName": "Finnegan",
                            "lastName": "Southey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Southey"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Sindhwani and Keerthi (2006) proposed a fast algorithm for linear S3VMs, suitable for large scale text applications."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2605508,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "094271a91510105dd01eb3b8cf97a5f210b1c938",
            "isKey": false,
            "numCitedBy": 49,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a general approach to model selection and regularization that exploits unlabeled data to adaptively control hypothesis complexity in supervised learning tasks. The idea is to impose a metric structure on hypotheses by determining the discrepancy between their predictions across the distribution of unlabeled data. We show how this metric can be used to detect untrustworthy training error estimates, and devise novel model selection strategies that exhibit theoretical guarantees against over-fitting (while still avoiding under-fitting). We then extend the approach to derive a general training criterion for supervised learning\u2014yielding an adaptive regularization method that uses unlabeled data to automatically set regularization parameters. This new criterion adjusts its regularization level to the specific set of training data received, and performs well on a variety of regression and conditional density estimation tasks. The only proviso for these methods is that sufficient unlabeled training data be available."
            },
            "slug": "Metric-Based-Methods-for-Adaptive-Model-Selection-Schuurmans-Southey",
            "title": {
                "fragments": [],
                "text": "Metric-Based Methods for Adaptive Model Selection and Regularization"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "A general approach to model selection and regularization that exploits unlabeled data to adaptively control hypothesis complexity in supervised learning tasks and derives a general training criterion for supervised learning that adjusts its regularization level to the specific set of training data received, and performs well on a variety of regression and conditional density estimation tasks."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40975594"
                        ],
                        "name": "Tom Michael Mitchell",
                        "slug": "Tom-Michael-Mitchell",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Mitchell",
                            "middleNames": [
                                "Michael"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tom Michael Mitchell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 36
                            }
                        ],
                        "text": "Co-training (Blum & Mitchell, 1998) (Mitchell, 1999) assumes that features c an be split into two sets; Each sub-feature set is sufficient to train a good clas sifier; The two sets are conditionally independent given the class."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1167,
                                "start": 20
                            }
                        ],
                        "text": "Co-training (Blum & Mitchell, 1998) (Mitchell, 1999) assumes that features c an be split into two sets; Each sub-feature set is sufficient to train a good clas sifier; The two sets are conditionally independent given the class. Initially two sepa rat classifiers are trained with the labeled data, on the two sub-feature sets res pectively. Each classifier then classifies the unlabeled data, and \u2018teaches\u2019 the other classifier with the few unlabeled examples (and the predicted labels) they feel most con fident. Each classifier is retrained with the additional training examples given b y the other classifier, and the process repeats. In co-training, unlabeled data helps by reducing the version space size. In other words, the two classifiers (or hypotheses) must agree on the much larger unlabeled data as well as the labeled data. We need the assumption that sub-features are sufficiently good, so that w e c n trust the labels by each learner on U . We need the sub-features to be conditionally independent so that one classifier\u2019s high confident data points are iid samples for the other classifier. Figure 4 visualizes the assumption. Nigam and Ghani (2000) perform extensive empirical experiments to comp are"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9487658,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f1a93c99341f8ac28d2b7e19532294e16458a091",
            "isKey": false,
            "numCitedBy": 188,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Most computational models of supervised learning rely only on labeled training examples, and ignore the possible role of unlabeled data. This is true both for cognitive science models of learning such as SOAR [Newell 1990] and ACT\u2013R [Anderson, et al. 1995], and for machine learning and data mining algorithms such as decision tree learning and inductive logic programming (see, e.g., [Mitchell 1997]). In this paper we consider the potential role of unlabeled data in supervised learning. We present an algorithm and experimental results demonstrating that unlabeled data can significantly improve learning accuracy in certain practical problems. We then identify the abstract problem structure that enables the algorithm to successfully utilize this unlabeled data, and prove that unlabeled data will boost learning accuracy for problems in this class. The problem class we identify includes problems where the features describing the examples are redundantly sufficient for classifying the example; a notion we make precise in this paper. This problem class includes many natural learning problems faced by humans, such as learning a semantic lexicon over noun phrases in natural language, and learning to recognize objects from multiple sensor inputs. We argue that models of human and animal learning should consider more strongly the potential role of unlabeled data, and that many natural learning problems fit the class we identify."
            },
            "slug": "The-Role-of-Unlabeled-Data-in-Supervised-Learning-Mitchell",
            "title": {
                "fragments": [],
                "text": "The Role of Unlabeled Data in Supervised Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "It is argued that models of human and animal learning should consider more strongly the potential role of unlabeled data, and that many natural learning problems fit the problem class identified in this paper."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1778989"
                        ],
                        "name": "M. Szummer",
                        "slug": "M.-Szummer",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Szummer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Szummer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132120"
                        ],
                        "name": "T. Jaakkola",
                        "slug": "T.-Jaakkola",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Jaakkola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jaakkola"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9743839,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8e6779bb55f7fbed5684ded55df51747ea678a84",
            "isKey": false,
            "numCitedBy": 669,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "To classify a large number of unlabeled examples we combine a limited number of labeled examples with a Markov random walk representation over the unlabeled examples. The random walk representation exploits any low dimensional structure in the data in a robust, probabilistic manner. We develop and compare several estimation criteria/algorithms suited to this representation. This includes in particular multi-way classification with an average margin criterion which permits a closed form solution. The time scale of the random walk regularizes the representation and can be set through a margin-based criterion favoring unambiguous classification. We also extend this basic regularization by adapting time scales for individual examples. We demonstrate the approach on synthetic examples and on text classification problems."
            },
            "slug": "Partially-labeled-classification-with-Markov-random-Szummer-Jaakkola",
            "title": {
                "fragments": [],
                "text": "Partially labeled classification with Markov random walks"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "This work combines a limited number of labeled examples with a Markov random walk representation over the unlabeled examples and develops and compares several estimation criteria/algorithms suited to this representation."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32150954"
                        ],
                        "name": "S. Goldman",
                        "slug": "S.-Goldman",
                        "structuredName": {
                            "firstName": "Sally",
                            "lastName": "Goldman",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Goldman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2150921406"
                        ],
                        "name": "Yan Zhou",
                        "slug": "Yan-Zhou",
                        "structuredName": {
                            "firstName": "Yan",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yan Zhou"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Recently Grady and Funka-Lea (2004) applied the harmonic function method to medical image segmentation tasks, where a user labels classes (e."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1215747,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ea4feb953b86f6a099d61ffa70d21c59be99f76a",
            "isKey": false,
            "numCitedBy": 530,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "In a wide variety of supervised learning scenarios, there is a small set of labeled data, along with a large pool of unlabeled data. In this thesis, we present a new semi-supervised learning method called co-learning that is designed to use unlabeled data to enhance standard supervised learning algorithms. The idea is that two or more standard supervised learning algorithms can leverage off the fact that they have different representations of the hypotheses and they are likely to detect different patterns in labeled data. We also design an active co-learning strategy to bootstrap our co-leaning procedure when the originally labeled data set is too small to provide accurate confidence estimate for the learned hypotheses. We provide a priority sampling technique as the selection component in our active co-learning method. We evaluate our co-learning algorithms on several datasets from a commonly used data repository in the machine learning community. We also test our co-learning method on text categorization. The contribution of this research is to put forward a new semi-supervised learning approach for learning with a small number of labeled examples, and explore the applicability of our co-learning strategy in real world applications."
            },
            "slug": "Enhancing-Supervised-Learning-with-Unlabeled-Data-Goldman-Zhou",
            "title": {
                "fragments": [],
                "text": "Enhancing Supervised Learning with Unlabeled Data"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "A new semi-supervised learning method called co-learning that is designed to use unlabeled data to enhance standard supervised learning algorithms to leverage off the fact that they have different representations of the hypotheses and are likely to detect different patterns in labeled data."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "24982365"
                        ],
                        "name": "Dengyong Zhou",
                        "slug": "Dengyong-Zhou",
                        "structuredName": {
                            "firstName": "Dengyong",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dengyong Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698617"
                        ],
                        "name": "O. Bousquet",
                        "slug": "O.-Bousquet",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Bousquet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Bousquet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2312432"
                        ],
                        "name": "T. N. Lal",
                        "slug": "T.-N.-Lal",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Lal",
                            "middleNames": [
                                "Navin"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. N. Lal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 378,
                                "start": 49
                            }
                        ],
                        "text": "For semi-supervised learning on directed graphs, Zhou et al. (2005b) take a hub - authority approach and essentially convert a directed graph into an undirected one. Two hub nodes are connected by an undirected edge with appropr iate weight if they co-link to authority nodes, and vice versa. Semi-supervised learnin g then proceeds on the undirected graph. Zhou et al. (2005a) generalize the work further."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 49
                            }
                        ],
                        "text": "For semi-supervised learning on directed graphs, Zhou et al. (2005b) take a hub - authority approach and essentially convert a directed graph into an undirected one."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 508435,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "46770a8e7e2af28f5253e5961f709be74e34c1f6",
            "isKey": false,
            "numCitedBy": 3895,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the general problem of learning from labeled and unlabeled data, which is often called semi-supervised learning or transductive inference. A principled approach to semi-supervised learning is to design a classifying function which is sufficiently smooth with respect to the intrinsic structure collectively revealed by known labeled and unlabeled points. We present a simple algorithm to obtain such a smooth solution. Our method yields encouraging experimental results on a number of classification problems and demonstrates effective use of unlabeled data."
            },
            "slug": "Learning-with-Local-and-Global-Consistency-Zhou-Bousquet",
            "title": {
                "fragments": [],
                "text": "Learning with Local and Global Consistency"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A principled approach to semi-supervised learning is to design a classifying function which is sufficiently smooth with respect to the intrinsic structure collectively revealed by known labeled and unlabeled points."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48173155"
                        ],
                        "name": "Jason D. R. Farquhar",
                        "slug": "Jason-D.-R.-Farquhar",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Farquhar",
                            "middleNames": [
                                "D.",
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jason D. R. Farquhar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763894"
                        ],
                        "name": "D. Hardoon",
                        "slug": "D.-Hardoon",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Hardoon",
                            "middleNames": [
                                "Roi"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Hardoon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37192632"
                        ],
                        "name": "H. Meng",
                        "slug": "H.-Meng",
                        "structuredName": {
                            "firstName": "Hongying",
                            "lastName": "Meng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Meng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404459229"
                        ],
                        "name": "J. Shawe-Taylor",
                        "slug": "J.-Shawe-Taylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shawe-Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shawe-Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2540580"
                        ],
                        "name": "S. Szedm\u00e1k",
                        "slug": "S.-Szedm\u00e1k",
                        "structuredName": {
                            "firstName": "S\u00e1ndor",
                            "lastName": "Szedm\u00e1k",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Szedm\u00e1k"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 326,
                                "start": 97
                            }
                        ],
                        "text": "However, the experiment uses a single positive labeled e xampl and no negative labeled examples, making it a one-class setting similar to novelty detection or quantile estimation instead of binary classification. In addition, the fish stimulus is a familiar real-world concept which might induce prior bias. Zhu et al. (2007) show that human binary classification behavior confor ms well to a generative model (Gaussian Mixture Models) for semi-supervised lea rning."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6938954,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "38a93d6d335ba266979534d7f8ee279f439b7af3",
            "isKey": false,
            "numCitedBy": 327,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Kernel methods make it relatively easy to define complex high-dimensional feature spaces. This raises the question of how we can identify the relevant subspaces for a particular learning task. When two views of the same phenomenon are available kernel Canonical Correlation Analysis (KCCA) has been shown to be an effective preprocessing step that can improve the performance of classification algorithms such as the Support Vector Machine (SVM). This paper takes this observation to its logical conclusion and proposes a method that combines this two stage learning (KCCA followed by SVM) into a single optimisation termed SVM-2K. We present both experimental and theoretical analysis of the approach showing encouraging results and insights."
            },
            "slug": "Two-view-learning:-SVM-2K,-Theory-and-Practice-Farquhar-Hardoon",
            "title": {
                "fragments": [],
                "text": "Two view learning: SVM-2K, Theory and Practice"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper proposes a method that combines this two stage learning (KCCA followed by SVM) into a single optimisation termed SVM-2K and presents both experimental and theoretical analysis of the approach showing encouraging results and insights."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730609"
                        ],
                        "name": "O. Chapelle",
                        "slug": "O.-Chapelle",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Chapelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Chapelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703272"
                        ],
                        "name": "M. Chi",
                        "slug": "M.-Chi",
                        "structuredName": {
                            "firstName": "Mingmin",
                            "lastName": "Chi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Chi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2281542"
                        ],
                        "name": "A. Zien",
                        "slug": "A.-Zien",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Zien",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Zien"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 39
                            }
                        ],
                        "text": "A book on semi-supervised learning is (Chapelle et al., 2006)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1603238,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a036987b90ecce3dcac92c183728b4247e66d498",
            "isKey": false,
            "numCitedBy": 159,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Semi-Supervised Support Vector Machines (S3VMs) are an appealing method for using unlabeled data in classification: their objective function favors decision boundaries which do not cut clusters. However their main problem is that the optimization problem is non-convex and has many local minima, which often results in suboptimal performances. In this paper we propose to use a global optimization technique known as continuation to alleviate this problem. Compared to other algorithms minimizing the same objective function, our continuation method often leads to lower test errors."
            },
            "slug": "A-continuation-method-for-semi-supervised-SVMs-Chapelle-Chi",
            "title": {
                "fragments": [],
                "text": "A continuation method for semi-supervised SVMs"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper proposes to use a global optimization technique known as continuation to alleviate the problem of non-convex optimization of S3VMs, which often results in suboptimal performances."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056999643"
                        ],
                        "name": "Wei Tong",
                        "slug": "Wei-Tong",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Tong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Tong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144723884"
                        ],
                        "name": "Rong Jin",
                        "slug": "Rong-Jin",
                        "structuredName": {
                            "firstName": "Rong",
                            "lastName": "Jin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rong Jin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14463728,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0b2c37c2fa9e9848f750c779545e4237b91aec33",
            "isKey": false,
            "numCitedBy": 48,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent studies have shown that graph-based approaches are effective for semi-supervised learning. The key idea behind many graph-based approaches is to enforce the consistency between the class assignment of unlabeled examples and the pairwise similarity between examples. One major limitation with most graph-based approaches is that they are unable to explore dissimilarity or negative similarity. This is because the dissimilar relation is not transitive, and therefore is difficult to be propagated. Furthermore, negative similarity could result in unbounded energy functions, which makes most graph-based algorithms unapplicable. In this paper, we propose a new graph-based approach, termed as \"mixed label propagation\" which is able to effectively explore both similarity and dissimilarity simultaneously. In particular, the new framework determines the assignment of class labels by (1) minimizing the energy function associated with positive similarity, and (2) maximizing the energy function associated with negative similarity. Our empirical study with collaborative filtering shows promising performance of the proposed approach."
            },
            "slug": "Semi-Supervised-Learning-by-Mixed-Label-Propagation-Tong-Jin",
            "title": {
                "fragments": [],
                "text": "Semi-Supervised Learning by Mixed Label Propagation"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper proposes a new graph-based approach, termed as \"mixed label propagation\" which is able to effectively explore both similarity and dissimilarity simultaneously, and determines the assignment of class labels by minimizing the energy function associated with positive similarity, and maximizing the energyfunction associated with negative similarity."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2676309"
                        ],
                        "name": "C. Burges",
                        "slug": "C.-Burges",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Burges",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burges"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144189092"
                        ],
                        "name": "John C. Platt",
                        "slug": "John-C.-Platt",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Platt",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John C. Platt"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Burges and Platt (2005) propose a directed graphical model, called Conditional Harmonic Mixing, that is somewhat between graph-based semi-supervised learning and standard Bayes nets."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 124982552,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4ecf556d7a0e51e53a14c8025e08fcf6fc3c0f50",
            "isKey": false,
            "numCitedBy": 25,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently graph-based algorithms, in which nodes represent data points and links encode similarities, have become popular for semi-supervised learning. In this chapter we introduce a general probabilistic formulation called `Conditional Harmonic Mixing\u2019, in which the links are directed, a conditional probability matrix is associated with each link, and where the numbers of classes can vary from node to node. The posterior class probability at each node is updated by minimizing the KL divergence between its distribution and that predicted by its neighbours. We show that for arbitrary graphs, as long as each unlabeled point is reachable from at least one training point, a solution always exists, is unique, and can be found by solving a sparse linear system iteratively. This result holds even if the graph contains loops, or if the conditional probability matrices are not consistent. We show how, given a classifier for a task, CHM can learn its transition probabilities. Using the Reuters database, we show that CHM improves the accuracy of the best available classifier, for small training set sizes."
            },
            "slug": "Semi-Supervised-Learning-with-Conditional-Harmonic-Burges-Platt",
            "title": {
                "fragments": [],
                "text": "Semi-Supervised Learning with Conditional Harmonic Mixing"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This chapter introduces a general probabilistic formulation called `Conditional Harmonic Mixing\u2019, in which the links are directed, a conditional probability matrix is associated with each link, and where the numbers of classes can vary from node to node."
            },
            "venue": {
                "fragments": [],
                "text": "Semi-Supervised Learning"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1911526"
                        ],
                        "name": "A. Corduneanu",
                        "slug": "A.-Corduneanu",
                        "structuredName": {
                            "firstName": "Adrian",
                            "lastName": "Corduneanu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Corduneanu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132120"
                        ],
                        "name": "T. Jaakkola",
                        "slug": "T.-Jaakkola",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Jaakkola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jaakkola"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1761253,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "7de4569c7353030fec21bbb38c06323dd69f777c",
            "isKey": false,
            "numCitedBy": 86,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "We formulate a principle for classification with the knowledge of the marginal distribution over the data points (unlabeled data). The principle is cast in terms of Tikhonov style regularization where the regularization penalty articulates the way in which the marginal density should constrain otherwise unrestricted conditional distributions. Specifically, the regularization penalty penalizes any information introduced between the examples and labels beyond what is provided by the available labeled examples. The work extends Szummer and Jaakkola's information regularization (NIPS 2002) to multiple dimensions, providing a regularizer independent of the covering of the space used in the derivation. We show in addition how the information regularizer can be used as a measure of complexity of the classification task with unlabeled data and prove a relevant sample-complexity bound. We illustrate the regularization principle in practice by restricting the class of conditional distributions to be logistic regression models and constructing the regularization penalty from a finite set of unlabeled examples."
            },
            "slug": "On-Information-Regularization-Corduneanu-Jaakkola",
            "title": {
                "fragments": [],
                "text": "On Information Regularization"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The work extends Szummer and Jaakkola's information regularization to multiple dimensions, providing a regularizer independent of the covering of the space used in the derivation, and shows in addition how the information regularizer can be used as a measure of complexity of the classification task with unlabeled data and prove a relevant sample-complexity bound."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145624000"
                        ],
                        "name": "Zhi-Hua Zhou",
                        "slug": "Zhi-Hua-Zhou",
                        "structuredName": {
                            "firstName": "Zhi-Hua",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhi-Hua Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40655383"
                        ],
                        "name": "Ming Li",
                        "slug": "Ming-Li",
                        "structuredName": {
                            "firstName": "Ming",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming Li"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14969576,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "437c85ad1c05f60574544d31e96bd8e60393fc92",
            "isKey": false,
            "numCitedBy": 231,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "In many practical machine learning and data mining applications, unlabeled training examples are readily available but labeled ones are fairly expensive to obtain. Therefore, semi-supervised learning algorithms such as co-training have attracted much attention. Previous research mainly focuses on semi-supervised classification. In this paper, a co-training style semi-supervised regression algorithm, i.e. COREG, is proposed. This algorithm uses two k-nearest neighbor regressors with different distance metrics, each of which labels the unlabeled data for the other regressor where the labeling confidence is estimated through consulting the influence of the labeling of unlabeled examples on the labeled ones. Experiments show that COREG can effectively exploit unlabeled data to improve regression estimates."
            },
            "slug": "Semi-Supervised-Regression-with-Co-Training-Zhou-Li",
            "title": {
                "fragments": [],
                "text": "Semi-Supervised Regression with Co-Training"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Experiments show that COREG can effectively exploit unlabeled data to improve regression estimates and is proposed as a co-training style semi-supervised regression algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144160673"
                        ],
                        "name": "Alex Holub",
                        "slug": "Alex-Holub",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Holub",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Holub"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 238138,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d3cb669a67a09d6fd86a4b216ff6299662f2ed5e",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a semi-supervised learning algorithm for visual object categorization which utilizes statistical information from unlabelled data to increase classification performance. We build on an earlier hybrid generative-discriminative approach by Holub et al. [6] which extracts Fisher scores from generative models. The hybrid model allows us to combine the modelling power and flexibility of generative models with discriminative classifiers. Here we illustrate how the generative framework can be used to add prior knowledge obtained from unlabelled images, which the discriminative classifier can subsequently exploit. We illustrate the effects of using different sets of images as prior knowledge and find that the greatest benefits are incurred when the prior exemplars have similar statistics to the classes being discriminated between. Our tests show that strong performance (85%) can be obtained in discriminating between the faces of two different people using prior knowledge and only 2-3 training examples. Furthermore, we extend our approach to multi-class discrimination and show state-of-the-art performance on the Caltech101."
            },
            "slug": "Exploiting-Unlabelled-Data-for-Hybrid-Object-Holub",
            "title": {
                "fragments": [],
                "text": "Exploiting Unlabelled Data for Hybrid Object Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "A semi-supervised learning algorithm for visual object categorization which utilizes statistical information from unlabelled data to increase classification performance and extends to multi-class discrimination and shows state-of-the-art performance on the Caltech101."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46597039"
                        ],
                        "name": "P. Sinha",
                        "slug": "P.-Sinha",
                        "structuredName": {
                            "firstName": "Pawan",
                            "lastName": "Sinha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Sinha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46609120"
                        ],
                        "name": "B. Balas",
                        "slug": "B.-Balas",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Balas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Balas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7252638"
                        ],
                        "name": "Yuri Ostrovsky",
                        "slug": "Yuri-Ostrovsky",
                        "structuredName": {
                            "firstName": "Yuri",
                            "lastName": "Ostrovsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuri Ostrovsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144575094"
                        ],
                        "name": "Richard Russell",
                        "slug": "Richard-Russell",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Russell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Richard Russell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2541311,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d5c22cb54bd23f17289c31abb84baaf0cd439540",
            "isKey": false,
            "numCitedBy": 715,
            "numCiting": 206,
            "paperAbstract": {
                "fragments": [],
                "text": "A key goal of computer vision researchers is to create automated face recognition systems that can equal, and eventually surpass, human performance. To this end, it is imperative that computational researchers know of the key findings from experimental studies of face recognition by humans. These findings provide insights into the nature of cues that the human visual system relies upon for achieving its impressive performance and serve as the building blocks for efforts to artificially emulate these abilities. In this paper, we present what we believe are 19 basic results, with implications for the design of computational systems. Each result is described briefly and appropriate pointers are provided to permit an in-depth study of any particular result"
            },
            "slug": "Face-Recognition-by-Humans:-Nineteen-Results-All-Sinha-Balas",
            "title": {
                "fragments": [],
                "text": "Face Recognition by Humans: Nineteen Results All Computer Vision Researchers Should Know About"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Findings from experimental studies of face recognition by humans provide insights into the nature of cues that the human visual system relies upon for achieving its impressive performance and serve as the building blocks for efforts to artificially emulate these abilities."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the IEEE"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2230211"
                        ],
                        "name": "Linli Xu",
                        "slug": "Linli-Xu",
                        "structuredName": {
                            "firstName": "Linli",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Linli Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714772"
                        ],
                        "name": "Dale Schuurmans",
                        "slug": "Dale-Schuurmans",
                        "structuredName": {
                            "firstName": "Dale",
                            "lastName": "Schuurmans",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dale Schuurmans"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Xu and Schuurmans (2005) present a similar multi-class version of SDP formulation, which results in multi-class SVM for semi-supervised learning."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2198368,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dbbb3272236a3188b7332ff929627bb7414e6e68",
            "isKey": false,
            "numCitedBy": 161,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We present new unsupervised and semi-supervised training algorithms for multi-class support vector machines based on semidefinite programming. Although support vector machines (SVMs) have been a dominant machine learning technique for the past decade, they have generally been applied to supervised learning problems. Developing unsupervised extensions to SVMs has in fact proved to be difficult. In this paper, we present a principled approach to unsupervised SVM training by formulating convex relaxations of the natural training criterion: find a labeling that would yield an optimal SVM classifier on the resulting training data. The problem is hard, but semidefinite relaxations can approximate this objective surprisingly well. While previous work has concentrated on the two-class case, we present a general, multi-class formulation that can be applied to a wider range of natural data sets. The resulting training procedures are computationally intensive, but produce high quality generalization results."
            },
            "slug": "Unsupervised-and-Semi-Supervised-Multi-Class-Vector-Xu-Schuurmans",
            "title": {
                "fragments": [],
                "text": "Unsupervised and Semi-Supervised Multi-Class Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A principled approach to unsupervised SVM training is presented by formulating convex relaxations of the natural training criterion: find a labeling that would yield an optimal SVM classifier on the resulting training data."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057046890"
                        ],
                        "name": "Wei Chu",
                        "slug": "Wei-Chu",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Chu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Chu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1808676"
                        ],
                        "name": "V. Sindhwani",
                        "slug": "V.-Sindhwani",
                        "structuredName": {
                            "firstName": "Vikas",
                            "lastName": "Sindhwani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Sindhwani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744700"
                        ],
                        "name": "Zoubin Ghahramani",
                        "slug": "Zoubin-Ghahramani",
                        "structuredName": {
                            "firstName": "Zoubin",
                            "lastName": "Ghahramani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zoubin Ghahramani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144106136"
                        ],
                        "name": "S. Keerthi",
                        "slug": "S.-Keerthi",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Keerthi",
                            "middleNames": [
                                "Sathiya"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Keerthi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Chu et al. (2006) develop Guassian process models that incorporate pairwise label relations (e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7665438,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0c12d38ee14e5fbc5d53540534abec84f34b8888",
            "isKey": false,
            "numCitedBy": 101,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Correlation between instances is often modelled via a kernel function using input attributes of the instances. Relational knowledge can further reveal additional pairwise correlations between variables of interest. In this paper, we develop a class of models which incorporates both reciprocal relational information and input attributes using Gaussian process techniques. This approach provides a novel non-parametric Bayesian framework with a data-dependent covariance function for supervised learning tasks. We also apply this framework to semi-supervised learning. Experimental results on several real world data sets verify the usefulness of this algorithm."
            },
            "slug": "Relational-Learning-with-Gaussian-Processes-Chu-Sindhwani",
            "title": {
                "fragments": [],
                "text": "Relational Learning with Gaussian Processes"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A class of models which incorporates both reciprocal relational information and input attributes using Gaussian process techniques are developed, providing a novel non-parametric Bayesian framework with a data-dependent covariance function for supervised learning tasks."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145624000"
                        ],
                        "name": "Zhi-Hua Zhou",
                        "slug": "Zhi-Hua-Zhou",
                        "structuredName": {
                            "firstName": "Zhi-Hua",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhi-Hua Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40655383"
                        ],
                        "name": "Ming Li",
                        "slug": "Ming-Li",
                        "structuredName": {
                            "firstName": "Ming",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming Li"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Zhu and Ghahramani (2002) attempted exactly this, but were limited by the MCMC sampling techniques (they used global Metropolis and Swendsen-Wang sampling)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 406684,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a7d685ff8532ecb972c9382f86dea53ee7528264",
            "isKey": false,
            "numCitedBy": 908,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "In many practical data mining applications, such as Web page classification, unlabeled training examples are readily available, but labeled ones are fairly expensive to obtain. Therefore, semi-supervised learning algorithms such as co-training have attracted much attention. In this paper, a new co-training style semi-supervised learning algorithm, named tri-training, is proposed. This algorithm generates three classifiers from the original labeled example set. These classifiers are then refined using unlabeled examples in the tri-training process. In detail, in each round of tri-training, an unlabeled example is labeled for a classifier if the other two classifiers agree on the labeling, under certain conditions. Since tri-training neither requires the instance space to be described with sufficient and redundant views nor does it put any constraints on the supervised learning algorithm, its applicability is broader than that of previous co-training style algorithms. Experiments on UCI data sets and application to the Web page classification task indicate that tri-training can effectively exploit unlabeled data to enhance the learning performance."
            },
            "slug": "Tri-training:-exploiting-unlabeled-data-using-three-Zhou-Li",
            "title": {
                "fragments": [],
                "text": "Tri-training: exploiting unlabeled data using three classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "Experiments on UCI data sets and application to the Web page classification task indicate that tri-training can effectively exploit unlabeled data to enhance the learning performance."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Knowledge and Data Engineering"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145624000"
                        ],
                        "name": "Zhi-Hua Zhou",
                        "slug": "Zhi-Hua-Zhou",
                        "structuredName": {
                            "firstName": "Zhi-Hua",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhi-Hua Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2429024"
                        ],
                        "name": "Ke-Jia Chen",
                        "slug": "Ke-Jia-Chen",
                        "structuredName": {
                            "firstName": "Ke-Jia",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ke-Jia Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2192443"
                        ],
                        "name": "Yuan Jiang",
                        "slug": "Yuan-Jiang",
                        "structuredName": {
                            "firstName": "Yuan",
                            "lastName": "Jiang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuan Jiang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 0
                            }
                        ],
                        "text": "Zhu et al. (2003c) propose that new test point be classified by its neare st neighbor inL\u222aU ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8849752,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3981a49910a153209c80040ff0e7f6f87f9abd4f",
            "isKey": false,
            "numCitedBy": 92,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, the SSAIR (Semi-Supervised Active Image Retrieval) approach, which attempts to exploit unlabeled data to improve the performance of content-based image retrieval (CBIR), is proposed. This approach combines the merits of semi-supervised learning and active learning. In detail, in each round of relevance feedback, two simple learners are trained from the labeled data, i.e. images from user query and user feedback. Each learner then classifies the unlabeled images in the database and passes the most relevant/irrelevant images to the other learner. After re-training with the additional labeled data, the learners classify the images in the database again and then their classifications are merged. Images judged to be relevant with high confidence are returned as the retrieval result, while these judged with low confidence are put into the pool which is used in the next round of relevance feedback. Experiments show that semi-supervised learning and active learning mechanisms are both beneficial to CBIR."
            },
            "slug": "Exploiting-Unlabeled-Data-in-Content-Based-Image-Zhou-Chen",
            "title": {
                "fragments": [],
                "text": "Exploiting Unlabeled Data in Content-Based Image Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "The SSAIR (Semi-Supervised Active Image Retrieval) approach, which attempts to exploit unlabeled data to improve the performance of content-based image retrieval (CBIR), is proposed and experiments show that semi-supervised learning and active learning mechanisms are both beneficial to CBIR."
            },
            "venue": {
                "fragments": [],
                "text": "ECML"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1911526"
                        ],
                        "name": "A. Corduneanu",
                        "slug": "A.-Corduneanu",
                        "structuredName": {
                            "firstName": "Adrian",
                            "lastName": "Corduneanu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Corduneanu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132120"
                        ],
                        "name": "T. Jaakkola",
                        "slug": "T.-Jaakkola",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Jaakkola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jaakkola"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 874376,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6bc9a9902f34a9af62bcff0789840a476b583cdd",
            "isKey": false,
            "numCitedBy": 25,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "An increasing number of parameter estimation tasks involve the use of at least two information sources, one complete but limited, the other abundant but incomplete. Standard algorithms such as EM (or em) used in this context are unfortunately not stable in the sense that they can lead to a dramatic loss of accuracy with the inclusion of incomplete observations. We provide a more controlled solution to this problem through differential equations that govern the evolution of locally optimal solutions (fixed points) as a function of the source weighting. This approach permits us to explicitly identify any critical (bifurcation) points leading to choices unsupported by the available complete data. The approach readily applies to any graphical model in O(n) time where n is the number of parameters. We use the naive Bayes model to illustrate these ideas and demonstrate the effectiveness of our approach in the context of text classification problems. Acknowledgements This work was supported in part by Nippon Telegraph and Telephone Corporation, by ARO MURI grant DAAD19-00-1-0466, and by NSF ITR grant IIS-0085836."
            },
            "slug": "Stable-Mixing-of-Complete-and-Incomplete-Corduneanu-Jaakkola",
            "title": {
                "fragments": [],
                "text": "Stable Mixing of Complete and Incomplete Information"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work permits us to explicitly identify any critical (bifurcation) points leading to choices unsupported by the available complete data and readily applies to any graphical model in O(n) time where n is the number of parameters."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1767737"
                        ],
                        "name": "D. Elworthy",
                        "slug": "D.-Elworthy",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Elworthy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Elworthy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 255,
                                "start": 241
                            }
                        ],
                        "text": "For example peoplehave long realized that training Hidden Markov Model with unlabeled data (the Baum-Welsh algorithm, which by the way qualifies as semi-supervised learning on sequences) can reduce accuracy under certain initial conditions (Elworthy, 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 259,
                                "start": 243
                            }
                        ],
                        "text": "For example people have long realized that training Hidden Markov Model with unlabeled data (the Baum-We lsh algorithm, which by the way qualifies as semi-supervised learning on sequen ces) can reduce accuracy under certain initial conditions (Elworthy, 1994) ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 38
                            }
                        ],
                        "text": "Some cautionary notes can be found in (Elworthy, 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1900253,
            "fieldsOfStudy": [
                "Environmental Science"
            ],
            "id": "5301d28e78992a588ee5699c305ebb949dc83613",
            "isKey": true,
            "numCitedBy": 186,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "In part of speech tagging by Hidden Markov Model, a statistical model is used to assign grammatical categories to words in a text. Early work in the field relied on a corpus which had been tagged by a human annotator to train the model. More recently, Cutting et al. (1992) suggest that training can be achieved with a minimal lexicon and a limited amount of a priori information about probabilities, by using an Baum-Welch re-estimation to automatically refine the model. In this paper, I report two experiments designed to determine how much manual training information is needed. The first experiment suggests that initial biasing of either lexical or transition probabilities is essential to achieve a good accuracy. The second experiment reveals that there are three distinct patterns of Baum-Welch reestimation. In two of the patterns, the re-estimation ultimately reduces the accuracy of the tagging rather than improving it. The pattern which is applicable can be predicted from the quality of the initial model and the similarity between the tagged training corpus (if any) and the corpus to be tagged. Heuristics for deciding how to use re-estimation in an effective manner are given. The conclusions are broadly in agreement with those of Merialdo (1994), but give greater detail about the contributions of different parts of the model."
            },
            "slug": "Does-Baum-Welch-Re-estimation-Help-Taggers-Elworthy",
            "title": {
                "fragments": [],
                "text": "Does Baum-Welch Re-estimation Help Taggers?"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Two experiments designed to determine how much manual training information is needed for speech tagging by Hidden Markov Model suggest that initial biasing of either lexical or transition probabilities is essential to achieve a good accuracy and reveal three distinct patterns of Baum-Welch reestimation."
            },
            "venue": {
                "fragments": [],
                "text": "ANLP"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745410"
                        ],
                        "name": "Maria-Florina Balcan",
                        "slug": "Maria-Florina-Balcan",
                        "structuredName": {
                            "firstName": "Maria-Florina",
                            "lastName": "Balcan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Maria-Florina Balcan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690967"
                        ],
                        "name": "A. Blum",
                        "slug": "A.-Blum",
                        "structuredName": {
                            "firstName": "Avrim",
                            "lastName": "Blum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Blum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39701662"
                        ],
                        "name": "P. Choi",
                        "slug": "P.-Choi",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Choi",
                            "middleNames": [
                                "Pakyan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Choi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "114987044"
                        ],
                        "name": "Brian Pantano",
                        "slug": "Brian-Pantano",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Pantano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Brian Pantano"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714122"
                        ],
                        "name": "M. Rwebangira",
                        "slug": "M.-Rwebangira",
                        "structuredName": {
                            "firstName": "Mugizi",
                            "lastName": "Rwebangira",
                            "middleNames": [
                                "Robert"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Rwebangira"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1832364"
                        ],
                        "name": "Xiaojin Zhu",
                        "slug": "Xiaojin-Zhu",
                        "structuredName": {
                            "firstName": "Xiaojin",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaojin Zhu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9267548,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "76ed9e63f7f750c5df2c3a9d1aec3fd883efc3c3",
            "isKey": false,
            "numCitedBy": 97,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "An application of semi-supervised learning is\nmade to the problem of person identification in\nlow quality webcam images. Using a set of images\nof ten people collected over a period of four\nmonths, the person identification task is posed\nas a graph-based semi-supervised learning problem,\nwhere only a few training images are labeled.\nThe importance of domain knowledge\nin graph construction is discussed, and experiments\nare presented that clearly show the advantage\nof semi-supervised learning over standard\nsupervised learning. The data used in the study\nis available to the research community to encourage\nfurther investigation of this problem."
            },
            "slug": "Person-Identification-in-Webcam-Images:-An-of-Balcan-Blum",
            "title": {
                "fragments": [],
                "text": "Person Identification in Webcam Images: An Application of Semi-Supervised Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "The person identification task is posed as a graph-based semi-supervised learning problem, where only a few training images are labeled and the importance of domain knowledge in graph construction is discussed, and experiments are presented that clearly show the advantage of semi- supervised learning over standard supervised learning."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145172877"
                        ],
                        "name": "K. Nigam",
                        "slug": "K.-Nigam",
                        "structuredName": {
                            "firstName": "Kamal",
                            "lastName": "Nigam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Nigam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791498"
                        ],
                        "name": "R. Ghani",
                        "slug": "R.-Ghani",
                        "structuredName": {
                            "firstName": "Rayid",
                            "lastName": "Ghani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Ghani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7464925,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "847546aa1cbcb017cb16041cba4927b76f57461a",
            "isKey": false,
            "numCitedBy": 1061,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently there has been signi cant interest in supervised learning algorithms that combine labeled and unlabeled data for text learning tasks. The co-training setting [1] applies to datasets that have a natural separation of their features into two disjoint sets. We demonstrate that when learning from labeled and unlabeled data, algorithms explicitly leveraging a natural independent split of the features outperform algorithms that do not. When a natural split does not exist, co-training algorithms that manufacture a feature split may out-perform algorithms not using a split. These results help explain why co-training algorithms are both discriminative in nature and robust to the assumptions of their embedded classi ers."
            },
            "slug": "Analyzing-the-effectiveness-and-applicability-of-Nigam-Ghani",
            "title": {
                "fragments": [],
                "text": "Analyzing the effectiveness and applicability of co-training"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "It is demonstrated that when learning from labeled and unlabeled data, algorithms explicitly leveraging a natural independent split of the features outperform algorithms that do not and may out-perform algorithms not using a split."
            },
            "venue": {
                "fragments": [],
                "text": "CIKM '00"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132120"
                        ],
                        "name": "T. Jaakkola",
                        "slug": "T.-Jaakkola",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Jaakkola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jaakkola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2316129"
                        ],
                        "name": "M. Meil\u0103",
                        "slug": "M.-Meil\u0103",
                        "structuredName": {
                            "firstName": "Marina",
                            "lastName": "Meil\u0103",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Meil\u0103"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768120"
                        ],
                        "name": "T. Jebara",
                        "slug": "T.-Jebara",
                        "structuredName": {
                            "firstName": "Tony",
                            "lastName": "Jebara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jebara"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "(Joachims, 1999) (Bennett & Demiriz, 1999) (Demirez & Bennett, 2000) (Fung & Mangasarian, 1999) (Chapelle & Zien, 2005). Xu and Schuurmans (2005) present a training method based on semi-definite programming (SDP, which applies to the completely unsupervised SVMs as well)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The maximum entropy discrimination approach (Jaakkola et al., 1999) also"
                    },
                    "intents": []
                }
            ],
            "corpusId": 872496,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8f9d6be4fee69c9a859c9055b09f2ee94864f5b7",
            "isKey": false,
            "numCitedBy": 291,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a general framework for discriminative estimation based on the maximum entropy principle and its extensions. All calculations involve distributions over structures and/or parameters rather than specific settings and reduce to relative entropy projections. This holds even when the data is not separable within the chosen parametric class, in the context of anomaly detection rather than classification, or when the labels in the training set are uncertain or incomplete. Support vector machines are naturally subsumed under this class and we provide several extensions. We are also able to estimate exactly and efficiently discriminative distributions over tree structures of class-conditional models within this framework. Preliminary experimental results are indicative of the potential in these techniques."
            },
            "slug": "Maximum-Entropy-Discrimination-Jaakkola-Meil\u0103",
            "title": {
                "fragments": [],
                "text": "Maximum Entropy Discrimination"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "A general framework for discriminative estimation based on the maximum entropy principle and its extensions is presented and preliminary experimental results are indicative of the potential in these techniques."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1832364"
                        ],
                        "name": "Xiaojin Zhu",
                        "slug": "Xiaojin-Zhu",
                        "structuredName": {
                            "firstName": "Xiaojin",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaojin Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 0
                            }
                        ],
                        "text": "Zhu et al. (2003c) propose that new test point be classified by its neare st neighbor inL\u222aU ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14278266,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "81032f72599eae019d6a0812c037c59dd694c23a",
            "isKey": false,
            "numCitedBy": 179,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Graph-based methods for semi-supervised learning have recently been shown to be promising for combining labeled and unlabeled data in classification problems. However, inference for graph-based methods often does not scale well to very large data sets, since it requires inversion of a large matrix or solution of a large linear program. Moreover, such approaches are inherently transductive, giving predictions for only those points in the unlabeled set, and not for an arbitrary test point. In this paper a new approach is presented that preserves the strengths of graph-based semi-supervised learning while overcoming the limitations of scalability and non-inductive inference, through a combination of generative mixture models and discriminative regularization using the graph Laplacian. Experimental results show that this approach preserves the accuracy of purely graph-based transductive methods when the data has \"manifold structure,\" and at the same time achieves inductive learning with significantly reduced computational cost."
            },
            "slug": "Harmonic-mixtures:-combining-mixture-models-and-for-Zhu-Lafferty",
            "title": {
                "fragments": [],
                "text": "Harmonic mixtures: combining mixture models and graph-based methods for inductive and scalable semi-supervised learning"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "Experimental results show that this approach preserves the accuracy of purely graph-based transductive methods when the data has \"manifold structure,\" and at the same time achieves inductive learning with significantly reduced computational cost."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145728220"
                        ],
                        "name": "K. Bennett",
                        "slug": "K.-Bennett",
                        "structuredName": {
                            "firstName": "Kristin",
                            "lastName": "Bennett",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Bennett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2932893"
                        ],
                        "name": "A. Demiriz",
                        "slug": "A.-Demiriz",
                        "structuredName": {
                            "firstName": "Ayhan",
                            "lastName": "Demiriz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Demiriz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7635678,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8198e70878c907e1bd05e7a3fa4280d8c338df60",
            "isKey": false,
            "numCitedBy": 873,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a semi-supervised support vector machine (S3VM) method. Given a training set of labeled data and a working set of unlabeled data, S3VM constructs a support vector machine using both the training and working sets. We use S3VM to solve the transduction problem using overall risk minimization (ORM) posed by Vapnik. The transduction problem is to estimate the value of a classification function at the given points in the working set. This contrasts with the standard inductive learning problem of estimating the classification function at all possible values and then using the fixed function to deduce the classes of the working set data. We propose a general S3VM model that minimizes both the misclassification error and the function capacity based on all the available data. We show how the S3VM model for 1-norm linear support vector machines can be converted to a mixed-integer program and then solved exactly using integer programming. Results of S3VM and the standard 1-norm support vector machine approach are compared on ten data sets. Our computational results support the statistical learning theory results showing that incorporating working data improves generalization when insufficient training information is available. In every case, S3VM either improved or showed no significant difference in generalization compared to the traditional approach."
            },
            "slug": "Semi-Supervised-Support-Vector-Machines-Bennett-Demiriz",
            "title": {
                "fragments": [],
                "text": "Semi-Supervised Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A general S3VM model is proposed that minimizes both the misclassification error and the function capacity based on all the available data that can be converted to a mixed-integer program and then solved exactly using integer programming."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730609"
                        ],
                        "name": "O. Chapelle",
                        "slug": "O.-Chapelle",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Chapelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Chapelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2281542"
                        ],
                        "name": "A. Zien",
                        "slug": "A.-Zien",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Zien",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Zien"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In Chapelle and Zien (2005), it is a constraint on continuous function predictions: 1 u l+u"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14283441,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d2c5d2cafc35856832f2b478790f0af119baab92",
            "isKey": false,
            "numCitedBy": 839,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We believe that the cluster assumption is key to successful semi-supervised learning. Based on this, we propose three semi-supervised algorithms: 1. deriving graph-based distances that emphazise low density regions between clusters, followed by training a standard SVM; 2. optimizing the Transductive SVM objective function, which places the decision boundary in low density regions, by gradient descent; 3. combining the first two to make maximum use of the cluster assumption. We compare with state of the art algorithms and demonstrate superior accuracy for the latter two methods."
            },
            "slug": "Semi-Supervised-Classification-by-Low-Density-Chapelle-Zien",
            "title": {
                "fragments": [],
                "text": "Semi-Supervised Classification by Low Density Separation"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "Three semi-supervised algorithms are proposed: deriving graph-based distances that emphazise low density regions between clusters, followed by training a standard SVM, and optimizing the Transductive SVM objective function by gradient descent."
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143707112"
                        ],
                        "name": "M. Collins",
                        "slug": "M.-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 631,
                                "start": 0
                            }
                        ],
                        "text": "Collins and Singer (1999); Jones (2005) used co-training, co-EM and other related methods for information extraction from text. Balcan and Blum (2006) show that co-training can be quite effective, that in the extre me case only one labeled point is needed to learn the classifier. Zhou et al. (2007) give a co-training algorithm using Canonical Correlation Analysis which also need only one labeled point. Dasgupta et al. (Dasgupta et al., 2001) provide a PACstyle theoretical analysis. Co-training makes strong assumptions on the splitting of features. One might wonder if these conditions can be relaxed. Goldman and Zhou (2000) use two learners of different type but both takes the whole feature set, and ess entially use"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 300,
                                "start": 0
                            }
                        ],
                        "text": "Collins and Singer (1999); Jones (2005) used co-training, co-EM and other related methods for information extraction from text. Balcan and Blum (2006) show that co-training can be quite effective, that in the extre me case only one labeled point is needed to learn the classifier. Zhou et al. (2007) give a co-training algorithm using Canonical Correlation Analysis which also need only one labeled point."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 0
                            }
                        ],
                        "text": "Collins and Singer (1999); Jones (2005) used co-training, co-EM and other related methods for information extraction from text. Balcan and Blum (2006) show that co-training can be quite effective, that in the extre me case only one labeled point is needed to learn the classifier."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 0
                            }
                        ],
                        "text": "Collins and Singer (1999); Jones (2005) used co-training, co-EM and other related methods for information extraction from text."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 859162,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1c0ece611643cfb8f3a23e4802c754ea583ebe37",
            "isKey": true,
            "numCitedBy": 1013,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper discusses the use of unlabeled examples for the problem of named entity classification. A large number of rules is needed for coverage of the domain, suggesting that a fairly large number of labeled examples should be required to train a classifier. However, we show that the use of unlabeled data can reduce the requirements for supervision to just 7 simple \"seed\" rules. The approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context inwhich it appears are sufficient to determine its type. We present two algorithms. The first method uses a similar algorithm to that of (Yarowsky 95), with modifications motivated by (Blum and Mitchell 98). The second algorithm extends ideas from boosting algorithms, designed for supervised learning tasks, to the framework suggested by (Blum and Mitchell 98)."
            },
            "slug": "Unsupervised-Models-for-Named-Entity-Classification-Collins-Singer",
            "title": {
                "fragments": [],
                "text": "Unsupervised Models for Named Entity Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "It is shown that the use of unlabeled data can reduce the requirements for supervision to just 7 simple \"seed\" rules, gaining leverage from natural redundancy in the data."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2987520"
                        ],
                        "name": "Katharine Graf Estes",
                        "slug": "Katharine-Graf-Estes",
                        "structuredName": {
                            "firstName": "Katharine",
                            "lastName": "Estes",
                            "middleNames": [
                                "Graf"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Katharine Graf Estes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1898233"
                        ],
                        "name": "Julia L Evans",
                        "slug": "Julia-L-Evans",
                        "structuredName": {
                            "firstName": "Julia",
                            "lastName": "Evans",
                            "middleNames": [
                                "L"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Julia L Evans"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3177547"
                        ],
                        "name": "M. Alibali",
                        "slug": "M.-Alibali",
                        "structuredName": {
                            "firstName": "Martha",
                            "lastName": "Alibali",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Alibali"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2769848"
                        ],
                        "name": "J. Saffran",
                        "slug": "J.-Saffran",
                        "structuredName": {
                            "firstName": "Jenny",
                            "lastName": "Saffran",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Saffran"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14387074,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "d460bec270ff4efeda927c3a200746144994b5fb",
            "isKey": false,
            "numCitedBy": 424,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "The present experiments investigated how the process of statistically segmenting words from fluent speech is linked to the process of mapping meanings to words. Seventeen-month-old infants first participated in a statistical word segmentation task, which was immediately followed by an object-label-learning task. Infants presented with labels that were words in the fluent speech used in the segmentation task were able to learn the object labels. However, infants presented with labels consisting of novel syllable sequences (nonwords; Experiment 1) or familiar sequences with low internal probabilities (part-words; Experiment 2) did not learn the labels. Thus, prior segmentation opportunities, but not mere frequency of exposure, facilitated infants\u2217 learning of object labels. This work provides the first demonstration that exposure to word forms in a statistical word segmentation task facilitates subsequent word learning."
            },
            "slug": "Can-Infants-Map-Meaning-to-Newly-Segmented-Words-Estes-Evans",
            "title": {
                "fragments": [],
                "text": "Can Infants Map Meaning to Newly Segmented Words?"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work provides the first demonstration that exposure to word forms in a statistical word segmentation task facilitates subsequent word learning, and prior segmentation opportunities, but not mere frequency of exposure, facilitated infants learning of object labels."
            },
            "venue": {
                "fragments": [],
                "text": "Psychological science"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50033175"
                        ],
                        "name": "Andreas Argyriou",
                        "slug": "Andreas-Argyriou",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Argyriou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andreas Argyriou"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11145965,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ee5e80ede31c39c6483179119d76c3274eae9579",
            "isKey": false,
            "numCitedBy": 4,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "An algorithm for semi-supervised learning using Gaussian fields and harmonic functions has been recently proposed by Zhu, Ghahramani and Lafferty. In this approach, a Gaussian field is defined on a weighted graph representing degrees of similarity between data points. A limited number of the points are labeled and it is shown that the unlabeled points can be classified according to the solution of a linear system involving the Laplacian of the graph. Thus, learning reduces to solving several systems of the form Ax = b, where A is a fixed symmetric, positive definite matrix. The objective of this work is to construct the weight matrix of the graph in an efficient way and apply numerical methods to the solution of the linear systems. Principal Component Analysis is used to compress the weight information to a smaller dimensionality. Then, kd-tree nearest neighbour algorithms are applied to form the k or -neighbourhoods around each point in O(n log n) time. The result is a sparse weight matrix and its corresponding Laplacian matrix. Classification and hyperparameter learning are then accelerated by using iterative methods for linear systems. These methods, in combination with a simple preconditioning technique, cost much less than the superquadratic direct methods and enable the application of the learning algorithm to larger data sets. Finally, the improvements in performance are demonstrated by some real-life experiments."
            },
            "slug": "Efficient-Approximation-Methods-for-Harmonic-Argyriou",
            "title": {
                "fragments": [],
                "text": "Efficient Approximation Methods for Harmonic Semi-Supervised Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 35,
                "text": "The objective of this work is to construct the weight matrix of the graph in an efficient way and apply numerical methods to the solution of the linear systems."
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740222"
                        ],
                        "name": "Wee Sun Lee",
                        "slug": "Wee-Sun-Lee",
                        "structuredName": {
                            "firstName": "Wee",
                            "lastName": "Lee",
                            "middleNames": [
                                "Sun"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wee Sun Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145321667"
                        ],
                        "name": "B. Liu",
                        "slug": "B.-Liu",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Liu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Leskes (2005) presents a generalization error bound for semi-supervised learning with multiple learners, an extension to co-training."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14322823,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "41d9b4104b6d8cb8c135560b5f775bc8cd7a720d",
            "isKey": false,
            "numCitedBy": 334,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of learning with positive and unlabeled examples arises frequently in retrieval applications. We transform the problem into a problem of learning with noise by labeling all unlabeled examples as negative and use a linear function to learn from the noisy examples. To learn a linear function with noise, we perform logistic regression after weighting the examples to handle noise rates of greater than a half. With appropriate regularization, the cost function of the logistic regression problem is convex, allowing the problem to be solved efficiently. We also propose a performance measure that can be estimated from positive and unlabeled examples for evaluating retrieval performance. The measure, which is proportional to the product of precision and recall, can be used with a validation set to select regularization parameters for logistic regression. Experiments on a text classification corpus show that the methods proposed are effective."
            },
            "slug": "Learning-with-Positive-and-Unlabeled-Examples-Using-Lee-Liu",
            "title": {
                "fragments": [],
                "text": "Learning with Positive and Unlabeled Examples Using Weighted Logistic Regression"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "A performance measure that can be estimated from positive and unlabeled examples for evaluating retrieval performance, which is proportional to the product of precision and recall, can be used with a validation set to select regularization parameters for logistic regression."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "150062221"
                        ],
                        "name": "C. Oliveira",
                        "slug": "C.-Oliveira",
                        "structuredName": {
                            "firstName": "Clayton",
                            "lastName": "Oliveira",
                            "middleNames": [
                                "Silva"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Oliveira"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15070753,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "90bb74b0738e293719f9f9599b37504b55368bad",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we investigate techniques for semi-supervised learning that split their unsupervised and supervised components \u2014 that is, an initial unsupervised phase is followed by a supervised learning phase. We first analyze the relative value of labeled and unlabeled data. We then present methods that perform \u201csplit\u201d semi-supervised learning and show promising empirical results."
            },
            "slug": "Splitting-the-Unsupervised-and-Supervised-of-Oliveira",
            "title": {
                "fragments": [],
                "text": "Splitting the Unsupervised and Supervised Components of Semi-Supervised Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "This paper investigates techniques for semi-supervised learning that split their unsupervised and supervised components \u2014 that is, an initial unsuper supervised phase is followed by a supervised learning phase."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1832364"
                        ],
                        "name": "Xiaojin Zhu",
                        "slug": "Xiaojin-Zhu",
                        "structuredName": {
                            "firstName": "Xiaojin",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaojin Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744700"
                        ],
                        "name": "Zoubin Ghahramani",
                        "slug": "Zoubin-Ghahramani",
                        "structuredName": {
                            "firstName": "Zoubin",
                            "lastName": "Ghahramani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zoubin Ghahramani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1052837,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "125842668eab7decac136db8a59d392dc5e4e395",
            "isKey": false,
            "numCitedBy": 3711,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "An approach to semi-supervised learning is proposed that is based on a Gaussian random field model. Labeled and unlabeled data are represented as vertices in a weighted graph, with edge weights encoding the similarity between instances. The learning problem is then formulated in terms of a Gaussian random field on this graph, where the mean of the field is characterized in terms of harmonic functions, and is efficiently obtained using matrix methods or belief propagation. The resulting learning algorithms have intimate connections with random walks, electric networks, and spectral graph theory. We discuss methods to incorporate class priors and the predictions of classifiers obtained by supervised learning. We also propose a method of parameter learning by entropy minimization, and show the algorithm's ability to perform feature selection. Promising experimental results are presented for synthetic data, digit classification, and text classification tasks."
            },
            "slug": "Semi-Supervised-Learning-Using-Gaussian-Fields-and-Zhu-Ghahramani",
            "title": {
                "fragments": [],
                "text": "Semi-Supervised Learning Using Gaussian Fields and Harmonic Functions"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "An approach to semi-supervised learning is proposed that is based on a Gaussian random field model, and methods to incorporate class priors and the predictions of classifiers obtained by supervised learning are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144539424"
                        ],
                        "name": "N. Chawla",
                        "slug": "N.-Chawla",
                        "structuredName": {
                            "firstName": "N.",
                            "lastName": "Chawla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Chawla"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770463"
                        ],
                        "name": "Grigoris I. Karakoulas",
                        "slug": "Grigoris-I.-Karakoulas",
                        "structuredName": {
                            "firstName": "Grigoris",
                            "lastName": "Karakoulas",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Grigoris I. Karakoulas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Chawla and Karakoulas (2005) perform empirical studies on this version of co-training and compared it against several other methods, in particular for the case where labeled and unlabeled data do not follow the same distribution."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 9144332,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e1d3006a1191f04131db8a26afb6949d7fec700d",
            "isKey": false,
            "numCitedBy": 159,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "There has been increased interest in devising learning techniques that combine unlabeled data with labeled data -- i.e. semi-supervised learning. However, to the best of our knowledge, no study has been performed across various techniques and different types and amounts of labeled and unlabeled data. Moreover, most of the published work on semi-supervised learning techniques assumes that the labeled and unlabeled data come from the same distribution. It is possible for the labeling process to be associated with a selection bias such that the distributions of data points in the labeled and unlabeled sets are different. Not correcting for such bias can result in biased function approximation with potentially poor performance. In this paper, we present an empirical study of various semi-supervised learning techniques on a variety of datasets. We attempt to answer various questions such as the effect of independence or relevance amongst features, the effect of the size of the labeled and unlabeled sets and the effect of noise. We also investigate the impact of sample-selection bias on the semi -supervised learning techniques under study and implement a bivariate probit technique particularly designed to correct for such bias."
            },
            "slug": "Learning-From-Labeled-And-Unlabeled-Data:-An-Study-Chawla-Karakoulas",
            "title": {
                "fragments": [],
                "text": "Learning From Labeled And Unlabeled Data: An Empirical Study Across Techniques And Domains"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "This paper presents an empirical study of various semi-supervised learning techniques on a variety of datasets, and attempts to answer various questions such as the effect of independence or relevance amongst features, theeffect of the size of the labeled and unlabeled sets and the effects of noise."
            },
            "venue": {
                "fragments": [],
                "text": "J. Artif. Intell. Res."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680188"
                        ],
                        "name": "T. Joachims",
                        "slug": "T.-Joachims",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "Joachims",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Joachims"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6027413,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "49b8dff62cccc26023c876460234bf29084a382f",
            "isKey": false,
            "numCitedBy": 734,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new method for transductive learning, which can be seen as a transductive version of the k nearest-neighbor classifier. Unlike for many other transductive learning methods, the training problem has a meaningful relaxation that can be solved globally optimally using spectral methods. We propose an algorithm that robustly achieves good generalization performance and that can be trained efficiently. A key advantage of the algorithm is that it does not require additional heuristics to avoid unbalanced splits. Furthermore, we show a connection to transductive Support Vector Machines, and that an effective Co-Training algorithm arises as a special case."
            },
            "slug": "Transductive-Learning-via-Spectral-Graph-Joachims",
            "title": {
                "fragments": [],
                "text": "Transductive Learning via Spectral Graph Partitioning"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work proposes an algorithm that robustly achieves good generalization performance and that can be trained efficiently, and shows a connection to transductive Support Vector Machines, and that an effective Co-Training algorithm arises as a special case."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145624000"
                        ],
                        "name": "Zhi-Hua Zhou",
                        "slug": "Zhi-Hua-Zhou",
                        "structuredName": {
                            "firstName": "Zhi-Hua",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhi-Hua Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2429024"
                        ],
                        "name": "Ke-Jia Chen",
                        "slug": "Ke-Jia-Chen",
                        "structuredName": {
                            "firstName": "Ke-Jia",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ke-Jia Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065114421"
                        ],
                        "name": "Honghua Dai",
                        "slug": "Honghua-Dai",
                        "structuredName": {
                            "firstName": "Honghua",
                            "lastName": "Dai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Honghua Dai"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "For example, Zhu et al. (2003a) use a heuristic \u201cclass mean normalization\u201d procedure to move towards the desired class proportions; S3VM methods explicitly fit the desired class proportions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17172023,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "552146d209bfe303fc7248d8e11e512c435ee39c",
            "isKey": false,
            "numCitedBy": 154,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "Relevance feedback is an effective scheme bridging the gap between high-level semantics and low-level features in content-based image retrieval (CBIR). In contrast to previous methods which rely on labeled images provided by the user, this article attempts to enhance the performance of relevance feedback by exploiting unlabeled images existing in the database. Concretely, this article integrates the merits of semisupervised learning and active learning into the relevance feedback process. In detail, in each round of relevance feedback two simple learners are trained from the labeled data, that is, images from user query and user feedback. Each learner then labels some unlabeled images in the database for the other learner. After retraining with the additional labeled data, the learners reclassify the images in the database and then their classifications are merged. Images judged to be positive with high confidence are returned as the retrieval result, while those judged with low confidence are put into the pool which is used in the next round of relevance feedback. Experiments show that using semisupervised learning and active learning simultaneously in CBIR is beneficial, and the proposed method achieves better performance than some existing methods."
            },
            "slug": "Enhancing-relevance-feedback-in-image-retrieval-Zhou-Chen",
            "title": {
                "fragments": [],
                "text": "Enhancing relevance feedback in image retrieval using unlabeled data"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Experiments show that using semisupervised learning and active learning simultaneously in CBIR is beneficial, and the proposed method achieves better performance than some existing methods."
            },
            "venue": {
                "fragments": [],
                "text": "ACM Trans. Inf. Syst."
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709392"
                        ],
                        "name": "D. Donoho",
                        "slug": "D.-Donoho",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Donoho",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Donoho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2684296"
                        ],
                        "name": "C. Grimes",
                        "slug": "C.-Grimes",
                        "structuredName": {
                            "firstName": "Carrie",
                            "lastName": "Grimes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Grimes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This can be viewed as an application of the Nystr\u00f6m method (Fowlkes et al., 2004). Yu et al. (2004) report an early attempt on semi-supervised induction using RBF basis functions in a regularization framework."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1810410,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "57a66ac4a4e0a00d2cdee8711ce0a18b49e9f7a2",
            "isKey": false,
            "numCitedBy": 1588,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a method for recovering the underlying parametrization of scattered data (mi) lying on a manifold M embedded in high-dimensional Euclidean space. The method, Hessian-based locally linear embedding, derives from a conceptual framework of local isometry in which the manifold M, viewed as a Riemannian submanifold of the ambient Euclidean space \u211dn, is locally isometric to an open, connected subset \u0398 of Euclidean space \u211dd. Because \u0398 does not have to be convex, this framework is able to handle a significantly wider class of situations than the original ISOMAP algorithm. The theoretical framework revolves around a quadratic form \u210b(f) = \u222bM\u2009\u2225Hf(m)\u2225\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} \\begin{equation*}{\\mathrm{_{{\\mathit{F}}}^{2}}}\\end{equation*}\\end{document}dm defined on functions f : M \u21a6 \u211d. Here Hf denotes the Hessian of f, and \u210b(f) averages the Frobenius norm of the Hessian over M. To define the Hessian, we use orthogonal coordinates on the tangent planes of M. The key observation is that, if M truly is locally isometric to an open, connected subset of \u211dd, then \u210b(f) has a (d + 1)-dimensional null space consisting of the constant functions and a d-dimensional space of functions spanned by the original isometric coordinates. Hence, the isometric coordinates can be recovered up to a linear isometry. Our method may be viewed as a modification of locally linear embedding and our theoretical framework as a modification of the Laplacian eigenmaps framework, where we substitute a quadratic form based on the Hessian in place of one based on the Laplacian."
            },
            "slug": "Hessian-eigenmaps:-Locally-linear-embedding-for-Donoho-Grimes",
            "title": {
                "fragments": [],
                "text": "Hessian eigenmaps: Locally linear embedding techniques for high-dimensional data"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "The Hessian-based locally linear embedding method for recovering the underlying parametrization of scattered data (mi) lying on a manifold M embedded in high-dimensional Euclidean space is described, where the isometric coordinates can be recovered up to a linear isometry."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences of the United States of America"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132120"
                        ],
                        "name": "T. Jaakkola",
                        "slug": "T.-Jaakkola",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Jaakkola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jaakkola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 265,
                                "start": 240
                            }
                        ],
                        "text": "However instead of directly using the generative model for classification, each labeled example is converted into a fixed-length Fisher score vector, i.e. the derivatives of log likelihood w.r.t. model parameters, for all component models (Jaakkola & Haussler, 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Jones (2005) used co-training, co-EM and other related methods for information extraction from text."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14336127,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e45c2420e6dc59ba6d357fb0c996ebf43c861560",
            "isKey": false,
            "numCitedBy": 1619,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Generative probability models such as hidden Markov models provide a principled way of treating missing information and dealing with variable length sequences. On the other hand, discriminative methods such as support vector machines enable us to construct flexible decision boundaries and often result in classification performance superior to that of the model based approaches. An ideal classifier should combine these two complementary approaches. In this paper, we develop a natural way of achieving this combination by deriving kernel functions for use in discriminative methods such as support vector machines from generative probability models. We provide a theoretical justification for this combination as well as demonstrate a substantial improvement in the classification performance in the context of DNA and protein sequence analysis."
            },
            "slug": "Exploiting-Generative-Models-in-Discriminative-Jaakkola-Haussler",
            "title": {
                "fragments": [],
                "text": "Exploiting Generative Models in Discriminative Classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A natural way of achieving this combination by deriving kernel functions for use in discriminative methods such as support vector machines from generative probability models is developed."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1778989"
                        ],
                        "name": "M. Szummer",
                        "slug": "M.-Szummer",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Szummer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Szummer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132120"
                        ],
                        "name": "T. Jaakkola",
                        "slug": "T.-Jaakkola",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Jaakkola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jaakkola"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 29
                            }
                        ],
                        "text": "3 Information Regularization Szummer and Jaakkola (2002) propose the information regularization frame work to control the label conditionals p(y|x) by p(x), wherep(x) may be estimated from unlabeled data."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 150
                            }
                        ],
                        "text": "5.1 Transductive SVM . . . . . . . . . . . . . . . . . . . . . . . . . 12 5.2 Null Category Noise Model for Gaussian Processes . . . . . . . . 14 5.3 Information Regularization . . . . . . . . . . . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 761,
                                "start": 29
                            }
                        ],
                        "text": "3 Information Regularization Szummer and Jaakkola (2002) propose the information regularization frame work to control the label conditionals p(y|x) by p(x), wherep(x) may be estimated from unlabeled data. The idea is that labels shouldn\u2019t change too much in regions where p(x) is high. The authors use the mutual information I(x; y) betweenx andy as a measure of label complexity. I(x; y) is small when the labels are homogeneous, and large when labels vary. This motives the minimization of the product of p(x) mass in a region withI(x; y) (normalized by a variance term). The minimization is carried out on multiple overlapping regions covering the data space. The theory is developed further in (Corduneanu & Jaakkola, 2003). C orduneanu and Jaakkola (2005) extend the work by formulating semi-super vised learning as a communication problem."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2731006,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "ca8219c2a7753872ac5343c68140014d57470fef",
            "isKey": true,
            "numCitedBy": 120,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Classification with partially labeled data requires using a large number of unlabeled examples (or an estimated marginal P(x)), to further constrain the conditional P(y|x) beyond a few available labeled examples. We formulate a regularization approach to linking the marginal and the conditional in a general way. The regularization penalty measures the information that is implied about the labels over covering regions. No parametric assumptions are required and the approach remains tractable even for continuous marginal densities P(x). We develop algorithms for solving the regularization problem for finite covers, establish a limiting differential equation, and exemplify the behavior of the new regularization approach in simple cases."
            },
            "slug": "Information-Regularization-with-Partially-Labeled-Szummer-Jaakkola",
            "title": {
                "fragments": [],
                "text": "Information Regularization with Partially Labeled Data"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A regularization approach to linking the marginal and the conditional in a general way is formulated and the regularization penalty measures the information that is implied about the labels over covering regions."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2349519"
                        ],
                        "name": "Joel Ratsaby",
                        "slug": "Joel-Ratsaby",
                        "structuredName": {
                            "firstName": "Joel",
                            "lastName": "Ratsaby",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joel Ratsaby"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144694846"
                        ],
                        "name": "S. Venkatesh",
                        "slug": "S.-Venkatesh",
                        "structuredName": {
                            "firstName": "Santosh",
                            "lastName": "Venkatesh",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Venkatesh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 87
                            }
                        ],
                        "text": "More discussions on identifiability and semi-supervised learning can be found in e.g. (Ratsaby & Venkatesh, 1995) and (Cordunean & Jaakkola, 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17561403,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "53dcb8199cda481d67663efd29f0d80f6f29bf32",
            "isKey": false,
            "numCitedBy": 110,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "1 INTRODUCTION We investigate the tradeoff between labeled The classical problem of learning a classification rule and unlabeled sample complexities in learning can be stated as follows: patterns from classes \" 1 \" and a classification rule for a parametric two-class \" 2 \" (or \" states of nature \") appear with probabilities problem. In the problem considered, a sam-P 1 = P and P2 = 1 \u2013 p, respectively; the pattern classes ple of m labeled examples and n unlabeled ex-are represented by feature vectors x in a common N-amples generated from a two-class, N-variate dimensional Euclidean space R N, the patterns of class Gaussian mixture is provided together with \" i \" distributed according to the class-conditional prob-side information specifying the parametric form ability density fi (x) (i = 1, 2), Labeled pairs (x, y) E of the probability densities. The class means RN x {1,2} are assumed generated according to the fol-and a priori class probabilities are, however, lowing mechanism: a pattern class (or \" label \") y e {1,2} unknown parameters. In this framework we is first drawn randomly according to the distribution of use the maximum likelihood estimation method classes {p 1, P2}; a corresponding random feature vector to estimate the unknown parameters and uti-x c RN is then drawn according to the class-conditional lize rates of convergence of uniform strong laws density fv. In the supervised learning scenario, a labeled to determine the tradeoff between error rate m-sample { (xj, y j), 1 < j < m } is acquired by inde-and sample complexity. In particular, we show pendent sampling from the distribution of pairs (x, y). that for the algorithm used, the misclassifi-Using the sample, the objective is to construct a deci-mation probability deviates from the minimal sion rule which when presented with a random pattern Bayes error rate by Cl(N3/5n-' /5) + Cl(e-cm) x drawn from the mixture density where N is the dimension of the feature space, f(x) = plfl (x) + P2f2(x) m is the number of labeled examples, n is the number of unlabeled examples, and c is a pos-produces a label which disagrees with the true class it ive constant. of origin by a probability P~,,O, close to the minimal pB~yw error rate. Formally this learning problem can be formulated in the framework of the Probably Approximately Correct (PAC) learning model (cf. [9, 10]) as follows: Given e \u2026"
            },
            "slug": "Learning-from-a-mixture-of-labeled-and-unlabeled-Ratsaby-Venkatesh",
            "title": {
                "fragments": [],
                "text": "Learning from a mixture of labeled and unlabeled examples with parametric side information"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "The tradeoff between labeled and unlabeled sample complexities in learning is investigated and pendent sampling from the distribution of pairs (x, y) is shown."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '95"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730609"
                        ],
                        "name": "O. Chapelle",
                        "slug": "O.-Chapelle",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Chapelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Chapelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1808676"
                        ],
                        "name": "V. Sindhwani",
                        "slug": "V.-Sindhwani",
                        "structuredName": {
                            "firstName": "Vikas",
                            "lastName": "Sindhwani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Sindhwani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144106136"
                        ],
                        "name": "S. Keerthi",
                        "slug": "S.-Keerthi",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Keerthi",
                            "middleNames": [
                                "Sathiya"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Keerthi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 39
                            }
                        ],
                        "text": "A book on semi-supervised learning is (Chapelle et al., 2006)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 239378,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "19e96b12f266d7ce1f6e67a5da9df744ab800023",
            "isKey": false,
            "numCitedBy": 136,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Semi-supervised SVMs (S3VM) attempt to learn low-density separators by maximizing the margin over labeled and unlabeled examples. The associated optimization problem is non-convex. To examine the full potential of S3VMs modulo local minima problems in current implementations, we apply branch and bound techniques for obtaining exact, globally optimal solutions. Empirical evidence suggests that the globally optimal solution can return excellent generalization performance in situations where other implementations fail completely. While our current implementation is only applicable to small datasets, we discuss variants that can potentially lead to practically useful algorithms."
            },
            "slug": "Branch-and-Bound-for-Semi-Supervised-Support-Vector-Chapelle-Sindhwani",
            "title": {
                "fragments": [],
                "text": "Branch and Bound for Semi-Supervised Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Empirical evidence suggests that the globally optimal solution of S3VMs modulo local minima problems in current implementations can return excellent generalization performance in situations where other implementations fail completely."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145306271"
                        ],
                        "name": "Neil D. Lawrence",
                        "slug": "Neil-D.-Lawrence",
                        "structuredName": {
                            "firstName": "Neil",
                            "lastName": "Lawrence",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Neil D. Lawrence"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "2 Null Category Noise Model for Gaussian Processes Lawrence and Jordan (2005) proposed a Gaussian process approach, which can be viewed as the Gaussian process parallel of TSVM."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6944818,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9186ed615f4c9d18f7440afdd03a74ff70a2ed78",
            "isKey": false,
            "numCitedBy": 167,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a probabilistic approach to learning a Gaussian Process classifier in the presence of unlabeled data. Our approach involves a \"null category noise model\" (NCNM) inspired by ordered categorical noise models. The noise model reflects an assumption that the data density is lower between the class-conditional densities. We illustrate our approach on a toy problem and present comparative results for the semi-supervised classification of handwritten digits."
            },
            "slug": "Semi-supervised-Learning-via-Gaussian-Processes-Lawrence-Jordan",
            "title": {
                "fragments": [],
                "text": "Semi-supervised Learning via Gaussian Processes"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "A probabilistic approach to learning a Gaussian Process classifier in the presence of unlabeled data using a \"null category noise model\" (NCNM) inspired by ordered categorical noise models."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7446832"
                        ],
                        "name": "Kilian Q. Weinberger",
                        "slug": "Kilian-Q.-Weinberger",
                        "structuredName": {
                            "firstName": "Kilian",
                            "lastName": "Weinberger",
                            "middleNames": [
                                "Q."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kilian Q. Weinberger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1409971380"
                        ],
                        "name": "Ben Packer",
                        "slug": "Ben-Packer",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Packer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ben Packer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796044"
                        ],
                        "name": "L. Saul",
                        "slug": "L.-Saul",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Saul",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Saul"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": ", 2004) (Weinberger et al., 2005)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 15074762,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8381c212e081c319e17c65a2138d46ba5f91cd90",
            "isKey": false,
            "numCitedBy": 184,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe an algorithm for nonlinear dimensionality reduction based on semidefinite programming and kernel matrix factorization. The algorithm learns a kernel matrix for high dimensional data that lies on or near a low dimensional manifold. In earlier work, the kernel matrix was learned by maximizing the variance in feature space while preserving the distances and angles between nearest neighbors. In this paper, adapting recent ideas from semi-supervised learning on graphs, we show that the full kernel matrix can be very well approximated by a product of smaller matrices. Representing the kernel matrix in this way, we can reformulate the semidefinite program in terms of a much smaller submatrix of inner products between randomly chosen landmarks. The new framework leads to order-of-magnitude reductions in computation time and makes it possible to study much larger problems in manifold learning."
            },
            "slug": "Nonlinear-Dimensionality-Reduction-by-Semidefinite-Weinberger-Packer",
            "title": {
                "fragments": [],
                "text": "Nonlinear Dimensionality Reduction by Semidefinite Programming and Kernel Matrix Factorization"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown that the full kernel matrix can be very well approximated by a product of smaller matrices, which leads to order-of-magnitude reductions in computation time and makes it possible to study much larger problems in manifold learning."
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1832364"
                        ],
                        "name": "Xiaojin Zhu",
                        "slug": "Xiaojin-Zhu",
                        "structuredName": {
                            "firstName": "Xiaojin",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaojin Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47909531"
                        ],
                        "name": "Yan Liu",
                        "slug": "Yan-Liu",
                        "structuredName": {
                            "firstName": "Yan",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yan Liu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3096549,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0e785b4d666158c58fdd1885b8af4908b8e23eed",
            "isKey": false,
            "numCitedBy": 176,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Kernel conditional random fields (KCRFs) are introduced as a framework for discriminative modeling of graph-structured data. A representer theorem for conditional graphical models is given which shows how kernel conditional random fields arise from risk minimization procedures defined using Mercer kernels on labeled graphs. A procedure for greedily selecting cliques in the dual representation is then proposed, which allows sparse representations. By incorporating kernels and implicit feature spaces into conditional graphical models, the framework enables semi-supervised learning algorithms for structured data through the use of graph kernels. The framework and clique selection methods are demonstrated in synthetic data experiments, and are also applied to the problem of protein secondary structure prediction."
            },
            "slug": "Kernel-conditional-random-fields:-representation-Lafferty-Zhu",
            "title": {
                "fragments": [],
                "text": "Kernel conditional random fields: representation and clique selection"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "Kernel conditional random fields are introduced as a framework for discriminative modeling of graph-structured data and a procedure for greedily selecting cliques in the dual representation is proposed, which allows sparse representations."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34433830"
                        ],
                        "name": "Akinori Fujino",
                        "slug": "Akinori-Fujino",
                        "structuredName": {
                            "firstName": "Akinori",
                            "lastName": "Fujino",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Akinori Fujino"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735221"
                        ],
                        "name": "N. Ueda",
                        "slug": "N.-Ueda",
                        "structuredName": {
                            "firstName": "Naonori",
                            "lastName": "Ueda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Ueda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1727070"
                        ],
                        "name": "Kazumi Saito",
                        "slug": "Kazumi-Saito",
                        "structuredName": {
                            "firstName": "Kazumi",
                            "lastName": "Saito",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kazumi Saito"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "Fujino et al. (2005) extend generative mixture models by including a \u2018bias correction\u2019 term and discriminative training using the maximum entropy principle."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10999066,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1e834157de8b7a090747842359fbb7060680c45a",
            "isKey": false,
            "numCitedBy": 112,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Semi-supervised classifier design that simultaneously utilizes both labeled and unlabeled samples is a major research issue in machine learning. Existing semisupervised learning methods belong to either generative or discriminative approaches. This paper focuses on probabilistic semi-supervised classifier design and presents a hybrid approach to take advantage of the generative and discriminative approaches. Our formulation considers a generative model trained on labeled samples and a newly introduced bias correction model. Both models belong to the same model family. The proposed hybrid model is constructed by combining both generative and bias correction models based on the maximum entropy principle. The parameters of the bias correction model are estimated by using training data, and combination weights are estimated so that labeled samples are correctly classified. We use naive Bayes models as the generative models to apply the hybrid approach to text classification problems. In our experimental results on three text data sets, we confirmed that the proposed method significantly outperformed pure generative and discriminative methods when the classification performances of the both methods were comparable."
            },
            "slug": "A-Hybrid-Generative/Discriminative-Approach-to-Fujino-Ueda",
            "title": {
                "fragments": [],
                "text": "A Hybrid Generative/Discriminative Approach to Semi-Supervised Classifier Design"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The proposed hybrid model is constructed by combining both generative and bias correction models based on the maximum entropy principle and significantly outperformed pureGenerative and discriminative methods when the classification performances of the both methods were comparable."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7446832"
                        ],
                        "name": "Kilian Q. Weinberger",
                        "slug": "Kilian-Q.-Weinberger",
                        "structuredName": {
                            "firstName": "Kilian",
                            "lastName": "Weinberger",
                            "middleNames": [
                                "Q."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kilian Q. Weinberger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796044"
                        ],
                        "name": "L. Saul",
                        "slug": "L.-Saul",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Saul",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Saul"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Yu et al. (2005) solve the large scale semi-supervised learning problem by using a bipartite graph."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 291166,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8f70d8019618a7c1dfc8118711a2e72ea5a510e7",
            "isKey": false,
            "numCitedBy": 638,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "Can we detect low dimensional structure in high dimensional data sets of images? In this paper, we propose an algorithm for unsupervised learning of image manifolds by semidefinite programming. Given a data set of images, our algorithm computes a low dimensional representation of each image with the property that distances between nearby images are preserved. More generally, it can be used to analyze high dimensional data that lies on or near a low dimensional manifold. We illustrate the algorithm on easily visualized examples of curves and surfaces, as well as on actual images of faces, handwritten digits, and solid objects."
            },
            "slug": "Unsupervised-Learning-of-Image-Manifolds-by-Weinberger-Saul",
            "title": {
                "fragments": [],
                "text": "Unsupervised Learning of Image Manifolds by Semidefinite Programming"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "An algorithm for unsupervised learning of image manifolds by semidefinite programming that computes a low dimensional representation of each image with the property that distances between nearby images are preserved."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1767244"
                        ],
                        "name": "S. Baluja",
                        "slug": "S.-Baluja",
                        "structuredName": {
                            "firstName": "Shumeet",
                            "lastName": "Baluja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Baluja"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 0
                            }
                        ],
                        "text": "Baluja (1998) uses the same algorithm on a face orientation discrimination task. Fujino et al. (2005) extend generative mixture models by including a \u2018bias correction\u2019 term and discriminative training using th e maximum entropy principle."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 0
                            }
                        ],
                        "text": "Baluja (1998) uses the same algorithm on a face orientation discrimination task."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3035742,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e79f93de1c06dfed28df4de7c2cc9d3822c2f401",
            "isKey": false,
            "numCitedBy": 115,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents probabilistic modeling methods to solve the problem of discriminating between five facial orientations with very little labeled data. Three models are explored. The first model maintains no inter-pixel dependencies, the second model is capable of modeling a set of arbitrary pair-wise dependencies, and the last model allows dependencies only between neighboring pixels. We show that for all three of these models, the accuracy of the learned models can be greatly improved by augmenting a small number of labeled training images with a large set of unlabeled images using Expectation-Maximization. This is important because it is often difficult to obtain image labels, while many unlabeled images are readily available. Through a large set of empirical tests, we examine the benefits of unlabeled data for each of the models. By using only two randomly selected labeled examples per class, we can discriminate between the five facial orientations with an accuracy of 94%; with six labeled examples, we achieve an accuracy of 98%."
            },
            "slug": "Probabilistic-Modeling-for-Face-Orientation-from-Baluja",
            "title": {
                "fragments": [],
                "text": "Probabilistic Modeling for Face Orientation Discrimination: Learning from Labeled and Unlabeled Data"
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "118237765"
                        ],
                        "name": "K. Nigam",
                        "slug": "K.-Nigam",
                        "structuredName": {
                            "firstName": "Kanal",
                            "lastName": "Nigam",
                            "middleNames": [
                                "Paul"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Nigam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40975594"
                        ],
                        "name": "Tom Michael Mitchell",
                        "slug": "Tom-Michael-Mitchell",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Mitchell",
                            "middleNames": [
                                "Michael"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tom Michael Mitchell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 67
                            }
                        ],
                        "text": "Remedies include smart choice of starting point by active learning (Nigam, 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 68
                            }
                        ],
                        "text": "Remedies include smart choice\nof starting point by active learning (Nigam, 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 294,
                                "start": 67
                            }
                        ],
                        "text": "be better modeled by multiple multinomial instead of a single one (Nigam et al., 2000). Some other examples are (Shahshahani & Landgrebe, 1994) ( Miller & Uyar, 1997). Another solution is to down-weighing unlabeled data (Cordu neanu & Jaakkola, 2001), which is also used by Nigam et al. (2000), and by Callis on-Burch et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 332,
                                "start": 67
                            }
                        ],
                        "text": "be better modeled by multiple multinomial instead of a single one (Nigam et al., 2000). Some other examples are (Shahshahani & Landgrebe, 1994) ( Miller & Uyar, 1997). Another solution is to down-weighing unlabeled data (Cordu neanu & Jaakkola, 2001), which is also used by Nigam et al. (2000), and by Callis on-Burch et al. (2004) who estimate word alignment for machine translation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15161133,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "50c56af1eb05cfb6ec81e84a6924fb46cb202747",
            "isKey": true,
            "numCitedBy": 165,
            "numCiting": 198,
            "paperAbstract": {
                "fragments": [],
                "text": "One key difficulty with text classification learning algorithms is that they require many hand-labeled examples to learn accurately. This dissertation demonstrates that supervised learning algorithms that use a small number of labeled examples and many inexpensive unlabeled examples can create high-accuracy text classifiers. By assuming that documents are created by a parametric generative model, Expectation-Maximization (EM) finds local maximum a posteriori models and classifiers from all the data\u2014labeled and unlabeled. These generative models do not capture all the intricacies of text; however on some domains this technique substantially improves classification accuracy, especially when labeled data are sparse. \nTwo problems arise from this basic approach. First, unlabeled data can hurt performance in domains where the generative modeling assumptions are too strongly violated. In this case the assumptions can be made more representative in two ways: by modeling sub-topic class structure, and by modeling super-topic hierarchical class relationships. By doing so, model probability and classification accuracy come into correspondence, allowing unlabeled data to improve classification performance. The second problem is that even with a representative model, the improvements given by unlabeled data do not sufficiently compensate for a paucity of labeled data. Here, limited labeled data provide EM initializations that lead to low-probability models. Performance can be significantly improved by using active learning to select high-quality initializations, and by using alternatives to EM that avoid low-probability local maxima."
            },
            "slug": "Using-unlabeled-data-to-improve-text-classification-Nigam-Mitchell",
            "title": {
                "fragments": [],
                "text": "Using unlabeled data to improve text classification"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This dissertation demonstrates that supervised learning algorithms that use a small number of labeled examples and many inexpensive unlabeled examples can create high-accuracy text classifiers."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The decision boundary has the smallest generalization error bound on unlabeled data (Vapnik, 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 145
                            }
                        ],
                        "text": "The name TSVM originates from the intention to work only on the observed data (though people use them for induction anyway), which according to (Vapnik, 1998) is solving a simpler problem."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 28637672,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "385197d4c02593e2823c71e4f90a0993b703620e",
            "isKey": false,
            "numCitedBy": 26322,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "A comprehensive look at learning and generalization theory. The statistical theory of learning and generalization concerns the problem of choosing desired functions on the basis of empirical data. Highly applicable to a variety of computer science and robotics fields, this book offers lucid coverage of the theory as a whole. Presenting a method for determining the necessary and sufficient conditions for consistency of learning process, the author covers function estimates from small data pools, applying these estimations to real-life problems, and much more."
            },
            "slug": "Statistical-learning-theory-Vapnik",
            "title": {
                "fragments": [],
                "text": "Statistical learning theory"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Presenting a method for determining the necessary and sufficient conditions for consistency of learning process, the author covers function estimates from small data pools, applying these estimations to real-life problems, and much more."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2150440259"
                        ],
                        "name": "Xin Yang",
                        "slug": "Xin-Yang",
                        "structuredName": {
                            "firstName": "Xin",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xin Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713425"
                        ],
                        "name": "Haoying Fu",
                        "slug": "Haoying-Fu",
                        "structuredName": {
                            "firstName": "Haoying",
                            "lastName": "Fu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Haoying Fu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145203884"
                        ],
                        "name": "H. Zha",
                        "slug": "H.-Zha",
                        "structuredName": {
                            "firstName": "Hongyuan",
                            "lastName": "Zha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Zha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40527110"
                        ],
                        "name": "J. Barlow",
                        "slug": "J.-Barlow",
                        "structuredName": {
                            "firstName": "Jesse",
                            "lastName": "Barlow",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Barlow"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "One approach for this setting is presented in (Yang et al., 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16902834,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c2266c38da7701fd1feb1ef457df3fc7f503bd46",
            "isKey": false,
            "numCitedBy": 139,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of nonlinear dimensionality reduction is considered. We focus on problems where prior information is available, namely, semi-supervised dimensionality reduction. It is shown that basic nonlinear dimensionality reduction algorithms, such as Locally Linear Embedding (LLE), Isometric feature mapping (ISOMAP), and Local Tangent Space Alignment (LTSA), can be modified by taking into account prior information on exact mapping of certain data points. The sensitivity analysis of our algorithms shows that prior information will improve stability of the solution. We also give some insight on what kind of prior information best improves the solution. We demonstrate the usefulness of our algorithm by synthetic and real life examples."
            },
            "slug": "Semi-supervised-nonlinear-dimensionality-reduction-Yang-Fu",
            "title": {
                "fragments": [],
                "text": "Semi-supervised nonlinear dimensionality reduction"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The sensitivity analysis of the algorithms shows that prior information will improve stability of the solution, and some insight is given on what kind of prior information best improves the solution."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47392513"
                        ],
                        "name": "Jonathan Baxter",
                        "slug": "Jonathan-Baxter",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Baxter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Baxter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 28
                            }
                        ],
                        "text": "In a separate line of work, Baxter (1997) proves that there is a unique o ptimal metric for classification if we use 1-nearest-neighbor."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 910,
                                "start": 28
                            }
                        ],
                        "text": "In a separate line of work, Baxter (1997) proves that there is a unique o ptimal metric for classification if we use 1-nearest-neighbor. The metric, named Ca nonical Distortion Measure (CDM), defines a distance d(x1, x2) as the expected loss if we classifyx1 with x2\u2019s label. The distance measure proposed in (Yianilos, 1995) can be viewed as a special case. Yianilos assume a Gaussian mixture model h as been learned fromU , such that a class correspond to a component, but the correspondence is unknown. In this case CDM d(x1, x2) = p(x1, x2from same component ) and can be computed analytically. Now that a metric has been learned from U , we can find withinL the 1-nearest-neighbor of a new data point x, and classifyx with the nearest neighbor\u2019s label. It will be interesting to compare this scheme with EM based semi-supervised learning, where L is used to label mixture components. Weston et al. (2004) propose the neighborhood mismatch kernel and the b agg d mismatch kernel."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2045411,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0bb8ccbef10f3996cf6a17af689260b8291a8245",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "To measure the quality of a set of vector quantization points a means of measuring the distance between a random point and its quantization is required. Common metrics such as the Hamming and Euclidean metrics, while mathematically simple, are inappropriate for comparing natural signals such as speech or images. In this paper it is shown how an environment of functions on an input space X induces a canonical distortion measure (CDM) on X. The depiction \u201ccanonical\u201d is justified because it is shown that optimizing the reconstruction error of X with respect to the CDM gives rise to optimal piecewise constant approximations of the functions in the environment. The CDM is calculated in closed form for several different function classes. An algorithm for training neural networks to implement the CDM is presented along with some encouraging experimental results."
            },
            "slug": "The-Canonical-Distortion-Measure-for-Vector-and-Baxter",
            "title": {
                "fragments": [],
                "text": "The Canonical Distortion Measure for Vector Quantization and Function Approximation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is shown how an environment of functions on an input space X induces a canonical distortion measure (CDM) on X that gives rise to optimal piecewise constant approximations of the functions in the environment."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067958230"
                        ],
                        "name": "Thanh Phong Pham",
                        "slug": "Thanh-Phong-Pham",
                        "structuredName": {
                            "firstName": "Thanh",
                            "lastName": "Pham",
                            "middleNames": [
                                "Phong"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thanh Phong Pham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34789794"
                        ],
                        "name": "H. Ng",
                        "slug": "H.-Ng",
                        "structuredName": {
                            "firstName": "Hwee",
                            "lastName": "Ng",
                            "middleNames": [
                                "Tou"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740222"
                        ],
                        "name": "Wee Sun Lee",
                        "slug": "Wee-Sun-Lee",
                        "structuredName": {
                            "firstName": "Wee",
                            "lastName": "Lee",
                            "middleNames": [
                                "Sun"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wee Sun Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5595109,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "776e9192019f74489dcec285bd6e553f30172652",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Current word sense disambiguation (WSD) systems based on supervised learning are still limited in that they do not work well for all words in a language. One of the main reasons is the lack of sufficient training data. In this paper, we investigate the use of unlabeled training data for WSD, in the framework of semi-supervised learning. Four semisupervised leaming algorithms are evaluated on 29 nouns of Senseval-2 (SE2) English lexical sample task and SE2 English all-words task. Empirical results show that unlabeled data can bring significant improvement in WSD accuracy."
            },
            "slug": "Word-Sense-Disambiguation-with-Semi-Supervised-Pham-Ng",
            "title": {
                "fragments": [],
                "text": "Word Sense Disambiguation with Semi-Supervised Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Empirical results show that unlabeled data can bring significant improvement in WSD accuracy, and four semisupervised leaming algorithms are evaluated on 29 nouns of Senseval-2 (SE2) English lexical sample task and SE2 English all-words task."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117523996"
                        ],
                        "name": "Qing Lu",
                        "slug": "Qing-Lu",
                        "structuredName": {
                            "firstName": "Qing",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qing Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746034"
                        ],
                        "name": "L. Getoor",
                        "slug": "L.-Getoor",
                        "structuredName": {
                            "firstName": "Lise",
                            "lastName": "Getoor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Getoor"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12810639,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "09b675f16a35e6898fd241c1da718733d71c38cd",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "There has been a surge of interest in learning using a mix of labeled and unlabeled data. General approaches include semi-supervised learning and tranductive inference. In this paper we look at some of the unique ways in which unlabeled data can improve performance when doing link-based classification, the classification of objects making use of both object descriptions and the links between objects."
            },
            "slug": "Link-based-Classifi-cation-using-Labeled-and-Data-Lu-Getoor",
            "title": {
                "fragments": [],
                "text": "Link-based Classifi-cation using Labeled and Unlabeled Data"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Some of the unique ways in which unlabeled data can improve performance when doing link-based classification, the classification of objects making use of both object descriptions and the links between objects are looked at."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145780619"
                        ],
                        "name": "David J. Miller",
                        "slug": "David-J.-Miller",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Miller",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David J. Miller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2309473"
                        ],
                        "name": "H. S. Uyar",
                        "slug": "H.-S.-Uyar",
                        "structuredName": {
                            "firstName": "Hasan",
                            "lastName": "Uyar",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. S. Uyar"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1167,
                                "start": 20
                            }
                        ],
                        "text": "Co-training (Blum & Mitchell, 1998) (Mitchell, 1999) assumes that features c an be split into two sets; Each sub-feature set is sufficient to train a good clas sifier; The two sets are conditionally independent given the class. Initially two sepa rat classifiers are trained with the labeled data, on the two sub-feature sets res pectively. Each classifier then classifies the unlabeled data, and \u2018teaches\u2019 the other classifier with the few unlabeled examples (and the predicted labels) they feel most con fident. Each classifier is retrained with the additional training examples given b y the other classifier, and the process repeats. In co-training, unlabeled data helps by reducing the version space size. In other words, the two classifiers (or hypotheses) must agree on the much larger unlabeled data as well as the labeled data. We need the assumption that sub-features are sufficiently good, so that w e c n trust the labels by each learner on U . We need the sub-features to be conditionally independent so that one classifier\u2019s high confident data points are iid samples for the other classifier. Figure 4 visualizes the assumption. Nigam and Ghani (2000) perform extensive empirical experiments to comp are co-training with generative mixture models and EM."
                    },
                    "intents": []
                }
            ],
            "corpusId": 17425751,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c608ec27a937361122d178b38b6b7387440b58eb",
            "isKey": false,
            "numCitedBy": 346,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "We address statistical classifier design given a mixed training set consisting of a small labelled feature set and a (generally larger) set of unlabelled features. This situation arises, e.g., for medical images, where although training features may be plentiful, expensive expertise is required to extract their class labels. We propose a classifier structure and learning algorithm that make effective use of unlabelled data to improve performance. The learning is based on maximization of the total data likelihood, i.e. over both the labelled and unlabelled data subsets. Two distinct EM learning algorithms are proposed, differing in the EM formalism applied for unlabelled data. The classifier, based on a joint probability model for features and labels, is a \"mixture of experts\" structure that is equivalent to the radial basis function (RBF) classifier, but unlike RBFs, is amenable to likelihood-based training. The scope of application for the new method is greatly extended by the observation that test data, or any new data to classify, is in fact additional, unlabelled data - thus, a combined learning/classification operation - much akin to what is done in image segmentation - can be invoked whenever there is new data to classify. Experiments with data sets from the UC Irvine database demonstrate that the new learning algorithms and structure achieve substantial performance gains over alternative approaches."
            },
            "slug": "A-Mixture-of-Experts-Classifier-with-Learning-Based-Miller-Uyar",
            "title": {
                "fragments": [],
                "text": "A Mixture of Experts Classifier with Learning Based on Both Labelled and Unlabelled Data"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A classifier structure and learning algorithm that make effective use of unlabelled data to improve performance and is a \"mixture of experts\" structure that is equivalent to the radial basis function (RBF) classifier, but unlike RBFs, is amenable to likelihood-based training."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1808676"
                        ],
                        "name": "V. Sindhwani",
                        "slug": "V.-Sindhwani",
                        "structuredName": {
                            "firstName": "Vikas",
                            "lastName": "Sindhwani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Sindhwani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770745"
                        ],
                        "name": "P. Niyogi",
                        "slug": "P.-Niyogi",
                        "structuredName": {
                            "firstName": "Partha",
                            "lastName": "Niyogi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Niyogi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145520115"
                        ],
                        "name": "Mikhail Belkin",
                        "slug": "Mikhail-Belkin",
                        "structuredName": {
                            "firstName": "Mikhail",
                            "lastName": "Belkin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mikhail Belkin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 306,
                                "start": 86
                            }
                        ],
                        "text": "It ha been suggested that temporal correlation serves as the glue, as summarized by ( Sinha et al., 2006) (Result 14). It seems when we observe an object with chan ging angles, we link the images as \u2018containing the same object\u2019 by the virtue that the images are close in time. Wallis and B\u0308 ulthoff (2001) created artificial image sequences where a frontal face is morphed into the profile face of a different person."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14807890,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5de430fffd090658af35538fd2ceffa5dda8d96e",
            "isKey": false,
            "numCitedBy": 68,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "The enormous wealth of unlabeled data in many applications of machine learning is beginning to pose challenges to the designers of semi-supervised learning methods. We are interested in developing linear classification algorithms to efficiently learn from massive partially labeled datasets. In this paper, we propose Linear Laplacian Support Vector Machines and Linear Laplacian Regularized Least Squares as promising solutions to this problem."
            },
            "slug": "Linear-Manifold-Regularization-for-Large-Scale-Sindhwani-Niyogi",
            "title": {
                "fragments": [],
                "text": "Linear Manifold Regularization for Large Scale Semi-supervised Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper proposes Linear Laplacian Support Vector Machines and Linear LaPlacian Regularized Least Squares as promising solutions to the problem of efficiently learning from massive partially labeled datasets."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1799860"
                        ],
                        "name": "T. Griffiths",
                        "slug": "T.-Griffiths",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Griffiths",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Griffiths"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804885"
                        ],
                        "name": "M. Steyvers",
                        "slug": "M.-Steyvers",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Steyvers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Steyvers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796335"
                        ],
                        "name": "D. Blei",
                        "slug": "D.-Blei",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Blei",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Blei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763295"
                        ],
                        "name": "J. Tenenbaum",
                        "slug": "J.-Tenenbaum",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Tenenbaum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tenenbaum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11408454,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0ecc5ffeae38689dd2fe6ed4c32a6745744d7641",
            "isKey": false,
            "numCitedBy": 593,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Statistical approaches to language learning typically focus on either short-range syntactic dependencies or long-range semantic dependencies between words. We present a generative model that uses both kinds of dependencies, and can be used to simultaneously find syntactic classes and semantic topics despite having no representation of syntax or semantics beyond statistical dependency. This model is competitive on tasks like part-of-speech tagging and document classification with models that exclusively use short- and long-range dependencies respectively."
            },
            "slug": "Integrating-Topics-and-Syntax-Griffiths-Steyvers",
            "title": {
                "fragments": [],
                "text": "Integrating Topics and Syntax"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This work presents a generative model that uses both kinds of dependencies, and can be used to simultaneously find syntactic classes and semantic topics despite having no representation of syntax or semantics beyond statistical dependency."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2189118"
                        ],
                        "name": "Ashish Kapoor",
                        "slug": "Ashish-Kapoor",
                        "structuredName": {
                            "firstName": "Ashish",
                            "lastName": "Kapoor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ashish Kapoor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2114755206"
                        ],
                        "name": "Yuan Qi",
                        "slug": "Yuan-Qi",
                        "structuredName": {
                            "firstName": "Yuan",
                            "lastName": "Qi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuan Qi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2259303"
                        ],
                        "name": "Hyung-il Ahn",
                        "slug": "Hyung-il-Ahn",
                        "structuredName": {
                            "firstName": "Hyung-il",
                            "lastName": "Ahn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hyung-il Ahn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719389"
                        ],
                        "name": "Rosalind W. Picard",
                        "slug": "Rosalind-W.-Picard",
                        "structuredName": {
                            "firstName": "Rosalind",
                            "lastName": "Picard",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rosalind W. Picard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5198353,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "df8d59a962e00cc4cc34da822913d71b61c8851a",
            "isKey": false,
            "numCitedBy": 69,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "There have been many graph-based approaches for semi-supervised classification. One problem is that of hyperparameter learning: performance depends greatly on the hyperparameters of the similarity graph, transformation of the graph Laplacian and the noise model. We present a Bayesian framework for learning hyperparameters for graph-based semi-supervised classification. Given some labeled data, which can contain inaccurate labels, we pose the semi-supervised classification as an inference problem over the unknown labels. Expectation Propagation is used for approximate inference and the mean of the posterior is used for classification. The hyperparameters are learned using EM for evidence maximization. We also show that the posterior mean can be written in terms of the kernel matrix, providing a Bayesian classifier to classify new points. Tests on synthetic and real datasets show cases where there are significant improvements in performance over the existing approaches."
            },
            "slug": "Hyperparameter-and-Kernel-Learning-for-Graph-Based-Kapoor-Qi",
            "title": {
                "fragments": [],
                "text": "Hyperparameter and Kernel Learning for Graph Based Semi-Supervised Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "A Bayesian framework for learning hyperparameters for graph-based semi-supervised classification and shows that the posterior mean can be written in terms of the kernel matrix, providing a Bayesian classifier to classify new points."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108029096"
                        ],
                        "name": "Xinhua Zhang",
                        "slug": "Xinhua-Zhang",
                        "structuredName": {
                            "firstName": "Xinhua",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xinhua Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740222"
                        ],
                        "name": "Wee Sun Lee",
                        "slug": "Wee-Sun-Lee",
                        "structuredName": {
                            "firstName": "Wee",
                            "lastName": "Lee",
                            "middleNames": [
                                "Sun"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wee Sun Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9358755,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b7e0cbed52273986e01406875f06b15435f77132",
            "isKey": false,
            "numCitedBy": 60,
            "numCiting": 132,
            "paperAbstract": {
                "fragments": [],
                "text": "Semi-supervised learning algorithms have been successfully applied in many applications with scarce labeled data, by utilizing the unlabeled data. One important category is graph based semi-supervised learning algorithms, for which the performance depends considerably on the quality of the graph, or its hyperparameters. In this paper, we deal with the less explored problem of learning the graphs. We propose a graph learning method for the harmonic energy minimization method; this is done by minimizing the leave-one-out prediction error on labeled data points. We use a gradient based method and designed an efficient algorithm which significantly accelerates the calculation of the gradient by applying the matrix inversion lemma and using careful pre-computation. Experimental results show that the graph learning method is effective in improving the performance of the classification algorithm."
            },
            "slug": "Hyperparameter-Learning-for-Graph-Based-Learning-Zhang-Lee",
            "title": {
                "fragments": [],
                "text": "Hyperparameter Learning for Graph Based Semi-supervised Learning Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper proposes a graph learning method for the harmonic energy minimization method by minimizing the leave-one-out prediction error on labeled data points and designed an efficient algorithm which significantly accelerates the calculation of the gradient by applying the matrix inversion lemma and using careful pre-computation."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7446832"
                        ],
                        "name": "Kilian Q. Weinberger",
                        "slug": "Kilian-Q.-Weinberger",
                        "structuredName": {
                            "firstName": "Kilian",
                            "lastName": "Weinberger",
                            "middleNames": [
                                "Q."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kilian Q. Weinberger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145757665"
                        ],
                        "name": "Fei Sha",
                        "slug": "Fei-Sha",
                        "structuredName": {
                            "firstName": "Fei",
                            "lastName": "Sha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fei Sha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796044"
                        ],
                        "name": "L. Saul",
                        "slug": "L.-Saul",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Saul",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Saul"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": ", 2000), locally linear embedding (LLE) (Roweis & Saul, 2000) (Saul & Roweis, 2003), Hessian LLE (Donoho & Grimes, 2003), Laplacian eigenmaps (Belkin & Niyogi, 2003), and semidefinite embedding (SDE) (Weinberger & Saul, 2004) (Weinberger et al., 2004) (Weinberger et al."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Consider the Teapot dataset used in (Zhu & Lafferty, 2005) (originally from (Weinberger et al., 2004)), with images of a teapot viewed from different angles."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8585434,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2ff87fef3fc79b5bb47f2783e5b28084ef6b83c4",
            "isKey": false,
            "numCitedBy": 551,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate how to learn a kernel matrix for high dimensional data that lies on or near a low dimensional manifold. Noting that the kernel matrix implicitly maps the data into a nonlinear feature space, we show how to discover a mapping that \"unfolds\" the underlying manifold from which the data was sampled. The kernel matrix is constructed by maximizing the variance in feature space subject to local constraints that preserve the angles and distances between nearest neighbors. The main optimization involves an instance of semidefinite programming---a fundamentally different computation than previous algorithms for manifold learning, such as Isomap and locally linear embedding. The optimized kernels perform better than polynomial and Gaussian kernels for problems in manifold learning, but worse for problems in large margin classification. We explain these results in terms of the geometric properties of different kernels and comment on various interpretations of other manifold learning algorithms as kernel methods."
            },
            "slug": "Learning-a-kernel-matrix-for-nonlinear-reduction-Weinberger-Sha",
            "title": {
                "fragments": [],
                "text": "Learning a kernel matrix for nonlinear dimensionality reduction"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "This work investigates how to learn a kernel matrix for high dimensional data that lies on or near a low dimensional manifold and shows how to discover a mapping that \"unfolds\" the underlying manifold from which the data was sampled."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2104994444"
                        ],
                        "name": "Kamal Nigamyknigam",
                        "slug": "Kamal-Nigamyknigam",
                        "structuredName": {
                            "firstName": "Kamal",
                            "lastName": "Nigamyknigam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kamal Nigamyknigam"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 35
                            }
                        ],
                        "text": "Mixture of multivariate Bernoulli (McCallum & Nigam, 1998a) is not identifiable."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 0
                            }
                        ],
                        "text": "McCallum and Nigam (1998b) use EM with unlabeled data integrated into the active learning algorithm. Muslea et al. (2002) propose CO-EMT which co mbines multi-view (e."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 550,
                                "start": 0
                            }
                        ],
                        "text": "McCallum and Nigam (1998b) use EM with unlabeled data integrated into the active learning algorithm. Muslea et al. (2002) propose CO-EMT which co mbines multi-view (e.g. co-training) learning with active learning. Zhou et al. (2004 c); Zhou et al. (2006b) apply semi-supervised learning together with active le arning to content-based image retrieval. Many active learning algorithms naively select as query the point with maximum label ambiguity (entropy), or least confidence, or maximum disagreemen t between multiple learners. Zhu et al. (2003b) show that these are not nec essarily the right things to do, if one is interested in classification error."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 0
                            }
                        ],
                        "text": "McCallum and Nigam (1998b) use EM with unlabeled data integrated into the active learning algorithm."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 256,
                                "start": 0
                            }
                        ],
                        "text": "McCallum and Nigam (1998b) use EM with unlabeled data integrated into the active learning algorithm. Muslea et al. (2002) propose CO-EMT which co mbines multi-view (e.g. co-training) learning with active learning. Zhou et al. (2004 c); Zhou et al. (2006b) apply semi-supervised learning together with active le arning to content-based image retrieval."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14133176,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6f17768a9fe231a2fd38708be90f98db3890c986",
            "isKey": true,
            "numCitedBy": 210,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper shows how a text classiier's need for labeled training data can be reduced by a combination of active learning and Expectation Maximization (EM) on a pool of unlabeled data. Query-by-Committee is used to actively select documents for labeling, then EM with a naive Bayes model further improves classiication accuracy by concurrently estimating probabilistic labels for the remaining unlabeled documents and using them to improve the model. We also present a metric for better measuring disagreement among committee members; it accounts for the strength of their disagreement and for the distribution of the documents. Experimental results show that our method of combining EM and active learning requires only half as many labeled training examples to achieve the same accuracy as either EM or active learning alone."
            },
            "slug": "Employing-Em-in-Pool-based-Active-Learning-for-Text-Nigamyknigam",
            "title": {
                "fragments": [],
                "text": "Employing Em in Pool-based Active Learning for Text Classiication"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "This paper shows how a text classiier's need for labeled training data can be reduced by a combination of active learning and Expectation Maximization on a pool of unlabeled data and presents a metric for better measuring disagreement among committee members."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693517"
                        ],
                        "name": "David Yarowsky",
                        "slug": "David-Yarowsky",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Yarowsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Yarowsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Yarowsky (1995) uses self-training for word sense disambiguation, e."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1487550,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "944cba683d10d8c1a902e05cd68e32a9f47b372e",
            "isKey": false,
            "numCitedBy": 2536,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents an unsupervised learning algorithm for sense disambiguation that, when trained on unannotated English text, rivals the performance of supervised techniques that require time-consuming hand annotations. The algorithm is based on two powerful constraints---that words tend to have one sense per discourse and one sense per collocation---exploited in an iterative bootstrapping procedure. Tested accuracy exceeds 96%."
            },
            "slug": "Unsupervised-Word-Sense-Disambiguation-Rivaling-Yarowsky",
            "title": {
                "fragments": [],
                "text": "Unsupervised Word Sense Disambiguation Rivaling Supervised Methods"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "An unsupervised learning algorithm for sense disambiguation that, when trained on unannotated English text, rivals the performance of supervised techniques that require time-consuming hand annotations."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2879453"
                        ],
                        "name": "V. Castelli",
                        "slug": "V.-Castelli",
                        "structuredName": {
                            "firstName": "Vittorio",
                            "lastName": "Castelli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Castelli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752732"
                        ],
                        "name": "T. Cover",
                        "slug": "T.-Cover",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Cover",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Cover"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 94
                            }
                        ],
                        "text": "If the mixture model assumption is correct, unlabeled data is guaranteed to improve accuracy (Castelli & Cover, 1995) (Castelli & Cover, 1996) (Ratsaby& Venkatesh, 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 35473938,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "f67e9a6bd7c688f1c9c653584a4fa1f9c7fda2a6",
            "isKey": false,
            "numCitedBy": 235,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-the-exponential-value-of-labeled-samples-Castelli-Cover",
            "title": {
                "fragments": [],
                "text": "On the exponential value of labeled samples"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit. Lett."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38551815"
                        ],
                        "name": "Anat Levin",
                        "slug": "Anat-Levin",
                        "structuredName": {
                            "firstName": "Anat",
                            "lastName": "Levin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anat Levin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1684384"
                        ],
                        "name": "Dani Lischinski",
                        "slug": "Dani-Lischinski",
                        "structuredName": {
                            "firstName": "Dani",
                            "lastName": "Lischinski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dani Lischinski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30400079"
                        ],
                        "name": "Yair Weiss",
                        "slug": "Yair-Weiss",
                        "structuredName": {
                            "firstName": "Yair",
                            "lastName": "Weiss",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yair Weiss"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11713946,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bad97714e7b7ca4f8e49dd1fefc05e7d9bea6658",
            "isKey": false,
            "numCitedBy": 1193,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Colorization is a computer-assisted process of adding color to a monochrome image or movie. The process typically involves segmenting images into regions and tracking these regions across image sequences. Neither of these tasks can be performed reliably in practice; consequently, colorization requires considerable user intervention and remains a tedious, time-consuming, and expensive task.In this paper we present a simple colorization method that requires neither precise image segmentation, nor accurate region tracking. Our method is based on a simple premise; neighboring pixels in space-time that have similar intensities should have similar colors. We formalize this premise using a quadratic cost function and obtain an optimization problem that can be solved efficiently using standard techniques. In our approach an artist only needs to annotate the image with a few color scribbles, and the indicated colors are automatically propagated in both space and time to produce a fully colorized image or sequence. We demonstrate that high quality colorizations of stills and movie clips may be obtained from a relatively modest amount of user input."
            },
            "slug": "Colorization-using-optimization-Levin-Lischinski",
            "title": {
                "fragments": [],
                "text": "Colorization using optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper presents a simple colorization method that requires neither precise image segmentation, nor accurate region tracking, and demonstrates that high quality colorizations of stills and movie clips may be obtained from a relatively modest amount of user input."
            },
            "venue": {
                "fragments": [],
                "text": "ACM Trans. Graph."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9330607"
                        ],
                        "name": "S. Roweis",
                        "slug": "S.-Roweis",
                        "structuredName": {
                            "firstName": "Sam",
                            "lastName": "Roweis",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roweis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796044"
                        ],
                        "name": "L. Saul",
                        "slug": "L.-Saul",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Saul",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Saul"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Sajama and Orlitsky (2005) analyze the lower and upper bounds on estimating data-density-based distance."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5987139,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "afcd6da7637ddeef6715109aca248da7a24b1c65",
            "isKey": false,
            "numCitedBy": 13981,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Many areas of science depend on exploratory data analysis and visualization. The need to analyze large amounts of multivariate data raises the fundamental problem of dimensionality reduction: how to discover compact representations of high-dimensional data. Here, we introduce locally linear embedding (LLE), an unsupervised learning algorithm that computes low-dimensional, neighborhood-preserving embeddings of high-dimensional inputs. Unlike clustering methods for local dimensionality reduction, LLE maps its inputs into a single global coordinate system of lower dimensionality, and its optimizations do not involve local minima. By exploiting the local symmetries of linear reconstructions, LLE is able to learn the global structure of nonlinear manifolds, such as those generated by images of faces or documents of text."
            },
            "slug": "Nonlinear-dimensionality-reduction-by-locally-Roweis-Saul",
            "title": {
                "fragments": [],
                "text": "Nonlinear dimensionality reduction by locally linear embedding."
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Locally linear embedding (LLE) is introduced, an unsupervised learning algorithm that computes low-dimensional, neighborhood-preserving embeddings of high-dimensional inputs that learns the global structure of nonlinear manifolds."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1911526"
                        ],
                        "name": "A. Corduneanu",
                        "slug": "A.-Corduneanu",
                        "structuredName": {
                            "firstName": "Adrian",
                            "lastName": "Corduneanu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Corduneanu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132120"
                        ],
                        "name": "T. Jaakkola",
                        "slug": "T.-Jaakkola",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Jaakkola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jaakkola"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The heuristics in (Delalleau et al., 2005) similarly create a small graph with a subset of the unlabeled data. They enables fast approximate computation by reducing the problem size. Garcke and Griebel (2005) propose the use of sparse grids for semi-supervised learning."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5430060,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1097ebc8459336b3f425ce4684f7463c10b50818",
            "isKey": false,
            "numCitedBy": 48,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "We provide a principle for semi-supervised learning based on optimizing the rate of communicating labels for unlabeled points with side information. The side information is expressed in terms of identities of sets of points or regions with the purpose of biasing the labels in each region to be the same. The resulting regularization objective is convex, has a unique solution, and the solution can be found with a pair of local propagation operations on graphs induced by the regions. We analyze the properties of the algorithm and demonstrate its performance on document classification tasks."
            },
            "slug": "Distributed-Information-Regularization-on-Graphs-Corduneanu-Jaakkola",
            "title": {
                "fragments": [],
                "text": "Distributed Information Regularization on Graphs"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "A principle for semi-supervised learning based on optimizing the rate of communicating labels for unlabeled points with side information that has a unique solution and demonstrates its performance on document classification tasks."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144494993"
                        ],
                        "name": "R. Jones",
                        "slug": "R.-Jones",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Jones",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Jones"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "(2005) propose a simple procedure for semi-supervised learning: First one runs PCA onL \u222a U (ignoring the labels)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 297,
                                "start": 86
                            }
                        ],
                        "text": "The mean is known as a harmonic function, which has many interesting properties (Zhu, 2005). Recently Grady and Funka-Lea (2004) applied the harmonic function metho d to medical image segmentation tasks, where a user labels classes (e.g. differ ent organs) with a few strokes. Levin et al. (2004) use the equivalent of h armonic functions for colorization of gray-scale images."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 86
                            }
                        ],
                        "text": "The mean is known as a harmonic function, which has many interesting properties (Zhu, 2005). Recently Grady and Funka-Lea (2004) applied the harmonic function metho d to medical image segmentation tasks, where a user labels classes (e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 330,
                                "start": 72
                            }
                        ],
                        "text": "Such k ernel machines include the kernelized conditional random fields (Lafferty et al., 2 004) and maximum margin Markov networks (Taskar et al., 2003), which differ primar ily by the loss function they use. With a graph kernel the kernel machine thus perform semi-supervised lea rning on structured data. Lafferty et al. (2004) hinted this idea and tested it o n a bioinformatics dataset."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 26239002,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e71ce0ca28516ef20f066f3c756960798086aa28",
            "isKey": true,
            "numCitedBy": 71,
            "numCiting": 99,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe and evaluate algorithms for learning to extract semantic classes from sentences in text documents, using the minimum of training information. The thesis of this research is that we can efficiently automate information extraction, that is, learn from tens of examples of labeled training data instead of requiring thousands, by exploiting redundancy and separability of the features noun-phrases and contexts. We exploit this redundancy and separability in two ways: (1) in the algorithms for learning semantic classes, and (2) in novel algorithms for active learning, leading to better extractors for a given amount of user labeling effort."
            },
            "slug": "Learning-to-Extract-Entities-from-Labeled-and-Text-Jones",
            "title": {
                "fragments": [],
                "text": "Learning to Extract Entities from Labeled and Unlabeled Text"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The thesis of this research is that it can efficiently automate information extraction, that is, learn from tens of examples of labeled training data instead of requiring thousands, by exploiting redundancy and separability of the features noun-phrases and contexts."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691993"
                        ],
                        "name": "E. Riloff",
                        "slug": "E.-Riloff",
                        "structuredName": {
                            "firstName": "Ellen",
                            "lastName": "Riloff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Riloff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144120827"
                        ],
                        "name": "J. Wiebe",
                        "slug": "J.-Wiebe",
                        "structuredName": {
                            "firstName": "Janyce",
                            "lastName": "Wiebe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Wiebe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47481818"
                        ],
                        "name": "Theresa Wilson",
                        "slug": "Theresa-Wilson",
                        "structuredName": {
                            "firstName": "Theresa",
                            "lastName": "Wilson",
                            "middleNames": [],
                            "suffix": ""
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Theresa Wilson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1164969,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "96f9e8ee890b4e142dd34dcf93fd52678eecd2b5",
            "isKey": false,
            "numCitedBy": 541,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We explore the idea of creating a subjectivity classifier that uses lists of subjective nouns learned by bootstrapping algorithms. The goal of our research is to develop a system that can distinguish subjective sentences from objective sentences. First, we use two bootstrapping algorithms that exploit extraction patterns to learn sets of subjective nouns. Then we train a Naive Bayes classifier using the subjective nouns, discourse features, and subjectivity clues identified in prior research. The bootstrapping algorithms learned over 1000 subjective nouns, and the subjectivity classifier performed well, achieving 77% recall with 81% precision."
            },
            "slug": "Learning-subjective-nouns-using-extraction-pattern-Riloff-Wiebe",
            "title": {
                "fragments": [],
                "text": "Learning subjective nouns using extraction pattern bootstrapping"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The goal of the research is to develop a system that can distinguish subjective sentences from objective sentences, and a Naive Bayes classifier is trained using the subjective nouns, discourse features, and subjectivity clues identified in prior research."
            },
            "venue": {
                "fragments": [],
                "text": "CoNLL"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144316140"
                        ],
                        "name": "G. Wallis",
                        "slug": "G.-Wallis",
                        "structuredName": {
                            "firstName": "Guy",
                            "lastName": "Wallis",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wallis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747836"
                        ],
                        "name": "H. B\u00fclthoff",
                        "slug": "H.-B\u00fclthoff",
                        "structuredName": {
                            "firstName": "Heinrich",
                            "lastName": "B\u00fclthoff",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. B\u00fclthoff"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6556548,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "6ed561e682823c87a38dd96dccac35e732ad81ce",
            "isKey": false,
            "numCitedBy": 245,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "The influence of temporal association on the representation and recognition of objects was investigated. Observers were shown sequences of novel faces in which the identity of the face changed as the head rotated. As a result, observers showed a tendency to treat the views as if they were of the same person. Additional experiments revealed that this was only true if the training sequences depicted head rotations rather than jumbled views; in other words, the sequence had to be spatially as well as temporally smooth. Results suggest that we are continuously associating views of objects to support later recognition, and that we do so not only on the basis of the physical similarity, but also the correlated appearance in time of the objects."
            },
            "slug": "Effects-of-temporal-association-on-recognition-Wallis-B\u00fclthoff",
            "title": {
                "fragments": [],
                "text": "Effects of temporal association on recognition memory"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Results suggest that the authors are continuously associating views of objects to support later recognition, and that they do so not only on the basis of the physical similarity, but also the correlated appearance in time of the objects."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences of the United States of America"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "24982365"
                        ],
                        "name": "Dengyong Zhou",
                        "slug": "Dengyong-Zhou",
                        "structuredName": {
                            "firstName": "Dengyong",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dengyong Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143936663"
                        ],
                        "name": "Thomas Hofmann",
                        "slug": "Thomas-Hofmann",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Hofmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Hofmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1402489,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ddb2567f1e3630529b05c19c4fe99720de4b481b",
            "isKey": false,
            "numCitedBy": 200,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Given a directed graph in which some of the nodes are labeled, we investigate the question of how to exploit the link structure of the graph to infer the labels of the remaining unlabeled nodes. To that extent we propose a regularization framework for functions defined over nodes of a directed graph that forces the classification function to change slowly on densely linked subgraphs. A powerful, yet computationally simple classification algorithm is derived within the proposed framework. The experimental evaluation on real-world Web classification problems demonstrates encouraging results that validate our approach."
            },
            "slug": "Semi-supervised-Learning-on-Directed-Graphs-Zhou-Sch\u00f6lkopf",
            "title": {
                "fragments": [],
                "text": "Semi-supervised Learning on Directed Graphs"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A regularization framework for functions defined over nodes of a directed graph that forces the classification function to change slowly on densely linked subgraphs is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2715551"
                        ],
                        "name": "Zheng-Yu Niu",
                        "slug": "Zheng-Yu-Niu",
                        "structuredName": {
                            "firstName": "Zheng-Yu",
                            "lastName": "Niu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zheng-Yu Niu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719916"
                        ],
                        "name": "D. Ji",
                        "slug": "D.-Ji",
                        "structuredName": {
                            "firstName": "Dong-Hong",
                            "lastName": "Ji",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ji"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679749"
                        ],
                        "name": "C. Tan",
                        "slug": "C.-Tan",
                        "structuredName": {
                            "firstName": "Chew",
                            "lastName": "Tan",
                            "middleNames": [
                                "Lim"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Tan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6264696,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4d5faafe1b839a2b3f280555ddbb6288b02e01ae",
            "isKey": false,
            "numCitedBy": 120,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "Shortage of manually sense-tagged data is an obstacle to supervised word sense disambiguation methods. In this paper we investigate a label propagation based semi-supervised learning algorithm for WSD, which combines labeled and unlabeled data in learning process to fully realize a global consistency assumption: similar examples should have similar labels. Our experimental results on benchmark corpora indicate that it consistently outperforms SVM when only very few labeled examples are available, and its performance is also better than monolingual bootstrapping, and comparable to bilingual bootstrapping."
            },
            "slug": "Word-Sense-Disambiguation-Using-Label-Propagation-Niu-Ji",
            "title": {
                "fragments": [],
                "text": "Word Sense Disambiguation Using Label Propagation Based Semi-Supervised Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "This paper investigates a label propagation based semi-supervised learning algorithm for WSD, which combines labeled and unlabeled data in learning process to fully realize a global consistency assumption: similar examples should have similar labels."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1400347470"
                        ],
                        "name": "M. A. Carreira-Perpi\u00f1\u00e1n",
                        "slug": "M.-A.-Carreira-Perpi\u00f1\u00e1n",
                        "structuredName": {
                            "firstName": "Miguel",
                            "lastName": "Carreira-Perpi\u00f1\u00e1n",
                            "middleNames": [
                                "\u00c1."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. A. Carreira-Perpi\u00f1\u00e1n"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804104"
                        ],
                        "name": "R. Zemel",
                        "slug": "R.-Zemel",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Zemel",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Zemel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2661550,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6193da2b80be4f952292e00258415dab7ee7e172",
            "isKey": false,
            "numCitedBy": 130,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Many machine learning algorithms for clustering or dimensionality reduction take as input a cloud of points in Euclidean space, and construct a graph with the input data points as vertices. This graph is then partitioned (clustering) or used to redefine metric information (dimensionality reduction). There has been much recent work on new methods for graph-based clustering and dimensionality reduction, but not much on constructing the graph itself. Graphs typically used include the fully-connected graph, a local fixed-grid graph (for image segmentation) or a nearest-neighbor graph. We suggest that the graph should adapt locally to the structure of the data. This can be achieved by a graph ensemble that combines multiple minimum spanning trees, each fit to a perturbed version of the data set. We show that such a graph ensemble usually produces a better representation of the data manifold than standard methods; and that it provides robustness to a subsequent clustering or dimensionality reduction algorithm based on the graph."
            },
            "slug": "Proximity-Graphs-for-Clustering-and-Manifold-Carreira-Perpi\u00f1\u00e1n-Zemel",
            "title": {
                "fragments": [],
                "text": "Proximity Graphs for Clustering and Manifold Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown that a graph ensemble usually produces a better representation of the data manifold than standard methods; and that it provides robustness to a subsequent clustering or dimensionality reduction algorithm based on the graph."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1832364"
                        ],
                        "name": "Xiaojin Zhu",
                        "slug": "Xiaojin-Zhu",
                        "structuredName": {
                            "firstName": "Xiaojin",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaojin Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145536952"
                        ],
                        "name": "J. Kandola",
                        "slug": "J.-Kandola",
                        "structuredName": {
                            "firstName": "Jaz",
                            "lastName": "Kandola",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kandola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744700"
                        ],
                        "name": "Zoubin Ghahramani",
                        "slug": "Zoubin-Ghahramani",
                        "structuredName": {
                            "firstName": "Zoubin",
                            "lastName": "Ghahramani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zoubin Ghahramani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2283419,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "96fe05eae94127593ec36858ea4f5d12af28ff93",
            "isKey": false,
            "numCitedBy": 194,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an algorithm based on convex optimization for constructing kernels for semi-supervised learning. The kernel matrices are derived from the spectral decomposition of graph Laplacians, and combine labeled and unlabeled data in a systematic fashion. Unlike previous work using diffusion kernels and Gaussian random field kernels, a nonparametric kernel approach is presented that incorporates order constraints during optimization. This results in flexible kernels and avoids the need to choose among different parametric forms. Our approach relies on a quadratically constrained quadratic program (QCQP), and is computationally feasible for large datasets. We evaluate the kernels on real datasets using support vector machines, with encouraging results."
            },
            "slug": "Nonparametric-Transforms-of-Graph-Kernels-for-Zhu-Kandola",
            "title": {
                "fragments": [],
                "text": "Nonparametric Transforms of Graph Kernels for Semi-Supervised Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "An algorithm based on convex optimization for constructing kernels for semi-supervised learning that incorporates order constraints during optimization results in flexible kernels and avoids the need to choose among different parametric forms."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145321667"
                        ],
                        "name": "B. Liu",
                        "slug": "B.-Liu",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740222"
                        ],
                        "name": "Wee Sun Lee",
                        "slug": "Wee-Sun-Lee",
                        "structuredName": {
                            "firstName": "Wee",
                            "lastName": "Lee",
                            "middleNames": [
                                "Sun"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wee Sun Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144019071"
                        ],
                        "name": "Philip S. Yu",
                        "slug": "Philip-S.-Yu",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Yu",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philip S. Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39952499"
                        ],
                        "name": "Xiaoli Li",
                        "slug": "Xiaoli-Li",
                        "structuredName": {
                            "firstName": "Xiaoli",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaoli Li"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Another set of methods heuristically identify some \u2018reliable\u2019 negative examples in the unlabeled set, and use EM on generative (Naive Bayes) models (Liu et al., 2002) or logistic regression (Lee & Liu, 2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2748729,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "006b577653e0e91e1cfd37c32c6d1cfb198a730e",
            "isKey": false,
            "numCitedBy": 559,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate the following problem: Given a set of documents of a particular topic or class P , and a large set M of mixed documents that contains documents from class P and other types of documents, identify the documents from class P in M . The key feature of this problem is that there is no labeled nonP document, which makes traditional machine learning techniques inapplicable, as they all need labeled documents of both classes. We call this problem partially supervised classification. In this paper, we show that this problem can be posed as a constrained optimization problem and that under appropriate conditions, solutions to the constrained optimization problem will give good solutions to the partially supervised classification problem. We present a novel technique to solve the problem and demonstrate the effectiveness of the technique through extensive experimentation."
            },
            "slug": "Partially-Supervised-Classification-of-Text-Liu-Lee",
            "title": {
                "fragments": [],
                "text": "Partially Supervised Classification of Text Documents"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "This paper shows that the problem of identifying documents from a set of documents of a particular topic or class P and a large set M of mixed documents, and that under appropriate conditions, solutions to the constrained optimization problem will give good solution to the partially supervised classification problem."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685083"
                        ],
                        "name": "N. Cristianini",
                        "slug": "N.-Cristianini",
                        "structuredName": {
                            "firstName": "Nello",
                            "lastName": "Cristianini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cristianini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404459229"
                        ],
                        "name": "J. Shawe-Taylor",
                        "slug": "J.-Shawe-Taylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shawe-Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shawe-Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1727076"
                        ],
                        "name": "H. Lodhi",
                        "slug": "H.-Lodhi",
                        "structuredName": {
                            "firstName": "Huma",
                            "lastName": "Lodhi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Lodhi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5516891,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "942ed8a81716c1f1038ed091ca6149bc3ae22c8f",
            "isKey": false,
            "numCitedBy": 319,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Kernel methods like support vector machines have successfully been used for text categorization. A standard choice of kernel function has been the inner product between the vector-space representation of two documents, in analogy with classical information retrieval (IR) approaches.Latent semantic indexing (LSI) has been successfully used for IR purposes as a technique for capturing semantic relations between terms and inserting them into the similarity measure between two documents. One of its main drawbacks, in IR, is its computational cost.In this paper we describe how the LSI approach can be implemented in a kernel-defined feature space.We provide experimental results demonstrating that the approach can significantly improve performance, and that it does not impair it."
            },
            "slug": "Latent-Semantic-Kernels-Cristianini-Shawe-Taylor",
            "title": {
                "fragments": [],
                "text": "Latent Semantic Kernels"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper describes how the LSI approach can be implemented in a kernel-defined feature space and provides experimental results demonstrating that the approach can significantly improve performance, and that it does not impair it."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Intelligent Information Systems"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690967"
                        ],
                        "name": "A. Blum",
                        "slug": "A.-Blum",
                        "structuredName": {
                            "firstName": "Avrim",
                            "lastName": "Blum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Blum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144500070"
                        ],
                        "name": "Shuchi Chawla",
                        "slug": "Shuchi-Chawla",
                        "structuredName": {
                            "firstName": "Shuchi",
                            "lastName": "Chawla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shuchi Chawla"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "1 Mincut Blum and Chawla (2001) pose semi-supervised learning as a graph mincut (also known as st-cut) problem."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5892518,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0eedbab3ae55fd6a4e7bbc75fcc261293384f883",
            "isKey": false,
            "numCitedBy": 1057,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Many application domains suffer from not having enough labeled training data for learning. However, large amounts of unlabeled examples can often be gathered cheaply. As a result, there has been a great deal of work in recent years on how unlabeled data can be used to aid classification. We consider an algorithm based on finding minimum cuts in graphs, that uses pairwise relationships among the examples in order to learn from both labeled and unlabeled data."
            },
            "slug": "Learning-from-Labeled-and-Unlabeled-Data-using-Blum-Chawla",
            "title": {
                "fragments": [],
                "text": "Learning from Labeled and Unlabeled Data using Graph Mincuts"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "An algorithm based on finding minimum cuts in graphs, that uses pairwise relationships among the examples in order to learn from both labeled and unlabeled data is considered."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30400079"
                        ],
                        "name": "Yair Weiss",
                        "slug": "Yair-Weiss",
                        "structuredName": {
                            "firstName": "Yair",
                            "lastName": "Weiss",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yair Weiss"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 66
                            }
                        ],
                        "text": "There are s ev ral criteria on what constitutes a good clustering (Weiss, 1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15872360,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e9d7f589a3d368a3701832e28d90ca09ec9e5577",
            "isKey": false,
            "numCitedBy": 857,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Automatic grouping and segmentation of images remains a challenging problem in computer vision. Recently, a number of authors have demonstrated good performance on this task using methods that are based on eigenvectors of the affinity matrix. These approaches are extremely attractive in that they are based on simple eigendecomposition algorithms whose stability is well understood. Nevertheless, the use of eigendecompositions in the context of segmentation is far from well understood. In this paper we give a unified treatment of these algorithms, and show the close connections between them while highlighting their distinguishing features. We then prove results on eigenvectors of block matrices that allow us to analyze the performance of these algorithms in simple grouping settings. Finally, we use our analysis to motivate a variation on the existing methods that combines aspects from different eigenvector segmentation algorithms. We illustrate our analysis with results on real and synthetic images."
            },
            "slug": "Segmentation-using-eigenvectors:-a-unifying-view-Weiss",
            "title": {
                "fragments": [],
                "text": "Segmentation using eigenvectors: a unifying view"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A unified treatment of eigenvectors of block matrices based on eigendecompositions in the context of segmentation is given, and close connections between them are shown while highlighting their distinguishing features."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Seventh IEEE International Conference on Computer Vision"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145172877"
                        ],
                        "name": "K. Nigam",
                        "slug": "K.-Nigam",
                        "structuredName": {
                            "firstName": "Kamal",
                            "lastName": "Nigam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Nigam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144867807"
                        ],
                        "name": "S. Thrun",
                        "slug": "S.-Thrun",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Thrun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Thrun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40975594"
                        ],
                        "name": "Tom Michael Mitchell",
                        "slug": "Tom-Michael-Mitchell",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Mitchell",
                            "middleNames": [
                                "Michael"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tom Michael Mitchell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This is similar to semisupervised learning with mixture models (Nigam et al., 2000) or clusters (Dara et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 105
                            }
                        ],
                        "text": "Another solution is to down-weighing unlabeled data (Corduneanu & Jaakkola, 2001), which is also used by Nigam et al. (2000), and by Callison-Burch et al. (2004) who estimate word alignment for machine translation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "be better modeled by multiple multinomial instead of a single one (Nigam et al., 2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 0
                            }
                        ],
                        "text": "Nigam et al. (2000) apply the EM algorithm on mixture of multinomial for the task of text classification."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 151
                            }
                        ],
                        "text": "For example in text categorization a topic may contain several sub-topics, and will\nbe better modeled by multiple multinomial instead of a single one (Nigam et al., 2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 686980,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e2de29049d62de925cf709024b92774cd82b0a5a",
            "isKey": true,
            "numCitedBy": 3072,
            "numCiting": 79,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper shows that the accuracy of learned text classifiers can be improved by augmenting a small number of labeled training documents with a large pool of unlabeled documents. This is important because in many text classification problems obtaining training labels is expensive, while large quantities of unlabeled documents are readily available.We introduce an algorithm for learning from labeled and unlabeled documents based on the combination of Expectation-Maximization (EM) and a naive Bayes classifier. The algorithm first trains a classifier using the available labeled documents, and probabilistically labels the unlabeled documents. It then trains a new classifier using the labels for all the documents, and iterates to convergence. This basic EM procedure works well when the data conform to the generative assumptions of the model. However these assumptions are often violated in practice, and poor performance can result. We present two extensions to the algorithm that improve classification accuracy under these conditions: (1) a weighting factor to modulate the contribution of the unlabeled data, and (2) the use of multiple mixture components per class. Experimental results, obtained using text from three different real-world tasks, show that the use of unlabeled data reduces classification error by up to 30%."
            },
            "slug": "Text-Classification-from-Labeled-and-Unlabeled-EM-Nigam-McCallum",
            "title": {
                "fragments": [],
                "text": "Text Classification from Labeled and Unlabeled Documents using EM"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "This paper shows that the accuracy of learned text classifiers can be improved by augmenting a small number of labeled training documents with a large pool of unlabeled documents, and presents two extensions to the algorithm that improve classification accuracy under these conditions."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143936663"
                        ],
                        "name": "Thomas Hofmann",
                        "slug": "Thomas-Hofmann",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Hofmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Hofmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 46
                            }
                        ],
                        "text": "Probabilistic Latent Semantic Analysis (PLSA) (Hofmann, 1999) is an important improvement over LSI."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 653762,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "355b86dafd852e4df905f6ad9402c7d03831d618",
            "isKey": false,
            "numCitedBy": 2635,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Probabilistic Latent Semantic Analysis is a novel statistical technique for the analysis of two-mode and co-occurrence data, which has applications in information retrieval and filtering, natural language processing, machine learning from text, and in related areas. Compared to standard Latent Semantic Analysis which stems from linear algebra and performs a Singular Value Decomposition of co-occurrence tables, the proposed method is based on a mixture decomposition derived from a latent class model. This results in a more principled approach which has a solid foundation in statistics. In order to avoid overfitting, we propose a widely applicable generalization of maximum likelihood model fitting by tempered EM. Our approach yields substantial and consistent improvements over Latent Semantic Analysis in a number of experiments."
            },
            "slug": "Probabilistic-Latent-Semantic-Analysis-Hofmann",
            "title": {
                "fragments": [],
                "text": "Probabilistic Latent Semantic Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work proposes a widely applicable generalization of maximum likelihood model fitting by tempered EM, based on a mixture decomposition derived from a latent class model which results in a more principled approach which has a solid foundation in statistics."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51204489"
                        ],
                        "name": "T. D. Bie",
                        "slug": "T.-D.-Bie",
                        "structuredName": {
                            "firstName": "Tijl",
                            "lastName": "Bie",
                            "middleNames": [
                                "De"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. D. Bie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685083"
                        ],
                        "name": "N. Cristianini",
                        "slug": "N.-Cristianini",
                        "structuredName": {
                            "firstName": "Nello",
                            "lastName": "Cristianini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cristianini"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11441492,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2d8eaad2d960c5f073f464c3e8c7ba8c91458703",
            "isKey": false,
            "numCitedBy": 58,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "The rise of convex programming has changed the face of many research fields in recent years, machine learning being one of the ones that benefitted the most. A very recent developement, the relaxation of combinatorial problems to semi-definite programs (SDP), has gained considerable attention over the last decade (Helmberg, 2000; De Bie and Cristianini, 2004a). Although SDP problems can be solved in polynomial time, for many relaxations the exponent in the polynomial complexity bounds is too high for scaling to large problem sizes. This has hampered their uptake as a powerful new tool in machine learning. \n \nIn this paper, we present a new and fast SDP relaxation of the normalized graph cut problem, and investigate its usefulness in unsupervised and semi-supervised learning. In particular, this provides a convex algorithm for transduction, as well as approaches to clustering. We further propose a whole cascade of fast relaxations that all hold the middle between older spectral relaxations and the new SDP relaxation, allowing one to trade off computational cost versus relaxation accuracy. Finally, we discuss how the methodology developed in this paper can be applied to other combinatorial problems in machine learning, and we treat the max-cut problem as an example."
            },
            "slug": "Fast-SDP-Relaxations-of-Graph-Cut-Clustering,-and-Bie-Cristianini",
            "title": {
                "fragments": [],
                "text": "Fast SDP Relaxations of Graph Cut Clustering, Transduction, and Other Combinatorial Problem"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A new and fast SDP relaxation of the normalized graph cut problem is presented, and its usefulness in unsupervised and semi-supervised learning is investigated, providing a convex algorithm for transduction, as well as approaches to clustering."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2939803"
                        ],
                        "name": "Ronan Collobert",
                        "slug": "Ronan-Collobert",
                        "structuredName": {
                            "firstName": "Ronan",
                            "lastName": "Collobert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronan Collobert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50095217"
                        ],
                        "name": "Fabian H Sinz",
                        "slug": "Fabian-H-Sinz",
                        "structuredName": {
                            "firstName": "Fabian",
                            "lastName": "Sinz",
                            "middleNames": [
                                "H"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fabian H Sinz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5175370,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "33fd991b4e05becfdec93585d25b6369a0519133",
            "isKey": false,
            "numCitedBy": 363,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "Convex learning algorithms, such as Support Vector Machines (SVMs), are often seen as highly desirable because they offer strong practical properties and are amenable to theoretical analysis. However, in this work we show how non-convexity can provide scalability advantages over convexity. We show how concave-convex programming can be applied to produce (i) faster SVMs where training errors are no longer support vectors, and (ii) much faster Transductive SVMs."
            },
            "slug": "Trading-convexity-for-scalability-Collobert-Sinz",
            "title": {
                "fragments": [],
                "text": "Trading convexity for scalability"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is shown how concave-convex programming can be applied to produce faster SVMs where training errors are no longer support vectors, and much faster Transductive SVMs."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680188"
                        ],
                        "name": "T. Joachims",
                        "slug": "T.-Joachims",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "Joachims",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Joachims"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14591650,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "74b1a9e50f18af8a7b9f8dd38f40e0466ad7a8e8",
            "isKey": false,
            "numCitedBy": 3047,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper introduces Transductive Support Vector Machines (TSVMs) for text classi cation. While regular Support Vector Machines (SVMs) try to induce a general decision function for a learning task, Transductive Support Vector Machines take into account a particular test set and try to minimize misclassi cations of just those particular examples. The paper presents an analysis of why TSVMs are well suited for text classi cation. These theoretical ndings are supported by experiments on three test collections. The experiments show substantial improvements over inductive methods, especially for small training sets, cutting the number of labeled training examples down to a twentieth on some tasks. This work also proposes an algorithm for training TSVMs e ciently, handling 10,000 examples and more."
            },
            "slug": "Transductive-Inference-for-Text-Classification-Joachims",
            "title": {
                "fragments": [],
                "text": "Transductive Inference for Text Classification using Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "An analysis of why Transductive Support Vector Machines are well suited for text classi cation is presented, and an algorithm for training TSVMs, handling 10,000 examples and more is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2932893"
                        ],
                        "name": "A. Demiriz",
                        "slug": "A.-Demiriz",
                        "structuredName": {
                            "firstName": "Ayhan",
                            "lastName": "Demiriz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Demiriz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145728220"
                        ],
                        "name": "K. Bennett",
                        "slug": "K.-Bennett",
                        "structuredName": {
                            "firstName": "Kristin",
                            "lastName": "Bennett",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Bennett"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 239,
                                "start": 219
                            }
                        ],
                        "text": "We shall also mention that instead of using an probabilistic generative mixture model, some approaches employ various clustering algorithms to cluster the whole dataset, then label each cluster with labeled data, e.g. (Demiriz et al., 1999) (Dara et al., 2000)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2341726,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5662f02cee0afea188e1d59443b60263d4f7b92a",
            "isKey": false,
            "numCitedBy": 232,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "A semi-supervised clustering algorithm is proposed that combines the benefits of supervised and unsupervised learning methods. The approach allows unlabeled data with no known class to be used to improve classification accuracy. The objective function of an unsupervised technique, e.g. K-means clustering, is modified to minimize both the cluster dispersion of the input attributes and a measure of cluster impurity based on the class labels. Minimizing the cluster dispersion of the examples is a form of capacity control to prevent overfitting. For the the output labels, impurity measures from decision tree algorithms such as the Gini index can be used. A genetic algorithm optimizes the objective function to produce clusters. Experimental results show that using class information improves the generalization ability compared to unsupervised methods based only on the input attributes."
            },
            "slug": "Semi-Supervised-Clustering-Using-Genetic-Algorithms-Demiriz-Bennett",
            "title": {
                "fragments": [],
                "text": "Semi-Supervised Clustering Using Genetic Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Experimental results show that using class information improves the generalization ability compared to unsupervised methods based only on the input attributes."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35662379"
                        ],
                        "name": "Beatriz Maeireizo",
                        "slug": "Beatriz-Maeireizo",
                        "structuredName": {
                            "firstName": "Beatriz",
                            "lastName": "Maeireizo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Beatriz Maeireizo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737616"
                        ],
                        "name": "D. Litman",
                        "slug": "D.-Litman",
                        "structuredName": {
                            "firstName": "Diane",
                            "lastName": "Litman",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Litman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726601"
                        ],
                        "name": "R. Hwa",
                        "slug": "R.-Hwa",
                        "structuredName": {
                            "firstName": "Rebecca",
                            "lastName": "Hwa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Hwa"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 32080089,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b0f60f5b87396006262e0bb6adff36a70556d3b3",
            "isKey": false,
            "numCitedBy": 95,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Natural Language Processing applications often require large amounts of annotated training data, which are expensive to obtain. In this paper we investigate the applicability of Co-training to train classifiers that predict emotions in spoken dialogues. In order to do so, we have first applied the wrapper approach with Forward Selection and Naive Bayes, to reduce the dimensionality of our feature set. Our results show that Co-training can be highly effective when a good set of features are chosen."
            },
            "slug": "Co-training-for-Predicting-Emotions-with-Spoken-Maeireizo-Litman",
            "title": {
                "fragments": [],
                "text": "Co-training for Predicting Emotions with Spoken Dialogue Data"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper investigates the applicability of Co-training to train classifiers that predict emotions in spoken dialogues and applies the wrapper approach with Forward Selection and Naive Bayes, to reduce the dimensionality of the feature set."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712692"
                        ],
                        "name": "F. Odone",
                        "slug": "F.-Odone",
                        "structuredName": {
                            "firstName": "Francesca",
                            "lastName": "Odone",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Odone"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690976"
                        ],
                        "name": "L. Rosasco",
                        "slug": "L.-Rosasco",
                        "structuredName": {
                            "firstName": "Lorenzo",
                            "lastName": "Rosasco",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Rosasco"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 59
                            }
                        ],
                        "text": "The authors start from the same regularization pr oblem of (Belkin et al., 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 50
                            }
                        ],
                        "text": "In fact th ey are equivalent to LapSVM and LapRLS (Belkin et al., 2005) with a certain para meter."
                    },
                    "intents": []
                }
            ],
            "corpusId": 10368334,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7f666be963a921837340b63867637b39fb28141b",
            "isKey": false,
            "numCitedBy": 132,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "In this lecture we introduce a class of learning algorithms, collectively called manifold regularization algorithms, suited for predicting/classifying data embedded in high-dimensional spaces. We introduce manifold regularization in the framework of semi-supervised learning, a generalization of the supervised learning setting in which our training set may consist of unlabeled as well as labeled examples."
            },
            "slug": "Manifold-Regularization-Odone-Rosasco",
            "title": {
                "fragments": [],
                "text": "Manifold Regularization"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This lecture introduces manifold regularization in the framework of semi-supervised learning, a generalization of the supervised learning setting in which the training set may consist of unlabeled as well as labeled examples."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2209630"
                        ],
                        "name": "Behzad M. Shahshahani",
                        "slug": "Behzad-M.-Shahshahani",
                        "structuredName": {
                            "firstName": "Behzad",
                            "lastName": "Shahshahani",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Behzad M. Shahshahani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1773449"
                        ],
                        "name": "D. Landgrebe",
                        "slug": "D.-Landgrebe",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Landgrebe",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Landgrebe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Similarly Sindhwani et al. (2005b); Brefeld et al."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 25
                            }
                        ],
                        "text": "Some other examples are (Shahshahani & Landgrebe, 1994) (Miller & Uyar, 1997)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 11081015,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "334867ed99a0af07d8a53dae4f7fdeffffdecc09",
            "isKey": false,
            "numCitedBy": 581,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we study the use of unlabeled sam- ples in reducing the problem of small training sample size that can severely affect the recognition rate of classifiers when the dimensionality of the multispectral data is high. We show that by using additional unlabeled samples that are available at no extra cost, the performance may be improved, and therefore the Hughes phenomenon can be mitigated. Furthermore, by ex- periments, we show that by using additional unlabeled samples more representative estimates can be obtained. We also pro- pose a semiparametric method for incorporating the training (Le., labeled) and unlabeled samples simultaneously into the parameter estimation process."
            },
            "slug": "The-effect-of-unlabeled-samples-in-reducing-the-and-Shahshahani-Landgrebe",
            "title": {
                "fragments": [],
                "text": "The effect of unlabeled samples in reducing the small sample size problem and mitigating the Hughes phenomenon"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "By using additional unlabeled samples that are available at no extra cost, the performance may be improved, and therefore the Hughes phenomenon can be mitigated and therefore more representative estimates can be obtained."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Geosci. Remote. Sens."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "24982365"
                        ],
                        "name": "Dengyong Zhou",
                        "slug": "Dengyong-Zhou",
                        "structuredName": {
                            "firstName": "Dengyong",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dengyong Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708497"
                        ],
                        "name": "A. Gretton",
                        "slug": "A.-Gretton",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "Gretton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gretton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698617"
                        ],
                        "name": "O. Bousquet",
                        "slug": "O.-Bousquet",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Bousquet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Bousquet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7724688,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "429eb35f06a90892a9595545d18e46ddfe981152",
            "isKey": false,
            "numCitedBy": 737,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "The Google search engine has enjoyed huge success with its web page ranking algorithm, which exploits global, rather than local, hyperlink structure of the web using random walks. Here we propose a simple universal ranking algorithm for data lying in the Euclidean space, such as text or image data. The core idea of our method is to rank the data with respect to the intrinsic manifold structure collectively revealed by a great amount of data. Encouraging experimental results from synthetic, image, and text data illustrate the validity of our method."
            },
            "slug": "Ranking-on-Data-Manifolds-Zhou-Weston",
            "title": {
                "fragments": [],
                "text": "Ranking on Data Manifolds"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A simple universal ranking algorithm for data lying in the Euclidean space, such as text or image data, to rank the data with respect to the intrinsic manifold structure collectively revealed by a great amount of data."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763295"
                        ],
                        "name": "J. Tenenbaum",
                        "slug": "J.-Tenenbaum",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Tenenbaum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tenenbaum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2094778056"
                        ],
                        "name": "V. De Silva",
                        "slug": "V.-De-Silva",
                        "structuredName": {
                            "firstName": "Vin",
                            "lastName": "De Silva",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. De Silva"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46657367"
                        ],
                        "name": "J. Langford",
                        "slug": "J.-Langford",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Langford",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Langford"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Representative methods include Isomap (Tenenbaum et al., 2000), locally linear embedding (LLE) (Roweis & Saul, 2000) (Saul & Roweis, 2003), Hessian LLE (Donoho & Grimes, 2003), Laplacian eigenmaps (Belkin & Niyogi, 2003), and semidefinite embedding (SDE) (Weinberger & Saul, 2004) (Weinberger et al."
                    },
                    "intents": []
                }
            ],
            "corpusId": 221338160,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3537fcd0ff99a3b3cb3d279012df826358420556",
            "isKey": false,
            "numCitedBy": 12182,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "Scientists working with large volumes of high-dimensional data, such as global climate patterns, stellar spectra, or human gene distributions, regularly confront the problem of dimensionality reduction: finding meaningful low-dimensional structures hidden in their high-dimensional observations. The human brain confronts the same problem in everyday perception, extracting from its high-dimensional sensory inputs-30,000 auditory nerve fibers or 10(6) optic nerve fibers-a manageably small number of perceptually relevant features. Here we describe an approach to solving dimensionality reduction problems that uses easily measured local metric information to learn the underlying global geometry of a data set. Unlike classical techniques such as principal component analysis (PCA) and multidimensional scaling (MDS), our approach is capable of discovering the nonlinear degrees of freedom that underlie complex natural observations, such as human handwriting or images of a face under different viewing conditions. In contrast to previous algorithms for nonlinear dimensionality reduction, ours efficiently computes a globally optimal solution, and, for an important class of data manifolds, is guaranteed to converge asymptotically to the true structure."
            },
            "slug": "A-global-geometric-framework-for-nonlinear-Tenenbaum-Silva",
            "title": {
                "fragments": [],
                "text": "A global geometric framework for nonlinear dimensionality reduction."
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "An approach to solving dimensionality reduction problems that uses easily measured local metric information to learn the underlying global geometry of a data set and efficiently computes a globally optimal solution, and is guaranteed to converge asymptotically to the true structure."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2879453"
                        ],
                        "name": "V. Castelli",
                        "slug": "V.-Castelli",
                        "structuredName": {
                            "firstName": "Vittorio",
                            "lastName": "Castelli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Castelli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752732"
                        ],
                        "name": "T. Cover",
                        "slug": "T.-Cover",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Cover",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Cover"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 119
                            }
                        ],
                        "text": "If the mixture model assumption is correct, unlabeled data is guaranteed to improve accuracy (Castelli & Cover, 1995) (Castelli & Cover, 1996) (Ratsaby& Venkatesh, 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1389637,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "22834aa74138de7f4da42fb9dfb480cef4e7b177",
            "isKey": false,
            "numCitedBy": 229,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We observe a training set Q composed of l labeled samples {(X/sub 1/,/spl theta//sub 1/),...,(X/sub l/, /spl theta//sub l/)} and u unlabeled samples {X/sub 1/',...,X/sub u/'}. The labels /spl theta//sub i/ are independent random variables satisfying Pr{/spl theta//sub i/=1}=/spl eta/, Pr{/spl theta//sub i/=2}=1-/spl eta/. The labeled observations X/sub i/ are independently distributed with conditional density f/sub /spl theta/i/(/spl middot/) given /spl theta//sub i/. Let (X/sub 0/,/spl theta//sub 0/) be a new sample, independently distributed as the samples in the training set. We observe X/sub 0/ and we wish to infer the classification /spl theta//sub 0/. In this paper we first assume that the distributions f/sub 1/(/spl middot/) and f/sub 2/(/spl middot/) are given and that the mixing parameter is unknown. We show that the relative value of labeled and unlabeled samples in reducing the risk of optimal classifiers is the ratio of the Fisher informations they carry about the parameter /spl eta/. We then assume that two densities g/sub 1/(/spl middot/) and g/sub 2/(/spl middot/) are given, but we do not know whether g/sub 1/(/spl middot/)=f/sub 1/(/spl middot/) and g/sub 2/(/spl middot/)=f/sub 2/(/spl middot/) or if the opposite holds, nor do we know /spl eta/. Thus the learning problem consists of both estimating the optimum partition of the observation space and assigning the classifications to the decision regions. Here, we show that labeled samples are necessary to construct a classification rule and that they are exponentially more valuable than unlabeled samples."
            },
            "slug": "The-relative-value-of-labeled-and-unlabeled-samples-Castelli-Cover",
            "title": {
                "fragments": [],
                "text": "The relative value of labeled and unlabeled samples in pattern recognition with an unknown mixing parameter"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that labeled samples are necessary to construct a classification rule and that they are exponentially more valuable than unlabeled samples."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143800213"
                        ],
                        "name": "Charless C. Fowlkes",
                        "slug": "Charless-C.-Fowlkes",
                        "structuredName": {
                            "firstName": "Charless",
                            "lastName": "Fowlkes",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charless C. Fowlkes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50172592"
                        ],
                        "name": "Serge J. Belongie",
                        "slug": "Serge-J.-Belongie",
                        "structuredName": {
                            "firstName": "Serge",
                            "lastName": "Belongie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Serge J. Belongie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1718206"
                        ],
                        "name": "F. Graham",
                        "slug": "F.-Graham",
                        "structuredName": {
                            "firstName": "Fan",
                            "lastName": "Graham",
                            "middleNames": [
                                "Chung"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Graham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2384316,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "13c82489c1568b67265d17a15720001a5737171e",
            "isKey": false,
            "numCitedBy": 1341,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "Spectral graph theoretic methods have recently shown great promise for the problem of image segmentation. However, due to the computational demands of these approaches, applications to large problems such as spatiotemporal data and high resolution imagery have been slow to appear. The contribution of this paper is a method that substantially reduces the computational requirements of grouping algorithms based on spectral partitioning making it feasible to apply them to very large grouping problems. Our approach is based on a technique for the numerical solution of eigenfunction problems known as the Nystrom method. This method allows one to extrapolate the complete grouping solution using only a small number of samples. In doing so, we leverage the fact that there are far fewer coherent groups in a scene than pixels."
            },
            "slug": "Spectral-grouping-using-the-Nystrom-method-Fowlkes-Belongie",
            "title": {
                "fragments": [],
                "text": "Spectral grouping using the Nystrom method"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The contribution of this paper is a method that substantially reduces the computational requirements of grouping algorithms based on spectral partitioning making it feasible to apply them to very large grouping problems."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2417893"
                        ],
                        "name": "A. Goldberg",
                        "slug": "A.-Goldberg",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Goldberg",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Goldberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1832364"
                        ],
                        "name": "Xiaojin Zhu",
                        "slug": "Xiaojin-Zhu",
                        "structuredName": {
                            "firstName": "Xiaojin",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaojin Zhu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2795175,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6fec21a78eb9279c87cc89ef7efa0acf22ff4abd",
            "isKey": false,
            "numCitedBy": 353,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a graph-based semi-supervised learning algorithm to address the sentiment analysis task of rating inference. Given a set of documents (e.g., movie reviews) and accompanying ratings (e.g., \"4 stars\"), the task calls for inferring numerical ratings for unlabeled documents based on the perceived sentiment expressed by their text. In particular, we are interested in the situation where labeled data is scarce. We place this task in the semi-supervised setting and demonstrate that considering unlabeled reviews in the learning process can improve rating-inference performance. We do so by creating a graph on both labeled and unlabeled data to encode certain assumptions for this task. We then solve an optimization problem to obtain a smooth rating function over the whole graph. When only limited labeled data is available, this method achieves significantly better predictive accuracy over other methods that ignore the unlabeled examples during training."
            },
            "slug": "Seeing-stars-when-there-aren\u2019t-many-stars:-learning-Goldberg-Zhu",
            "title": {
                "fragments": [],
                "text": "Seeing stars when there aren\u2019t many stars: Graph-based semi-supervised learning for sentiment categorization"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "A graph-based semi-supervised learning algorithm is presented to address the sentiment analysis task of rating inference and achieves significantly better predictive accuracy over other methods that ignore the unlabeled examples during training."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1767444"
                        ],
                        "name": "Sarah Zelikovitz",
                        "slug": "Sarah-Zelikovitz",
                        "structuredName": {
                            "firstName": "Sarah",
                            "lastName": "Zelikovitz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sarah Zelikovitz"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 17491039,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cf0f236955ede57a5677359342a99e9edd5044e2",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We present work in progress that uses Latent Semantic Indexing (LSI) in conjunction with background knowledge and unlabeled examples to improve text classification accuracy. The singular value decomposition (SVD) that is performed by LSI is done on an expanded term by document matrix that includes the labeled training examples as well as the unlabeled examples. We report classification accuracy on different data sets both with and without the inclusion of background knowledge and compare it to other known work."
            },
            "slug": "Improving-Text-Classification-with-LSI-Using-Zelikovitz",
            "title": {
                "fragments": [],
                "text": "Improving Text Classification with LSI Using Background Knowledge"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "Work in progress that uses Latent Semantic Indexing (LSI) in conjunction with background knowledge and unlabeled examples to improve text classification accuracy is presented."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "24982365"
                        ],
                        "name": "Dengyong Zhou",
                        "slug": "Dengyong-Zhou",
                        "structuredName": {
                            "firstName": "Dengyong",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dengyong Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8192534"
                        ],
                        "name": "Jiayuan Huang",
                        "slug": "Jiayuan-Huang",
                        "structuredName": {
                            "firstName": "Jiayuan",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiayuan Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 735750,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9f53740cc80ecf013b3646d10c6357e80e5e6b1e",
            "isKey": false,
            "numCitedBy": 949,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "We usually endow the investigated objects with pairwise relationships, which can be illustrated as graphs. In many real-world problems, however, relationships among the objects of our interest are more complex than pair-wise. Naively squeezing the complex relationships into pairwise ones will inevitably lead to loss of information which can be expected valuable for our learning tasks however. Therefore we consider using hypergraphs instead to completely represent complex relationships among the objects of our interest, and thus the problem of learning with hypergraphs arises. Our main contribution in this paper is to generalize the powerful methodology of spectral clustering which originally operates on undirected graphs to hypergraphs, and further develop algorithms for hypergraph embedding and transductive classification on the basis of the spectral hypergraph clustering approach. Our experiments on a number of benchmarks showed the advantages of hypergraphs over usual graphs."
            },
            "slug": "Learning-with-Hypergraphs:-Clustering,-and-Zhou-Huang",
            "title": {
                "fragments": [],
                "text": "Learning with Hypergraphs: Clustering, Classification, and Embedding"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This paper generalizes the powerful methodology of spectral clustering which originally operates on undirected graphs to hypergraphs, and further develop algorithms for hypergraph embedding and transductive classification on the basis of the spectral hypergraph clustering approach."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143610806"
                        ],
                        "name": "Matthias Hein",
                        "slug": "Matthias-Hein",
                        "structuredName": {
                            "firstName": "Matthias",
                            "lastName": "Hein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthias Hein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46753623"
                        ],
                        "name": "Markus Maier",
                        "slug": "Markus-Maier",
                        "structuredName": {
                            "firstName": "Markus",
                            "lastName": "Maier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Markus Maier"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Hein and Maier (2006) propose an algorithm to denoise points sampled from a manifold."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11920160,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "038409c577587ba74528a2128fe7efbaa5679ece",
            "isKey": false,
            "numCitedBy": 179,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of denoising a noisily sampled submanifold M in \u211dd, where the submanifold M is a priori unknown and we are only given a noisy point sample. The presented denoising algorithm is based on a graph-based diffusion process of the point sample. We analyze this diffusion process using recent results about the convergence of graph Laplacians. In the experiments we show that our method is capable of dealing with non-trivial high-dimensional noise. Moreover using the denoising algorithm as pre-processing method we can improve the results of a semi-supervised learning algorithm."
            },
            "slug": "Manifold-Denoising-Hein-Maier",
            "title": {
                "fragments": [],
                "text": "Manifold Denoising"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The presented denoising algorithm is based on a graph-based diffusion process of the point sample and is analyzed using recent results about the convergence of graph Laplacians to improve the results of a semi-supervised learning algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730609"
                        ],
                        "name": "O. Chapelle",
                        "slug": "O.-Chapelle",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Chapelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Chapelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1411416797"
                        ],
                        "name": "Bernhard Sch\u00c3\u00b6lkopf",
                        "slug": "Bernhard-Sch\u00c3\u00b6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00c3\u00b6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bernhard Sch\u00c3\u00b6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2281542"
                        ],
                        "name": "A. Zien",
                        "slug": "A.-Zien",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Zien",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Zien"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 63820100,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2bc9fe21de471db0b7ec0ca28f7e0e73302a5d39",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter contains sections titled: Introduction, A Formal Framework, Sample Complexity Results, Algorithmic Results, Related Models and Discussion"
            },
            "slug": "An-Augmented-PAC-Model-for-Semi-Supervised-Learning-Chapelle-Sch\u00c3\u00b6lkopf",
            "title": {
                "fragments": [],
                "text": "An Augmented PAC Model for Semi-Supervised Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This chapter contains sections titled: Introduction, A Formal Framework, Sample Complexity Results, Algorithmic Results, Related Models and Discussion."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145172877"
                        ],
                        "name": "K. Nigam",
                        "slug": "K.-Nigam",
                        "structuredName": {
                            "firstName": "Kamal",
                            "lastName": "Nigam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Nigam"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 35
                            }
                        ],
                        "text": "Mixture of multivariate Bernoulli (McCallum & Nigam, 1998a) is not identifiable."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 0
                            }
                        ],
                        "text": "McCallum and Nigam (1998b) use EM with unlabeled data integrated into the active learning algorithm."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 0
                            }
                        ],
                        "text": "McCallum and Nigam (1998b) use EM with unlabeled data integrated into the active learning algorithm. Muslea et al. (2002) propose CO-EMT which co mbines multi-view (e."
                    },
                    "intents": []
                }
            ],
            "corpusId": 7311285,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "04ce064505b1635583fa0d9cc07cac7e9ea993cc",
            "isKey": false,
            "numCitedBy": 3833,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent work in text classification has used two different first-order probabilistic models for classification, both of which make the naive Bayes assumption. Some use a multi-variate Bernoulli model, that is, a Bayesian Network with no dependencies between words and binary word features (e.g. Larkey and Croft 1996; Koller and Sahami 1997). Others use a multinomial model, that is, a uni-gram language model with integer word counts (e.g. Lewis and Gale 1994; Mitchell 1997). This paper aims to clarify the confusion by describing the differences and details of these two models, and by empirically comparing their classification performance on five text corpora. We find that the multi-variate Bernoulli performs well with small vocabulary sizes, but that the multinomial performs usually performs even better at larger vocabulary sizes--providing on average a 27% reduction in error over the multi-variate Bernoulli model at any vocabulary size."
            },
            "slug": "A-comparison-of-event-models-for-naive-bayes-text-McCallum-Nigam",
            "title": {
                "fragments": [],
                "text": "A comparison of event models for naive bayes text classification"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is found that the multi-variate Bernoulli performs well with small vocabulary sizes, but that the multinomial performs usually performs even better at larger vocabulary sizes--providing on average a 27% reduction in error over the multi -variateBernoulli model at any vocabulary size."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI 1998"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796335"
                        ],
                        "name": "D. Blei",
                        "slug": "D.-Blei",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Blei",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Blei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1332,
                                "start": 35
                            }
                        ],
                        "text": "Latent Dirichlet Allocation (LDA) (Blei et al., 2003) is one step further. It assumes the topic proportion of each document is drawn from a Dirichlet dis tribution. With variational approximation, each document is represented by a pos terior Dirichlet over the topics. This is a much lower dimensional representation. Gr iffiths et al. (2005) extend LDA model to \u2018HMM-LDA\u2019 which uses both shortterm syntactic and long-term topical dependencies, as an effort to integrate s emantics and syntax. Li and McCallum (2005) apply the HMM-LDA model to obtain word clusters, as a rudimentary way for semi-supervised learning on sequenc es. Some algorithms derive a metric entirely from the density of U . These are motivated by unsupervised clustering and based on the intuition that data points in the same high density \u2018clump\u2019 should be close in the new metric. For instance, if U is generated from a single Gaussian, then the Mahalanobis distance induce d by the covariance matrix is such a metric. Tipping (1999) generalizes the Mahalano bis distance by fittingU with a mixture of Gaussian, and define a Riemannian manifold with metric atx being the weighted average of individual component inverse covariance. The distance between x1 andx2 is computed along the straight line (in Euclidean space) between the two points. Rattray (2000) further genera lizes the metric so that it only depends on the change in log probabilities of the density, n ot on a particular Gaussian mixture assumption."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 343,
                                "start": 35
                            }
                        ],
                        "text": "Latent Dirichlet Allocation (LDA) (Blei et al., 2003) is one step further. It assumes the topic proportion of each document is drawn from a Dirichlet dis tribution. With variational approximation, each document is represented by a pos terior Dirichlet over the topics. This is a much lower dimensional representation. Gr iffiths et al. (2005) extend LDA model to \u2018HMM-LDA\u2019 which uses both shortterm syntactic and long-term topical dependencies, as an effort to integrate s emantics and syntax."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1918,
                                "start": 35
                            }
                        ],
                        "text": "Latent Dirichlet Allocation (LDA) (Blei et al., 2003) is one step further. It assumes the topic proportion of each document is drawn from a Dirichlet dis tribution. With variational approximation, each document is represented by a pos terior Dirichlet over the topics. This is a much lower dimensional representation. Gr iffiths et al. (2005) extend LDA model to \u2018HMM-LDA\u2019 which uses both shortterm syntactic and long-term topical dependencies, as an effort to integrate s emantics and syntax. Li and McCallum (2005) apply the HMM-LDA model to obtain word clusters, as a rudimentary way for semi-supervised learning on sequenc es. Some algorithms derive a metric entirely from the density of U . These are motivated by unsupervised clustering and based on the intuition that data points in the same high density \u2018clump\u2019 should be close in the new metric. For instance, if U is generated from a single Gaussian, then the Mahalanobis distance induce d by the covariance matrix is such a metric. Tipping (1999) generalizes the Mahalano bis distance by fittingU with a mixture of Gaussian, and define a Riemannian manifold with metric atx being the weighted average of individual component inverse covariance. The distance between x1 andx2 is computed along the straight line (in Euclidean space) between the two points. Rattray (2000) further genera lizes the metric so that it only depends on the change in log probabilities of the density, n ot on a particular Gaussian mixture assumption. And the distance is computed along a curve that minimizes the distance. The new metric is invariant to linear transfor mation of the features, and connected regions of relatively homogeneous d nsity in U will be close to each other. Such metric is attractive, yet it depends on the homogeneity of the initial Euclidean space. Their application in semi-supervise d learning needs further investigation. Sajama and Orlitsky (2005) analyze th e lower and upper bounds on estimating data-density-based distance."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1008,
                                "start": 35
                            }
                        ],
                        "text": "Latent Dirichlet Allocation (LDA) (Blei et al., 2003) is one step further. It assumes the topic proportion of each document is drawn from a Dirichlet dis tribution. With variational approximation, each document is represented by a pos terior Dirichlet over the topics. This is a much lower dimensional representation. Gr iffiths et al. (2005) extend LDA model to \u2018HMM-LDA\u2019 which uses both shortterm syntactic and long-term topical dependencies, as an effort to integrate s emantics and syntax. Li and McCallum (2005) apply the HMM-LDA model to obtain word clusters, as a rudimentary way for semi-supervised learning on sequenc es. Some algorithms derive a metric entirely from the density of U . These are motivated by unsupervised clustering and based on the intuition that data points in the same high density \u2018clump\u2019 should be close in the new metric. For instance, if U is generated from a single Gaussian, then the Mahalanobis distance induce d by the covariance matrix is such a metric. Tipping (1999) generalizes the Mahalano bis distance by fittingU with a mixture of Gaussian, and define a Riemannian manifold with metric atx being the weighted average of individual component inverse covariance."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 517,
                                "start": 35
                            }
                        ],
                        "text": "Latent Dirichlet Allocation (LDA) (Blei et al., 2003) is one step further. It assumes the topic proportion of each document is drawn from a Dirichlet dis tribution. With variational approximation, each document is represented by a pos terior Dirichlet over the topics. This is a much lower dimensional representation. Gr iffiths et al. (2005) extend LDA model to \u2018HMM-LDA\u2019 which uses both shortterm syntactic and long-term topical dependencies, as an effort to integrate s emantics and syntax. Li and McCallum (2005) apply the HMM-LDA model to obtain word clusters, as a rudimentary way for semi-supervised learning on sequenc es."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 34
                            }
                        ],
                        "text": "Latent Dirichlet Allocation (LDA) (Blei et al., 2003) is one step further."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3177797,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f198043a866e9187925a8d8db9a55e3bfdd47f2c",
            "isKey": true,
            "numCitedBy": 30949,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Latent-Dirichlet-Allocation-Blei-Ng",
            "title": {
                "fragments": [],
                "text": "Latent Dirichlet Allocation"
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117881943"
                        ],
                        "name": "Tong Zhang",
                        "slug": "Tong-Zhang",
                        "structuredName": {
                            "firstName": "Tong",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tong Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38070424"
                        ],
                        "name": "R. Ando",
                        "slug": "R.-Ando",
                        "structuredName": {
                            "firstName": "Rie",
                            "lastName": "Ando",
                            "middleNames": [
                                "Kubota"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Ando"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7827877,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0a8848a9a3f0494882a0025fc28e1df7d8e3fb28",
            "isKey": false,
            "numCitedBy": 75,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider a framework for semi-supervised learning using spectral decomposition based un-supervised kernel design. This approach subsumes a class of previously proposed semi-supervised learning methods on data graphs. We examine various theoretical properties of such methods. In particular, we derive a generalization performance bound, and obtain the optimal kernel design by minimizing the bound. Based on the theoretical analysis, we are able to demonstrate why spectral kernel design based methods can often improve the predictive performance. Experiments are used to illustrate the main consequences of our analysis."
            },
            "slug": "Analysis-of-Spectral-Kernel-Design-based-Learning-Zhang-Ando",
            "title": {
                "fragments": [],
                "text": "Analysis of Spectral Kernel Design based Semi-supervised Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown why spectral kernel design based methods can often improve the predictive performance and derive a generalization performance bound, and obtain the optimal kernel design by minimizing the bound."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1832364"
                        ],
                        "name": "Xiaojin Zhu",
                        "slug": "Xiaojin-Zhu",
                        "structuredName": {
                            "firstName": "Xiaojin",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaojin Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744700"
                        ],
                        "name": "Zoubin Ghahramani",
                        "slug": "Zoubin-Ghahramani",
                        "structuredName": {
                            "firstName": "Zoubin",
                            "lastName": "Ghahramani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zoubin Ghahramani"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 10587410,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "138b6767d572e84147da34dd38573b0eff5171b7",
            "isKey": false,
            "numCitedBy": 144,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract: \"We show that the Gaussian random fields and harmonic energy minimizing function framework for semi-supervised learning can be viewed in terms of Gaussian processes, with covariance matrices derived from the graph Laplacian. We derive hyperparameter learning with evidence maximization, and give an empirical study of various ways to parameterize the graph weights.\""
            },
            "slug": "Semi-supervised-learning-:-from-Gaussian-fields-to-Zhu-Lafferty",
            "title": {
                "fragments": [],
                "text": "Semi-supervised learning : from Gaussian fields to Gaussian processes"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "It is shown that the Gaussian random fields and harmonic energy minimizing function framework for semi-supervised learning can be viewed in terms of Gaussian processes, with covariance matrices derived from the graph Laplacian, to derive hyperparameter learning with evidence maximization."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117881943"
                        ],
                        "name": "Tong Zhang",
                        "slug": "Tong-Zhang",
                        "structuredName": {
                            "firstName": "Tong",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tong Zhang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 16025939,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9755f9993553131e5cc796d34ecaa624fe0ddffa",
            "isKey": false,
            "numCitedBy": 308,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently, there has been increasing interest in using unlabeled data for classiica-tion. However, whether these unlabeled data are truly useful is still under debate. In order to have a better understanding of relevant issues, it is worthwhile to precisely formulate the problem and carefully analyze the value of unlabeled data under certain learning models. In this paper, we approach this problem from the statistical point of view, where we assume that a correct model of the underlying distribution is given. We demonstrate that Fisher information matrices can be used to judge the asymp-totic value of unlabeled data. We apply this methodology to both \\passive partially supervised learning\" and \\active learning\", and draw conclusions from this analysis. Experiments will be provided to support our claims."
            },
            "slug": "The-Value-of-Unlabeled-Data-for-Classification-Zhang",
            "title": {
                "fragments": [],
                "text": "The Value of Unlabeled Data for Classification Problems"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is demonstrated that Fisher information matrices can be used to judge the asymp-totic value of unlabeled data and this methodology is applied to both passive partially supervised learning and active learning."
            },
            "venue": {
                "fragments": [],
                "text": "ICML 2000"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763608"
                        ],
                        "name": "Chris Callison-Burch",
                        "slug": "Chris-Callison-Burch",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Callison-Burch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris Callison-Burch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144251066"
                        ],
                        "name": "David Talbot",
                        "slug": "David-Talbot",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Talbot",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Talbot"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057788"
                        ],
                        "name": "M. Osborne",
                        "slug": "M.-Osborne",
                        "structuredName": {
                            "firstName": "Miles",
                            "lastName": "Osborne",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Osborne"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 133
                            }
                        ],
                        "text": "Another solution is to down-weighing unlabeled data (Corduneanu & Jaakkola, 2001), which is also used by Nigam et al. (2000), and by Callison-Burch et al. (2004) who estimate word alignment for machine translation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "(2000), and by Callison-Burch et al. (2004) who estimate word alignment for machine translation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13169626,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "efaffeae84ee93f87f70e42415b67feb639e0809",
            "isKey": false,
            "numCitedBy": 98,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "The parameters of statistical translation models are typically estimated from sentence-aligned parallel corpora. We show that significant improvements in the alignment and translation quality of such models can be achieved by additionally including word-aligned data during training. Incorporating word-level alignments into the parameter estimation of the IBM models reduces alignment error rate and increases the Bleu score when compared to training the same models only on sentence-aligned data. On the Verbmobil data set, we attain a 38% reduction in the alignment error rate and a higher Bleu score with half as many training examples. We discuss how varying the ratio of word-aligned to sentence-aligned data affects the expected performance gain."
            },
            "slug": "Statistical-Machine-Translation-with-Word-and-Callison-Burch-Talbot",
            "title": {
                "fragments": [],
                "text": "Statistical Machine Translation with Word- and Sentence-Aligned Parallel Corpora"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "It is shown that significant improvements in the alignment and translation quality of such models can be achieved by additionally including word-aligned data during training by incorporating word-level alignments into the parameter estimation of the IBM models."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "24982365"
                        ],
                        "name": "Dengyong Zhou",
                        "slug": "Dengyong-Zhou",
                        "structuredName": {
                            "firstName": "Dengyong",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dengyong Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8192534"
                        ],
                        "name": "Jiayuan Huang",
                        "slug": "Jiayuan-Huang",
                        "structuredName": {
                            "firstName": "Jiayuan",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiayuan Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 10
                            }
                        ],
                        "text": "Wu and Sc\u1e27olkopf (2007) showed that such local linear model regularization can be written as f\u22a4Rf ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16721854,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "df95ae968cb0b722143f6000fa0dc7ce21cc35e2",
            "isKey": false,
            "numCitedBy": 422,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a general framework for learning from labeled and unlabeled data on a directed graph in which the structure of the graph including the directionality of the edges is considered. The time complexity of the algorithm derived from this framework is nearly linear due to recently developed numerical techniques. In the absence of labeled instances, this framework can be utilized as a spectral clustering method for directed graphs, which generalizes the spectral clustering approach for undirected graphs. We have applied our framework to real-world web classification problems and obtained encouraging results."
            },
            "slug": "Learning-from-labeled-and-unlabeled-data-on-a-graph-Zhou-Huang",
            "title": {
                "fragments": [],
                "text": "Learning from labeled and unlabeled data on a directed graph"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "A general framework for learning from labeled and unlabeled data on a directed graph in which the structure of the graph including the directionality of the edges is considered, which generalizes the spectral clustering approach for undirected graphs."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145520115"
                        ],
                        "name": "Mikhail Belkin",
                        "slug": "Mikhail-Belkin",
                        "structuredName": {
                            "firstName": "Mikhail",
                            "lastName": "Belkin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mikhail Belkin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770745"
                        ],
                        "name": "P. Niyogi",
                        "slug": "P.-Niyogi",
                        "structuredName": {
                            "firstName": "Partha",
                            "lastName": "Niyogi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Niyogi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14879317,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "88816ae492956f3004daa41357166f1181c0c1bf",
            "isKey": false,
            "numCitedBy": 7047,
            "numCiting": 78,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the central problems in machine learning and pattern recognition is to develop appropriate representations for complex data. We consider the problem of constructing a representation for data lying on a low-dimensional manifold embedded in a high-dimensional space. Drawing on the correspondence between the graph Laplacian, the Laplace Beltrami operator on the manifold, and the connections to the heat equation, we propose a geometrically motivated algorithm for representing the high-dimensional data. The algorithm provides a computationally efficient approach to nonlinear dimensionality reduction that has locality-preserving properties and a natural connection to clustering. Some potential applications and illustrative examples are discussed."
            },
            "slug": "Laplacian-Eigenmaps-for-Dimensionality-Reduction-Belkin-Niyogi",
            "title": {
                "fragments": [],
                "text": "Laplacian Eigenmaps for Dimensionality Reduction and Data Representation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work proposes a geometrically motivated algorithm for representing the high-dimensional data that provides a computationally efficient approach to nonlinear dimensionality reduction that has locality-preserving properties and a natural connection to clustering."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2590197"
                        ],
                        "name": "R. Dara",
                        "slug": "R.-Dara",
                        "structuredName": {
                            "firstName": "Raju",
                            "lastName": "Dara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Dara"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2492622"
                        ],
                        "name": "S. C. Kremer",
                        "slug": "S.-C.-Kremer",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Kremer",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. C. Kremer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33942072"
                        ],
                        "name": "D. Stacey",
                        "slug": "D.-Stacey",
                        "structuredName": {
                            "firstName": "Deborah",
                            "lastName": "Stacey",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Stacey"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61764539,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "baccb4c73f51e26e3b2ab803ecc923144d40d625",
            "isKey": false,
            "numCitedBy": 87,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "We show the use of a self organizing map to cluster unlabeled data and to infer possible labelings from the clusters. Our inferred labels are presented to a multilayer perceptron along with labeled data, performance is improved over using only the labeled data. Results are presented for a number of popular real-world benchmark problems from domains other than text. This shows one way in which unlabeled data can be used to enhance supervised learning in a general-purpose neural network."
            },
            "slug": "Clustering-unlabeled-data-with-SOMs-improves-of-Dara-Kremer",
            "title": {
                "fragments": [],
                "text": "Clustering unlabeled data with SOMs improves classification of labeled real-world data"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "A self organizing map is used to cluster unlabeled data and to infer possible labelings from the clusters and results are presented for a number of popular real-world benchmark problems from domains other than text."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2002 International Joint Conference on Neural Networks. IJCNN'02 (Cat. No.02CH37290)"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145749654"
                        ],
                        "name": "M. Rattray",
                        "slug": "M.-Rattray",
                        "structuredName": {
                            "firstName": "Magnus",
                            "lastName": "Rattray",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Rattray"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 0
                            }
                        ],
                        "text": "Riloff et al. (2003) uses it to identify subjective nouns."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14628393,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "504d650dac515801674751afdfe0f074e734b5bf",
            "isKey": false,
            "numCitedBy": 21,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "A Riemannian distance is defined which is appropriate for clustering multivariate data. This distance requires that data is first fitted with a differentiable density model allowing the definition of an appropriate Riemannian metric. A tractable approximation is developed for the case of a Gaussian mixture model and the distance is tested on artificial data, demonstrating an ability to deal with differing length scales and linearly inseparable data clusters. Further work is required to investigate performance on larger data sets."
            },
            "slug": "A-model-based-distance-for-clustering-Rattray",
            "title": {
                "fragments": [],
                "text": "A model-based distance for clustering"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A tractable approximation is developed for the case of a Gaussian mixture model and the distance is tested on artificial data, demonstrating an ability to deal with differing length scales and linearly inseparable data clusters."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2418588"
                        ],
                        "name": "Sajama",
                        "slug": "Sajama",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Sajama",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sajama"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691155"
                        ],
                        "name": "A. Orlitsky",
                        "slug": "A.-Orlitsky",
                        "structuredName": {
                            "firstName": "Alon",
                            "lastName": "Orlitsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Orlitsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10731077,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "485fade71c20ece3f99d68a1321e73226c5edd4b",
            "isKey": false,
            "numCitedBy": 44,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Density-based distance metrics have applications in semi-supervised learning, nonlinear interpolation and clustering. We consider density-based metrics induced by Riemannian manifold structures and estimate them using kernel density estimators for the underlying data distribution. We lower bound the rate of convergence of these plug-in path-length estimates and hence of the metric, as the sample size increases. We present an upper bound on the rate of convergence of all estimators of the metric. We also show that the metric can be consistently computed using the shortest path algorithm on a suitably constructed graph on the data samples and lower bound the convergence rate of the computation error. We present experiments illustrating the use of the metrics for semi-supervised classification and non-linear interpolation."
            },
            "slug": "Estimating-and-computing-density-based-distance-Sajama-Orlitsky",
            "title": {
                "fragments": [],
                "text": "Estimating and computing density based distance metrics"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that the metric can be consistently computed using the shortest path algorithm on a suitably constructed graph on the data samples and the convergence rate of the computation error is lower bound."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7668712"
                        ],
                        "name": "Fabio Gagliardi Cozman",
                        "slug": "Fabio-Gagliardi-Cozman",
                        "structuredName": {
                            "firstName": "Fabio",
                            "lastName": "Cozman",
                            "middleNames": [
                                "Gagliardi"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fabio Gagliardi Cozman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49641404"
                        ],
                        "name": "I. Cohen",
                        "slug": "I.-Cohen",
                        "structuredName": {
                            "firstName": "Ira",
                            "lastName": "Cohen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Cohen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3156170"
                        ],
                        "name": "M. C. Cirelo",
                        "slug": "M.-C.-Cirelo",
                        "structuredName": {
                            "firstName": "Marcelo",
                            "lastName": "Cirelo",
                            "middleNames": [
                                "Cesar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. C. Cirelo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "Cozman et al. (2003) give a formal derivation on how this might happen."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 5
                            }
                        ],
                        "text": "See (Cozman et al., 2003) for a more recent argument."
                    },
                    "intents": []
                }
            ],
            "corpusId": 16974352,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "db9203b1a30b2edf51fdac23f661a99672459a85",
            "isKey": false,
            "numCitedBy": 215,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper analyzes the performance of semi-supervised learning of mixture models. We show that unlabeled data can lead to an increase in classification error even in situations where additional labeled data would decrease classification error. We present a mathematical analysis of this \"degradation\" phenomenon and show that it is due to the fact that bias may be adversely affected by unlabeled data. We discuss the impact of these theoretical results to practical situations."
            },
            "slug": "Semi-Supervised-Learning-of-Mixture-Models-Cozman-Cohen",
            "title": {
                "fragments": [],
                "text": "Semi-Supervised Learning of Mixture Models"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "This paper analyzes the performance of semi-supervised learning of mixture models and shows that unlabeled data can lead to an increase in classification error even in situations where additional labeled data would decrease classification error."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34913588"
                        ],
                        "name": "Hariharan Narayanan",
                        "slug": "Hariharan-Narayanan",
                        "structuredName": {
                            "firstName": "Hariharan",
                            "lastName": "Narayanan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hariharan Narayanan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145520115"
                        ],
                        "name": "Mikhail Belkin",
                        "slug": "Mikhail-Belkin",
                        "structuredName": {
                            "firstName": "Mikhail",
                            "lastName": "Belkin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mikhail Belkin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770745"
                        ],
                        "name": "P. Niyogi",
                        "slug": "P.-Niyogi",
                        "structuredName": {
                            "firstName": "Partha",
                            "lastName": "Niyogi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Niyogi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Nigam et al. (2000) apply the EM algorithm on mixture of multinomial for the task of text classification."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1122908,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0190b62a2a5b0c57675b9fd0a897136af98f07e1",
            "isKey": false,
            "numCitedBy": 53,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the intuitions underlying many graph-based methods for clustering and semi-supervised learning, is that class or cluster boundaries pass through areas of low probability density. In this paper we provide some formal analysis of that notion for a probability distribution. We introduce a notion of weighted boundary volume, which measures the length of the class/cluster boundary weighted by the density of the underlying probability distribution. We show that sizes of the cuts of certain commonly used data adjacency graphs converge to this continuous weighted volume of the boundary."
            },
            "slug": "On-the-Relation-Between-Low-Density-Separation,-and-Narayanan-Belkin",
            "title": {
                "fragments": [],
                "text": "On the Relation Between Low Density Separation, Spectral Clustering and Graph Cuts"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "A notion of weighted boundary volume is introduced, which measures the length of the class/cluster boundary weighted by the density of the underlying probability distribution, and it is shown that sizes of the cuts of certain commonly used data adjacency graphs converge to this continuous weighted volume of the boundary."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46865129"
                        ],
                        "name": "Jianbo Shi",
                        "slug": "Jianbo-Shi",
                        "structuredName": {
                            "firstName": "Jianbo",
                            "lastName": "Shi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianbo Shi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153652147"
                        ],
                        "name": "J. Malik",
                        "slug": "J.-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 306,
                                "start": 86
                            }
                        ],
                        "text": "It ha been suggested that temporal correlation serves as the glue, as summarized by ( Sinha et al., 2006) (Result 14). It seems when we observe an object with chan ging angles, we link the images as \u2018containing the same object\u2019 by the virtue that the images are close in time. Wallis and B\u0308 ulthoff (2001) created artificial image sequences where a frontal face is morphed into the profile face of a different person."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14848918,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b94c7ff9532ab26c3aedbee3988ec4c7a237c173",
            "isKey": false,
            "numCitedBy": 12819,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a novel approach for solving the perceptual grouping problem in vision. Rather than focusing on local features and their consistencies in the image data, our approach aims at extracting the global impression of an image. We treat image segmentation as a graph partitioning problem and propose a novel global criterion, the normalized cut, for segmenting the graph. The normalized cut criterion measures both the total dissimilarity between the different groups as well as the total similarity within the groups. We show that an efficient computational technique based on a generalized eigenvalue problem can be used to optimize this criterion. We have applied this approach to segmenting static images and found results very encouraging."
            },
            "slug": "Normalized-cuts-and-image-segmentation-Shi-Malik",
            "title": {
                "fragments": [],
                "text": "Normalized cuts and image segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work treats image segmentation as a graph partitioning problem and proposes a novel global criterion, the normalized cut, for segmenting the graph, which measures both the total dissimilarity between the different groups as well as the total similarity within the groups."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057046890"
                        ],
                        "name": "Wei Chu",
                        "slug": "Wei-Chu",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Chu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Chu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744700"
                        ],
                        "name": "Zoubin Ghahramani",
                        "slug": "Zoubin-Ghahramani",
                        "structuredName": {
                            "firstName": "Zoubin",
                            "lastName": "Ghahramani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zoubin Ghahramani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2788778,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "724a2430522c7c398c01f90330f41e187c6a7243",
            "isKey": false,
            "numCitedBy": 438,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a probabilistic kernel approach to ordinal regression based on Gaussian processes. A threshold model that generalizes the probit function is used as the likelihood function for ordinal variables. Two inference techniques, based on the Laplace approximation and the expectation propagation algorithm respectively, are derived for hyperparameter learning and model selection. We compare these two Gaussian process approaches with a previous ordinal regression method based on support vector machines on some benchmark and real-world data sets, including applications of ordinal regression to collaborative filtering and gene expression analysis. Experimental results on these data sets verify the usefulness of our approach."
            },
            "slug": "Gaussian-Processes-for-Ordinal-Regression-Chu-Ghahramani",
            "title": {
                "fragments": [],
                "text": "Gaussian Processes for Ordinal Regression"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "A probabilistic kernel approach to ordinal regression based on Gaussian processes is presented, where a threshold model that generalizes the probit function is used as the likelihood function for ordinal variables."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145520115"
                        ],
                        "name": "Mikhail Belkin",
                        "slug": "Mikhail-Belkin",
                        "structuredName": {
                            "firstName": "Mikhail",
                            "lastName": "Belkin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mikhail Belkin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2458509"
                        ],
                        "name": "Irina Matveeva",
                        "slug": "Irina-Matveeva",
                        "structuredName": {
                            "firstName": "Irina",
                            "lastName": "Matveeva",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Irina Matveeva"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770745"
                        ],
                        "name": "P. Niyogi",
                        "slug": "P.-Niyogi",
                        "structuredName": {
                            "firstName": "Partha",
                            "lastName": "Niyogi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Niyogi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1845,
                                "start": 60
                            }
                        ],
                        "text": "The authors start from the same regularization pr oblem of (Belkin et al., 2005). The key idea is to approximate the function space with a finite basis, with sparse grids. The minimizer f in this finite dimensional subspace can be efficiently computed. As the authors point out, this method is different from the general kernel methods which rely on the represen t r theorem for finite representation. In practice the method is limited by data dimensionality (around 20). A potential drawback is that the method employs a regular grid , and cannot \u2018zoom in\u2019 to small interesting data regions with higher resolution. Yu et al. (2005) solve the large scale semi-supervised learning problem b y using a bipartite graph. The labeled and unlabeled points form one side of th bipartite split, while a much smaller number of \u2018block-level\u2019 nodes form the other side. The authors show that the harmonic function can be computed using the block-level nodes. The computation involves inverting a much smaller matrix on block-level nodes. It is thus cheaper and more scalable than working dir ectly on the L\u222aU matrix. The authors propose two methods to construct the bipartite graph, so that it approximates the given weight matrix W on L \u222a U . One uses Nonnegative Matrix Factorization, the other uses mixture models. The latter method has the additional benefit of induction, and is similar to the harmonic mixtures (Zhu & Lafferty, 2005). However in the latter method the mixture model is derived ba sed on the given weight matrix W . But in harmonic mixturesW and the mixture model are independent, and the mixture model serves as a \u2018second knowledge s ourc \u2019 in addition toW . The original manifold regularization framework (Belkin et al., 2004b) need s to invert a(l+u)\u00d7 (l+u) matrix, and is not scalable. To speed up things, Sindhwani et al. (2005b) consider linear manifold regularization ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 633,
                                "start": 60
                            }
                        ],
                        "text": "The authors start from the same regularization pr oblem of (Belkin et al., 2005). The key idea is to approximate the function space with a finite basis, with sparse grids. The minimizer f in this finite dimensional subspace can be efficiently computed. As the authors point out, this method is different from the general kernel methods which rely on the represen t r theorem for finite representation. In practice the method is limited by data dimensionality (around 20). A potential drawback is that the method employs a regular grid , and cannot \u2018zoom in\u2019 to small interesting data regions with higher resolution. Yu et al. (2005) solve the large scale semi-supervised learning problem b y using a bipartite graph."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 44352521,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c5c6ea2f23fe8d3e986c4c99e83a90c204538619",
            "isKey": false,
            "numCitedBy": 583,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of labeling a partially labeled graph. This setting may arise in a number of situations from survey sampling to information retrieval to pattern recognition in manifold settings. It is also of potential practical importance, when the data is abundant, but labeling is expensive or requires human assistance."
            },
            "slug": "Regularization-and-Semi-supervised-Learning-on-Belkin-Matveeva",
            "title": {
                "fragments": [],
                "text": "Regularization and Semi-supervised Learning on Large Graphs"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "This work considers the problem of labeling a partially labeled graph, which may arise in a number of situations from survey sampling to information retrieval to pattern recognition in manifold settings."
            },
            "venue": {
                "fragments": [],
                "text": "COLT"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2417893"
                        ],
                        "name": "A. Goldberg",
                        "slug": "A.-Goldberg",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Goldberg",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Goldberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1832364"
                        ],
                        "name": "Xiaojin Zhu",
                        "slug": "Xiaojin-Zhu",
                        "structuredName": {
                            "firstName": "Xiaojin",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaojin Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144731788"
                        ],
                        "name": "Stephen J. Wright",
                        "slug": "Stephen-J.-Wright",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Wright",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephen J. Wright"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15058057,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7f59c9bf1f088edd07df88fd31aaedb6ea4cca37",
            "isKey": false,
            "numCitedBy": 123,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Label dissimilarity specifies that a pair of examples probably have different class labels. We present a semi-supervised classification algorithm that learns from dissimilarity and similarity information on labeled and unlabeled data. Our approach uses a novel graphbased encoding of dissimilarity that results in a convex problem, and can handle both binary and multiclass classification. Experiments on several tasks are promising."
            },
            "slug": "Dissimilarity-in-Graph-Based-Semi-Supervised-Goldberg-Zhu",
            "title": {
                "fragments": [],
                "text": "Dissimilarity in Graph-Based Semi-Supervised Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "A semi-supervised classification algorithm that learns from dissimilarity and similarity information on labeled and unlabeled data that results in a convex problem, and can handle both binary and multiclass classification."
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698617"
                        ],
                        "name": "O. Bousquet",
                        "slug": "O.-Bousquet",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Bousquet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Bousquet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730609"
                        ],
                        "name": "O. Chapelle",
                        "slug": "O.-Chapelle",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Chapelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Chapelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143610806"
                        ],
                        "name": "Matthias Hein",
                        "slug": "Matthias-Hein",
                        "structuredName": {
                            "firstName": "Matthias",
                            "lastName": "Hein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthias Hein"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3118640,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7e0d11533c411e3c0559762e7cfc6790c28ccf2b",
            "isKey": false,
            "numCitedBy": 129,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "We address in this paper the question of how the knowledge of the marginal distribution P(x) can be incorporated in a learning algorithm. We suggest three theoretical methods for taking into account this distribution for regularization and provide links to existing graph-based semi-supervised learning algorithms. We also propose practical implementations."
            },
            "slug": "Measure-Based-Regularization-Bousquet-Chapelle",
            "title": {
                "fragments": [],
                "text": "Measure Based Regularization"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper proposes three theoretical methods for taking into account this distribution P(x) for regularization and provides links to existing graph-based semi-supervised learning algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2834541"
                        ],
                        "name": "R. Kondor",
                        "slug": "R.-Kondor",
                        "structuredName": {
                            "firstName": "Risi",
                            "lastName": "Kondor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kondor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5525836,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "6320770fe216ebbba769b9f0a006669b616a03d0",
            "isKey": false,
            "numCitedBy": 888,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "The application of kernel-based learning algorithms has, so far, largely been confined to realvalued data and a few special data types, such as strings. In this paper we propose a general method of constructing natural families of kernels over discrete structures, based on the matrix exponentiation idea. In particular, we focus on generating kernels on graphs, for which we propose a special class of exponential kernels called diffusion kernels, which are based on the heat equation and can be regarded as the discretization of the familiar Gaussian kernel of Euclidean space."
            },
            "slug": "Diffusion-Kernels-on-Graphs-and-Other-Discrete-Kondor-Lafferty",
            "title": {
                "fragments": [],
                "text": "Diffusion Kernels on Graphs and Other Discrete Input Spaces"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper proposes a general method of constructing natural families of kernels over discrete structures, based on the matrix exponentiation idea, and focuses on generating kernels on graphs, for which a special class of exponential kernels called diffusion kernels are proposed."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2831141"
                        ],
                        "name": "Michael E. Tipping",
                        "slug": "Michael-E.-Tipping",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Tipping",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael E. Tipping"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14243706,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "08d20f55442aeb79edfaaaafa7ad54c513ee1dcb",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "The reliable detection of clusters in datasets of non-trivial dimensionality is notoriously difficult. Clustering algorithms are generally driven by some distance function (usually Euclidean) defined over pairs of examples, which implicitly treats distances within and between clusters alike. In this paper, a more effective distance measure is proposed, derived from an a priori estimated Gaussian mixture model. Examples are given to illustrate how the proposed approach can effectively de-emphasise within-cluster structure, and thus implicitly magnify the separation between regions of high data density."
            },
            "slug": "Deriving-cluster-analytic-distance-functions-from-Tipping",
            "title": {
                "fragments": [],
                "text": "Deriving cluster analytic distance functions from Gaussian mixture models"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A more effective distance measure is proposed, derived from an a priori estimated Gaussian mixture model, which can effectively de-emphasise within-cluster structure, and thus implicitly magnify the separation between regions of high data density."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1750819"
                        ],
                        "name": "L. Grady",
                        "slug": "L.-Grady",
                        "structuredName": {
                            "firstName": "Leo",
                            "lastName": "Grady",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Grady"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1405518469"
                        ],
                        "name": "G. Funka-Lea",
                        "slug": "G.-Funka-Lea",
                        "structuredName": {
                            "firstName": "Gareth",
                            "lastName": "Funka-Lea",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Funka-Lea"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5342757,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1c373fef1659c81375d8c66f115075feddffeebe",
            "isKey": false,
            "numCitedBy": 206,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "A novel method is proposed for performing multi-label, semi-automated image segmentation. Given a small number of pixels with user-defined labels, one can analytically (and quickly) determine the probability that a random walker starting at each unlabeled pixel will first reach one of the pre-labeled pixels. By assigning each pixel to the label for which the greatest probability is calculated, a high-quality image segmentation may be obtained. Theoretical properties of this algorithm are developed along with the corresponding connections to discrete potential theory and electrical circuits. This algorithm is formulated in discrete space (i.e., on a graph) using combinatorial analogues of standard operators and principles from continuous potential theory, allowing it to be applied in arbitrary dimension."
            },
            "slug": "Multi-label-Image-Segmentation-for-Medical-Based-on-Grady-Funka-Lea",
            "title": {
                "fragments": [],
                "text": "Multi-label Image Segmentation for Medical Applications Based on Graph-Theoretic Electrical Potentials"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "A novel method is proposed for performing multi-label, semi-automated image segmentation using combinatorial analogues of standard operators and principles from continuous potential theory, allowing it to be applied in arbitrary dimension."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV Workshops CVAMIA and MMBIA"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730609"
                        ],
                        "name": "O. Chapelle",
                        "slug": "O.-Chapelle",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Chapelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Chapelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 331378,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6bbc0c752570c46a772f2982728f9ad4191f25dd",
            "isKey": false,
            "numCitedBy": 507,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a framework to incorporate unlabeled data in kernel classifier, based on the idea that two points in the same cluster are more likely to have the same label. This is achieved by modifying the eigenspectrum of the kernel matrix. Experimental results assess the validity of this approach."
            },
            "slug": "Cluster-Kernels-for-Semi-Supervised-Learning-Chapelle-Weston",
            "title": {
                "fragments": [],
                "text": "Cluster Kernels for Semi-Supervised Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "A framework to incorporate unlabeled data in kernel classifier, based on the idea that two points in the same cluster are more likely to have the same label is proposed by modifying the eigenspectrum of the kernel matrix."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728654"
                        ],
                        "name": "U. V. Luxburg",
                        "slug": "U.-V.-Luxburg",
                        "structuredName": {
                            "firstName": "Ulrike",
                            "lastName": "Luxburg",
                            "middleNames": [
                                "von"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. V. Luxburg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698617"
                        ],
                        "name": "O. Bousquet",
                        "slug": "O.-Bousquet",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Bousquet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Bousquet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145520115"
                        ],
                        "name": "Mikhail Belkin",
                        "slug": "Mikhail-Belkin",
                        "structuredName": {
                            "firstName": "Mikhail",
                            "lastName": "Belkin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mikhail Belkin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 356393,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "32b21a53f527c8ed00f28006df5285164d830912",
            "isKey": false,
            "numCitedBy": 122,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "An important aspect of clustering algorithms is whether the partitions constructed on finite samples converge to a useful clustering of the whole data space as the sample size increases. This paper investigates this question for normalized and unnormalized versions of the popular spectral clustering algorithm. Surprisingly, the convergence of unnormalized spectral clustering is more difficult to handle than the normalized case. Even though recently some first results on the convergence of normalized spectral clustering have been obtained, for the unnormalized case we have to develop a completely new approach combining tools from numerical integration, spectral and perturbation theory, and probability. It turns out that while in the normalized case, spectral clustering usually converges to a nice partition of the data space, in the unnormalized case the same only holds under strong additional assumptions which are not always satisfied. We conclude that our analysis gives strong evidence for the superiority of normalized spectral clustering. It also provides a basis for future exploration of other Laplacian-based methods."
            },
            "slug": "Limits-of-Spectral-Clustering-Luxburg-Bousquet",
            "title": {
                "fragments": [],
                "text": "Limits of Spectral Clustering"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "This paper investigates whether the partitions constructed on finite samples converge to a useful clustering of the whole data space as the sample size increases and concludes that while in the normalized case, spectral clustering usually converges to a nice partition of the data space, in the unnormalized case the same only holds under strong additional assumptions which are not always satisfied."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30400079"
                        ],
                        "name": "Yair Weiss",
                        "slug": "Yair-Weiss",
                        "structuredName": {
                            "firstName": "Yair",
                            "lastName": "Weiss",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yair Weiss"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "be better modeled by multiple multinomial instead of a single one (Nigam et al., 2000). Some other examples are (Shahshahani & Landgrebe, 1994) (Miller & Uyar, 1997). Another solution is to down-weighing unlabeled data (Corduneanu & Jaakkola, 2001), which is also used by Nigam et al. (2000), and by Callison-Burch et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The data points are mapped into a new space spanned by the first k eigenvectors of the normalized Laplacian in (Ng et al., 2001), with special normalization."
                    },
                    "intents": []
                }
            ],
            "corpusId": 18764978,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c02dfd94b11933093c797c362e2f8f6a3b9b8012",
            "isKey": false,
            "numCitedBy": 8412,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Despite many empirical successes of spectral clustering methods\u2014 algorithms that cluster points using eigenvectors of matrices derived from the data\u2014there are several unresolved issues. First. there are a wide variety of algorithms that use the eigenvectors in slightly different ways. Second, many of these algorithms have no proof that they will actually compute a reasonable clustering. In this paper, we present a simple spectral clustering algorithm that can be implemented using a few lines of Matlab. Using tools from matrix perturbation theory, we analyze the algorithm, and give conditions under which it can be expected to do well. We also show surprisingly good experimental results on a number of challenging clustering problems."
            },
            "slug": "On-Spectral-Clustering:-Analysis-and-an-algorithm-Ng-Jordan",
            "title": {
                "fragments": [],
                "text": "On Spectral Clustering: Analysis and an algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "A simple spectral clustering algorithm that can be implemented using a few lines of Matlab is presented, and tools from matrix perturbation theory are used to analyze the algorithm, and give conditions under which it can be expected to do well."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144865353"
                        ],
                        "name": "B. Pang",
                        "slug": "B.-Pang",
                        "structuredName": {
                            "firstName": "Bo",
                            "lastName": "Pang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Pang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145810617"
                        ],
                        "name": "Lillian Lee",
                        "slug": "Lillian-Lee",
                        "structuredName": {
                            "firstName": "Lillian",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lillian Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 388,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "167e1359943b96b9e92ee73db1df69a1f65d731d",
            "isKey": false,
            "numCitedBy": 3565,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Sentiment analysis seeks to identify the viewpoint(s) underlying a text span; an example application is classifying a movie review as \"thumbs up\" or \"thumbs down\". To determine this sentiment polarity, we propose a novel machine-learning method that applies text-categorization techniques to just the subjective portions of the document. Extracting these portions can be implemented using efficient techniques for finding minimum cuts in graphs; this greatly facilitates incorporation of cross-sentence contextual constraints."
            },
            "slug": "A-Sentimental-Education:-Sentiment-Analysis-Using-Pang-Lee",
            "title": {
                "fragments": [],
                "text": "A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A novel machine-learning method is proposed that applies text-categorization techniques to just the subjective portions of the document, which greatly facilitates incorporation of cross-sentence contextual constraints."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2834541"
                        ],
                        "name": "R. Kondor",
                        "slug": "R.-Kondor",
                        "structuredName": {
                            "firstName": "Risi",
                            "lastName": "Kondor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kondor"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 29
                            }
                        ],
                        "text": "3 Information Regularization Szummer and Jaakkola (2002) propose the information regularization frame work to control the label conditionals p(y|x) by p(x), wherep(x) may be estimated from unlabeled data."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 11
                            }
                        ],
                        "text": "(2002) and Smola and Kondor (2003) both show the spectr al transformation of a Laplacian results in kernels suitable for semi-supervise d learning."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 150
                            }
                        ],
                        "text": "5.1 Transductive SVM . . . . . . . . . . . . . . . . . . . . . . . . . 12 5.2 Null Category Noise Model for Gaussian Processes . . . . . . . . 14 5.3 Information Regularization . . . . . . . . . . . . . . . . . . . . ."
                    },
                    "intents": []
                }
            ],
            "corpusId": 7326173,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "60de4b6068407defa3c88f5feeb8b74d8e55fe9c",
            "isKey": true,
            "numCitedBy": 858,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a family of kernels on graphs based on the notion of regularization operators. This generalizes in a natural way the notion of regularization and Greens functions, as commonly used for real valued functions, to graphs. It turns out that diffusion kernels can be found as a special case of our reasoning. We show that the class of positive, monotonically decreasing functions on the unit interval leads to kernels and corresponding regularization operators."
            },
            "slug": "Kernels-and-Regularization-on-Graphs-Smola-Kondor",
            "title": {
                "fragments": [],
                "text": "Kernels and Regularization on Graphs"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is shown that the class of positive, monotonically decreasing functions on the unit interval leads to kernels and corresponding regularization operators and can be found as a special case of the reasoning."
            },
            "venue": {
                "fragments": [],
                "text": "COLT"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 689,
                                "start": 13
                            }
                        ],
                        "text": "Zhou and Li (2005a) proposed using co-training for semi-supervised regression. The paper used two kNN regressors, each with a different p-norm as distance measure. Like in co-training, each regressor makes prediction on unlabe led data, and the most confident predictions are used to train the other regressor. The confidence of a prediction on unlabeled point is measured by the MSE on labeled set before and after adding this prediction as training data to the current r egressor. Similarly Sindhwani et al. (2005b); Brefeld et al. (2006) perform multi-view regression, where a regularization term depends on the disagreement a o g regressors on different views. Cortes and Mohri (2006) propose a simple yet efficient transductive r egression model."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 298,
                                "start": 2
                            }
                        ],
                        "text": ", 2005) are con structed from the spectrum of the Laplacian, with non-parametric convex o ptimization. Learning the optimal eigenvalues for a graph kernel is in fact a wa y to (at least partially) improve an imperfect graph. In this sense it is related to g raph construction. Kapoor et al. (2005) learn both the graph weight hyperparameter, the h yperparameter for Laplacian spectrum transformation r(\u03bb) = \u03bb + \u03b4, and the noise model hyperparameter with evidence maximization."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 732,
                                "start": 1
                            }
                        ],
                        "text": "(2005) (von Luxburg et al., 2004) study the c onsistency of spectral clustering methods . The authors find that the normalized Laplacian is better than the unnormalized Laplacian for spectral clustering. The conve rgence of the eigenvectors of the unnormalized Laplacian is not clear, while the normaliz ed Laplacian always converges under general conditions. There are e x mples where the top eigenvectors of the unnormalized Laplacian do not yield a sensible clu stering. The corresponding problem in semi-supervised classification nee ds further study. One reason is that in semi-supervised learning the whole Laplacian ( normalized or not) is often used for regularization, not only the top eigenvector s. Zhang and Ando (2006) prove that semi-supervised learning based on graph"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 860,
                                "start": 1
                            }
                        ],
                        "text": "(2005b) take a hub - authority approach and essentially convert a directed graph into an und irected one. Two hub nodes are connected by an undirected edge with appropr iate weight if they co-link to authority nodes, and vice versa. Semi-supervised learnin g then proceeds on the undirected graph. Zhou et al. (2005a) generalize the work further. The algorithm takes a tr ansition matrix (with a unique stationary distribution) as input, and gives a closed fo rm solution on unlabeled data. The solution parallels and generalizes the normaliz ed Laplacian solution for undirected graphs (Zhou et al., 2004a). The pre vious work (Zhou et al., 2005b) is a special case with the 2-step random walk transition matrix. In the absence of labels, the algorithm is the generalization of the normalized c ut (Shi & Malik, 2000) on directed graphs. Lu and Getoor (2003) convert the link structure in a directed graph into pe rnode features, and combines them with per-node object features in logistic regression."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 41
                            }
                        ],
                        "text": "2 Gaussian Processes Lawrence and Jordan (2005) proposed a Gaussian process appro ach, which can be viewed as the Gaussian process parallel of TSVM."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 2
                            }
                        ],
                        "text": ", 2005) also uses unlabeled data fo r model selection and active learning. Kaariainen (2005) uses the metric to de rive a generalization error bound, see Section 9."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 541,
                                "start": 13
                            }
                        ],
                        "text": "Zhou and Li (2005a) proposed using co-training for semi-supervised regression. The paper used two kNN regressors, each with a different p-norm as distance measure. Like in co-training, each regressor makes prediction on unlabe led data, and the most confident predictions are used to train the other regressor. The confidence of a prediction on unlabeled point is measured by the MSE on labeled set before and after adding this prediction as training data to the current r egressor. Similarly Sindhwani et al. (2005b); Brefeld et al. (2006) perform multi-view regression, where a regularization term depends on the disagreement a o g regressors on different views."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 543,
                                "start": 8
                            }
                        ],
                        "text": "Leskes (2005) presents a generalization error bound for semi-super vised learning with multiple learners, an extension to co-training. The author shows that if multiple learning algorithms are forced to produce similar hypotheses (i.e. to agree) given the same training set, and such hypotheses still have low train ing error, then the generalization error bound is tighter. The unlabeled data is us ed to assess the agreement among hypotheses. The author proposes a new A gre mentBoost algorithm to implement the procedure. Kaariainen (2005) presents another generalization error bound for s emi-supervised learning."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 20
                            }
                        ],
                        "text": "Sajama and Orlitsky (2005) analyze th e lower and upper bounds on estimating data-density-based distance."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1069,
                                "start": 1
                            }
                        ],
                        "text": "(2005b) take a hub - authority approach and essentially convert a directed graph into an und irected one. Two hub nodes are connected by an undirected edge with appropr iate weight if they co-link to authority nodes, and vice versa. Semi-supervised learnin g then proceeds on the undirected graph. Zhou et al. (2005a) generalize the work further. The algorithm takes a tr ansition matrix (with a unique stationary distribution) as input, and gives a closed fo rm solution on unlabeled data. The solution parallels and generalizes the normaliz ed Laplacian solution for undirected graphs (Zhou et al., 2004a). The pre vious work (Zhou et al., 2005b) is a special case with the 2-step random walk transition matrix. In the absence of labels, the algorithm is the generalization of the normalized c ut (Shi & Malik, 2000) on directed graphs. Lu and Getoor (2003) convert the link structure in a directed graph into pe rnode features, and combines them with per-node object features in logistic regression. They also use an EM-like iterative algorithm. Zhou et al. (2006a) propose to formulate relational objects using hyperg raphs, where an edge can connect more than two vertices, and extend spectral clustering, classification and embedding to such hypergraphs."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Hyperparameter and k"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 449,
                                "start": 0
                            }
                        ],
                        "text": "Muslea et al. (2002) propose CO-EMT which co mbines multi-view (e.g. co-training) learning with active learning. Zhou et al. (2004 c); Zhou et al. (2006b) apply semi-supervised learning together with active le arning to content-based image retrieval. Many active learning algorithms naively select as query the point with maximum label ambiguity (entropy), or least confidence, or maximum disagreemen t between multiple learners. Zhu et al. (2003b) show that these are not nec essarily the right things to do, if one is interested in classification error."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 0
                            }
                        ],
                        "text": "Muslea et al. (2002) propose CO-EMT which co mbines multi-view (e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 0
                            }
                        ],
                        "text": "Nigam et al. (2000) apply the EM algorithm on mixture of multinomial for the task of text classification. They showed the resulting classifiers perfo rm better than those trained only fromL. Baluja (1998) uses the same algorithm on a face orientation discrimination task."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 0
                            }
                        ],
                        "text": "Muslea et al. (2002) propose CO-EMT which co mbines multi-view (e.g. co-training) learning with active learning. Zhou et al. (2004 c); Zhou et al. (2006b) apply semi-supervised learning together with active le arning to content-based image retrieval."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "Nigam et al. (2000) apply the EM algorithm on mixture of multinomial for the task of text classification."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Active + semi-supervised"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 284,
                                "start": 0
                            }
                        ],
                        "text": "Goldman and Zhou (2000) use two learners of different type but both takes the whole feature set, and ess entially use one learner\u2019s high confidence data points, identified with a set of statistical tests, in U to teach the other learning and vice versa. Later Zhou and Goldman (2004) p ropose a single-view multiple-learner Democratic Co-learning algorithm."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 894,
                                "start": 0
                            }
                        ],
                        "text": "Goldman and Zhou (2000) use two learners of different type but both takes the whole feature set, and ess entially use one learner\u2019s high confidence data points, identified with a set of statistical tests, in U to teach the other learning and vice versa. Later Zhou and Goldman (2004) p ropose a single-view multiple-learner Democratic Co-learning algorithm. An ense mble of learners with different inductive bias are trained separately on the complete feature of the labeled data. They then make predictions on the unlabeled data . If a majority of learners confidently agree on the class of an unlabeled point xu, that classification is used as the label of xu. xu and its label is added to the training data. All learners are retrained on the updated training set. The final pre diction is made with a variant of a weighted majority vote among all the learners. Similarly Zhou and Li (2005b) propose \u2018tri-training\u2019 which uses three learners ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 0
                            }
                        ],
                        "text": "Goldman and Zhou (2000) use two learners of different type but both takes the whole feature set, and ess entially use one learner\u2019s high confidence data points, identified with a set of statistical tests, in U to teach the other learning and vice versa."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 0
                            }
                        ],
                        "text": "Getz et al. (2005) computes the marginal probabilities of the discrete Markov random field at any temperature with the Multi-canonical Monte-Carlo method, which seems to be able to overcome the energy trap faced by the standard Me tropolis or Swendsen-Wang method."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Semi-supervised learning \u2013 a s"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 525,
                                "start": 0
                            }
                        ],
                        "text": "Jones (2005) used co-training, co-EM and oth er related methods for information extraction from text. Balcan and Blum (2006) show th at co-training can be quite effective, that in the extreme case only one labeled point is needed to learn the classifier. Zhou et al. (2007) give a co-training algo rithm using Canonical Correlation Analysis which also need only one labeled point. Co-training makes strong assumptions on the splitting of features. One might wonder if these conditions can be relaxed. Goldman and Zhou (2000) use two learners of different type but both takes the whole feature set, and ess entially use one learner\u2019s high confidence data points, identified with a set of statistical tests, in U to teach the other learning and vice versa."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 22
                            }
                        ],
                        "text": "Similarly Zhou and Li (2005b) propose \u2018tri-training\u2019 which uses three learners."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 0
                            }
                        ],
                        "text": "Jones (2005) used co-training, co-EM and oth er related methods for information extraction from text."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 274,
                                "start": 0
                            }
                        ],
                        "text": "Jones (2005) used co-training, co-EM and oth er related methods for information extraction from text. Balcan and Blum (2006) show th at co-training can be quite effective, that in the extreme case only one labeled point is needed to learn the classifier. Zhou et al. (2007) give a co-training algo rithm using Canonical Correlation Analysis which also need only one labeled point."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 0
                            }
                        ],
                        "text": "Jones (2005) used co-training, co-EM and oth er related methods for information extraction from text. Balcan and Blum (2006) show th at co-training can be quite effective, that in the extreme case only one labeled point is needed to learn the classifier."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 0
                            }
                        ],
                        "text": "(2005a) give a semi-supervised kernel that is not limited to the unlabeled points, but defined over all input space."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Two-view feature generation model"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "Altun et al. (2005) defines a graph kernel over the whole space by linearly combining the norms of a standard kernel and a graph regularization term, resulting in a nonlinear graph kernel similar to Sindhwani et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 224,
                                "start": 0
                            }
                        ],
                        "text": "Altun et al. (2005) defines a graph kernel over the whole space by linearly combining the norms of a standard kernel and a graph regularization term, resulting in a nonlinear graph kernel similar to Sindhwani et al. (2005a). They use the kernel with a marg in loss."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 294,
                                "start": 0
                            }
                        ],
                        "text": "Altun et al. (2005) defines a graph kernel over the whole space by linearly combining the norms of a standard kernel and a graph regularization term, resulting in a nonlinear graph kernel similar to Sindhwani et al. (2005a). They use the kernel with a marg in loss. Brefeld and Scheffer (2006) extend structured SVM with a multi-view regu larizer, which penalizes disagreements between classifications on unlabeled data, w here the classifiers operate on different feature subsets."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Maximum margin semi-supervise"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 870,
                                "start": 21
                            }
                        ],
                        "text": "2 Gaussian Processes Lawrence and Jordan (2005) proposed a Gaussian process appro ach, which can be viewed as the Gaussian process parallel of TSVM. The key difference to a standard Gaussian process is in the noise model. A \u2018null category noise model\u2019 maps the hidden continuous variable f to three instead of two labels, specifically to the never used label \u20180\u2019 whenf is around zero. On top of that, it is restricted that unlabeled data points cannot take the label 0. This pushes the posterior of f away from zero for the unlabeled points. It achieves the similar effect of TSVM where the ma rgin avoids dense unlabeled data region. However nothing special is done on th process model. Therefore all the benefit of unlabeled data comes from the noise mod l. A very similar noise model is proposed in (Chu & Ghahramani, 2004) for ordin al regression. Chu et al. (2006) develop Gaussian process models that incorporate pa irwise label relations (e."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 108
                            }
                        ],
                        "text": "5.1 Transductive SVM . . . . . . . . . . . . . . . . . . . . . . . . . 12 5.2 Null Category Noise Model for Gaussian Processes . . . . . . . . 14 5.3 Information Regularization . . . . . . . . . . . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 21
                            }
                        ],
                        "text": "2 Gaussian Processes Lawrence and Jordan (2005) proposed a Gaussian process appro ach, which can be viewed as the Gaussian process parallel of TSVM."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Semi-supervised learning v"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 63
                            }
                        ],
                        "text": "This is similar to semisupervised learning with mixture models (Nigam et al., 2000) or clusters (Dar a et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 105
                            }
                        ],
                        "text": "Another solution is to down-weighing unlabeled data (Corduneanu & Jaakkola, 2001), which is also used by Nigam et al. (2000), and by Callison-Burch et al. (2004) who estimate word alignment for machine translation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 0
                            }
                        ],
                        "text": "Oliveira et al. (2005) propose a simple procedure for semi-supervised learning: First one runs PCA onL \u222a U (ignoring the labels)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 0
                            }
                        ],
                        "text": "Nigam et al. (2000) apply the EM algorithm on mixture of multinomial for the task of text classification."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 151
                            }
                        ],
                        "text": "For example in text categorization a topic may contain several sub-topics, and will\nbe better modeled by multiple multinomial instead of a single one (Nigam et al., 2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 151
                            }
                        ],
                        "text": "For example in text categorization a topic may contain several sub-topics, an d will be better modeled by multiple multinomial instead of a single one (Nigam et al., 2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Text classificatio"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 742,
                                "start": 45
                            }
                        ],
                        "text": "With the Branch and Bound sea rch technique, Chapelle et al. (2006b) finds the global optimal solution for small datasets. The results indicate excellent accuracy. Although Branch and Bound will probably never be useful for large datasets, the results provide so me ground truth, and points to the potentials of TSVMs with better approximation methods. Weston et al. (2006) learn with a \u2018universum\u2019, which is a set of unlabeled data that is known to come fromneitherof the two classes. The decision boundary is encouraged to pass through the universum. One interpretation is similar to the maximum entropy principle: the classifier should be confident on labeled example s, yet maximally ignorant on unrelated examples. Zhang and Oles (2000) point out that TSVMs may not behave well under some circumstances."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 20
                            }
                        ],
                        "text": "In Joachims (1999); Chapelle et al. (2006b), it is a c onstraint on hard labels 1 u l+u"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 21
                            }
                        ],
                        "text": "In a similar spirit, Chapelle et al. (2006a) use a continuation approach, which a lso starts by minimizing an easy convex objective function, and gradually defor ms it to the TSVM objective (with Gaussian instead of hat loss), using the solution o f previous iterations to initialize the next ones."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 834,
                                "start": 21
                            }
                        ],
                        "text": "In a similar spirit, Chapelle et al. (2006a) use a continuation approach, which a lso starts by minimizing an easy convex objective function, and gradually defor ms it to the TSVM objective (with Gaussian instead of hat loss), using the solution o f previous iterations to initialize the next ones. Collobert et al. (2006) optimize the hard TSVM directly, using an approximate optimization procedure known a s concave-convex procedure (CCCP). The key is to notice that the hat lo ss is a sum of a convex function and a concave function. By replacing the concave fu nction with a linear upper bound, one can perform convex minimization to produce an u pper bound of the loss function. This is repeated until a local minimum is reached. T he authors report significant speed up of TSVM training with CCCP. Sindhwa ni and Keerthi (2006) proposed a fast algorithm for linear S3VMs, suitable for large scale"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 323,
                                "start": 21
                            }
                        ],
                        "text": "In a similar spirit, Chapelle et al. (2006a) use a continuation approach, which a lso starts by minimizing an easy convex objective function, and gradually defor ms it to the TSVM objective (with Gaussian instead of hat loss), using the solution o f previous iterations to initialize the next ones. Collobert et al. (2006) optimize the hard TSVM directly, using an approximate optimization procedure known a s concave-convex procedure (CCCP)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 0
                            }
                        ],
                        "text": "Chapelle et al. (2002) and Smola and Kondor (2003) both show the spectr al transformation of a Laplacian results in kernels suitable for semi-supervise d learning."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 45
                            }
                        ],
                        "text": "With the Branch and Bound sea rch technique, Chapelle et al. (2006b) finds the global optimal solution for small datasets."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 374,
                                "start": 45
                            }
                        ],
                        "text": "With the Branch and Bound sea rch technique, Chapelle et al. (2006b) finds the global optimal solution for small datasets. The results indicate excellent accuracy. Although Branch and Bound will probably never be useful for large datasets, the results provide so me ground truth, and points to the potentials of TSVMs with better approximation methods. Weston et al. (2006) learn with a \u2018universum\u2019, which is a set of unlabeled data that is known to come fromneitherof the two classes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 39
                            }
                        ],
                        "text": "A book on semi-supervised learning is (Chapelle et al., 2006)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A continuation method for semisupervised SVMs.ICML06"
            },
            "venue": {
                "fragments": [],
                "text": "23rd International Conference on Machine Learning"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 10
                            }
                        ],
                        "text": "19 6.1.10 Some Other Methods . . . . . . . . . . . . . . . . . . . . 20\n6.2 Graph Construction . . . . . . . . . . . . . . . . . . . . . . . . . 20 6.3 Fast Computation . . . . . . . . . . . . . . . . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 22
                            }
                        ],
                        "text": "11 Some Other Methods Szummer and Jaakkola (2001) perform a t-step Markov random walk on the graph."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 85
                            }
                        ],
                        "text": "It has been suggested that temporal correlation serves as the glue, as summarized by ( Sinha et al., 2006) (Result 14)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Face recogn"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 742,
                                "start": 45
                            }
                        ],
                        "text": "With the Branch and Bound sea rch technique, Chapelle et al. (2006b) finds the global optimal solution for small datasets. The results indicate excellent accuracy. Although Branch and Bound will probably never be useful for large datasets, the results provide so me ground truth, and points to the potentials of TSVMs with better approximation methods. Weston et al. (2006) learn with a \u2018universum\u2019, which is a set of unlabeled data that is known to come fromneitherof the two classes. The decision boundary is encouraged to pass through the universum. One interpretation is similar to the maximum entropy principle: the classifier should be confident on labeled example s, yet maximally ignorant on unrelated examples. Zhang and Oles (2000) point out that TSVMs may not behave well under some circumstances."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 21
                            }
                        ],
                        "text": "In a similar spirit, Chapelle et al. (2006a) use a continuation approach, which a lso starts by minimizing an easy convex objective function, and gradually defor ms it to the TSVM objective (with Gaussian instead of hat loss), using the solution o f previous iterations to initialize the next ones."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 20
                            }
                        ],
                        "text": "In Joachims (1999); Chapelle et al. (2006b), it is a c onstraint on"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 834,
                                "start": 21
                            }
                        ],
                        "text": "In a similar spirit, Chapelle et al. (2006a) use a continuation approach, which a lso starts by minimizing an easy convex objective function, and gradually defor ms it to the TSVM objective (with Gaussian instead of hat loss), using the solution o f previous iterations to initialize the next ones. Collobert et al. (2006) optimize the hard TSVM directly, using an approximate optimization procedure known a s concave-convex procedure (CCCP). The key is to notice that the hat lo ss is a sum of a convex function and a concave function. By replacing the concave fu nction with a linear upper bound, one can perform convex minimization to produce an u pper bound of the loss function. This is repeated until a local minimum is reached. T he authors report significant speed up of TSVM training with CCCP. Sindhwa ni and Keerthi (2006) proposed a fast algorithm for linear S3VMs, suitable for large scale"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 323,
                                "start": 21
                            }
                        ],
                        "text": "In a similar spirit, Chapelle et al. (2006a) use a continuation approach, which a lso starts by minimizing an easy convex objective function, and gradually defor ms it to the TSVM objective (with Gaussian instead of hat loss), using the solution o f previous iterations to initialize the next ones. Collobert et al. (2006) optimize the hard TSVM directly, using an approximate optimization procedure known a s concave-convex procedure (CCCP)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 0
                            }
                        ],
                        "text": "Chapelle et al. (2002) and Smola and Kondor (2003) both show the spectr al transformation of a Laplacian results in kernels suitable for semi-supervise d learning."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 45
                            }
                        ],
                        "text": "With the Branch and Bound sea rch technique, Chapelle et al. (2006b) finds the global optimal solution for small datasets."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 374,
                                "start": 45
                            }
                        ],
                        "text": "With the Branch and Bound sea rch technique, Chapelle et al. (2006b) finds the global optimal solution for small datasets. The results indicate excellent accuracy. Although Branch and Bound will probably never be useful for large datasets, the results provide so me ground truth, and points to the potentials of TSVMs with better approximation methods. Weston et al. (2006) learn with a \u2018universum\u2019, which is a set of unlabeled data that is known to come fromneitherof the two classes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 39
                            }
                        ],
                        "text": "A book on semi-supervised learning is (Chapelle et al., 2006)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A continuation method for semi"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 10
                            }
                        ],
                        "text": "19 6.1.10 Some Other Methods . . . . . . . . . . . . . . . . . . . . 20\n6.2 Graph Construction . . . . . . . . . . . . . . . . . . . . . . . . . 20 6.3 Fast Computation . . . . . . . . . . . . . . . . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 22
                            }
                        ],
                        "text": "11 Some Other Methods Szummer and Jaakkola (2001) perform a t-step Markov random walk on the graph."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A co-regularized appro"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 10
                            }
                        ],
                        "text": "19 6.1.10 Some Other Methods . . . . . . . . . . . . . . . . . . . . 20\n6.2 Graph Construction . . . . . . . . . . . . . . . . . . . . . . . . . 20 6.3 Fast Computation . . . . . . . . . . . . . . . . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 22
                            }
                        ],
                        "text": "10 Some Other Methods Szummer and Jaakkola (2001) perform a t-step Markov random walk on the graph."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Face recogn"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 0
                            }
                        ],
                        "text": "Balcan et al. (2005b) relax the conditional independence assumption with a much weaker expansion condition, and justify the iterative co-training proc edure."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 17
                            }
                        ],
                        "text": "In another work, Balcan et al. (2005a) constru ct a graph on webcam images using temporal links (as well as color, face similarity links) for semi-supervised learning."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 322,
                                "start": 0
                            }
                        ],
                        "text": "Balcan et al. (2005a) build gra phs for video surveillance using strong domain knowledge, where the graph of we bcam images consists of time edges, color edges and face edges. Such graphs reflect a deep understanding of the problem structure and how unlabeled data is ex pect d to help. Carreira-Perpinan and Zemel (2005) build robust graphs from multiple minimum spanning trees by perturbation and edge removal."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 0
                            }
                        ],
                        "text": "Balcan et al. (2005a) build gra phs for video surveillance using strong domain knowledge, where the graph of we bcam images consists of time edges, color edges and face edges."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Co-training and expansion"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 16
                            }
                        ],
                        "text": "Balcan and Blum (2005) propose a PAC-style model for semi-supervised learning."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "(2005) perform empirical experiments on word sense disambig uation, comparing variants of co-training and spectral graph transducer."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 66
                            }
                        ],
                        "text": "Another solution is to down-weighing unlabeled data (Corduneanu & Jaakkola, 2001), which is also used by Nigam et al. (2000), and by Callison-Burch et al. (2004) who estimate word alignment for machine translation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 131
                            }
                        ],
                        "text": "More discussions on identifiability and semi-supervised learning can be found in e.g. (Ratsaby & Venkatesh, 1995) and (Cordunean & Jaakkola, 2001)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 0
                            }
                        ],
                        "text": "(2005b) take a hub - authority approach and essentially convert a directed graph into an und irected one."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 25
                            }
                        ],
                        "text": "C orduneanu and Jaakkola (2005) extend the work by formulating semi-super vised learning as a communication problem."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "(2005) used this approach for image categorization."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "(2005) apply self-training to object detection syste ms from images, and show the semi-supervised technique compares favorably with a stateof-the-art detector."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 7
                            }
                        ],
                        "text": "Leskes (2005) presents a generalization error bound for semi-super vised learning with multiple learners, an extension to co-training."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 17
                            }
                        ],
                        "text": "Burges and Platt (2005) propose a directedgraphical model, called Conditional Harmonic Mixing, that is somewhat between graph-based semi-supervised learning and standard Bayes nets."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 248,
                                "start": 240
                            }
                        ],
                        "text": "However instead of directly using the generative model for classification, each labeled example is converted into a fixed-length Fisher score vector, i.e. the derivatives of log likelihood w.r.t. model parameters, for all component models (Jaakkola & Haussler, 1998)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 12
                            }
                        ],
                        "text": "Zhou and Li (2005a) proposed using co-training for semi-supervised regression."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Diffusion kernels on graphs"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 29
                            }
                        ],
                        "text": "3 Information Regularization Szummer and Jaakkola (2002) propose the information regularization frame work to control the label conditionals p(y|x) by p(x), wherep(x) may be estimated from unlabeled data."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 150
                            }
                        ],
                        "text": "5.1 Transductive SVM . . . . . . . . . . . . . . . . . . . . . . . . . 12 5.2 Null Category Noise Model for Gaussian Processes . . . . . . . . 14 5.3 Information Regularization . . . . . . . . . . . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 761,
                                "start": 29
                            }
                        ],
                        "text": "3 Information Regularization Szummer and Jaakkola (2002) propose the information regularization frame work to control the label conditionals p(y|x) by p(x), wherep(x) may be estimated from unlabeled data. The idea is that labels shouldn\u2019t change too much in regions where p(x) is high. The authors use the mutual information I(x; y) betweenx andy as a measure of label complexity. I(x; y) is small when the labels are homogeneous, and large when labels vary. This motives the minimization of the product of p(x) mass in a region withI(x; y) (normalized by a variance term). The minimization is carried out on multiple overlapping regions covering the data space. The theory is developed further in (Corduneanu & Jaakkola, 2003). C orduneanu and Jaakkola (2005) extend the work by formulating semi-super vised learning as a communication problem."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Partially labeled classification with Mark"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 3
                            }
                        ],
                        "text": "In Chapelle and Zien (2005), it is a constraint on c ontinuous function predictions: 1 u l+u"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 0
                            }
                        ],
                        "text": "Chapelle and Zien (2005) use a density-sensitive connectivity distance b etw en nodesi, j (a given path between i, j consists of several segments, one of them is the longest; now consider all paths between i, j and find the shortest \u2018longest segment\u2019)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 335,
                                "start": 0
                            }
                        ],
                        "text": "Chapelle and Zien (2005) use a density-sensitive connectivity distance b etw en nodesi, j (a given path between i, j consists of several segments, one of them is the longest; now consider all paths between i, j and find the shortest \u2018longest segment\u2019). Exponentiating the negative distance gives a graph kernel. Bousquet et al. (2004) propose \u2018measure-based regularization\u2019, th e continuous counterpart of graph-based regularization."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Semi-supervised classification by low den"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "12772030"
                        ],
                        "name": "S. Coates",
                        "slug": "S.-Coates",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Coates",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Coates"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 221927721,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "389570e45a3622f630c318913e9b109cc95de109",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "AOAC-Research-Institute-Coates",
            "title": {
                "fragments": [],
                "text": "AOAC Research Institute"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4231116"
                        ],
                        "name": "F. Chung",
                        "slug": "F.-Chung",
                        "structuredName": {
                            "firstName": "Fan",
                            "lastName": "Chung",
                            "middleNames": [
                                "R.",
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Chung"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 209973425,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "851eb4b78f6deb8aba9d39529e462c5319940f51",
            "isKey": false,
            "numCitedBy": 172,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Spectral-Graph-Theory,-Regional-Conference-Series-Chung",
            "title": {
                "fragments": [],
                "text": "Spectral Graph Theory, Regional Conference Series in Math."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1513640199"
                        ],
                        "name": "F. E. R. Pollard-Urquhart",
                        "slug": "F.-E.-R.-Pollard-Urquhart",
                        "structuredName": {
                            "firstName": "F.",
                            "lastName": "Pollard-Urquhart",
                            "middleNames": [
                                "E.",
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. E. R. Pollard-Urquhart"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 165951252,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "641c90bb4964acbe2cc0cd4d17e22a98a0dd23d9",
            "isKey": false,
            "numCitedBy": 101,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "San-sebastian,-spain-Pollard-Urquhart",
            "title": {
                "fragments": [],
                "text": "San sebastian, spain"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1902
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055460158"
                        ],
                        "name": "John Hart",
                        "slug": "John-Hart",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Hart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John Hart"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 125819190,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9d30b773355fa32d8e685aba6924e233e1fa4a27",
            "isKey": false,
            "numCitedBy": 290,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "ACM-Transactions-on-Graphics:-Editorial-Hart",
            "title": {
                "fragments": [],
                "text": "ACM Transactions on Graphics: Editorial"
            },
            "venue": {
                "fragments": [],
                "text": "SIGGRAPH 2003"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728216"
                        ],
                        "name": "A. Storkey",
                        "slug": "A.-Storkey",
                        "structuredName": {
                            "firstName": "Amos",
                            "lastName": "Storkey",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Storkey"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 208023192,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fe95a7a1d66ebdc7d3097ebbe97de60c9630bf84",
            "isKey": false,
            "numCitedBy": 553,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "ADVANCES-IN-NEURAL-INFORMATION-PROCESSING-SYSTEMS-Storkey",
            "title": {
                "fragments": [],
                "text": "ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16"
            },
            "venue": {
                "fragments": [],
                "text": "NIPS 2004"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40492233"
                        ],
                        "name": "E. Watanabe",
                        "slug": "E.-Watanabe",
                        "structuredName": {
                            "firstName": "Eiji",
                            "lastName": "Watanabe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Watanabe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113701334"
                        ],
                        "name": "K. Mori",
                        "slug": "K.-Mori",
                        "structuredName": {
                            "firstName": "Katsumi",
                            "lastName": "Mori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Mori"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 196077515,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "13387c0f07a20eb397318ca10fca0f3b2c1b4a3e",
            "isKey": false,
            "numCitedBy": 14,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Distributed-Cooperative-Learning-Algorithm-for-a-Watanabe-Mori",
            "title": {
                "fragments": [],
                "text": "A Distributed-Cooperative Learning Algorithm for Multi-Layered Neural Networks using a PC Cluster"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745410"
                        ],
                        "name": "Maria-Florina Balcan",
                        "slug": "Maria-Florina-Balcan",
                        "structuredName": {
                            "firstName": "Maria-Florina",
                            "lastName": "Balcan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Maria-Florina Balcan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690967"
                        ],
                        "name": "A. Blum",
                        "slug": "A.-Blum",
                        "structuredName": {
                            "firstName": "Avrim",
                            "lastName": "Blum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Blum"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 64074970,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0ad524fa8aaacac8c5e400aa904302ec180b51b4",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "An-Augmented-PAC-Model-for-Semi-Supervised-Learning-Balcan-Blum",
            "title": {
                "fragments": [],
                "text": "An Augmented PAC Model for Semi-Supervised Learning"
            },
            "venue": {
                "fragments": [],
                "text": "Semi-Supervised Learning"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713919"
                        ],
                        "name": "P. Langley",
                        "slug": "P.-Langley",
                        "structuredName": {
                            "firstName": "Pat",
                            "lastName": "Langley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Langley"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6655909,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "028165965fdf066821e1b65ac1de3ae6c503c30d",
            "isKey": false,
            "numCitedBy": 857,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Editorial:-On-Machine-Learning-Langley",
            "title": {
                "fragments": [],
                "text": "Editorial: On Machine Learning"
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145918972"
                        ],
                        "name": "F. Denis",
                        "slug": "F.-Denis",
                        "structuredName": {
                            "firstName": "Fran\u00e7ois",
                            "lastName": "Denis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Denis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741371"
                        ],
                        "name": "R\u00e9mi Gilleron",
                        "slug": "R\u00e9mi-Gilleron",
                        "structuredName": {
                            "firstName": "R\u00e9mi",
                            "lastName": "Gilleron",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R\u00e9mi Gilleron"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144640325"
                        ],
                        "name": "M. Tommasi",
                        "slug": "M.-Tommasi",
                        "structuredName": {
                            "firstName": "Marc",
                            "lastName": "Tommasi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Tommasi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16355407,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3cb4ab6af4b69e3c5e0457fe075e285b348c51bc",
            "isKey": false,
            "numCitedBy": 107,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Text-Classification-from-Positive-and-Unlabeled-Denis-Gilleron",
            "title": {
                "fragments": [],
                "text": "Text Classification from Positive and Unlabeled Examples"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1832364"
                        ],
                        "name": "Xiaojin Zhu",
                        "slug": "Xiaojin-Zhu",
                        "structuredName": {
                            "firstName": "Xiaojin",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaojin Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744700"
                        ],
                        "name": "Zoubin Ghahramani",
                        "slug": "Zoubin-Ghahramani",
                        "structuredName": {
                            "firstName": "Zoubin",
                            "lastName": "Ghahramani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zoubin Ghahramani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59634378,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7f85812c23e81502daecb0d1cc5d3d0c2a98fd05",
            "isKey": false,
            "numCitedBy": 47,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Towards-semi-supervised-classification-with-Markov-Zhu-Ghahramani",
            "title": {
                "fragments": [],
                "text": "Towards semi-supervised classification with Markov random fields"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Branch and boun"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Automatic alignment of local representatio"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "SPECTRAL GRAPH THEORY (CBMS Regional Conference Series in Mathematics 92)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 259,
                                "start": 242
                            }
                        ],
                        "text": "We shall also mention that instead of using an probabilistic generative mixture model, some approaches employ various clustering algorithms to cluster the whole dataset, then label each cluster with labeled data, e.g. (Demiriz et al., 1999) (Dara et al., 2000)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Clsutering unlabeled data with SOMs improves classification of labeled real-world data"
            },
            "venue": {
                "fragments": [],
                "text": "Clsutering unlabeled data with SOMs improves classification of labeled real-world data"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Efficient approximation methods for harmonic semisupervised learning. Master's thesis"
            },
            "venue": {
                "fragments": [],
                "text": "Efficient approximation methods for harmonic semisupervised learning. Master's thesis"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Exploiting unlabeled data"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A probability analysis on the value of unla"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 0
                            }
                        ],
                        "text": "Grandvalet and Bengio (2005) used the label entropy on unlab eled data as a regularizer."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 0
                            }
                        ],
                        "text": "Grandvalet and Bengio (2005) used the label entropy on unlab eled data as a regularizer. By minimizing the entropy, the method assumes a prior whic h prefers minimal class overlap. Lee et al. (2006) apply the principle of entropy minimization for semi-supervis d learning on 2-D conditional random fields for image pixel classification."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Semi-supervised learning by entro"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "Nigam et al. (2000) apply the EM algorithm on mixture of multinomial for the task of text classification."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 0
                            }
                        ],
                        "text": "Nigam et al. (2000) apply the EM algorithm on mixture of multinomial for the task of text classification. They showed the resulting classifiers perfo rm better than those trained only fromL. Baluja (1998) uses the same algorithm on a face orientation discrimination task."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Active + semi-supervised"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 307,
                                "start": 87
                            }
                        ],
                        "text": "It has been suggested that temporal correlation serves as the glue, as summarized by ( Sinha et al., 2006) (Result 14). It seems when we observe an object with chan ging angles, we link the images as \u2018containing the same object\u2019 by the virtue that the images are close in time. Wallis and B\u0308 ulthoff (2001) created artificial image sequences where a frontal face is morphed into the profile face of a different person."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Large scale semisupervised linea"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Max-margin Markov networks. NIPS'03"
            },
            "venue": {
                "fragments": [],
                "text": "Max-margin Markov networks. NIPS'03"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 42
                            }
                        ],
                        "text": "We refer readers to a recent short survey (Grira et al., 2004) for the literatures."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Unsupervised and semisupervised clustering: a brief survey. in \u2018A Review of Machine Learnin"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 60
                            }
                        ],
                        "text": ", 2 006), and the more challenging structured output spaces (Brefeld et al., 2005; Brefe ld & Scheffer, 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 9
                            }
                        ],
                        "text": "(2005b); Brefeld et al. (2006) perform multi-view regression, where a regularization term depends on the disagreement a o g regressors on different views. Cortes and Mohri (2006) propose a simple yet efficient transductive r egression model."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 9
                            }
                        ],
                        "text": "(2005b); Brefeld et al. (2006) perform multi-view regression, where a regularization term depends on the disagreement a o g regressors on different views."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Multiview discriminative sequential learning.European Conference on Machine Learning (ECML"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Nonlinear dimen"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 0
                            }
                        ],
                        "text": "Nigam and Ghani (2000) perform extensive empirical experiments to comp are co-training with generative mixture models and EM."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The discipline of machine learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Blockwise supervised inference"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Semi-supervised support vector machin"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Humans perform semisupervised classification too"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 211,
                                "start": 19
                            }
                        ],
                        "text": "The heuristics in (Delalleau et al., 2005) similarly create a small graph with a sub set of the unlabeled data. They enables fast approximate computation by re ducing the problem size. Garcke and Griebel (2005) propose the use of sparse grids for semisupervised learning."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Clsutering unlabeled data with S"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Co-training for predicting emotio"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Efficient coregularized least squares regression"
            },
            "venue": {
                "fragments": [],
                "text": "ICML06, 23rd International Conference on Machine Learning"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 44
                            }
                        ],
                        "text": "The maximum entropy discrimination approach (Jaakkola et al., 1999) also"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 1
                            }
                        ],
                        "text": "(Joachims, 1999) (Bennett & Demiriz, 1999) (Demirez & Bennett, 2000) (F ung & Mangasarian, 1999) (Chapelle & Zien, 2005). Xu and Schuurmans (2005) present a training method based on semi-defin ite programming (SDP, which applies to the completely unsupervised SVMs as well) ."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Maximum entropy discriminatio"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Information Processing Systems"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 0
                            }
                        ],
                        "text": "Burges and Platt (2005) propose a directedgraphical model, called Conditional Harmonic Mixing, that is somewhat between graph-based semi-supervised learning and standard Bayes nets."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Semi-supervised learning with conditio"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Limits of spectral clu"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 11
                            }
                        ],
                        "text": "However as (Rosset et al., 2005) points out that is not always the case."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A method for inferring la"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Classification learning from both classified and unclassified examples"
            },
            "venue": {
                "fragments": [],
                "text": "Classification learning from both classified and unclassified examples"
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Semi-supervised learning with conditional harmonic mixing Semi-supervised learning"
            },
            "venue": {
                "fragments": [],
                "text": "Semi-supervised learning with conditional harmonic mixing Semi-supervised learning"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 71
                            }
                        ],
                        "text": "Such kernel machines include the kernelized conditional random fi elds (Lafferty et al., 2004) and maximum margin Markov networks (Taskar et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Kernel conditional random fie"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 145
                            }
                        ],
                        "text": "The name TSVM originates from the intention to work only on the observed data (though people use them for induction anyway) , which according to (Vapnik, 1998) is solving a simpler problem."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 85
                            }
                        ],
                        "text": "The dec ision boundary has the smallest generalization error bound on unlabeled data ( Vapnik, 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 145
                            }
                        ],
                        "text": "The name TSVM originates from the intention to work only on the observed data (though people use them for induction anyway), which according to (Vapnik, 1998) is solving a simpler problem."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "1998).Statistical learning theory"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 9
                            }
                        ],
                        "text": "Recently Grady and Funka-Lea (2004) applied the harmonic function metho d to medical image segmentation tasks, where a user labels classes (e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 204,
                                "start": 9
                            }
                        ],
                        "text": "Recently Grady and Funka-Lea (2004) applied the harmonic function metho d to medical image segmentation tasks, where a user labels classes (e.g. differ ent organs) with a few strokes. Levin et al. (2004) use the equivalent of h armonic functions for colorization of gray-scale images."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Enhancing supervised learning with unla"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "Cozman et al. (2003) give a formal derivation on how this might happen."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 5
                            }
                        ],
                        "text": "See (Cozman et al., 2003) for a more recent argument."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Semi - supervised learning ofmixture models"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Unsupervised and semi-supervised"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "Pang and Lee (2004) use mincut to improve the classification of a sentence in to either \u2018objective\u2019 or \u2018subjective\u2019, with the assumption that sentences clos e t each other tend to have the same class."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A sentimental education: Sentiment analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Large-scale sparsified manifold regula"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 0
                            }
                        ],
                        "text": "Riloff et al. (2003) uses it to identify subjective nouns."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A sentimental education: Sentiment analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Trading convexity for sca"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 149
                            }
                        ],
                        "text": "Another set of methods heuristically identify some \u2018reliable\u2019 negative example s in the unlabeled set, and use EM on generative (Naive Bayes) models (Liu et al., 2002) or logistic regression (Lee & Liu, 2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Partially supervised classifi"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 869,
                                "start": 0
                            }
                        ],
                        "text": "Chawla and Karakoulas (2005 ) perform empirical studies on this version of co-training and compared it ag a nst several other methods, in particular for the case where labeled and unlab eled data do not follow the same distribution. Later Zhou and Goldman (2004) propose a single-view multiple-learner Democratic Co-learning algorithm. An ensemble of learners with different inductive bias are trained separately on the comple te feature of the labeled data. They then make predictions on the unlabeled data. If majority of learners confidently agree on the class of an unlabeled point xu, that classification is used as the label of xu. xu and its label is added to the training data. All learners are retrained on the updated training set. The final pre diction is made with a variant of a weighted majority vote among all the learners. Similarly Zhou and Li (2005b) propose \u2018tri-training\u2019 which uses three learners ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 263,
                                "start": 0
                            }
                        ],
                        "text": "Chawla and Karakoulas (2005 ) perform empirical studies on this version of co-training and compared it ag a nst several other methods, in particular for the case where labeled and unlab eled data do not follow the same distribution. Later Zhou and Goldman (2004) propose a single-view multiple-learner Democratic Co-learning algorithm."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 39
                            }
                        ],
                        "text": "A book on semi-supervised learning is (Chapelle et al., 2006)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Branch and boun"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Co-training for predicting emotio"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Semi-supervised nonline"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 0
                            }
                        ],
                        "text": "Sajama and Orlitsky (2005) analyze th e lower and upper bounds on estimating data-density-based distance."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 211,
                                "start": 19
                            }
                        ],
                        "text": "The heuristics in (Delalleau et al., 2005) similarly create a small graph with a sub set of the unlabeled data. They enables fast approximate computation by re ducing the problem size. Garcke and Griebel (2005) propose the use of sparse grids for semisupervised learning."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Semi-supervised learning using semidefinite programming"
            },
            "venue": {
                "fragments": [],
                "text": "In O"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Transductive classification via local learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Manifold denoising. Advances in Neural Information Processing Systems (NIPS) 19"
            },
            "venue": {
                "fragments": [],
                "text": "Manifold denoising. Advances in Neural Information Processing Systems (NIPS) 19"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Hyperparameter learning for graph b"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Distributed information regularization on graphs Advances in neural information processing systems 17"
            },
            "venue": {
                "fragments": [],
                "text": "Distributed information regularization on graphs Advances in neural information processing systems 17"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Laplacian eigenmaps for dimensionality redu"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Distributed information regu"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Semi - supervised support vector machines for unlabeled data classification ( Technical Report 9905 )"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 0
                            }
                        ],
                        "text": "Yu et al. (2004) report an early attempt on semi-supervised induction usin g RBF basis functions in a regularization framework."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Unsupervised and semi-supervised"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Unsupervised and semisupervised clustering: a brief survey. in 'A Review of Machine Learning Techniques for Processing Multimedia Content"
            },
            "venue": {
                "fragments": [],
                "text": "Unsupervised and semisupervised clustering: a brief survey. in 'A Review of Machine Learning Techniques for Processing Multimedia Content"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 265,
                                "start": 240
                            }
                        ],
                        "text": "However instead of directly using the generative model for classification, each labeled example is converted into a fixed-length Fisher score vector, i.e. the derivatives of log likelihood w.r.t. model parameters, for all component models (Jaakkola & Haussler, 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Exploiting generative models in disc"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 0
                            }
                        ],
                        "text": "Yu et al. (2004) report an early attempt on semi-supervised induction usin g RBF basis functions in a regularization framework."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Unsupervised learning of imag"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On semi-supervised classification Advances in neural information processing systems 17"
            },
            "venue": {
                "fragments": [],
                "text": "On semi-supervised classification Advances in neural information processing systems 17"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 0
                            }
                        ],
                        "text": "(Dasgupta et al., 2001) provide a PACstyle theoretical analysis."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "PAC generalization bo"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 25
                            }
                        ],
                        "text": "Some other examples are (Shahshahani & Landgrebe, 1994) (Miller & Uyar, 1997)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 0
                            }
                        ],
                        "text": "Sindhwani et al. (2005a) give a semi-supervised kernel that is not limited to the unlabeled points, but defined over all input space."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The effect of unlabeled"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 0
                            }
                        ],
                        "text": "Narayanan et al. (2006) prove that the \u2018weighted boundary vo lume\u2019, i."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "Nigam et al. (2000) apply the EM algorithm on mixture of multinomial for the task of text classification."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On the relation between low"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Cluster kernels for semi"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Semi-supervised learning using semidefinite programming Semisupervised learning"
            },
            "venue": {
                "fragments": [],
                "text": "Semi-supervised learning using semidefinite programming Semisupervised learning"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 13
                            }
                        ],
                        "text": "Similarly in (Delalleau et al., 2005) the authors proposes an indu ction scheme to classify a new point x by"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 18
                            }
                        ],
                        "text": "The heuristics in (Delalleau et al., 2005) similarly create a small graph with a sub set of the unlabeled data."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Efficient non-parametr"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 0
                            }
                        ],
                        "text": "Oliveira et al. (2005) propose a simple procedure for semi-supervised learning: First one runs PCA onL \u222a U (ignoring the labels)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Analyzing the effectiveness and applicab"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Cluster kernels for semi"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning with hyperg"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 0
                            }
                        ],
                        "text": "Fujino et al. (2005) extend generative mixture models by including a \u2018bias correction\u2019 term and discriminative training using th e maximum entropy principle."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "Fujino et al. (2005) extend generative mixture models by including a \u2018bias correction\u2019 term and discriminative training using the maximum entropy principle."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A hybrid generative/discriminativ"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 230,
                                "start": 0
                            }
                        ],
                        "text": "Lu and Getoor (2003) convert the link structure in a directed graph into pe rnode features, and combines them with per-node object features in logistic regression. They also use an EM-like iterative algorithm. Zhou et al. (2006a) propose to formulate relational objects using hyperg raphs, where an edge can connect more than two vertices, and extend spectral clustering, classification and embedding to such hypergraphs."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 0
                            }
                        ],
                        "text": "Lu and Getoor (2003) convert the link structure in a directed graph into pe rnode features, and combines them with per-node object features in logistic regression."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Link-based classification using labeled and"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 0
                            }
                        ],
                        "text": "Holub et al. (2005) used this approach for image categorization."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Exploiting unlabelled data for h"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Harmonic mixtures: combining mixture models"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Think globally, fit locally: unsuperv"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "2004).Semi-supervised induction with basis functions (Technical Report 141)"
            },
            "venue": {
                "fragments": [],
                "text": "Max Planck Institute for Biological Cybernetic"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Analysis of spectral kernel design"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Face recognition by humans: 20 results all computer vision researchers should know about. (under review)"
            },
            "venue": {
                "fragments": [],
                "text": "Face recognition by humans: 20 results all computer vision researchers should know about. (under review)"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Maximum entropy discrimination. Neural Information Processing Systems"
            },
            "venue": {
                "fragments": [],
                "text": "Maximum entropy discrimination. Neural Information Processing Systems"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Cluster kernels for semisupervised learning"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing Systems"
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Optimization approaches to semisupervised learning"
            },
            "venue": {
                "fragments": [],
                "text": "Applications and algorithms of complementarity"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 0
                            }
                        ],
                        "text": "Yarowsky (1995) uses self-training for word sense disambiguation, e."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Unsupervised word sense disambiguation riva"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 278,
                                "start": 0
                            }
                        ],
                        "text": "Riloff et al. (2003) uses it to identify subjective nouns. Maeireizo et al. ( 2004) classify dialogues as \u2018emotional\u2019 or \u2018non-emotional\u2019 with a procedure involv ing two classifiers.Self-training has also been applied to parsing and machine tra nslation. Rosenberg et al. (2005) apply self-training to object detection syste ms from images, and show the semi-supervised technique compares favorably with a stateof-the-art detector."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 0
                            }
                        ],
                        "text": "Riloff et al. (2003) uses it to identify subjective nouns."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning subjective nouns"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 68
                            }
                        ],
                        "text": "This document is a chapter excerpt from the au or\u2019s doctoral thesis (Zhu, 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 148
                            }
                        ],
                        "text": "\u2026obtain thelatest version at http://www.cs.wisc.edu/\u223cjerryzhu/pub/sslsurvey.pdf Please cite the survey using the following bibtex entry:\n@techreport{zhu05survey, author = \"Xiaojin Zhu\", title = \"Semi-Supervised Learning Literature Survey\", institution = \"Computer Sciences, University of\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Semi-supervised learning with graphs. Doctoral dissertation"
            },
            "venue": {
                "fragments": [],
                "text": "Semi-supervised learning with graphs. Doctoral dissertation"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Word sense disambiguation with semisupervised learning"
            },
            "venue": {
                "fragments": [],
                "text": "AAAI-05, The Twentieth National Conference on Artificial Intelligence"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 12
                            }
                        ],
                        "text": "Zhou and Li (2005a) proposed using co-training for semi-supervised regression."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "2005).Learning to extract entities from labeled and unlabeled text (Technical Report CMU-LTI-05-191)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Semi-supervised nonlinear dimensionality reduction. ICML-06, 23nd International Conference on Machine Learning"
            },
            "venue": {
                "fragments": [],
                "text": "Semi-supervised nonlinear dimensionality reduction. ICML-06, 23nd International Conference on Machine Learning"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Proximity graphs for c"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Semi - supervised learning with sparsegrids"
            },
            "venue": {
                "fragments": [],
                "text": "Proc . of the 22 nd ICML Workshop on Learning with Partially Classified Training Data"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Inference with the universum. ICML06, 23rd International Conference on Machine Learning"
            },
            "venue": {
                "fragments": [],
                "text": "Inference with the universum. ICML06, 23rd International Conference on Machine Learning"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 0
                            }
                        ],
                        "text": "Zhu et al. (2003b) show that these are not nec essarily the right things to do, if one is interested in classification error."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Combining active learn"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 0
                            }
                        ],
                        "text": "Mann and McCallum (2007) show that class prop ortion by itself can be a useful regularizer for semi-supervised learning."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Simple, robust, scalable semi-supervis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 33
                            }
                        ],
                        "text": "The distance measure proposed in (Yianilos, 1995) can be viewed as a special case."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 634,
                                "start": 34
                            }
                        ],
                        "text": "The distance measure proposed in (Yianilos, 1995) can be viewed as a special case. Yianilos assume a Gaussian mixture model h as been learned fromU , such that a class correspond to a component, but the correspondence is unknown. In this case CDM d(x1, x2) = p(x1, x2from same component ) and can be computed analytically. Now that a metric has been learned from U , we can find withinL the 1-nearest-neighbor of a new data point x, and classifyx with the nearest neighbor\u2019s label. It will be interesting to compare this scheme with EM based semi-supervised learning, where L is used to label mixture components. Weston et al. (2004) propose the neighborhood mismatch kernel and the b agg d mismatch kernel."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "1995).Metric learning via normal mixtures"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 0
                            }
                        ],
                        "text": "Yu et al. (2005) solve the large scale semi-supervised learning problem b y using a bipartite graph."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Label propagation through linear neigh"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 354,
                                "start": 36
                            }
                        ],
                        "text": "Like in co-training, each regressor makes prediction on unlabe led data, and the most confident predictions are used to train the other regressor. The confidence of a prediction on unlabeled point is measured by the MSE on labeled set before and after adding this prediction as training data to the current r egressor. Similarly Sindhwani et al. (2005b); Brefeld et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Hessian eigenmaps: locally linear e"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 206,
                                "start": 10
                            }
                        ],
                        "text": "Similarly Sindhwani et al. (2005b); Brefeld et al. (2006) perform multi-view regression, where a regularization term depends on the disagreement a o g regressors on different views. Cortes and Mohri (2006) propose a simple yet efficient transductive r egression model."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 10
                            }
                        ],
                        "text": "Similarly Sindhwani et al. (2005b); Brefeld et al. (2006) perform multi-view regression, where a regularization term depends on the disagreement a o g regressors on different views."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Deterministic annealing fo"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Semi-supervised learning \u2013 a s"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Analysis of spectral kernel design based semisupervised learning Advances in neural information processing systems 18"
            },
            "venue": {
                "fragments": [],
                "text": "Analysis of spectral kernel design based semisupervised learning Advances in neural information processing systems 18"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Metric learning via normal mixtures (Technical Report)"
            },
            "venue": {
                "fragments": [],
                "text": "Metric learning via normal mixtures (Technical Report)"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 869,
                                "start": 0
                            }
                        ],
                        "text": "Chawla and Karakoulas (2005 ) perform empirical studies on this version of co-training and compared it ag ainst several other methods, in particular for the case where labeled and unlab eled data do not follow the same distribution. Later Zhou and Goldman (2004) propose a single-view multiple-learner Democratic Co-learning algorithm. An ensemble of learners with different inductive bias are trained separately on the comple te feature of the labeled data. They then make predictions on the unlabeled data. If majority of learners confidently agree on the class of an unlabeled point xu, that classification is used as the label of xu. xu and its label is added to the training data. All learners are retrained on the updated training set. The final pre diction is made with a variant of a weighted majority vote among all the learners. Similarly Zhou and Li (2005b) propose \u2018tri-training\u2019 which uses three learners ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 263,
                                "start": 0
                            }
                        ],
                        "text": "Chawla and Karakoulas (2005 ) perform empirical studies on this version of co-training and compared it ag ainst several other methods, in particular for the case where labeled and unlab eled data do not follow the same distribution. Later Zhou and Goldman (2004) propose a single-view multiple-learner Democratic Co-learning algorithm."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Cluster kernels for semisupervised learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 60
                            }
                        ],
                        "text": ", 2 006), and the more challenging structured output spaces (Brefeld et al., 2005; Brefe ld & Scheffer, 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 9
                            }
                        ],
                        "text": "(2005b); Brefeld et al. (2006) perform multi-view regression, where a regularization term depends on the disagreement a o g regressors on different views. Cortes and Mohri (2006) propose a simple yet efficient transductive r egression model."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 9
                            }
                        ],
                        "text": "(2005b); Brefeld et al. (2006) perform multi-view regression, where a regularization term depends on the disagreement a o g regressors on different views."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Multiview discriminative sequen"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 39
                            }
                        ],
                        "text": "Rep resentative methods include Isomap (Tenenbaum et al., 2000), locally linear embedding ( LLE) (Roweis & Saul, 2000) (Saul & Roweis, 2003), Hessian LLE (Donoho & Grimes, 2003), Laplacian eigenmaps (Belkin & Niyogi, 2003), and semidefinite embed ding (SDE) (Weinberger & Saul, 2004) (Weinberger et al."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A global ge"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Optimization approaches to semisupervise"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Combining labeled and unlabeled data"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 239,
                                "start": 219
                            }
                        ],
                        "text": "We shall also mention that instead of using an probabilistic generative mixture model, some approaches employ various clustering algorithms to cluster the whole dataset, then label each cluster with labeled data, e.g. (Demiriz et al., 1999) (Dara et al., 2000)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Semi-supervised clusterin"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 60
                            }
                        ],
                        "text": ", 2 006), and the more challenging structured output spaces (Brefeld et al., 2005; Brefe ld & Scheffer, 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 9
                            }
                        ],
                        "text": "(2005b); Brefeld et al. (2006) perform multi-view regression, where a regularization term depends on the disagreement a o g regressors on different views. Cortes and Mohri (2006) propose a simple yet efficient transductive r egression model."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 9
                            }
                        ],
                        "text": "(2005b); Brefeld et al. (2006) perform multi-view regression, where a regularization term depends on the disagreement a o g regressors on different views."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Multiview discriminative sequen"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 39
                            }
                        ],
                        "text": "Rep resentative methods include Isomap (Tenenbaum et al., 2000), locally linear embedding ( LLE) (Roweis & Saul, 2000) (Saul & Roweis, 2003), Hessian LLE (Donoho & Grimes, 2003), Laplacian eigenmaps (Belkin & Niyogi, 2003), and semidefinite embed ding (SDE) (Weinberger & Saul, 2004) (Weinberger et al."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A global ge"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Blockwise supervised inference"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Democratic co-learing"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Semi-supervised learning with sparse"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Dissimilarity in graph-based se"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 0
                            }
                        ],
                        "text": "Riloff et al. (2003) uses it to identify subjective nouns."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Splitting the unsuperv"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1049,
                                "start": 91
                            }
                        ],
                        "text": "Fast computation of the harmonic function with conjugate gradient methods is discussed in (Argyriou, 2004). A comparison of three iterative methods : label propagation, conjugate gradient and loopy belief propagation is presen ted in (Zhu, 2005) Appendix F. Recently numerical methods for fast N-body problems have been applied todensegraphs in semi-supervised learning, reducing the computational cost fromO(n(3)) to O(n) (Mahdaviani et al., 2005). This is achieved with Krylov subspace methods and the fast Gauss transform. The harmonic mixture models (Zhu & Lafferty, 2005) convert the original graph into a much smaller backbone graph, by using a mixture model to \u2018carve up\u2019 the originalL \u222a U dataset. Learning on the smaller graph is much faster. Similar ideas have been used for e.g. dimensionality reduction (Teh & Roweis, 2 002). The heuristics in (Delalleau et al., 2005) similarly create a small graph with a sub set of the unlabeled data. They enables fast approximate computation by re ducing the problem size. Garcke and Griebel (2005) propose the use of sparse grids for semisupervised learning."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 90
                            }
                        ],
                        "text": "Fast computation of the harmonic function with conjugate gradient methods is discussed in (Argyriou, 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Efficient approximation methods for harmonic semisupervised learning. Master\u2019s thesis, University College London"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Co-validation: Using model disagreement to validate classification algorithms Advances in neural information processing systems 17"
            },
            "venue": {
                "fragments": [],
                "text": "Co-validation: Using model disagreement to validate classification algorithms Advances in neural information processing systems 17"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Semi-supervised learning via Gaussian processes Advances in neural information processing systems 17"
            },
            "venue": {
                "fragments": [],
                "text": "Semi-supervised learning via Gaussian processes Advances in neural information processing systems 17"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Ranking on data manifolds. Advances in Neural Information Processing System 16"
            },
            "venue": {
                "fragments": [],
                "text": "Ranking on data manifolds. Advances in Neural Information Processing System 16"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Optimization approaches to semisupervised learning Applications and algorithms of complementarity"
            },
            "venue": {
                "fragments": [],
                "text": "Optimization approaches to semisupervised learning Applications and algorithms of complementarity"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Beyond the point cloud"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 111
                            }
                        ],
                        "text": "The data points are mapped into a new space spanned by the first k eigenvectors of the normalized Laplacian in (Ng et al., 2001), with special normalization ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "Nigam et al. (2000) apply the EM algorithm on mixture of multinomial for the task of text classification."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 0
                            }
                        ],
                        "text": "Nigam et al. (2000) apply the EM algorithm on mixture of multinomial for the task of text classification. They showed the resulting classifiers perfo rm better than those trained only fromL. Baluja (1998) uses the same algorithm on a face orientation discrimination task."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On spectral clustering: Analys"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 39
                            }
                        ],
                        "text": "A book on semi-supervised learning is (Chapelle et al., 2006)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A continuation method for semisupervised SVMs"
            },
            "venue": {
                "fragments": [],
                "text": "ICML06, 23rd International Conference on Machine Learning"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Harmonic mixtures: combining mixture models"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "Denis et al. (2002) use this fact for text classification with Naive Bayes models."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Text classification from positi"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Semi-supervised learning for str"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Think globally, fit locally: unsuperv"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 509,
                                "start": 48
                            }
                        ],
                        "text": "The original manifold regularization framework (Belkin et al., 2004b) need s to invert a(l+u)\u00d7 (l+u) matrix, and is not scalable. To speed up things, Sindhwani et al. (2005c) consider linear manifold regularization . Effectively this is a special case when the base kernel is taken to be the linear kernel. The authors sh ow that it is advantageous to work with the primal variables. The resulting optimization problem can be much smaller if the data dimensionality is small, or sparse. Tsang and Kwok (2006) scale manifold regularization up by adding in an \u01ebinsensitive loss into the energy function, i."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 48
                            }
                        ],
                        "text": "The original manifold regularization framework (Belkin et al., 2004b) need s to invert a(l+u)\u00d7 (l+u) matrix, and is not scalable. To speed up things, Sindhwani et al. (2005c) consider linear manifold regularization ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Regularization and semi"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 0
                            }
                        ],
                        "text": "Chu et al. (2006) develop Gaussian process models that incorporate pa irwise label relations (e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Gaussian processes for ordinal regression (Technical Report)"
            },
            "venue": {
                "fragments": [],
                "text": "Relationa"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 39
                            }
                        ],
                        "text": "A: An existing survey can be found in (Seeger, 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning with labeled and unlabeled data (Technical Report)"
            },
            "venue": {
                "fragments": [],
                "text": "Learning with labeled and unlabeled data (Technical Report)"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 0
                            }
                        ],
                        "text": "(Joachims, 1999) (Bennett & Demiriz, 1999) (Demirez & Bennett, 2000) (F ung & Mangasarian, 1999) (Chapelle & Zien, 2005)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 0
                            }
                        ],
                        "text": "Jones (2005) used co-training, co-EM and oth er related methods for information extraction from text."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Transductive inference for text classification"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 509,
                                "start": 48
                            }
                        ],
                        "text": "The original manifold regularization framework (Belkin et al., 2004b) need s to invert a(l+u)\u00d7 (l+u) matrix, and is not scalable. To speed up things, Sindhwani et al. (2005c) consider linear manifold regularization . Effectively this is a special case when the base kernel is taken to be the linear kernel. The authors sh ow that it is advantageous to work with the primal variables. The resulting optimization problem can be much smaller if the data dimensionality is small, or sparse. Tsang and Kwok (2006) scale manifold regularization up by adding in an \u01ebinsensitive loss into the energy function, i."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 48
                            }
                        ],
                        "text": "The original manifold regularization framework (Belkin et al., 2004b) need s to invert a(l+u)\u00d7 (l+u) matrix, and is not scalable. To speed up things, Sindhwani et al. (2005c) consider linear manifold regularization ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Regularization and semi"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 0
                            }
                        ],
                        "text": "Yu et al. (2004) report an early attempt on semi-supervised induction usin g RBF basis functions in a regularization framework."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 253,
                                "start": 228
                            }
                        ],
                        "text": ", 2000), locally linear embedding ( LLE) (Roweis & Saul, 2000) (Saul & Roweis, 2003), Hessian LLE (Donoho & Grimes, 2003), Laplacian eigenmaps (Belkin & Niyogi, 2003), and semidefinite embed ding (SDE) (Weinberger & Saul, 2004) (Weinberger et al., 2004) (Weinber ger et al."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 76
                            }
                        ],
                        "text": "Consider the Teapot dataset used in (Zhu & Lafferty, 2005) (originally from (Weinberger et al., 2004)), with images of a teapot viewed from diff erent angles."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning a kernel ma"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 0
                            }
                        ],
                        "text": "Collobert et al. (2006) optimize the hard TSVM directly, using an approximate optimization procedure known a s concave-convex procedure (CCCP)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 535,
                                "start": 0
                            }
                        ],
                        "text": "Collobert et al. (2006) optimize the hard TSVM directly, using an approximate optimization procedure known a s concave-convex procedure (CCCP). The key is to notice that the hat lo ss is a sum of a convex function and a concave function. By replacing the concave fu nction with a linear upper bound, one can perform convex minimization to produce an u pper bound of the loss function. This is repeated until a local minimum is reached. T he authors report significant speed up of TSVM training with CCCP. Sindhwa ni and Keerthi (2006) proposed a fast algorithm for linear S3VMs, suitable for large scale text applications."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 263,
                                "start": 0
                            }
                        ],
                        "text": "Chawla and Karakoulas (2005 ) perform empirical studies on this version of co-training and compared it ag a nst several other methods, in particular for the case where labeled and unlab eled data do not follow the same distribution. Later Zhou and Goldman (2004) propose a single-view multiple-learner Democratic Co-learning algorithm."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning from labeled and un"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 111
                            }
                        ],
                        "text": "It is possible that understanding of the human cognitive model will lead to novel machin e learning approaches (Langley, 2006; Mitchell, 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "Nigam et al. (2000) apply the EM algorithm on mixture of multinomial for the task of text classification."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The discipline of machine learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 111
                            }
                        ],
                        "text": "It is possible that understanding of the human cognitive model will lead to novel machin e learning approaches (Langley, 2006; Mitchell, 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "2006).Intelligent behavior in humans and machines"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 306,
                                "start": 86
                            }
                        ],
                        "text": "It ha been suggested that temporal correlation serves as the glue, as summarized by ( Sinha et al., 2006) (Result 14). It seems when we observe an object with chan ging angles, we link the images as \u2018containing the same object\u2019 by the virtue that the images are close in time. Wallis and B\u0308 ulthoff (2001) created artificial image sequences where a frontal face is morphed into the profile face of a different person."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Metric-based methods for adap"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 0
                            }
                        ],
                        "text": "Holub et al. (2005) used this approach for image categorization."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Exploiting unlabelled data"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning with hypergr"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 39
                            }
                        ],
                        "text": "A book on semi-supervised learning is (Chapelle et al., 2006)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Branch and bound for semisupervised support vector machines"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing Systems (NIPS)"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 0
                            }
                        ],
                        "text": "Holub et al. (2005) used this approach for image categorization."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Exploiting unlabelled data"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Semi-supervised learning by entropy minimization Advances in neural information processing systems 17"
            },
            "venue": {
                "fragments": [],
                "text": "Semi-supervised learning by entropy minimization Advances in neural information processing systems 17"
            },
            "year": 2005
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 97,
            "methodology": 67,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 324,
        "totalPages": 33
    },
    "page_url": "https://www.semanticscholar.org/paper/Semi-Supervised-Learning-Literature-Survey-Zhu/a007f46b3303bdb50e705b441c367e595666538c?sort=total-citations"
}