{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685978"
                        ],
                        "name": "B. Taskar",
                        "slug": "B.-Taskar",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Taskar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Taskar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689992"
                        ],
                        "name": "P. Abbeel",
                        "slug": "P.-Abbeel",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Abbeel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Abbeel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736370"
                        ],
                        "name": "D. Koller",
                        "slug": "D.-Koller",
                        "structuredName": {
                            "firstName": "Daphne",
                            "lastName": "Koller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Koller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 61
                            }
                        ],
                        "text": "We also trained the same AMN model using the RMN approach of Taskar et al. (2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Hypertext. We tested AMNs on collective hypertext classification, using the variant of the WebKB dataset (Craven et al., 1998) used by  Taskar et al. (2002) ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "For example, Markov networks can be trained to maximize the conditional likelihood of the labels given the features of the objects (Lafferty et al., 2001;  Taskar et al., 2002 )."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 155
                            }
                        ],
                        "text": "For example, Markov networks can be trained to maximize the conditional likelihood of the labels given the features of the objects (Lafferty et al., 2001; Taskar et al., 2002)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 131
                            }
                        ],
                        "text": "In hypertext or bibliographic classification, labels of linked and cocited documents tend to be similar (Chakrabarti et al., 1998; Taskar et al., 2002)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 124
                            }
                        ],
                        "text": "We tested AMNs on collective hypertext classification, using the variant of the WebKB dataset (Craven et al., 1998) used by Taskar et al. (2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In hypertext or bibliographic classification, labels of linked and cocited documents tend to be similar (Chakrabarti et al., 1998;  Taskar et al., 2002 )."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We also trained the same AMN model using the RMN approach of  Taskar et al. (2002) ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2282762,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9cc36397e1fef5c922d64e88211a7e08ecc64759",
            "isKey": false,
            "numCitedBy": 798,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "In many supervised learning tasks, the entities to be labeled are related to each other in complex ways and their labels are not independent. For example, in hypertext classification, the labels of linked pages are highly correlated. A standard approach is to classify each entity independently, ignoring the correlations between them. Recently, Probabilistic Relational Models, a relational version of Bayesian networks, were used to define a joint probabilistic model for a collection of related entities. In this paper, we present an alternative framework that builds on (conditional) Markov networks and addresses two limitations of the previous approach. First, undirected models do not impose the acyclicity constraint that hinders representation of many important relational dependencies in directed models. Second, undirected models are well suited for discriminative training, where we optimize the conditional likelihood of the labels given the features, which generally improves classification accuracy. We show how to train these models effectively, and how to use approximate probabilistic inference over the learned model for collective classification of multiple related entities. We provide experimental results on a webpage classification task, showing that accuracy can be significantly improved by modeling relational dependencies."
            },
            "slug": "Discriminative-Probabilistic-Models-for-Relational-Taskar-Abbeel",
            "title": {
                "fragments": [],
                "text": "Discriminative Probabilistic Models for Relational Data"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "An alternative framework that builds on (conditional) Markov networks and addresses two limitations of the previous approach is presented, showing how to train these models effectively, and how to use approximate probabilistic inference over the learned model for collective classification of multiple related entities."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685978"
                        ],
                        "name": "B. Taskar",
                        "slug": "B.-Taskar",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Taskar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Taskar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730156"
                        ],
                        "name": "Carlos Guestrin",
                        "slug": "Carlos-Guestrin",
                        "structuredName": {
                            "firstName": "Carlos",
                            "lastName": "Guestrin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carlos Guestrin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736370"
                        ],
                        "name": "D. Koller",
                        "slug": "D.-Koller",
                        "structuredName": {
                            "firstName": "Daphne",
                            "lastName": "Koller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Koller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 44
                            }
                        ],
                        "text": "An alternative method, recently proposed by Taskar et al. (2003a), is to maximize the margin of confidence in the true label assignment y\u0302 over any other assignment y = y\u0302."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Recently, maximum margin-based training has been shown to additionally boost accuracy over conditional likelihood methods and allow a seamless integration of kernel methods with Markov networks ( Taskar et al., 2003a )."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 229,
                                "start": 209
                            }
                        ],
                        "text": "For example, there may be words which, in the test set, are highly indicative of a certain label, but are not present in the training set at all since they were very specific to a particular time period (see (Taskar et al., 2003b))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 121
                            }
                        ],
                        "text": "For example, the\nbest classification accuracy on the WebKB hypertext data set is obtained in a maximum margin framework (Taskar et al., 2003a), when we allow repulsive potentials on linked webpages (representing, for example, that students tend not to link to pages of students)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Our analysis is based on the maximum margin approach to training Markov networks, presented by  Taskar et al. (2003a) ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 95
                            }
                        ],
                        "text": "Our analysis is based on the maximum margin approach to training Markov networks, presented by Taskar et al. (2003a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 215,
                                "start": 195
                            }
                        ],
                        "text": "Recently, maximum margin-based training has been shown to additionally boost accuracy over conditional likelihood methods and allow a seamless integration of kernel methods with Markov networks (Taskar et al., 2003a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "An alternative method, recently proposed by  Taskar et al. (2003a) , is to maximize the margin of confidence in the true label assignment \u02c6 y over any other assignment y 6= \u02c6 They show that the margin-maximization criterion provides significant improvements in accuracy over a range of problems."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "For example, the best classification accuracy on the WebKB hypertext data set is obtained in a maximum margin framework ( Taskar et al., 2003a ), when we allow repulsive potentials on linked webpages (representing, for example, that students tend not to link to pages of students)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 201720,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0c450531e1121cfb657be5195e310217a4675397",
            "isKey": false,
            "numCitedBy": 1477,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "In typical classification tasks, we seek a function which assigns a label to a single object. Kernel-based approaches, such as support vector machines (SVMs), which maximize the margin of confidence of the classifier, are the method of choice for many such tasks. Their popularity stems both from the ability to use high-dimensional feature spaces, and from their strong theoretical guarantees. However, many real-world tasks involve sequential, spatial, or structured data, where multiple labels must be assigned. Existing kernel-based methods ignore structure in the problem, assigning labels independently to each object, losing much useful information. Conversely, probabilistic graphical models, such as Markov networks, can represent correlations between labels, by exploiting problem structure, but cannot handle high-dimensional feature spaces, and lack strong theoretical generalization guarantees. In this paper, we present a new framework that combines the advantages of both approaches: Maximum margin Markov (M3) networks incorporate both kernels, which efficiently deal with high-dimensional features, and the ability to capture correlations in structured data. We present an efficient algorithm for learning M3 networks based on a compact quadratic program formulation. We provide a new theoretical bound for generalization in structured domains. Experiments on the task of handwritten character recognition and collective hypertext classification demonstrate very significant gains over previous approaches."
            },
            "slug": "Max-Margin-Markov-Networks-Taskar-Guestrin",
            "title": {
                "fragments": [],
                "text": "Max-Margin Markov Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "Maximum margin Markov (M3) networks incorporate both kernels, which efficiently deal with high-dimensional features, and the ability to capture correlations in structured data, and a new theoretical bound for generalization in structured domains is provided."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721860"
                        ],
                        "name": "M. Wainwright",
                        "slug": "M.-Wainwright",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Wainwright",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Wainwright"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132120"
                        ],
                        "name": "T. Jaakkola",
                        "slug": "T.-Jaakkola",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Jaakkola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jaakkola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701607"
                        ],
                        "name": "A. Willsky",
                        "slug": "A.-Willsky",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Willsky",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Willsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10007532,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b28024225b22741035cf87203a3639c917959404",
            "isKey": false,
            "numCitedBy": 709,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop and analyze methods for computing provably optimal maximum a posteriori probability (MAP) configurations for a subclass of Markov random fields defined on graphs with cycles. By decomposing the original distribution into a convex combination of tree-structured distributions, we obtain an upper bound on the optimal value of the original problem (i.e., the log probability of the MAP assignment) in terms of the combined optimal values of the tree problems. We prove that this upper bound is tight if and only if all the tree distributions share an optimal configuration in common. An important implication is that any such shared configuration must also be a MAP configuration for the original distribution. Next we develop two approaches to attempting to obtain tight upper bounds: a) a tree-relaxed linear program (LP), which is derived from the Lagrangian dual of the upper bounds; and b) a tree-reweighted max-product message-passing algorithm that is related to but distinct from the max-product algorithm. In this way, we establish a connection between a certain LP relaxation of the mode-finding problem and a reweighted form of the max-product (min-sum) message-passing algorithm."
            },
            "slug": "MAP-estimation-via-agreement-on-trees:-and-linear-Wainwright-Jaakkola",
            "title": {
                "fragments": [],
                "text": "MAP estimation via agreement on trees: message-passing and linear programming"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "This work develops and analyze methods for computing provably optimal maximum a posteriori probability (MAP) configurations for a subclass of Markov random fields defined on graphs with cycles and establishes a connection between a certain LP relaxation of the mode-finding problem and a reweighted form of the max-product (min-sum) message-passing algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Information Theory"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3198578"
                        ],
                        "name": "J. Yedidia",
                        "slug": "J.-Yedidia",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Yedidia",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Yedidia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30400079"
                        ],
                        "name": "Yair Weiss",
                        "slug": "Yair-Weiss",
                        "structuredName": {
                            "firstName": "Yair",
                            "lastName": "Weiss",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yair Weiss"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 124
                            }
                        ],
                        "text": "In this approach, the Markov network is trained to maximize the conditional log-likelihood, using loopy belief propagation (Yedidia et al., 2000) for computing the posterior probabilities needed for optimization."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15300022,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b2799fd1254689eec52f86daf3668a5aac3ea943",
            "isKey": false,
            "numCitedBy": 1127,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Belief propagation (BP) was only supposed to work for treelike networks but works surprisingly well in many applications involving networks with loops, including turbo codes. However, there has been little understanding of the algorithm or the nature of the solutions it finds for general graphs. \n \nWe show that BP can only converge to a stationary point of an approximate free energy, known as the Bethe free energy in statistical physics. This result characterizes BP fixed-points and makes connections with variational approaches to approximate inference. \n \nMore importantly, our analysis lets us build on the progress made in statistical physics since Bethe's approximation was introduced in 1935. Kikuchi and others have shown how to construct more accurate free energy approximations, of which Bethe's approximation is the simplest. Exploiting the insights from our analysis, we derive generalized belief propagation (GBP) versions of these Kikuchi approximations. These new message passing algorithms can be significantly more accurate than ordinary BP, at an adjustable increase in complexity. We illustrate such a new GBP algorithm on a grid Markov network and show that it gives much more accurate marginal probabilities than those found using ordinary BP."
            },
            "slug": "Generalized-Belief-Propagation-Yedidia-Freeman",
            "title": {
                "fragments": [],
                "text": "Generalized Belief Propagation"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "It is shown that BP can only converge to a stationary point of an approximate free energy, known as the Bethe free energy in statistical physics, and generalized belief propagation (GBP) versions of these Kikuchi approximations are derived."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3371403"
                        ],
                        "name": "J. Kleinberg",
                        "slug": "J.-Kleinberg",
                        "structuredName": {
                            "firstName": "Jon",
                            "lastName": "Kleinberg",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kleinberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746222"
                        ],
                        "name": "\u00c9. Tardos",
                        "slug": "\u00c9.-Tardos",
                        "structuredName": {
                            "firstName": "\u00c9va",
                            "lastName": "Tardos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u00c9. Tardos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 196,
                                "start": 172
                            }
                        ],
                        "text": "For K > 2, the MAP problem is NP-hard, but a procedure based on a relaxed linear program guarantees a factor 2 approximation of the optimal solution (Boykov et al., 1999b; Kleinberg & Tardos, 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 0
                            }
                        ],
                        "text": "Kleinberg and Tardos (1999) extend the multi-class Potts model to have more general edge potentials, under the constraints that negative log potentials \u2212 log \u03c6ij(k, l) form a metric on the set of labels."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16241328,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "a3b3aad58ecc6aed599c7567d4fe07ad3480a866",
            "isKey": false,
            "numCitedBy": 378,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "In a traditional classification problem, we wish to assign one of k labels (or classes) to each of n objects, in a way that is consistent with some observed data that we have about the problem. An active line of research in this area is concerned with classification when one has information about pairwise relationships among the objects to be classified; this issue is one of the principal motivations for the framework of Markov random fields, and it arises in areas such as image processing, biometry: and document analysis. In its most basic form, this style of analysis seeks a classification that optimizes a combinatorial function consisting of assignment costs-based on the individual choice of label we make for each object-and separation costs-based on the pair of choices we make for two \"related\" objects. We formulate a general classification problem of this type, the metric labeling problem; we show that it contains as special cases a number of standard classification frameworks, including several arising from the theory of Markov random fields. From the perspective of combinatorial optimization, our problem can be viewed as a substantial generalization of the multiway cut problem, and equivalent to a type of uncapacitated quadratic assignment problem. We provide the first non-trivial polynomial-time approximation algorithms for a general family of classification problems of this type. Our main result is an O(log k log log k)-approximation algorithm for the metric labeling problem, with respect to an arbitrary metric on a set of k labels, and an arbitrary weighted graph of relationships on a set of objects. For the special case in which the labels are endowed with the uniform metric-all distances are the same-our methods provide a 2-approximation."
            },
            "slug": "Approximation-algorithms-for-classification-with-Kleinberg-Tardos",
            "title": {
                "fragments": [],
                "text": "Approximation algorithms for classification problems with pairwise relationships: metric labeling and Markov random fields"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work provides the first non-trivial polynomial-time approximation algorithms for a general family of classification problems of this type, the metric labeling problem, and shows that it contains as special cases a number of standard classification frameworks, including several arising from the theory of Markov random fields."
            },
            "venue": {
                "fragments": [],
                "text": "JACM"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1692688"
                        ],
                        "name": "Yuri Boykov",
                        "slug": "Yuri-Boykov",
                        "structuredName": {
                            "firstName": "Yuri",
                            "lastName": "Boykov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuri Boykov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1922280"
                        ],
                        "name": "O. Veksler",
                        "slug": "O.-Veksler",
                        "structuredName": {
                            "firstName": "Olga",
                            "lastName": "Veksler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Veksler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2984143"
                        ],
                        "name": "R. Zabih",
                        "slug": "R.-Zabih",
                        "structuredName": {
                            "firstName": "Ramin",
                            "lastName": "Zabih",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Zabih"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 150
                            }
                        ],
                        "text": "For K > 2, the MAP problem is NP-hard, but a procedure based on a relaxed linear program guarantees a factor 2 approximation of the optimal solution (Boykov et al., 1999b; Kleinberg & Tardos, 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 152
                            }
                        ],
                        "text": "Note however, that we can also use min-cut to perform exact inference on the learned models for K = 2 and also in approximate inference for K > 2 as in Boykov et al. (1999a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 145
                            }
                        ],
                        "text": "Associative interactions arise naturally in the context of image processing, where nearby pixels are likely to have the same label (Besag, 1986; Boykov et al., 1999b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 137
                            }
                        ],
                        "text": "In image processing, neighboring pixels exhibit local label coherence in denoising, segmentation and stereo correspondence (Besag, 1986; Boykov et al., 1999a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6436838,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "b543d70d3e4fe6673a2e39832f005fa7ebc79cec",
            "isKey": true,
            "numCitedBy": 528,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Markov Random Fields (MRFs) can be used for a wide variety of vision problems. In this paper we focus on MRFs with two-valued clique potentials, which form a generalized Potts model. We show that the maximum a posteriori estimate of such an MRF can be obtained by solving a multiway minimum cut problem on a graph. We develop efficient algorithms for computing good approximations to the minimum multiway, cut. The visual correspondence problem can be formulated as an MRF in our framework; this yields quite promising results on real data with ground truth. We also apply our techniques to MRFs with linear clique potentials."
            },
            "slug": "Markov-random-fields-with-efficient-approximations-Boykov-Veksler",
            "title": {
                "fragments": [],
                "text": "Markov random fields with efficient approximations"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This paper shows that the maximum a posteriori estimate of such an MRF can be obtained by solving a multiway minimum cut problem on a graph, and develops efficient algorithms for computing good approximations to the minimum multiway, cut."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 1998 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.98CB36231)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685978"
                        ],
                        "name": "B. Taskar",
                        "slug": "B.-Taskar",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Taskar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Taskar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2397156"
                        ],
                        "name": "M. Wong",
                        "slug": "M.-Wong",
                        "structuredName": {
                            "firstName": "Ming",
                            "lastName": "Wong",
                            "middleNames": [
                                "Fai"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Wong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736370"
                        ],
                        "name": "D. Koller",
                        "slug": "D.-Koller",
                        "structuredName": {
                            "firstName": "Daphne",
                            "lastName": "Koller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Koller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 44
                            }
                        ],
                        "text": "An alternative method, recently proposed by Taskar et al. (2003a), is to maximize the margin of confidence in the true label assignment y\u0302 over any other assignment y = y\u0302."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 229,
                                "start": 209
                            }
                        ],
                        "text": "For example, there may be words which, in the test set, are highly indicative of a certain label, but are not present in the training set at all since they were very specific to a particular time period (see (Taskar et al., 2003b))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 121
                            }
                        ],
                        "text": "For example, the\nbest classification accuracy on the WebKB hypertext data set is obtained in a maximum margin framework (Taskar et al., 2003a), when we allow repulsive potentials on linked webpages (representing, for example, that students tend not to link to pages of students)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 95
                            }
                        ],
                        "text": "Our analysis is based on the maximum margin approach to training Markov networks, presented by Taskar et al. (2003a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 215,
                                "start": 195
                            }
                        ],
                        "text": "Recently, maximum margin-based training has been shown to additionally boost accuracy over conditional likelihood methods and allow a seamless integration of kernel methods with Markov networks (Taskar et al., 2003a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "For example, there may be words which, in the test set, are highly indicative of a certain label, but are not present in the training set at all since they were very specific to a particular time period (see ( Taskar et al., 2003b ))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9482640,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "16f9c9791eb29025c3832d49a4a0d34eae5c6304",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper addresses the problem of classification in situations where the data distribution is not homogeneous: Data instances might come from different locations or times, and therefore are sampled from related but different distributions. In particular, features may appear in some parts of the data that are rarely or never seen in others. In most situations with nonhomogeneous data, the training data is not representative of the distribution under which the classifier must operate. We propose a method, based on probabilistic graphical models, for utilizing unseen features during classification. Our method introduces, for each such unseen feature, a continuous hidden variable describing its influence on the class -- whether it tends to be associated with some label. We then use probabilistic inference over the test data to infer a distribution over the value of this hidden variable. Intuitively, we \"learn\" the role of this unseen feature from the test set, generalizing from those instances whose label we are fairly sure about. Our overall probabilistic model is learned from the training data. In particular, we also learn models for characterizing the role of unseen features; these models use \"meta-features\" of those features, such as words in the neighborhood of an unseen feature, to infer its role. We present results for this framework on the task of classifying news articles and web pages, showing significant improvements over models that do not use unseen features."
            },
            "slug": "Learning-on-the-Test-Data:-Leveraging-Unseen-Taskar-Wong",
            "title": {
                "fragments": [],
                "text": "Learning on the Test Data: Leveraging Unseen Features"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper proposes a method, based on probabilistic graphical models, for utilizing unseen features during classification, and presents results for this framework on the task of classifying news articles and web pages, showing significant improvements over models that do not use unseen features."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "113414328"
                        ],
                        "name": "Fernando Pereira",
                        "slug": "Fernando-Pereira",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pereira",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fernando Pereira"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 132
                            }
                        ],
                        "text": "For example, Markov networks can be trained to maximize the conditional likelihood of the labels given the features of the objects (Lafferty et al., 2001; Taskar et al., 2002)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 220,
                                "start": 199
                            }
                        ],
                        "text": "The standard approach of learning the weights w given (x, y\u0302) is to maximize the log Pw(y\u0302 | x), with an additional regularization term, which is usually taken to be the squared-norm of the weights w (Lafferty et al., 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 221,
                                "start": 198
                            }
                        ],
                        "text": "The standard approach of learning the weights w given (x, \u0177) is to maximize the log Pw(\u0177 | x), with an additional regularization term, which is usually taken to be the squared-norm of the weights w (Lafferty et al., 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 219683473,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f4ba954b0412773d047dc41231c733de0c1f4926",
            "isKey": false,
            "numCitedBy": 13413,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "We present conditional random fields , a framework for building probabilistic models to segment and label sequence data. Conditional random fields offer several advantages over hidden Markov models and stochastic grammars for such tasks, including the ability to relax strong independence assumptions made in those models. Conditional random fields also avoid a fundamental limitation of maximum entropy Markov models (MEMMs) and other discriminative Markov models based on directed graphical models, which can be biased towards states with few successor states. We present iterative parameter estimation algorithms for conditional random fields and compare the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data."
            },
            "slug": "Conditional-Random-Fields:-Probabilistic-Models-for-Lafferty-McCallum",
            "title": {
                "fragments": [],
                "text": "Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work presents iterative parameter estimation algorithms for conditional random fields and compares the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745655"
                        ],
                        "name": "C. Bouman",
                        "slug": "C.-Bouman",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Bouman",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Bouman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153530088"
                        ],
                        "name": "M. Shapiro",
                        "slug": "M.-Shapiro",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Shapiro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Shapiro"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 132
                            }
                        ],
                        "text": "Associative interactions arise naturally in the context of image processing, where nearby pixels are likely to have the same label (Besag, 1986; Boykov et al., 1999b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 124
                            }
                        ],
                        "text": "In image processing, neighboring pixels exhibit local label coherence in denoising, segmentation and stereo correspondence (Besag, 1986; Boykov et al., 1999a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 167
                            }
                        ],
                        "text": "Probabilistic inference is NP-hard in general, and requires exponential time in a broad range of practical Markov network structures, including gridtopology networks (Besag, 1986)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5571990,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "1dc2443a955a6d3ef06c10d1071998c267e3a229",
            "isKey": false,
            "numCitedBy": 724,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "Many approaches to Bayesian image segmentation have used maximum a posteriori (MAP) estimation in conjunction with Markov random fields (MRF). Although this approach performs well, it has a number of disadvantages. In particular, exact MAP estimates cannot be computed, approximate MAP estimates are computationally expensive to compute, and unsupervised parameter estimation of the MRF is difficult. The authors propose a new approach to Bayesian image segmentation that directly addresses these problems. The new method replaces the MRF model with a novel multiscale random field (MSRF) and replaces the MAP estimator with a sequential MAP (SMAP) estimator derived from a novel estimation criteria. Together, the proposed estimator and model result in a segmentation algorithm that is not iterative and can be computed in time proportional to MN where M is the number of classes and N is the number of pixels. The also develop a computationally efficient method for unsupervised estimation of model parameters. Simulations on synthetic images indicate that the new algorithm performs better and requires much less computation than MAP estimation using simulated annealing. The algorithm is also found to improve classification accuracy when applied to the segmentation of multispectral remotely sensed images with ground truth data."
            },
            "slug": "A-multiscale-random-field-model-for-Bayesian-image-Bouman-Shapiro",
            "title": {
                "fragments": [],
                "text": "A multiscale random field model for Bayesian image segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Simulations on synthetic images indicate that the new algorithm performs better and requires much less computation than MAP estimation using simulated annealing, and is found to improve classification accuracy when applied to the segmentation of multispectral remotely sensed images with ground truth data."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Image Process."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1692688"
                        ],
                        "name": "Yuri Boykov",
                        "slug": "Yuri-Boykov",
                        "structuredName": {
                            "firstName": "Yuri",
                            "lastName": "Boykov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuri Boykov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1922280"
                        ],
                        "name": "O. Veksler",
                        "slug": "O.-Veksler",
                        "structuredName": {
                            "firstName": "Olga",
                            "lastName": "Veksler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Veksler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2984143"
                        ],
                        "name": "R. Zabih",
                        "slug": "R.-Zabih",
                        "structuredName": {
                            "firstName": "Ramin",
                            "lastName": "Zabih",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Zabih"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 150
                            }
                        ],
                        "text": "For K > 2, the MAP problem is NP-hard, but a procedure based on a relaxed linear program guarantees a factor 2 approximation of the optimal solution (Boykov et al., 1999b; Kleinberg & Tardos, 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 152
                            }
                        ],
                        "text": "Note however, that we can also use min-cut to perform exact inference on the learned models for K = 2 and also in approximate inference for K > 2 as in Boykov et al. (1999a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 145
                            }
                        ],
                        "text": "Associative interactions arise naturally in the context of image processing, where nearby pixels are likely to have the same label (Besag, 1986; Boykov et al., 1999b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 137
                            }
                        ],
                        "text": "In image processing, neighboring pixels exhibit local label coherence in denoising, segmentation and stereo correspondence (Besag, 1986; Boykov et al., 1999a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2430892,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3120324069ec20eed853d3f9bbbceb32e4173b93",
            "isKey": true,
            "numCitedBy": 3914,
            "numCiting": 116,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we address the problem of minimizing a large class of energy functions that occur in early vision. The major restriction is that the energy function's smoothness term must only involve pairs of pixels. We propose two algorithms that use graph cuts to compute a local minimum even when very large moves are allowed. The first move we consider is an /spl alpha/-/spl beta/-swap: for a pair of labels /spl alpha/,/spl beta/, this move exchanges the labels between an arbitrary set of pixels labeled a and another arbitrary set labeled /spl beta/. Our first algorithm generates a labeling such that there is no swap move that decreases the energy. The second move we consider is an /spl alpha/-expansion: for a label a, this move assigns an arbitrary set of pixels the label /spl alpha/. Our second algorithm, which requires the smoothness term to be a metric, generates a labeling such that there is no expansion move that decreases the energy. Moreover, this solution is within a known factor of the global minimum. We experimentally demonstrate the effectiveness of our approach on image restoration, stereo and motion."
            },
            "slug": "Fast-approximate-energy-minimization-via-graph-cuts-Boykov-Veksler",
            "title": {
                "fragments": [],
                "text": "Fast approximate energy minimization via graph cuts"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper proposes two algorithms that use graph cuts to compute a local minimum even when very large moves are allowed, and generates a labeling such that there is no expansion move that decreases the energy."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Seventh IEEE International Conference on Computer Vision"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144653004"
                        ],
                        "name": "V. Kolmogorov",
                        "slug": "V.-Kolmogorov",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Kolmogorov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Kolmogorov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2984143"
                        ],
                        "name": "R. Zabih",
                        "slug": "R.-Zabih",
                        "structuredName": {
                            "firstName": "Ramin",
                            "lastName": "Zabih",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Zabih"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 15
                            }
                        ],
                        "text": "More recently, Kolmogorov and Zabih (2002) showed how to optimize energy functions containing binary and ternary interactions using graph cuts, as long as the parameters satisfy a certain regularity condition."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 786967,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "30c15f9be29524e72b9744f8dc14faf2a122d65f",
            "isKey": false,
            "numCitedBy": 1924,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "In the last few years, several new algorithms based on graph cuts have been developed to solve energy minimization problems in computer vision. Each of these techniques constructs a graph such that the minimum cut on the graph also minimizes the energy. Yet, because these graph constructions are complex and highly specific to a particular energy function, graph cuts have seen limited application to date. In this paper, we give a characterization of the energy functions that can be minimized by graph cuts. Our results are restricted to functions of binary variables. However, our work generalizes many previous constructions and is easily applicable to vision problems that involve large numbers of labels, such as stereo, motion, image restoration, and scene reconstruction. We give a precise characterization of what energy functions can be minimized using graph cuts, among the energy functions that can be written as a sum of terms containing three or fewer binary variables. We also provide a general-purpose construction to minimize such an energy function. Finally, we give a necessary condition for any energy function of binary variables to be minimized by graph cuts. Researchers who are considering the use of graph cuts to optimize a particular energy function can use our results to determine if this is possible and then follow our construction to create the appropriate graph. A software implementation is freely available."
            },
            "slug": "What-energy-functions-can-be-minimized-via-graph-Kolmogorov-Zabih",
            "title": {
                "fragments": [],
                "text": "What energy functions can be minimized via graph cuts?"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work gives a precise characterization of what energy functions can be minimized using graph cuts, among the energy functions that can be written as a sum of terms containing three or fewer binary variables."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3072213"
                        ],
                        "name": "J. Besag",
                        "slug": "J.-Besag",
                        "structuredName": {
                            "firstName": "Julian",
                            "lastName": "Besag",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Besag"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 132
                            }
                        ],
                        "text": "Associative interactions arise naturally in the context of image processing, where nearby pixels are likely to have the same label (Besag, 1986; Boykov et al., 1999b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 124
                            }
                        ],
                        "text": "In image processing, neighboring pixels exhibit local label coherence in denoising, segmentation and stereo correspondence (Besag, 1986; Boykov et al., 1999a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 167
                            }
                        ],
                        "text": "Probabilistic inference is NP-hard in general, and requires exponential time in a broad range of practical Markov network structures, including gridtopology networks (Besag, 1986)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15128952,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "47865b56fee61d9c9ff477f7c79f090cc6663d3a",
            "isKey": false,
            "numCitedBy": 4635,
            "numCiting": 79,
            "paperAbstract": {
                "fragments": [],
                "text": "may 7th, 1986, Professor A. F. M. Smith in the Chair] SUMMARY A continuous two-dimensional region is partitioned into a fine rectangular array of sites or \"pixels\", each pixel having a particular \"colour\" belonging to a prescribed finite set. The true colouring of the region is unknown but, associated with each pixel, there is a possibly multivariate record which conveys imperfect information about its colour according to a known statistical model. The aim is to reconstruct the true scene, with the additional knowledge that pixels close together tend to have the same or similar colours. In this paper, it is assumed that the local characteristics of the true scene can be represented by a nondegenerate Markov random field. Such information can be combined with the records by Bayes' theorem and the true scene can be estimated according to standard criteria. However, the computational burden is enormous and the reconstruction may reflect undesirable largescale properties of the random field. Thus, a simple, iterative method of reconstruction is proposed, which does not depend on these large-scale characteristics. The method is illustrated by computer simulations in which the original scene is not directly related to the assumed random field. Some complications, including parameter estimation, are discussed. Potential applications are mentioned briefly."
            },
            "slug": "On-the-Statistical-Analysis-of-Dirty-Pictures-Besag",
            "title": {
                "fragments": [],
                "text": "On the Statistical Analysis of Dirty Pictures"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144570279"
                        ],
                        "name": "F. Bach",
                        "slug": "F.-Bach",
                        "structuredName": {
                            "firstName": "Francis",
                            "lastName": "Bach",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Bach"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 23
                            }
                        ],
                        "text": "In other cases (e.g., (Bach & Jordan, 2001)),\nthe network structure is learned, subject to the constraint that inference on these networks is tractable."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In other cases (e.g., ( Bach & Jordan, 2001 )), the network structure is learned, subject to the constraint that inference on these networks is tractable."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3168839,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ce2bcdfcb2118f5d42360265f27fe7000e416f20",
            "isKey": false,
            "numCitedBy": 160,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an algorithm that induces a class of models with thin junction trees\u2014models that are characterized by an upper bound on the size of the maximal cliques of their triangulated graph. By ensuring that the junction tree is thin, inference in our models remains tractable throughout the learning process. This allows both an efficient implementation of an iterative scaling parameter estimation algorithm and also ensures that inference can be performed efficiently with the final model. We illustrate the approach with applications in handwritten digit recognition and DNA splice site detection."
            },
            "slug": "Thin-Junction-Trees-Bach-Jordan",
            "title": {
                "fragments": [],
                "text": "Thin Junction Trees"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "An algorithm is presented that induces a class of models with thin junction trees\u2014models that are characterized by an upper bound on the size of the maximal cliques of their triangulated graph that allows both an efficient implementation of an iterative scaling parameter estimation algorithm and also ensures that inference can be performed efficiently with the final model."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144557047"
                        ],
                        "name": "M. Craven",
                        "slug": "M.-Craven",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Craven",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Craven"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2922396"
                        ],
                        "name": "Dan DiPasquo",
                        "slug": "Dan-DiPasquo",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "DiPasquo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dan DiPasquo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758106"
                        ],
                        "name": "Dayne Freitag",
                        "slug": "Dayne-Freitag",
                        "structuredName": {
                            "firstName": "Dayne",
                            "lastName": "Freitag",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dayne Freitag"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40975594"
                        ],
                        "name": "Tom Michael Mitchell",
                        "slug": "Tom-Michael-Mitchell",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Mitchell",
                            "middleNames": [
                                "Michael"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tom Michael Mitchell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145172877"
                        ],
                        "name": "K. Nigam",
                        "slug": "K.-Nigam",
                        "structuredName": {
                            "firstName": "Kamal",
                            "lastName": "Nigam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Nigam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682522"
                        ],
                        "name": "Se\u00e1n Slattery",
                        "slug": "Se\u00e1n-Slattery",
                        "structuredName": {
                            "firstName": "Se\u00e1n",
                            "lastName": "Slattery",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Se\u00e1n Slattery"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 95
                            }
                        ],
                        "text": "We tested AMNs on collective hypertext classification, using the variant of the WebKB dataset (Craven et al., 1998) used by Taskar et al. (2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 94
                            }
                        ],
                        "text": "We tested AMNs on collective hypertext classification, using the variant of the WebKB dataset (Craven et al., 1998) used by Taskar et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2312137,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8446830f3c05b97c4d12a0751c022d1ae6a5115b",
            "isKey": false,
            "numCitedBy": 800,
            "numCiting": 89,
            "paperAbstract": {
                "fragments": [],
                "text": "The World Wide Web is a vast source of information accessible to computers, but understandable only to humans. The goal of the research described here is to automatically create a computer understandable world wide knowledge base whose content mirrors that of the World Wide Web. Such a knowledge base would enable much more effective retrieval of Web information, and promote new uses of the Web to support knowledge-based inference and problem solving. Our approach is to develop a trainable information extraction system that takes two inputs: an ontology defining the classes and relations of interest, and a set of training data consisting of labeled regions of hypertext representing instances of these classes and relations. Given these inputs, the system learns to extract information from other pages and hyperlinks on the Web. This paper describes our general approach, several machine learning algorithms for this task, and promising initial results with a prototype system."
            },
            "slug": "Learning-to-Extract-Symbolic-Knowledge-from-the-Web-Craven-DiPasquo",
            "title": {
                "fragments": [],
                "text": "Learning to Extract Symbolic Knowledge from the World Wide Web"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "The goal of the research described here is to automatically create a computer understandable world wide knowledge base whose content mirrors that of the World Wide Web, and several machine learning algorithms for this task are described."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI/IAAI"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40941894"
                        ],
                        "name": "Soumen Chakrabarti",
                        "slug": "Soumen-Chakrabarti",
                        "structuredName": {
                            "firstName": "Soumen",
                            "lastName": "Chakrabarti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Soumen Chakrabarti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786444"
                        ],
                        "name": "B. Dom",
                        "slug": "B.-Dom",
                        "structuredName": {
                            "firstName": "Byron",
                            "lastName": "Dom",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Dom"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688317"
                        ],
                        "name": "P. Indyk",
                        "slug": "P.-Indyk",
                        "structuredName": {
                            "firstName": "Piotr",
                            "lastName": "Indyk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Indyk"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 105
                            }
                        ],
                        "text": "In hypertext or bibliographic classification, labels of linked and cocited documents tend to be similar (Chakrabarti et al., 1998; Taskar et al., 2002)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 207226010,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3261bb81085f59efae1e1c72453c47daaee777ac",
            "isKey": false,
            "numCitedBy": 926,
            "numCiting": 64,
            "paperAbstract": {
                "fragments": [],
                "text": "A major challenge in indexing unstructured hypertext databases is to automatically extract meta-data that enables structured search using topic taxonomies, circumvents keyword ambiguity, and improves the quality of search and profile-based routing and filtering. Therefore, an accurate classifier is an essential component of a hypertext database. Hyperlinks pose new problems not addressed in the extensive text classification literature. Links clearly contain high-quality semantic clues that are lost upon a purely term-based classifier, but exploiting link information is non-trivial because it is noisy. Naive use of terms in the link neighborhood of a document can even degrade accuracy. Our contribution is to propose robust statistical models and a relaxation labeling technique for better classification by exploiting link information in a small neighborhood around documents. Our technique also adapts gracefully to the fraction of neighboring documents having known topics. We experimented with pre-classified samples from Yahoo!1 and the US Patent Database2. In previous work, we developed a text classifier that misclassified only 13% of the documents in the well-known Reuters benchmark; this was comparable to the best results ever obtained. This classifier misclassified 36% of the patents, indicating that classifying hypertext can be more difficult than classifying text. Naively using terms in neighboring documents increased error to 38%; our hypertext classifier reduced it to 21%. Results with the Yahoo! sample were more dramatic: the text classifier showed 68% error, whereas our hypertext classifier reduced this to only 21%."
            },
            "slug": "Enhanced-hypertext-categorization-using-hyperlinks-Chakrabarti-Dom",
            "title": {
                "fragments": [],
                "text": "Enhanced hypertext categorization using hyperlinks"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work has developed a text classifier that misclassified only 13% of the documents in the well-known Reuters benchmark; this was comparable to the best results ever obtained and its technique also adapts gracefully to the fraction of neighboring documents having known topics."
            },
            "venue": {
                "fragments": [],
                "text": "SIGMOD '98"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50631189"
                        ],
                        "name": "A. Vazquez",
                        "slug": "A.-Vazquez",
                        "structuredName": {
                            "firstName": "Alexei",
                            "lastName": "Vazquez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Vazquez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1769960"
                        ],
                        "name": "A. Flammini",
                        "slug": "A.-Flammini",
                        "structuredName": {
                            "firstName": "Alessandro",
                            "lastName": "Flammini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Flammini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3123952"
                        ],
                        "name": "A. Maritan",
                        "slug": "A.-Maritan",
                        "structuredName": {
                            "firstName": "Amos",
                            "lastName": "Maritan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Maritan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690690"
                        ],
                        "name": "Alessandro Vespignani",
                        "slug": "Alessandro-Vespignani",
                        "structuredName": {
                            "firstName": "Alessandro",
                            "lastName": "Vespignani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alessandro Vespignani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "cation and function of proteins that interact are often highly correlated ( Vazquez et al., 2003 )."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 79
                            }
                        ],
                        "text": "For example, consider the problem of predicting whether two proteins interact (Vazquez et al., 2003); this probability may increase if they both interact with another protein."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "For example, consider the problem of predicting whether two proteins interact ( Vazquez et al., 2003 ); this probability may increase if they both interact with another protein."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 106
                            }
                        ],
                        "text": "Copyright 2004 by the authors.\ncation and function of proteins that interact are often highly correlated (Vazquez et al., 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2577381,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "b47e59a9e6f182a97fc6d93708bbbc624289de93",
            "isKey": true,
            "numCitedBy": 659,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Determining protein function is one of the most challenging problems of the post-genomic era. The availability of entire genome sequences and of high-throughput capabilities to determine gene coexpression patterns has shifted the research focus from the study of single proteins or small complexes to that of the entire proteome. In this context, the search for reliable methods for assigning protein function is of primary importance. There are various approaches available for deducing the function of proteins of unknown function using information derived from sequence similarity or clustering patterns of co-regulated genes, phylogenetic profiles, protein-protein interactions (refs. 5\u20138 and Samanta, M.P. and Liang, S., unpublished data), and protein complexes. Here we propose the assignment of proteins to functional classes on the basis of their network of physical interactions as determined by minimizing the number of protein interactions among different functional categories. Function assignment is proteome-wide and is determined by the global connectivity pattern of the protein network. The approach results in multiple functional assignments, a consequence of the existence of multiple equivalent solutions. We apply the method to analyze the yeast Saccharomyces cerevisiae protein-protein interaction network. The robustness of the approach is tested in a system containing a high percentage of unclassified proteins and also in cases of deletion and insertion of specific protein interactions."
            },
            "slug": "Global-protein-function-prediction-from-interaction-Vazquez-Flammini",
            "title": {
                "fragments": [],
                "text": "Global protein function prediction from protein-protein interaction networks"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The assignment of proteins to functional classes on the basis of their network of physical interactions as determined by minimizing the number of protein interactions among different functional categories is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "Nature Biotechnology"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713217"
                        ],
                        "name": "D. Bertsimas",
                        "slug": "D.-Bertsimas",
                        "structuredName": {
                            "firstName": "Dimitris",
                            "lastName": "Bertsimas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Bertsimas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144224173"
                        ],
                        "name": "J. Tsitsiklis",
                        "slug": "J.-Tsitsiklis",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Tsitsiklis",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tsitsiklis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 83
                            }
                        ],
                        "text": "However, here we can exploit fundamental properties of linear programming duality (Bertsimas & Tsitsiklis, 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60706246,
            "fieldsOfStudy": [
                "Biology",
                "Medicine"
            ],
            "id": "c8557a70ecdeec83f70954c5f169393c7f04fc9e",
            "isKey": false,
            "numCitedBy": 2692,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "p. 27, l. \u221211, replace \u201cSchwartz\u201d by \u201cSchwarz\u201d p. 69, l. \u221213: \u201cai\u2217x = bi\u201d should be \u201cai\u2217x = bi\u2217\u201d p. 126, l. 16, replace \u201cinequality constraints\u201d by \u201clinear inequality constraints\u201d p. 153, l. \u22128, replace aix 6= bi by aix 6= bi p. 163, Example 4.9, first line: replace \u201cfrom\u201d with \u201cform\u201d p. 165, l. 11, replace p\u2032Ax \u2265 0 by p\u2032Ax \u2265 0 p. 175, l. 1, replace \u201cTo this see\u201d by \u201cTo see this\u201d p. 203, l. 12: replace x \u2265 0 by x \u2265 0, xn+1 \u2265 0 p. 216, l. \u22126: replace \u201c\u2264 c}\u201d by \u201c\u2264 c\u2032}\u201d p. 216, l. \u22123: replace c\u2032 by (c1)\u2032 p. 216, l. \u22122: replace c\u2032 by (c2)\u2032 p. 216, l. \u22121: right-hand side should be \u03bb(c1)\u2032 + (1\u2212 \u03bb)(c2)\u2032 p. 220, l. \u221212: replace \u201cadded to the pivot row\u201d by \u201cadded to the zeroth row\u201d"
            },
            "slug": "Introduction-to-linear-optimization-Bertsimas-Tsitsiklis",
            "title": {
                "fragments": [],
                "text": "Introduction to linear optimization"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69038222"
                        ],
                        "name": "W. Garvin",
                        "slug": "W.-Garvin",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Garvin",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Garvin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12329438,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "f538816f51e0ce7ae9b362d17b37aae4cc3493db",
            "isKey": false,
            "numCitedBy": 294,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Suppose you manage a firm to produce m goods. These goods can be produced by n activities at varying levels x1, x2, ..., xn. One unit of activity j generates the amount aij of product i. Activity j has unit cost cj . Then \u2211n j=1 aijxj units of goods i are produced. If you want to produce goods of amounts b1, b2, ..., bm, there are conditions \u2211n j=1 aijxj \u2265 bi, i = 1, 2, ...,m. You want to minimize the cost \u2211n j=1 cjxj to produce the goods, then you have the following problem to solve,"
            },
            "slug": "Introduction-to-Linear-Programming-Garvin",
            "title": {
                "fragments": [],
                "text": "Introduction to Linear Programming"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "If you manage a firm to produce m goods and want to minimize the cost \u2211n j=1 cjxj to produce the goods, then you have the following problem to solve."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1960
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32129876"
                        ],
                        "name": "R. B. Potts",
                        "slug": "R.-B.-Potts",
                        "structuredName": {
                            "firstName": "Renfrey",
                            "lastName": "Potts",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. B. Potts"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 71
                            }
                        ],
                        "text": "In this setting, a common approach is to use a generalized Potts model (Potts, 1952), which penalizes assignments that do not have the same label across the edge: \u03c6ij(k, l) = \u03bbij , \u2200k 6= l and \u03c6ij(k, k) = 1, where \u03bbij \u2264 1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 72
                            }
                        ],
                        "text": "In this setting, a common approach is to use a\ngeneralized Potts model (Potts, 1952), which penalizes assignments that do not have the same label across the edge: \u03c6ij(k, l) = \u03bbij , \u2200k = l and \u03c6ij(k, k) = 1, where \u03bbij \u2264 1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 122689941,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "4aeea86e589383e2ec2d4214e919ebda9277c452",
            "isKey": false,
            "numCitedBy": 1685,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "In considering the statistics of the \u2018no-field\u2019 square Ising lattice in which each unit is capable of two configurations and only nearest neighbours interact, Kramers and Wannier (3) were able to deduce an inversion transformation under which the partition function of the lattice is invariant when the temperature is transformed from a low to a high (\u2018inverted\u2019) value. The important property of this inversion transformation is that its fixed point gives the transition point of the lattice."
            },
            "slug": "Some-generalized-order-disorder-transformations-Potts",
            "title": {
                "fragments": [],
                "text": "Some generalized order - disorder transformations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1952
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 44
                            }
                        ],
                        "text": "An alternative method, recently proposed by Taskar et al. (2003a), is to maximize the margin of confidence in the true label assignment y\u0302 over any other assignment y = y\u0302."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 229,
                                "start": 209
                            }
                        ],
                        "text": "For example, there may be words which, in the test set, are highly indicative of a certain label, but are not present in the training set at all since they were very specific to a particular time period (see (Taskar et al., 2003b))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 121
                            }
                        ],
                        "text": "For example, the\nbest classification accuracy on the WebKB hypertext data set is obtained in a maximum margin framework (Taskar et al., 2003a), when we allow repulsive potentials on linked webpages (representing, for example, that students tend not to link to pages of students)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 95
                            }
                        ],
                        "text": "Our analysis is based on the maximum margin approach to training Markov networks, presented by Taskar et al. (2003a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 215,
                                "start": 195
                            }
                        ],
                        "text": "Recently, maximum margin-based training has been shown to additionally boost accuracy over conditional likelihood methods and allow a seamless integration of kernel methods with Markov networks (Taskar et al., 2003a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Max margin markov networks. Proc. NIPS"
            },
            "venue": {
                "fragments": [],
                "text": "Max margin markov networks. Proc. NIPS"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49026551"
                        ],
                        "name": "D. Greig",
                        "slug": "D.-Greig",
                        "structuredName": {
                            "firstName": "Darryl",
                            "lastName": "Greig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Greig"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "146294528"
                        ],
                        "name": "B. Porteous",
                        "slug": "B.-Porteous",
                        "structuredName": {
                            "firstName": "Baroness",
                            "lastName": "Porteous",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Porteous"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1887111"
                        ],
                        "name": "A. Seheult",
                        "slug": "A.-Seheult",
                        "structuredName": {
                            "firstName": "Allan",
                            "lastName": "Seheult",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Seheult"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 32
                            }
                        ],
                        "text": "For binary-valued Potts models, Greig et al. (1989) show that the MAP problem can be formulated as a min-cut in an appropriately constructed graph."
                    },
                    "intents": []
                }
            ],
            "corpusId": 115691220,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "a717b20e99b76cb228b47694140ed3dce082b530",
            "isKey": false,
            "numCitedBy": 1257,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Exact-Maximum-A-Posteriori-Estimation-for-Binary-Greig-Porteous",
            "title": {
                "fragments": [],
                "text": "Exact Maximum A Posteriori Estimation for Binary Images"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 140
                            }
                        ],
                        "text": "For K > 2, the MAP problem is NP-hard, but a procedure based on a relaxed linear program guarantees a factor 2 approximation of the optimal solution (Boykov et al., 1999b; Kleinberg & Tardos, 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 13
                            }
                        ],
                        "text": "For example, Wainwright et al. (2002) provides a natural formulation of this form that is guaranteed to produce integral solutions for triangulated graphs."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Map estimation via agreement on (hyper)trees: Messagepassing and linear programming approaches"
            },
            "venue": {
                "fragments": [],
                "text": "Allerton Conference on Communication, Control and Computing"
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 15
                            }
                        ],
                        "text": "More recently, Kolmogorov and Zabih (2002) showed how to optimize energy functions containing binary and ternary interactions using graph cuts, as long as the parameters satisfy a certain regularity condition."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "What energy functions can be minimized using graph cuts? PAMI"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Thin junction trees Introduction to linear programming"
            },
            "venue": {
                "fragments": [],
                "text": "Thin junction trees Introduction to linear programming"
            },
            "year": 1997
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 17,
            "methodology": 11
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 24,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Learning-associative-Markov-networks-Taskar-Chatalbashev/702c2fde33ccb4328be06405c11e208a4b3ee347?sort=total-citations"
}