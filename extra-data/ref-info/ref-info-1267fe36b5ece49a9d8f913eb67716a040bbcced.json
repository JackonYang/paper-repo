{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2784955"
                        ],
                        "name": "J. Nocedal",
                        "slug": "J.-Nocedal",
                        "structuredName": {
                            "firstName": "Jorge",
                            "lastName": "Nocedal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Nocedal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9033333,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d2aaf56fb183ad66d099ac6c9110c5c365ab27f3",
            "isKey": false,
            "numCitedBy": 2414,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We study how to use the BFGS quasi-Newton matrices to precondition minimization methods for problems where the storage is critical. We give an update formula which generates matrices using information from the last m iterations, where m is any number supplied by the user. The quasi-Newton matrix is updated at every iteration by dropping the oldest information and replacing it by the newest informa- tion. It is shown that the matrices generated have some desirable properties. The resulting algorithms are tested numerically and compared with several well- known methods. 1. Introduction. For the problem of minimizing an unconstrained function / of n variables, quasi-Newton methods are widely employed (4). They construct a se- quence of matrices which in some way approximate the hessian of /(or its inverse). These matrices are symmetric; therefore, it is necessary to have n(n + l)/2 storage locations for each one. For large dimensional problems it will not be possible to re- tain the matrices in the high speed storage of a computer, and one has to resort to other kinds of algorithms. For example, one could use the methods (Toint (15), Shanno (12)) which preserve the sparsity structure of the hessian, or conjugate gradient methods (CG) which only have to store 3 or 4 vectors. Recently, some CG algorithms have been developed which use a variable amount of storage and which do not require knowledge about the sparsity structure of the problem (2), (7), (8). A disadvantage of these methods is that after a certain number of iterations the quasi-Newton matrix is discarded, and the algorithm is restarted using an initial matrix (usually a diagonal matrix). We describe an algorithm which uses a limited amount of storage and where the quasi-Newton matrix is updated continuously. At every step the oldest information contained in the matrix is discarded and replaced by new one. In this way we hope to have a more up to date model of our function. We will concentrate on the BFGS method since it is considered to be the most efficient. We believe that similar algo- rithms cannot be developed for the other members of the Broyden 0-class (1). Let / be the function to be nnnimized, g its gradient and h its hessian. We define"
            },
            "slug": "Updating-Quasi-Newton-Matrices-With-Limited-Storage-Nocedal",
            "title": {
                "fragments": [],
                "text": "Updating Quasi-Newton Matrices With Limited Storage"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "An update formula which generates matrices using information from the last m iterations, where m is any number supplied by the user, and the BFGS method is considered to be the most efficient."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2877506"
                        ],
                        "name": "S. Nash",
                        "slug": "S.-Nash",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Nash",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Nash"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 64336713,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c3dc812c32efe004b92f4dcc0562a51d7f9f7008",
            "isKey": false,
            "numCitedBy": 166,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we discuss the use of truncated-Newton methods, a flexible class of iterative methods, in the solution of large-scale unconstrained minimization problems. At each major iteration, the Newton equations are approximately solved by an inner iterative algorithm. The performance of the inner algorithm, and in addition the total method, can be greatly improved by the addition of preconditioning and scaling strategies. Preconditionings can be developed using either the outer nonlinear algorithm or using information computed during the inner iteration. Several preconditioning schemes are derived and tested.Numerical tests show that a carefully chosen truncated-Newton method can perform well in comparison with nonlinear conjugate-gradient-type algorithms. This is significant, since the two classes of methods have comparable storage and operation counts, and they are the only practical methods for solving many large-scale problems. In addition, with the Hessian matrix available, the truncated-Newton a..."
            },
            "slug": "Preconditioning-of-Truncated-Newton-Methods-Nash",
            "title": {
                "fragments": [],
                "text": "Preconditioning of Truncated-Newton Methods"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Numerical tests show that a carefully chosen truncated-Newton method can perform well in comparison with nonlinear conjugate-gradient-type algorithms, which is significant, since the two classes of methods have comparable storage and operation counts."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3104805"
                        ],
                        "name": "A. Buckley",
                        "slug": "A.-Buckley",
                        "structuredName": {
                            "firstName": "Albert",
                            "lastName": "Buckley",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Buckley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2762993"
                        ],
                        "name": "A. LeNir",
                        "slug": "A.-LeNir",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "LeNir",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. LeNir"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 143
                            }
                        ],
                        "text": "\u2026is used to accelerate convergence They are suitable for large scale problems because the amount of storage required by the algorithms and thus the cost of the iteration can be controlled by the user Alternatively limited memory methods can be viewed as implementations of quasi Newton\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 206818625,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b626629ccdf676f9e0536d3f891c7aa28d142521",
            "isKey": false,
            "numCitedBy": 130,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Both conjugate gradient and quasi-Newton methods are quite successful at minimizing smooth nonlinear functions of several variables, and each has its advantages. In particular, conjugate gradient methods require much less storage to implement than a quasi-Newton code and therefore find application when storage limitations occur. They are, however, slower, so there have recently been attempts to combine CG and QN algorithms so as to obtain an algorithm with good convergence properties and low storage requirements. One such method is the code CONMIN due to Shanno and Phua; it has proven quite successful but it has one limitation. It has no middle ground, in that it either operates as a quasi-Newton code using O(n2) storage locations, or as a conjugate gradient code using 7n locations, but it cannot take advantage of the not unusual situation where more than 7n locations are available, but a quasi-Newton code requires an excessive amount of storage.In this paper we present a way of looking at conjugate gradient algorithms which was in fact given by Shanno and Phua but which we carry further, emphasize and clarify. This applies in particular to Beale's 3-term recurrence relation. Using this point of view, we develop a new combined CG-QN algorithm which can use whatever storage is available; CONMIN occurs as a special case. We present numerical results to demonstrate that the new algorithm is never worse than CONMIN and that it is almost always better if even a small amount of extra storage is provided."
            },
            "slug": "QN-like-variable-storage-conjugate-gradients-Buckley-LeNir",
            "title": {
                "fragments": [],
                "text": "QN-like variable storage conjugate gradients"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A new combined CG-QN algorithm which can use whatever storage is available and is presented to demonstrate that the new algorithm is never worse than CONMIN and that it is almost always better if even a small amount of extra storage is provided."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Program."
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2213123"
                        ],
                        "name": "D. Shanno",
                        "slug": "D.-Shanno",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Shanno",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Shanno"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143786123"
                        ],
                        "name": "K. Phua",
                        "slug": "K.-Phua",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Phua",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Phua"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6738351,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c147055019fa759a986b619d4ff36d8cdd9d129a",
            "isKey": false,
            "numCitedBy": 219,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "The subroutine incorporates two nonlinear optimization methods, a conjugate gradient algorithm and a variable metric algorithm, with the choice of method left to the user. The conjugate gradient algorithm is the Beale restarted memoryless variable metric algorithm documented in Shanno [7]. This method requires approximately 7n double-precision words of working storage to be provided by the user. The variable metric method is the BFGS algorithm with initial scaling documented in Shanno and Phua [10], and required approximately n2/2 + l l n /2 double-precision words of working storage. Whichever method is chosen, the same linear search technique is used for both methods, with two differences. The basic linear search uses Davidon's cubic interpolation to find a step length a, which satisfies"
            },
            "slug": "Remark-on-\u201cAlgorithm-500:-Minimization-of-Functions-Shanno-Phua",
            "title": {
                "fragments": [],
                "text": "Remark on \u201cAlgorithm 500: Minimization of Unconstrained Multivariate Functions [E4]\u201d"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "The subroutine incorporates two nonlinear optimization methods, a conjugate gradient algorithm and a variable metric algorithm, with the choice of method left to the user, with two differences."
            },
            "venue": {
                "fragments": [],
                "text": "TOMS"
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3104805"
                        ],
                        "name": "A. Buckley",
                        "slug": "A.-Buckley",
                        "structuredName": {
                            "firstName": "Albert",
                            "lastName": "Buckley",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Buckley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2762993"
                        ],
                        "name": "A. LeNir",
                        "slug": "A.-LeNir",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "LeNir",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. LeNir"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 32264529,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fb171d802029f84a9a59c2fc689072ef173dce39",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "The algorithm is based on an earlier algorithm, namely, CONMIN, due to Shanno and Phua [6], but offers a fundamental facility which was not available in CONMIN. In the CONMIN code, one could either use a conjugate gradient code if little storage was available, or a quasi-Newton code if there was sufficient storage. BBVSCG offers the user the opportunity to specify the amount of available storage; the code then chooses an appropriate algorithm. What is significant is that, if there is not enough space to run the quasi-Newton part of the algorithm, then this new algorithm will use all of the space that has been allocated to it. If this is more than what is needed to run the conjugate gradient code of CONMIN, as one might often expect to be the case, then the algorithm BBVSCG is such that one can expect a more efficient and rapid convergence to the minimum. This"
            },
            "slug": "Algorithm-630:-BBVSCG\u2013a-variable-storage-algorithm-Buckley-LeNir",
            "title": {
                "fragments": [],
                "text": "Algorithm 630: BBVSCG\u2013a variable-storage algorithm for function minimization"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "The algorithm BBVSCG is such that one can expect a more efficient and rapid convergence to the minimum and is based on an earlier algorithm, namely, CONMIN, but offers a fundamental facility which was not available in CONMIN."
            },
            "venue": {
                "fragments": [],
                "text": "TOMS"
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35074400"
                        ],
                        "name": "Jean Charles Gilbert",
                        "slug": "Jean-Charles-Gilbert",
                        "structuredName": {
                            "firstName": "Jean",
                            "lastName": "Gilbert",
                            "middleNames": [
                                "Charles"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jean Charles Gilbert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8773884"
                        ],
                        "name": "C. Lemar\u00e9chal",
                        "slug": "C.-Lemar\u00e9chal",
                        "structuredName": {
                            "firstName": "Claude",
                            "lastName": "Lemar\u00e9chal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lemar\u00e9chal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2893408,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "6d78e5111b35dac99bd2cddddf6e8d9253e49d25",
            "isKey": false,
            "numCitedBy": 749,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes some numerical experiments with variable-storage quasi-Newton methods for the optimization of some large-scale models (coming from fluid mechanics and molecular biology). In addition to assessing these kinds of methods in real-life situations, we compare an algorithm of A. Buckley with a proposal by J. Nocedal. The latter seems generally superior, provided that careful attention is given to some nontrivial implementation aspects, which concern the general question of properly initializing a quasi-Newton matrix. In this context, we find it appropriate to use a diagonal matrix, generated by an update of the identity matrix, so as to fit the Rayleigh ellipsoid of the local Hessian in the direction of the change in the gradient.Also, a variational derivation of some rank one and rank two updates in Hilbert spaces is given."
            },
            "slug": "Some-numerical-experiments-with-variable-storage-Gilbert-Lemar\u00e9chal",
            "title": {
                "fragments": [],
                "text": "Some numerical experiments with variable-storage quasi-Newton algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is found appropriate to use a diagonal matrix, generated by an update of the identity matrix, so as to fit the Rayleigh ellipsoid of the local Hessian in the direction of the change in the gradient."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Program."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2213123"
                        ],
                        "name": "D. Shanno",
                        "slug": "D.-Shanno",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Shanno",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Shanno"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143786123"
                        ],
                        "name": "K. Phua",
                        "slug": "K.-Phua",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Phua",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Phua"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 206800401,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "838c0ff1ccb0fabf0c77438c3d4d62790f0ca3ec",
            "isKey": false,
            "numCitedBy": 218,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "In a series of recent papers, Oren, Oren and Luenberger, Oren and Spedicato, and Spedicato have developed the self-scaling variable metric algorithms. These algorithms alter Broyden's single parameter family of approximations to the inverse Hessian to a double parameter family. Conditions are given on the new parameter to minimize a bound on the condition number of the approximated inverse Hessian while insuring improved step-wise convergence.Davidon has devised an update which also minimizes the bound on the condition number while remaining in the Broyden single parameter family.This paper derives initial scalings for the approximate inverse Hessian which makes members of the Broyden class self-scaling. The Davidon, BFGS, and Oren\u2014Spedicato updates are tested for computational efficiency and stability on numerous test functions, with the results indicating strong superiority computationally for the Davidon and BFGS update over the self-scaling update, except on a special class of functions, the homogeneous functions."
            },
            "slug": "Matrix-conditioning-and-nonlinear-optimization-Shanno-Phua",
            "title": {
                "fragments": [],
                "text": "Matrix conditioning and nonlinear optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Results indicate strong superiority computationally for the Davidon and BFGS update over the self-scaling update, except on a special class of functions, the homogeneous functions."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Program."
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744288"
                        ],
                        "name": "P. Gill",
                        "slug": "P.-Gill",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Gill",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Gill"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143873253"
                        ],
                        "name": "W. Murray",
                        "slug": "W.-Murray",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Murray",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Murray"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 117925805,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9127f4edab7d8870f5c1669590186dc76d1f47bc",
            "isKey": false,
            "numCitedBy": 71,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : In this paper we discuss several recent conjugate-gradient type methods for solving large-scale nonlinear optimization problems. We demonstrate how the performance of these methods can be significantly improved by careful implementation. A method based upon iterative preconditioning will be suggested which performs reasonably efficiently on a wide variety of significant test problems. Our results indicate that nonlinear conjugate-gradient methods behave in a similar way to conjugate-gradient methods for the solution of systems of linear equations. These methods work best on problems whose Hessian matrices have sets of clustered eigenvalues. On more general problems, however, even the best method may require a prohibitively large number of iterations. We present numerical evidence that indicates that the use of theoretical analysis to predict the performance of algorithms on general problems is not straightforward. (Author)"
            },
            "slug": "Conjugate-Gradient-Methods-for-Large-Scale-Gill-Murray",
            "title": {
                "fragments": [],
                "text": "Conjugate-Gradient Methods for Large-Scale Nonlinear Optimization."
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "Numerical evidence is presented that indicates that the use of theoretical analysis to predict the performance of algorithms on general problems is not straightforward and a method based upon iterative preconditioning will be suggested which performs reasonably efficiently on a wide variety of significant test problems."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3104805"
                        ],
                        "name": "A. Buckley",
                        "slug": "A.-Buckley",
                        "structuredName": {
                            "firstName": "Albert",
                            "lastName": "Buckley",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Buckley"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 20866865,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b334490c44403724377ff43059818a105bfcd1ea",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Although quasi-Newton algorithms generally converge in fewer iterations than conjugate gradient algorithms, they have the disadvantage of requiring substantially more storage. An algorithm will be described which uses an intermediate (and variable) amount of storage and which demonstrates convergence which is also intermediate, that is, generally better than that observed for conjugate gradient algorithms but not so good as in a quasi-Newton approach. The new algorithm uses a strategy of generating a form of conjugate gradient search direction for most iterations, but it periodically uses a quasi-Newton step to improve the convergence. Some theoretical background for a new algorithm has been presented in an earlier paper; here we examine properties of the new algorithm and its implementation. We also present the results of some computational experience."
            },
            "slug": "A-combined-conjugate-gradient-quasi-Newton-Buckley",
            "title": {
                "fragments": [],
                "text": "A combined conjugate-gradient quasi-Newton minimization algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A new algorithm is described which uses an intermediate (and variable) amount of storage and which demonstrates convergence which is generally better than that observed for conjugate gradient algorithms but not so good as in a quasi-Newton approach."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Program."
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2425503"
                        ],
                        "name": "L. Nazareth",
                        "slug": "L.-Nazareth",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Nazareth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Nazareth"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121826220,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "27436cf278d5b11a0e6124169e0249154153893f",
            "isKey": false,
            "numCitedBy": 116,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Based upon analysis and numerical experience, the BFGS (Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno) algorithm is currently considered to be one of the most effective algorithms for finding a minimum of an unconstrained function, $f(x),x \\in \\mathbb{R}^n $. However, when computer storage is at a premium, the usual alternative is to use a conjugate gradient (CG) method. In this paper we show that the two algorithms are related to one another in a particularly close way. Based upon these observations a new family of algorithms is proposed."
            },
            "slug": "A-Relationship-between-the-BFGS-and-Conjugate-and-Nazareth",
            "title": {
                "fragments": [],
                "text": "A Relationship between the BFGS and Conjugate Gradient Algorithms and Its Implications for New Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that the BFGS and CG algorithms are related to one another in a particularly close way and a new family of algorithms is proposed."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1800149"
                        ],
                        "name": "J. Dennis",
                        "slug": "J.-Dennis",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Dennis",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Dennis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46190125"
                        ],
                        "name": "R. Schnabel",
                        "slug": "R.-Schnabel",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schnabel",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schnabel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 53416228,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "02c87d7506454c947c92cfb593ac3cab1b1b2ab3",
            "isKey": false,
            "numCitedBy": 100,
            "numCiting": 109,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : Finding the unconstrained minimizer of a function of more than one variable is an important problem with many practical applications, including data fitting, engineering design, and process control. In addition, techniques for solving unconstrained optimization problems form the basis for most methods for solving constrained optimization problems. This paper surveys the state of the art for solving unconstrained optimization problems and the closely related problem of solving systems of nonlinear equations. First the authors briefly give some mathematical background. Then they discuss Newton's method, the fundamental method underlying most approaches to these problems, as well as the inexact Newton method. The two main practical deficiencies of Newton's method, the need for analytic derivatives and the possible failure to converge to the solution from poor starting points, are the key issues in unconstrained optimization, and the extension of these techniques to solving large, sparse problems. Then this document discusses the main methods used to ensure convergence from poor starting points, line search methods and trust region methods. Also discusses are two rather different approaches to unconstrained optimization, the Nelder-Meade simplex method and conjugate direction methods."
            },
            "slug": "A-view-of-unconstrained-optimization-Dennis-Schnabel",
            "title": {
                "fragments": [],
                "text": "A view of unconstrained optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The main methods used to ensure convergence from poor starting points, line search methods and trust region methods are discussed, and two rather different approaches to unconstrained optimization, the Nelder-Meade simplex method and conjugate direction methods."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2213123"
                        ],
                        "name": "D. Shanno",
                        "slug": "D.-Shanno",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Shanno",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Shanno"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 9256912,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "27250b833d10ec7174c841171c5fc5e792c10a63",
            "isKey": false,
            "numCitedBy": 433,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Conjugate gradient methods are iterative methods for finding the minimizer of a scalar function fx of a vector variable x which do not update an approximation to the inverse Hessian matrix. This paper examines the effects of inexact linear searches on the methods and shows how the traditional Fletcher-Reeves and Polak-Ribiere algorithm may be modified in a form discovered by Perry to a sequence which can be interpreted as a memorytess BFGS algorithm. This algorithm may then be scaled optimally in the sense of Oren and Spedicalo. This scaling can be combined with Beale restarts and Powell's restart criterion. Computational results will show that this new method substantially outperforms known conjugate gradient methods on a wide class of problems."
            },
            "slug": "Conjugate-Gradient-Methods-with-Inexact-Searches-Shanno",
            "title": {
                "fragments": [],
                "text": "Conjugate Gradient Methods with Inexact Searches"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The traditional Fletcher-Reeves and Polak-Ribiere algorithm may be modified in a form discovered by Perry to a sequence which can be interpreted as a memorytess BFGS algorithm and this algorithm may then be scaled optimally in the sense of Oren and Spedicalo."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Oper. Res."
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144448267"
                        ],
                        "name": "R. Byrd",
                        "slug": "R.-Byrd",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Byrd",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Byrd"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2784955"
                        ],
                        "name": "J. Nocedal",
                        "slug": "J.-Nocedal",
                        "structuredName": {
                            "firstName": "Jorge",
                            "lastName": "Nocedal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Nocedal"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122800019,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9cea26ebaeccc01778ab2585bf97ab50122d0dad",
            "isKey": false,
            "numCitedBy": 313,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "The BFGS update formula is shown to have an important property that is inde- pendent of the algorithmic context of the update, and that is relevant to both constrained and unconstrained optimization. The BFGS method for unconstrained optimization, using a variety of line searches, including backtracking, is shown to be globally and superlinearly convergent on uniformly convex problems. The analysis is particularly simple due to the use of some new tools introduced in this paper."
            },
            "slug": "A-tool-for-the-analysis-of-Quasi-Newton-methods-to-Byrd-Nocedal",
            "title": {
                "fragments": [],
                "text": "A tool for the analysis of Quasi-Newton methods with application to unconstrained minimization"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The BFGS method for unconstrained optimization, using a variety of line searches, including backtracking, is shown to be globally and superlinearly convergent on uniformly convex problems."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145983059"
                        ],
                        "name": "A. Griewank",
                        "slug": "A.-Griewank",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Griewank",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Griewank"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145758858"
                        ],
                        "name": "P. Toint",
                        "slug": "P.-Toint",
                        "structuredName": {
                            "firstName": "Philippe",
                            "lastName": "Toint",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Toint"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121166840,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "7a619a74f575e9dbc43fc5c86c12f39fed465ba3",
            "isKey": false,
            "numCitedBy": 145,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "SummaryThis paper considers local convergence properties of inexact partitioned quasi-Newton algorithms for the solution of certain non-linear equations and, in particular, the optimization of partially separable objective functions. Using the bounded deterioration principle, one obtains local and linear convergence, which impliesQ-superlinear convergence under the usual conditions on the quasi-Newton updates. For the optimization case, these conditions are shown to be satisfied by any sequence of updates within the convex Broyden class, even if some Hessians are singular at the minimizer. Finally, local andQ-superlinear convergence is established for an inexact partitioned variable metric method under mild assumptions on the initial Hessian approximations."
            },
            "slug": "Local-convergence-analysis-for-partitioned-updates-Griewank-Toint",
            "title": {
                "fragments": [],
                "text": "Local convergence analysis for partitioned quasi-Newton updates"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2572541"
                        ],
                        "name": "T. Steihaug",
                        "slug": "T.-Steihaug",
                        "structuredName": {
                            "firstName": "Trond",
                            "lastName": "Steihaug",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Steihaug"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2832391,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0be514b583f7af1448f0873a1728ce658278527f",
            "isKey": false,
            "numCitedBy": 800,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Algorithms based on trust regions have been shown to be robust methods for unconstrained optimization problems. All existing methods, either based on the dogleg strategy or Hebden-More iterations, require solution of system of linear equations. In large scale optimization this may be prohibitively expensive. It is shown in this paper that an approximate solution of the trust region problem may be found by the preconditioned conjugate gradient method. This may be regarded as a generalized dogleg technique where we asymptotically take the inexact quasi-Newton step. We also show that we have the same convergence properties as existing methods based on the dogleg strategy using an approximate Hessian."
            },
            "slug": "The-Conjugate-Gradient-Method-and-Trust-Regions-in-Steihaug",
            "title": {
                "fragments": [],
                "text": "The Conjugate Gradient Method and Trust Regions in Large Scale Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "It is shown in this paper that an approximate solution of the trust region problem may be found by the preconditioned conjugate gradient method, and it is shown that the method has the same convergence properties as existing methods based on the dogleg strategy using an approximate Hessian."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1800149"
                        ],
                        "name": "J. Dennis",
                        "slug": "J.-Dennis",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Dennis",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Dennis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1795397"
                        ],
                        "name": "Bobby Schnabel",
                        "slug": "Bobby-Schnabel",
                        "structuredName": {
                            "firstName": "Bobby",
                            "lastName": "Schnabel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bobby Schnabel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 147
                            }
                        ],
                        "text": "\u2026that the limited memory BFGS method L BFGS is superior to the method of Buckley and LeNir They also show that for many problems the partitioned quasi Newton method is extremely e ective and is superior to the limited memory methods However we nd that for other problems the L BFGS method is very\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 27578127,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "e1053197256c6c3c0631377ec23a3f7dc1cb4781",
            "isKey": false,
            "numCitedBy": 7616,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface 1. Introduction. Problems to be considered Characteristics of 'real-world' problems Finite-precision arithmetic and measurement of error Exercises 2. Nonlinear Problems in One Variable. What is not possible Newton's method for solving one equation in one unknown Convergence of sequences of real numbers Convergence of Newton's method Globally convergent methods for solving one equation in one uknown Methods when derivatives are unavailable Minimization of a function of one variable Exercises 3. Numerical Linear Algebra Background. Vector and matrix norms and orthogonality Solving systems of linear equations-matrix factorizations Errors in solving linear systems Updating matrix factorizations Eigenvalues and positive definiteness Linear least squares Exercises 4. Multivariable Calculus Background Derivatives and multivariable models Multivariable finite-difference derivatives Necessary and sufficient conditions for unconstrained minimization Exercises 5. Newton's Method for Nonlinear Equations and Unconstrained Minimization. Newton's method for systems of nonlinear equations Local convergence of Newton's method The Kantorovich and contractive mapping theorems Finite-difference derivative methods for systems of nonlinear equations Newton's method for unconstrained minimization Finite difference derivative methods for unconstrained minimization Exercises 6. Globally Convergent Modifications of Newton's Method. The quasi-Newton framework Descent directions Line searches The model-trust region approach Global methods for systems of nonlinear equations Exercises 7. Stopping, Scaling, and Testing. Scaling Stopping criteria Testing Exercises 8. Secant Methods for Systems of Nonlinear Equations. Broyden's method Local convergence analysis of Broyden's method Implementation of quasi-Newton algorithms using Broyden's update Other secant updates for nonlinear equations Exercises 9. Secant Methods for Unconstrained Minimization. The symmetric secant update of Powell Symmetric positive definite secant updates Local convergence of positive definite secant methods Implementation of quasi-Newton algorithms using the positive definite secant update Another convergence result for the positive definite secant method Other secant updates for unconstrained minimization Exercises 10. Nonlinear Least Squares. The nonlinear least-squares problem Gauss-Newton-type methods Full Newton-type methods Other considerations in solving nonlinear least-squares problems Exercises 11. Methods for Problems with Special Structure. The sparse finite-difference Newton method Sparse secant methods Deriving least-change secant updates Analyzing least-change secant methods Exercises Appendix A. A Modular System of Algorithms for Unconstrained Minimization and Nonlinear Equations (by Robert Schnabel) Appendix B. Test Problems (by Robert Schnabel) References Author Index Subject Index."
            },
            "slug": "Numerical-methods-for-unconstrained-optimization-Dennis-Schnabel",
            "title": {
                "fragments": [],
                "text": "Numerical methods for unconstrained optimization and nonlinear equations"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Newton's Method for Nonlinear Equations and Unconstrained Minimization and methods for solving nonlinear least-squares problems with Special Structure."
            },
            "venue": {
                "fragments": [],
                "text": "Prentice Hall series in computational mathematics"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2213123"
                        ],
                        "name": "D. Shanno",
                        "slug": "D.-Shanno",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Shanno",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Shanno"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121706584,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "03c31ecd9ed16a3456c945d561642c4a6ab612a9",
            "isKey": false,
            "numCitedBy": 132,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper studies the convergence of a conjugate gradient algorithm proposed in a recent paper by Shanno. It is shown that under loose step length criteria similar to but slightly different from those of Lenard, the method converges to the minimizes of a convex function with a strictly bounded Hessian. Further, it is shown that for general functions that are bounded from below with bounded level sets and bounded second partial derivatives, false convergence in the sense that the sequence of approximations to the minimum converges to a point at which the gradient is bounded away from zero is impossible."
            },
            "slug": "On-the-Convergence-of-a-New-Conjugate-Gradient-Shanno",
            "title": {
                "fragments": [],
                "text": "On the Convergence of a New Conjugate Gradient Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is shown that under loose step length criteria similar to but slightly different from those of Lenard, the method converges to the minimizes of a convex function with a strictly bounded Hessian."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145983059"
                        ],
                        "name": "A. Griewank",
                        "slug": "A.-Griewank",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Griewank",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Griewank"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145758858"
                        ],
                        "name": "P. Toint",
                        "slug": "P.-Toint",
                        "structuredName": {
                            "firstName": "Philippe",
                            "lastName": "Toint",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Toint"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 117634960,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a21daea4d778110e9b9530bfc5ec4a5fd879d47b",
            "isKey": false,
            "numCitedBy": 46,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present some numerical experiments with an algorithm that uses the partial separability of an optimization problem. This research is motivated by the very large number of minimization problems in many variables having that particular property. The results discussed in the paper cover both unconstrained and bound constrained cases, as well as numerical estimation of gradient vectors. It is shown that exploiting the present underlying structure can lead to efficient algorithms, especially when the problem dimension is large."
            },
            "slug": "Numerical-experiments-with-partially-separable-Griewank-Toint",
            "title": {
                "fragments": [],
                "text": "Numerical experiments with partially separable optimization problems"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is shown that exploiting the present underlying structure can lead to efficient algorithms, especially when the problem dimension is large, and both unconstrained and bound constrained cases."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145758858"
                        ],
                        "name": "P. Toint",
                        "slug": "P.-Toint",
                        "structuredName": {
                            "firstName": "Philippe",
                            "lastName": "Toint",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Toint"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 122475294,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1149d6cd0199a8a0670cd7f5d75ab616682acdda",
            "isKey": false,
            "numCitedBy": 65,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a numerical comparison between algorithms for unconstrained optimization that take account of sparsity in the second derivative matrix of the objective function. Some of the methods included in the comparison use difference approximation schemes to evaluate the second derivative matrix and others use an approximation to it which is updated regularly using the changes in the gradient. These results show what method to use in what circumstances and also suggest interesting future developments."
            },
            "slug": "Some-numerical-results-using-a-sparse-matrix-in-Toint",
            "title": {
                "fragments": [],
                "text": "Some numerical results using a sparse matrix updating formula in unconstrained optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "A numerical comparison between algorithms for unconstrained optimization that take account of sparsity in the second derivative matrix of the objective function and what method to use in what circumstances is shown."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "96635129"
                        ],
                        "name": "J. D. Pearson",
                        "slug": "J.-D.-Pearson",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Pearson",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. D. Pearson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 117407502,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "deab7b2aef4b5e129c0edfa5e9675ad8e4707734",
            "isKey": false,
            "numCitedBy": 141,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : Two basic approaches to the generation of conjugate directions are considered for the problem of unconstrained minimization of a quadratic function. Using the principle of choosing a step direction orthogonal to the previous gradient changes, a projected gradient algorithm and a class of variable metric algorithms are derived. Three variants of the class are developed into algorithms, one of which is the Fletcher-Powell-Davidon scheme. Numerical results indicate the merits of the new algorithms compared to several now in use, for a variety of nonquadratic problems."
            },
            "slug": "ON-VARIABLE-METRIC-METHODS-OF-MINIMIZATION-Pearson",
            "title": {
                "fragments": [],
                "text": "ON VARIABLE METRIC METHODS OF MINIMIZATION"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "Two basic approaches to the generation of conjugate directions are considered for the problem of unconstrained minimization of a quadratic function, and a projected gradient algorithm and a class of variable metric algorithms are derived."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1968
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744288"
                        ],
                        "name": "P. Gill",
                        "slug": "P.-Gill",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Gill",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Gill"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143873253"
                        ],
                        "name": "W. Murray",
                        "slug": "W.-Murray",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Murray",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Murray"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685738"
                        ],
                        "name": "M. H. Wright",
                        "slug": "M.-H.-Wright",
                        "structuredName": {
                            "firstName": "Margaret",
                            "lastName": "Wright",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. H. Wright"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 147
                            }
                        ],
                        "text": "\u2026additional storage to accelerate convergence We show that the L BFGS method can be greatly accelerated by means of a simple scaling We then compare the L BFGSmethod with the partitioned quasi Newton method of Griewank and Toint a The results show that for some problems the partitioned quasi\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 20611582,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b8d9abd1c078573188b13d36c1b1efb7cb2fa865",
            "isKey": false,
            "numCitedBy": 7627,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Practical Optimization MethodsFree eBook: Practical Aspects of Structural Optimization [1701.01450] Practical optimization for hybrid quantum Practical Optimization | 4e70c9cf5faf796993a61adc9f46f2c5Acces PDF Practical OptimizationPractical Bayesian Optimization of Machine Learning Particle Swarm Optimization (PSO) An Overview Practical Issues Optimization Algorithms in Physics Practical Mathematical Optimization Universit T BremenA Practical Price Optimization Approach for Omnichannel A Gentle Introduction to Stochastic Optimization AlgorithmsApplied Sciences | Free Full-Text | Evolutionary 0387986316 Practical Optimization Methods: with A Lecture on Model Predictive ControlPractical Optimization : Algorithms and Engineering Wiley Series in Discrete Mathematics and Optimization Ser PRACTICAL OPTIMIZATION uCozEvolutionary practical optimization | DeepDyveA Practical Guide To Hyperparameter Optimization.Blood platelet production: a novel approach for practical [PDF] Practical Bilevel Optimization Download and Read Stability and Sample-based Approximations of Composite Practical portfolio optimization in Python (2/3) machine (PDF) Practical Financial Optimization. Decision making A Multiobjective Optimization Model for Prevention and Particle swarm optimization WikipediaPractical Methods Of Optimization|RPractical Portfolio Optimization London Business SchoolBao: Making Learned Query Optimization PracticalApache Spark Core Practical Optimization DatabricksPractical Methods of Optimization by R. FletcherChapter 11 Nonlinear Optimization Examples4.7 Applied Optimization Problems \u2013 Calculus Volume 1Practical bayesian optimization using Goptuna | by Masashi Practical Optimization Methods For 4th Generation Cellular Facility location problems \u2014 Mathematical Optimization Practical optimization (2004 edition) | Open Library[J726.Ebook] PDF Download Practical Optimization of Multi-objective Exploration for Practical Optimization Practical Optimization: a Gentle Introduction has moved!?Practical Rod Pumping Optimization on Apple Books(PDF) Practical Optimization with MATLAB The Free StudyPractical portfolio optimization in Python (3/3) code (PDF) Practical, Fast and Robust Point Cloud Registration Numerical Optimization Stanford UniversityPractical Optimization Methods with Mathematica ApplicationsPractical Optimization | 4e70c9cf5faf796993a61adc9f46f2c5Search Engine Optimization: Practical Marketing TechniquesLagout.orgMeter Placement in Active Distribution System using Manual: Practical guide to optimization for mobiles Unity"
            },
            "slug": "Practical-optimization-Gill-Murray",
            "title": {
                "fragments": [],
                "text": "Practical optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "This ebook Practical Optimization by Philip E. Gill is presented in pdf format and the full version of this ebook in DjVu, ePub, doc, txt, PDF forms is presented."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145983059"
                        ],
                        "name": "A. Griewank",
                        "slug": "A.-Griewank",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Griewank",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Griewank"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145758858"
                        ],
                        "name": "P. Toint",
                        "slug": "P.-Toint",
                        "structuredName": {
                            "firstName": "Philippe",
                            "lastName": "Toint",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Toint"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121362190,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d10c40235eea4c44929ce30c7414f7416dcbcf94",
            "isKey": false,
            "numCitedBy": 164,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "SummaryThis paper presents a minimization method based on the idea of partitioned updating of the Hessian matrix in the case where the objective function can be decomposed in a sum of convex \u201celement\u201d functions. This situation occurs in a large class of practical problems including nonlinear finite elements calculations. Some theoretical and algorithmic properties of the update are discussed and encouraging numerical results are presented."
            },
            "slug": "Partitioned-variable-metric-updates-for-large-Griewank-Toint",
            "title": {
                "fragments": [],
                "text": "Partitioned variable metric updates for large structured optimization problems"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "A minimization method based on the idea of partitioned updating of the Hessian matrix in the case where the objective function can be decomposed in a sum of convex \u201celement\u201d functions is presented."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2062748962"
                        ],
                        "name": "J. J. Mor\u00e9",
                        "slug": "J.-J.-Mor\u00e9",
                        "structuredName": {
                            "firstName": "Jorge",
                            "lastName": "Mor\u00e9",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. J. Mor\u00e9"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745867"
                        ],
                        "name": "B. S. Garbow",
                        "slug": "B.-S.-Garbow",
                        "structuredName": {
                            "firstName": "Burton",
                            "lastName": "Garbow",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. S. Garbow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3158856"
                        ],
                        "name": "K. E. Hillstrom",
                        "slug": "K.-E.-Hillstrom",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Hillstrom",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. E. Hillstrom"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 23550346,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ef840fa97be8c9425052672240ae0cdea7820c89",
            "isKey": false,
            "numCitedBy": 1387,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "Much of the testing of optimization software is inadequate because the number of test functmns is small or the starting points are close to the solution. In addition, there has been too much emphasm on measurmg the efficmncy of the software and not enough on testing reliability and robustness. To address this need, we have produced a relatwely large but easy-to-use collection of test functions and designed gmdelines for testing the reliability and robustness of unconstrained optimization software."
            },
            "slug": "Testing-Unconstrained-Optimization-Software-Mor\u00e9-Garbow",
            "title": {
                "fragments": [],
                "text": "Testing Unconstrained Optimization Software"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A relatwely large but easy-to-use collection of test functions and designed gmdelines for testing the reliability and robustness of unconstrained optimization software."
            },
            "venue": {
                "fragments": [],
                "text": "TOMS"
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "117489779"
                        ],
                        "name": "D. O\u2019Leary",
                        "slug": "D.-O\u2019Leary",
                        "structuredName": {
                            "firstName": "Dianne",
                            "lastName": "O\u2019Leary",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. O\u2019Leary"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5600980,
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "id": "2292666141b74355eb2c2b4acf4ef7a6a18b7393",
            "isKey": false,
            "numCitedBy": 67,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "A Newton-like method is presented for minimizing a function ofn variables. It uses only function and gradient values and is a variant of the discrete Newton algorithm. This variant requires fewer operations than the standard method whenn > 39, and storage is proportional ton rather thann2."
            },
            "slug": "A-discrete-Newton-algorithm-for-minimizing-a-of-O\u2019Leary",
            "title": {
                "fragments": [],
                "text": "A discrete Newton algorithm for minimizing a function of many variables"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "A Newton-like method is presented for minimizing a function ofn variables and is a variant of the discrete Newton algorithm that uses only function and gradient values and requires fewer operations than the standard method whenn > 39."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Program."
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144733293"
                        ],
                        "name": "R. Fletcher",
                        "slug": "R.-Fletcher",
                        "structuredName": {
                            "firstName": "Roger",
                            "lastName": "Fletcher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fletcher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 123487779,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1b84b383ad59f79e607ad0f08a8a10876631a0cd",
            "isKey": false,
            "numCitedBy": 9912,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface Table of Notation Part 1: Unconstrained Optimization Introduction Structure of Methods Newton-like Methods Conjugate Direction Methods Restricted Step Methods Sums of Squares and Nonlinear Equations Part 2: Constrained Optimization Introduction Linear Programming The Theory of Constrained Optimization Quadratic Programming General Linearly Constrained Optimization Nonlinear Programming Other Optimization Problems Non-Smooth Optimization References Subject Index."
            },
            "slug": "Practical-Methods-of-Optimization-Fletcher",
            "title": {
                "fragments": [],
                "text": "Practical Methods of Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "The aim of this book is to provide a Discussion of Constrained Optimization and its Applications to Linear Programming and Other Optimization Problems."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40769682"
                        ],
                        "name": "A. Perry",
                        "slug": "A.-Perry",
                        "structuredName": {
                            "firstName": "Avinoam",
                            "lastName": "Perry",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Perry"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 117158365,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "b456f3915bce73fadce4f24dbf693487cac337c3",
            "isKey": false,
            "numCitedBy": 84,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Class-of-Conjugate-Gradient-Algorithms-with-a-Perry",
            "title": {
                "fragments": [],
                "text": "A Class of Conjugate Gradient Algorithms with a Two-Step Variable Metric Memory"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145758858"
                        ],
                        "name": "P. Toint",
                        "slug": "P.-Toint",
                        "structuredName": {
                            "firstName": "Philippe",
                            "lastName": "Toint",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Toint"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 115391681,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "38f5e8f9c56bf86e6b2794d28b0b99f33ce29cbe",
            "isKey": false,
            "numCitedBy": 227,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Towards-an-efficient-sparsity-exploiting-newton-for-Toint",
            "title": {
                "fragments": [],
                "text": "Towards an efficient sparsity exploiting newton method for minimization"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143660080"
                        ],
                        "name": "G. Walsh",
                        "slug": "G.-Walsh",
                        "structuredName": {
                            "firstName": "Gordon",
                            "lastName": "Walsh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Walsh"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 118507308,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "db97110cd43f1398bacb7838316d72024f982c66",
            "isKey": false,
            "numCitedBy": 639,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Methods-Of-Optimization-Walsh",
            "title": {
                "fragments": [],
                "text": "Methods Of Optimization"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1970
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "87160862"
                        ],
                        "name": "T. M. Williams",
                        "slug": "T.-M.-Williams",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Williams",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. M. Williams"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62396338,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1e10e980f2c2a14ebeaa113c1c96062d2b2643e9",
            "isKey": false,
            "numCitedBy": 723,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Practical-Methods-of-Optimization.-Vol.-1:-Williams",
            "title": {
                "fragments": [],
                "text": "Practical Methods of Optimization. Vol. 1: Unconstrained Optimization"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "96635129"
                        ],
                        "name": "J. D. Pearson",
                        "slug": "J.-D.-Pearson",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Pearson",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. D. Pearson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 62545094,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "6ec37adaced07da1249f550199384ea681a98a02",
            "isKey": false,
            "numCitedBy": 119,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Variable-metric-methods-of-minimisation-Pearson",
            "title": {
                "fragments": [],
                "text": "Variable metric methods of minimisation"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. J."
            },
            "year": 1969
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\Test problems for partially separable optimization and results for the routine PSPMIN"
            },
            "venue": {
                "fragments": [],
                "text": "\\Test problems for partially separable optimization and results for the routine PSPMIN"
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Toint, \\Numerical experiments with partially separable optimization problems Numerical Analysis: Proceedings Dundee"
            },
            "venue": {
                "fragments": [],
                "text": "Lecture Notes in Mathematics"
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\Some global convergence properties of a variable metric algorithm for minimization without exact line search"
            },
            "venue": {
                "fragments": [],
                "text": "Nonlinear Programing, SIAM-AMS Proceedings IX (SIAM Publications"
            },
            "year": 1976
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Phua, \\Remark on algorithm 500: minimization of unconstrained multivariate functions"
            },
            "venue": {
                "fragments": [],
                "text": "ACM Transactions on Mathematical Software"
            },
            "year": 1980
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 147
                            }
                        ],
                        "text": "\u2026objective function and as we will see in this paper they can be very simple to program\nLimited memory methods originated with the work of Perry and Shanno b and were subsequently developed and analyzed by Buckley Nazareth Nocedal Shanno a Gill and Murray and Buckley and LeNir Numerical tests\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\A view of unconstrained optimization University of Colorado Tech. Rpt. CU-CS-376-87"
            },
            "venue": {
                "fragments": [],
                "text": "Handbooks in Operations Research and Management Science"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Toint, \\Towards an eecient sparsity exploiting Newton method for minimization"
            },
            "venue": {
                "fragments": [],
                "text": "Sparse Matrices and their Uses"
            },
            "year": 1981
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\VE08AD, a routine for partially separable optimization with bounded variables"
            },
            "venue": {
                "fragments": [],
                "text": "Harwell Subroutine Library, A.E.R.E. (UK"
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Phua, \\Matrix conditioning and nonlinear optimization"
            },
            "venue": {
                "fragments": [],
                "text": "Mathematical Programming"
            },
            "year": 1978
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\Update to TOMS Algorithm 630"
            },
            "venue": {
                "fragments": [],
                "text": "Rapports Techniques No. 91, Institut National de Recherche en Informatique et en Automatique"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "ACM Transactions on Mathematical Software"
            },
            "venue": {
                "fragments": [],
                "text": "ACM Transactions on Mathematical Software"
            },
            "year": 1981
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\Test results of two limited memory methods for large scale optimization"
            },
            "venue": {
                "fragments": [],
                "text": "\\Test results of two limited memory methods for large scale optimization"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\Algorithms for very large nonlinear optimization problems"
            },
            "venue": {
                "fragments": [],
                "text": "\\Algorithms for very large nonlinear optimization problems"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\A view of nonlinear optimization in a large number of variables"
            },
            "venue": {
                "fragments": [],
                "text": "\\A view of nonlinear optimization in a large number of variables"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\The global convergence of partitioned BFGS on semi-smooth problems with convex decompositions"
            },
            "venue": {
                "fragments": [],
                "text": "\\The global convergence of partitioned BFGS on semi-smooth problems with convex decompositions"
            },
            "year": 1987
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "methodology": 4
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 44,
        "totalPages": 5
    },
    "page_url": "https://www.semanticscholar.org/paper/On-the-limited-memory-BFGS-method-for-large-scale-Liu-Nocedal/1267fe36b5ece49a9d8f913eb67716a040bbcced?sort=total-citations"
}