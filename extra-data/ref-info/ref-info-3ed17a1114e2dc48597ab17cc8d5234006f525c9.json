{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748501"
                        ],
                        "name": "Claire Cardie",
                        "slug": "Claire-Cardie",
                        "structuredName": {
                            "firstName": "Claire",
                            "lastName": "Cardie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Claire Cardie"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 10020125,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "08e1d63442c8a2b3dc4c086a5d4d01ea654dda3d",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents Kenmore, a general framework for knowledge acquisition for natural language processing (NLP) systems. To ease the acquisition of knowledge in new domains, Kenmore exploits an online corpus using robust sentence analysis and embedded symbolic machine learning techniques while requiring only minimal human intervention. By treating all problems in ambiguity resolution as classification tasks, the framework uniformly addresses a range of subproblems in sentence analysis, each of which traditionally had required a separate computational mechanism. In a series of experiments, we demonstrate the successful use of Kenmore for learning solutions to several problems in lexical and structural ambiguity resolution. We argue that the learning and knowledge acquisition components should be embedded components of the NLP system in that (1) learning should take place within the larger natural language understanding system as it processes text, and (2) the learning components should be evaluated in the context of practical language-processing tasks."
            },
            "slug": "Embedded-machine-learning-systems-for-natural-a-Cardie",
            "title": {
                "fragments": [],
                "text": "Embedded machine learning systems for natural language processing: a general framework"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "Kenmore, a general framework for knowledge acquisition for natural language processing (NLP) systems, argues that the learning and knowledge acquisition components should be embedded components of the NLP system in that learning should take place within the larger natural language understanding system as it processes text."
            },
            "venue": {
                "fragments": [],
                "text": "Learning for Natural Language Processing"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144590225"
                        ],
                        "name": "D. Roth",
                        "slug": "D.-Roth",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Roth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Roth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3190501"
                        ],
                        "name": "D. Zelenko",
                        "slug": "D.-Zelenko",
                        "structuredName": {
                            "firstName": "Dmitry",
                            "lastName": "Zelenko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Zelenko"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 89
                            }
                        ],
                        "text": "F (k) denotes the features of\n1The basic unit studied can be a paragraph or any other unit, but for simplicity we will always call it a sentence."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1177419,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b1d3c5653dd4717df19b99709d81b08be44f268a",
            "isKey": false,
            "numCitedBy": 120,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an architecture and an on-line learning algorithm and apply it to the problem of part-of-speech tagging. The architecture presented, SNOW, is a network of linear separators in the feature space, utilizing the Winnow update algorithm.Multiplicative weight-update algorithms such as Winnow have been shown to have exceptionally good behavior when applied to very high dimensional problems, and especially when the target concepts depend on only a small subset of the features in the feature space. In this paper we describe an architecture that utilizes this mistake-driven algorithm for multi-class prediction-selecting the part of speech of a word. The experimental analysis presented here provides more evidence to that these algorithms are suitable for natural language problems.The algorithm used is an on-line algorithm: every example is used by the algorithm only once, and is then discarded. This has significance in terms of efficiency, as well as quick adaptation to new contexts.We present an extensive experimental study of our algorithm under various conditions; in particular, it is shown that the algorithm performs comparably to the best known algorithms for POS."
            },
            "slug": "Part-of-Speech-Tagging-Using-a-Network-of-Linear-Roth-Zelenko",
            "title": {
                "fragments": [],
                "text": "Part of Speech Tagging Using a Network of Linear Separators"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "An architecture and an on-line learning algorithm are presented that utilizes this mistake-driven algorithm for multi-class prediction-selecting the part of speech of a word and it is shown that the algorithm performs comparably to the best known algorithms for POS."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34938639"
                        ],
                        "name": "W. Gale",
                        "slug": "W.-Gale",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Gale",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Gale"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2244184"
                        ],
                        "name": "Kenneth Ward Church",
                        "slug": "Kenneth-Ward-Church",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Church",
                            "middleNames": [
                                "Ward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenneth Ward Church"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693517"
                        ],
                        "name": "David Yarowsky",
                        "slug": "David-Yarowsky",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Yarowsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Yarowsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "A partial list consists of Bayesian classifiers ( Gale, Church, & Yarowsky 1993 ), decision lists (Yarowsky 1994), Bayesian brids (Golding 1995), HMMs (Charniak 1993), inductive logic methods (Zelle & Mooney 1996), memorybased methods (Zavrel, Daelemans, & Veenstra 1997) and transformation-based learning (Brill 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17567112,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bf35ed0864ff6cf524a24f0a65aa6951f9d6f214",
            "isKey": false,
            "numCitedBy": 657,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Word sense disambiguation has been recognized as a major problem in natural language processing research for over forty years. Both quantitive and qualitative methods have been tried, but much of this work has been stymied by difficulties in acquiring appropriate lexical resources. The availability of this testing and training material has enabled us to develop quantitative disambiguation methods that achieve 92% accuracy in discriminating between two very distinct senses of a noun. In the training phase, we collect a number of instances of each sense of the polysemous noun. Then in the testing phase, we are given a new instance of the noun, and are asked to assign the instance to one of the senses. We attempt to answer this question by comparing the context of the unknown instance with contexts of known instances using a Bayesian argument that has been applied successfully in related tasks such as author identification and information retrieval. The proposed method is probably most appropriate for those aspects of sense disambiguation that are closest to the information retrieval task. In particular, the proposed method was designed to disambiguate senses that are usually associated with different topics."
            },
            "slug": "A-method-for-disambiguating-word-senses-in-a-large-Gale-Church",
            "title": {
                "fragments": [],
                "text": "A method for disambiguating word senses in a large corpus"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The proposed method was designed to disambiguate senses that are usually associated with different topics using a Bayesian argument that has been applied successfully in related tasks such as author identification and information retrieval."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Humanit."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749837"
                        ],
                        "name": "Eugene Charniak",
                        "slug": "Eugene-Charniak",
                        "structuredName": {
                            "firstName": "Eugene",
                            "lastName": "Charniak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eugene Charniak"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5371566,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b84276fe751ca4f1389549281383b151a746107b",
            "isKey": false,
            "numCitedBy": 1140,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nEugene Charniak breaks new ground in artificial intelligence research by presenting statistical language processing from an artificial intelligence point of view in a text for researchers and scientists with a traditional computer science background. \nNew, exacting empirical methods are needed to break the deadlock in such areas of artificial intelligence as robotics, knowledge representation, machine learning, machine translation, and natural language processing (NLP). It is time, Charniak observes, to switch paradigms. This text introduces statistical language processing techniques -- word tagging, parsing with probabilistic context free grammars, grammar induction, syntactic disambiguation, semantic word classes, word-sense disambiguation -- along with the underlying mathematics and chapter exercises. \nCharniak points out that as a method of attacking NLP problems, the statistical approach has several advantages. It is grounded in real text and therefore promises to produce usable results, and it offers an obvious way to approach learning: \"one simply gathers statistics.\" \nLanguage, Speech, and Communication"
            },
            "slug": "Statistical-language-learning-Charniak",
            "title": {
                "fragments": [],
                "text": "Statistical language learning"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Eugene Charniak points out that as a method of attacking NLP problems, the statistical approach has several advantages and is grounded in real text and therefore promises to produce usable results, and it offers an obvious way to approach learning."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2650619"
                        ],
                        "name": "Andrew R. Golding",
                        "slug": "Andrew-R.-Golding",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Golding",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew R. Golding"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 128
                            }
                        ],
                        "text": "A partial list consists of Bayesian classifiers (Gale, Church, & Yarowsky 1993), decision lists (Yarowsky 1994), Bayesian brids (Golding 1995), HMMs (Charniak 1993), inductive logic methods (Zelle & Mooney 1996), memory-"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 185
                            }
                        ],
                        "text": "The latter system presents a few augmentations over the simple naive Bayes (but still shares the same basic assumptions) and is among the most successful methods tried for the problem (Golding 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 131
                            }
                        ],
                        "text": "A partial list consists of Bayesian classifiers (Gale, Church, & Yarowsky 1993), decision lists (Yarowsky 1994), Bayesian hybrids (Golding 1995), HMMs (Charniak 1993), inductive logic methods (Zelle & Mooney 1996), memorybased methods (Zavrel, Daelemans, & Veenstra 1997) and transformation-based\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3204825,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "19f37330057a76d32f32f92b11f42c53fb6c2a87",
            "isKey": false,
            "numCitedBy": 152,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Two classes of methods have been shown to be useful for resolving lexical ambiguity. The first relies on the presence of particular words within some distance of the ambiguous target word; the second uses the pattern of words and part-of-speech tags around the target word. These methods have complementary coverage: the former captures the lexical ``atmosphere'' (discourse topic, tense, etc.), while the latter captures local syntax. Yarowsky has exploited this complementarity by combining the two methods using decision lists. The idea is to pool the evidence provided by the component methods, and to then solve a target problem by applying the single strongest piece of evidence, whatever type it happens to be. This paper takes Yarowsky's work as a starting point, applying decision lists to the problem of context-sensitive spelling correction. Decision lists are found, by and large, to outperform either component method. However, it is found that further improvements can be obtained by taking into account not just the single strongest piece of evidence, but ALL the available evidence. A new hybrid method, based on Bayesian classifiers, is presented for doing this, and its performance improvements are demonstrated."
            },
            "slug": "A-Bayesian-Hybrid-Method-for-Context-sensitive-Golding",
            "title": {
                "fragments": [],
                "text": "A Bayesian Hybrid Method for Context-sensitive Spelling Correction"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This paper takes Yarowsky's work as a starting point, applying decision lists to the problem of context-sensitive spelling correction, and finds that further improvements can be obtained by taking into account not just the single strongest piece of evidence, but ALL the available evidence."
            },
            "venue": {
                "fragments": [],
                "text": "VLC@ACL"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3316623"
                        ],
                        "name": "Jakub Zavrel",
                        "slug": "Jakub-Zavrel",
                        "structuredName": {
                            "firstName": "Jakub",
                            "lastName": "Zavrel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jakub Zavrel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735272"
                        ],
                        "name": "Walter Daelemans",
                        "slug": "Walter-Daelemans",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Daelemans",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Walter Daelemans"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143667674"
                        ],
                        "name": "J. Veenstra",
                        "slug": "J.-Veenstra",
                        "structuredName": {
                            "firstName": "Jorn",
                            "lastName": "Veenstra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Veenstra"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1742928,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "200224e8cd06bf822c05ad023e233bbfc6a96b64",
            "isKey": false,
            "numCitedBy": 81,
            "numCiting": 71,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we describe the application of Memory-Based Learning to the problem of Prepositional Phrase attachment disambiguation. We compare Memory-Based Learning, which stores examples in memory and generalizes by using intelligent similarity metrics, with a number of recently proposed statistical methods that are well suited to large numbers of features. We evaluate our methods on a common benchmark dataset and show that our method compares favorably to previous methods, and is well-suited to incorporating various unconventional representations of word patterns such as value difference metrics and Lexical Space."
            },
            "slug": "Resolving-PP-attachment-Ambiguities-with-Learning-Zavrel-Daelemans",
            "title": {
                "fragments": [],
                "text": "Resolving PP attachment Ambiguities with Memory-Based Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "The application of Memory-Based Learning to the problem of Prepositional Phrase attachment disambiguation is described and the method compares favorably to previous methods, and is well-suited to incorporating various unconventional representations of word patterns such as value difference metrics and Lexical Space."
            },
            "venue": {
                "fragments": [],
                "text": "CoNLL"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710450"
                        ],
                        "name": "J. Zelle",
                        "slug": "J.-Zelle",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Zelle",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Zelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797655"
                        ],
                        "name": "R. Mooney",
                        "slug": "R.-Mooney",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Mooney",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mooney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 263135,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b7c0e47f8b768258b7d536c21b218e6c46ab8791",
            "isKey": false,
            "numCitedBy": 642,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents recent work using the CHILL parser acquisition system to automate the construction of a natural-language interface for database queries. CHILL treats parser acquisition as the learning of search-control rules within a logic program representing a shift-reduce parser and uses techniques from Inductive Logic Programming to learn relational control knowledge. Starting with a general framework for constructing a suitable logical form, CHILL is able to train on a corpus comprising sentences paired with database queries and induce parsers that map subsequent sentences directly into executable queries. Experimental results with a complete database-query application for U.S. geography show that CHILL is able to learn parsers that outperform a preexisting, hand-crafted counterpart. These results demonstrate the ability of a corpus-based system to produce more than purely syntactic representations. They also provide direct evidence of the utility of an empirical approach at the level of a complete natural language application."
            },
            "slug": "Learning-to-Parse-Database-Queries-Using-Inductive-Zelle-Mooney",
            "title": {
                "fragments": [],
                "text": "Learning to Parse Database Queries Using Inductive Logic Programming"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Experimental results with a complete database-query application for U.S. geography show that CHILL is able to learn parsers that outperform a preexisting, hand-crafted counterpart, and provide direct evidence of the utility of an empirical approach at the level of a complete natural language application."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI/IAAI, Vol. 2"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2961287"
                        ],
                        "name": "N. Littlestone",
                        "slug": "N.-Littlestone",
                        "structuredName": {
                            "firstName": "Nick",
                            "lastName": "Littlestone",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Littlestone"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 35643382,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ba51a954699dd2df2c89c4972411a6f18235b81d",
            "isKey": false,
            "numCitedBy": 152,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Redundant-noisy-attributes,-attribute-errors,-and-Littlestone",
            "title": {
                "fragments": [],
                "text": "Redundant noisy attributes, attribute errors, and linear-threshold learning using winnow"
            },
            "venue": {
                "fragments": [],
                "text": "COLT '91"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2961287"
                        ],
                        "name": "N. Littlestone",
                        "slug": "N.-Littlestone",
                        "structuredName": {
                            "firstName": "Nick",
                            "lastName": "Littlestone",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Littlestone"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6334230,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a1dace286582d91916fe470d08f30381cf453f20",
            "isKey": false,
            "numCitedBy": 1612,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "Valiant (1984) and others have studied the problem of learning various classes of Boolean functions from examples. Here we discuss incremental learning of these functions. We consider a setting in which the learner responds to each example according to a current hypothesis. Then the learner updates the hypothesis, if necessary, based on the correct classification of the example. One natural measure of the quality of learning in this setting is the number of mistakes the learner makes. For suitable classes of functions, learning algorithms are available that make a bounded number of mistakes, with the bound independent of the number of examples seen by the learner. We present one such algorithm that learns disjunctive Boolean functions, along with variants for learning other classes of Boolean functions. The basic method can be expressed as a linear-threshold algorithm. A primary advantage of this algorithm is that the number of mistakes grows only logarithmically with the number of irrelevant attributes in the examples. At the same time, the algorithm is computationally efficient in both time and space."
            },
            "slug": "Learning-Quickly-When-Irrelevant-Attributes-Abound:-Littlestone",
            "title": {
                "fragments": [],
                "text": "Learning Quickly When Irrelevant Attributes Abound: A New Linear-Threshold Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work presents one such algorithm that learns disjunctive Boolean functions, along with variants for learning other classes of Boolean functions."
            },
            "venue": {
                "fragments": [],
                "text": "28th Annual Symposium on Foundations of Computer Science (sfcs 1987)"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2650619"
                        ],
                        "name": "Andrew R. Golding",
                        "slug": "Andrew-R.-Golding",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Golding",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew R. Golding"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144590225"
                        ],
                        "name": "D. Roth",
                        "slug": "D.-Roth",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Roth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Roth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2769937,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3a463db8048c67b48c7f4f019a4ab3a2f01f25fc",
            "isKey": false,
            "numCitedBy": 126,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Multiplicative weight-updating algorithms such as Winnow have been studied extensively in the COLT literature, but only recently have people started to use them in applications. In this paper, we apply a Winnow-based algorithm to a task in natural language: context-sensitive spelling correction. This is the task of fixing spelling errors that happen to result in valid words, such as substituting {\\it to\\/} for {\\it too}, {\\it casual\\/} for {\\it causal}, and so on. Previous approaches to this problem have been statistics-based; we compare Winnow to one of the more successful such approaches, which uses Bayesian classifiers. We find that: (1)~When the standard (heavily-pruned) set of features is used to describe problem instances, Winnow performs comparably to the Bayesian method; (2)~When the full (unpruned) set of features is used, Winnow is able to exploit the new features and convincingly outperform Bayes; and (3)~When a test set is encountered that is dissimilar to the training set, Winnow is better than Bayes at adapting to the unfamiliar test set, using a strategy we will present for combining learning on the training set with unsupervised learning on the (noisy) test set."
            },
            "slug": "Applying-Winnow-to-Context-Sensitive-Spelling-Golding-Roth",
            "title": {
                "fragments": [],
                "text": "Applying Winnow to Context-Sensitive Spelling Correction"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper applies a Winnow-based algorithm to a task in natural language: context-sensitive spelling correction, and finds that Winnow is better than Bayes at adapting to the unfamiliar test set, using a strategy for combining learning on the training set with unsupervised learn on the (noisy) test set."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693517"
                        ],
                        "name": "David Yarowsky",
                        "slug": "David-Yarowsky",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Yarowsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Yarowsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 97
                            }
                        ],
                        "text": "A partial list consists of Bayesian classifiers (Gale, Church, & Yarowsky 1993), decision lists (Yarowsky 1994), Bayesian hybrids (Golding 1995), HMMs (Charniak 1993), inductive logic methods (Zelle & Mooney 1996), memorybased methods (Zavrel, Daelemans, & Veenstra 1997) and transformation-based\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 73
                            }
                        ],
                        "text": "The surrounding context - word associations and syntactic patterns in this case - are sufficient to identify the correct form."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1580335,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "6903729430e70fa0a564dec6f294424f837781c8",
            "isKey": false,
            "numCitedBy": 484,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a statistical decision procedure for lexical ambiguity resolution. The algorithm exploits both local syntactic patterns and more distant collocational evidence, generating an efficient, effective, and highly perspicuous recipe for resolving a given ambiguity. By identifying and utilizing only the single best disambiguating evidence in a target context, the algorithm avoids the problematic complex modeling of statistical dependencies. Although directly applicable to a wide class of ambiguities, the algorithm is described and evaluated in a realistic case study, the problem of restoring missing accents in Spanish and French text. Current accuracy exceeds 99% on the full task, and typically is over 90% for even the most difficult ambiguities."
            },
            "slug": "Decision-Lists-for-Lexical-Ambiguity-Resolution:-to-Yarowsky",
            "title": {
                "fragments": [],
                "text": "Decision Lists for Lexical Ambiguity Resolution: Application to Accent Restoration in Spanish and French"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "This paper presents a statistical decision procedure for lexical ambiguity resolution that exploits both local syntactic patterns and more distant collocational evidence, generating an efficient, effective, and highly perspicuous recipe for resolving a given ambiguity."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2227181"
                        ],
                        "name": "K. H\u00f6ffgen",
                        "slug": "K.-H\u00f6ffgen",
                        "structuredName": {
                            "firstName": "Klaus-Uwe",
                            "lastName": "H\u00f6ffgen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. H\u00f6ffgen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723095"
                        ],
                        "name": "H. Simon",
                        "slug": "H.-Simon",
                        "structuredName": {
                            "firstName": "Hans",
                            "lastName": "Simon",
                            "middleNames": [
                                "Ulrich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Simon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1823251"
                        ],
                        "name": "K. S. V. Horn",
                        "slug": "K.-S.-V.-Horn",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Horn",
                            "middleNames": [
                                "S.",
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. S. V. Horn"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6707032,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fef36189265d90252106cdfd64e0a8d9d5c4c58d",
            "isKey": false,
            "numCitedBy": 184,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate the problem of learning concepts by presenting labeled and randomly chosen training\u2013examples to single neurons. It is well-known that linear halfspaces are learnable by the method of linear programming. The corresponding (Mc-Culloch-Pitts) neurons are therefore efficiently trainable to learn an unknown halfspace from examples. We want to analyze how fast the learning performance degrades when the representational power of the neuron is overstrained, i.e., if more complex concepts than just halfspaces are allowed. We show that a neuron cannot efficently find its probably almost optimal adjustment (unless RP = NP). If the weights and the threshold of the neuron have a fixed constant bound on their coding length, the situation is even worse: There is in general no polynomial time training method which bounds the resulting prediction error of the neuron by k.opt for a fixed constant k (unless RP = NP). Other variants of learning more complex concepts than halfspaces by single neurons are also investigated. We show that neither heuristical learning nor learning by sigmoidal neurons with a constant reject-rate is efficiently possible (unless RP = NP)."
            },
            "slug": "Robust-trainability-of-single-neurons-H\u00f6ffgen-Simon",
            "title": {
                "fragments": [],
                "text": "Robust trainability of single neurons"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown that a neuron cannot efficently find its probably almost optimal adjustment (unless RP = NP) and neither heuristical learning nor learning by sigmoidal neurons with a constant reject-rate is efficiently possible."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '92"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48816567"
                        ],
                        "name": "Janet D. Fodor",
                        "slug": "Janet-D.-Fodor",
                        "structuredName": {
                            "firstName": "Janet",
                            "lastName": "Fodor",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Janet D. Fodor"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14709073,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "76bf358d52f42e2adf8db5c8e306a3eca8b9ca59",
            "isKey": false,
            "numCitedBy": 404,
            "numCiting": 70,
            "paperAbstract": {
                "fragments": [],
                "text": "A strong claim about human sentence comprehension is that the processing mechanism is fully innate and applies differently to different languages only to the extent that their grammars differ. If so, there is hope for an explanatory project which attributes all parsing \u201cstrategies\u201d to fundamental design characteristics of the parsing device. However, the whole explanatory program is in peril because of the discovery (Cuetos & Mitchell, 1988) that Late Closure is not universal: Spanish, and also Dutch and other languages, favor Early Closure (high attachment) where English favors Late Closure flow attachment). I argue that the universal parser can weather this storm. Exceptions to Late Closure in Spanish and other languages are observed only in one construction (a relative clause attaching into a complex noun phrase [NP]), which is borderline in English too. For other constructions, low attachment is preferred in all languages tested. I propose that what differentiates the complex NP construction is the heaviness of the attachee compared to that of the host configuration. A relative clause is a heavy attachee, and the lower NP alone is small as a host; the relative is therefore better balanced if the whole complex NP is its host. A wide range of facts is accounted for by the principle that a constituent likes to have a sister of its own size. Light constituents will tend to attach low, and heavy ones to attach high, since larger constituents are dominated by higher nodes. A preference for balanced weight is familiar from work on prosodic phrasing. I suggest, therefore, that prosodic processing occurs in parallel with syntactic processing (even in reading) and influences structural ambiguity resolution. Height of attachment ambiguities are resolved by the prosodically motivated same-size-sister constraint. The exceptional behavior of English may be due to its prosodic packaging of a relative pronoun with the adjacent noun, overriding the balance tendency. If this explanation is correct, it is possible that all cross-language variations in parsing preferences are due to cross-language variations in the prosodic component of the competence grammar."
            },
            "slug": "Learning-To-Parse-Fodor",
            "title": {
                "fragments": [],
                "text": "Learning To Parse?"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is suggested that prosodic processing occurs in parallel with syntactic processing and influences structural ambiguity resolution and influencesStructural ambiguity resolution in cross-language parsing preferences is influenced by the prosodically motivated same-size-sister constraint."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741124"
                        ],
                        "name": "L. Valiant",
                        "slug": "L.-Valiant",
                        "structuredName": {
                            "firstName": "Leslie",
                            "lastName": "Valiant",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Valiant"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 2
                            }
                        ],
                        "text": ", (Valiant 1998)) (11)This is hard to define in the context of natural language; typically, this is understood as texts of similar nature; see a discussion of this issue in (Golding & Roth 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 126
                            }
                        ],
                        "text": "In addition, very few of the features in F are active in every example, yielding more efficient evaluation techniques (e.g., (Valiant 1998))\n11This is hard to define in the context of natural language; typically, this is understood as texts of similar nature; see a discussion of this issue in\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14204125,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "14a2e8174947556d4743cc11dae49751ea9867ab",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "A method of combining learning algorithms is described that preserves attribute-efficiency. It yields learning algorithms that require a number of examples that is polynomial in the number of relevant variables and logarithmic in the number of irrelevant ones. The algorithms are simple to implement and realizable on networks with a number of nodes linear in the total number of variables. They include generalizations of Littlestone's Winnow algorithm, and are, therefore, good candidates for experimentation on domains having very large numbers of attributes but where nonlinear hypotheses are sought."
            },
            "slug": "Projection-Learning-Valiant",
            "title": {
                "fragments": [],
                "text": "Projection learning"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "A method of combining learning algorithms is described that preserves attribute-efficiency and yields learning algorithms that require a number of examples that is polynomial in the number of relevant variables and logarithmic in the total number of variables."
            },
            "venue": {
                "fragments": [],
                "text": "COLT' 98"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693517"
                        ],
                        "name": "David Yarowsky",
                        "slug": "David-Yarowsky",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Yarowsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Yarowsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1487550,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "944cba683d10d8c1a902e05cd68e32a9f47b372e",
            "isKey": false,
            "numCitedBy": 2536,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents an unsupervised learning algorithm for sense disambiguation that, when trained on unannotated English text, rivals the performance of supervised techniques that require time-consuming hand annotations. The algorithm is based on two powerful constraints---that words tend to have one sense per discourse and one sense per collocation---exploited in an iterative bootstrapping procedure. Tested accuracy exceeds 96%."
            },
            "slug": "Unsupervised-Word-Sense-Disambiguation-Rivaling-Yarowsky",
            "title": {
                "fragments": [],
                "text": "Unsupervised Word Sense Disambiguation Rivaling Supervised Methods"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "An unsupervised learning algorithm for sense disambiguation that, when trained on unannotated English text, rivals the performance of supervised techniques that require time-consuming hand annotations."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056642528"
                        ],
                        "name": "M. Kearns",
                        "slug": "M.-Kearns",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Kearns",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kearns"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6243824,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d474299d7a51b89a1d7394d426cf881a89b8013d",
            "isKey": false,
            "numCitedBy": 217,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "A model of machine learning in which the concept to be learned may exhibit uncertain or probabilistic behavior is investigated. Such probabilistic concepts (or p-concepts) may arise in situations such as weather prediction, where the measured variables and their accuracy are insufficient to determine the outcome with certainty. It is required that learning algorithms be both efficient and general in the sense that they perform well for a wide class of p-concepts and for any distribution over the domain. Many efficient algorithms for learning natural classes of p-concepts are given, and an underlying theory of learning p-concepts is developed in detail.<<ETX>>"
            },
            "slug": "Efficient-distribution-free-learning-of-concepts-Kearns-Schapire",
            "title": {
                "fragments": [],
                "text": "Efficient distribution-free learning of probabilistic concepts"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "A model of machine learning in which the concept to be learned may exhibit uncertain or probabilistic behavior is investigated, and an underlying theory of learning p-concepts is developed in detail."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1718611"
                        ],
                        "name": "L. Mangu",
                        "slug": "L.-Mangu",
                        "structuredName": {
                            "firstName": "Lidia",
                            "lastName": "Mangu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Mangu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145022783"
                        ],
                        "name": "E. Brill",
                        "slug": "E.-Brill",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Brill",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Brill"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2019904,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e0ad46338401cf4259ed33f675833446455ac764",
            "isKey": false,
            "numCitedBy": 150,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a new approach to automatically learning linguistic knowledge for spelling correction. A major feature of this approach is the fact that the acquired knowledge is captured in a small set of easily understood rules, as opposed to a large set of opaque features and weights. A perspicuous representation is advantageous in order to best exploit human intuition to understand and improve upon the acquired knowledge of the system."
            },
            "slug": "Automatic-Rule-Acquisition-for-Spelling-Correction-Mangu-Brill",
            "title": {
                "fragments": [],
                "text": "Automatic Rule Acquisition for Spelling Correction"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "This paper describes a new approach to automatically learning linguistic knowledge for spelling correction that is captured in a small set of easily understood rules, as opposed to a large set of opaque features and weights."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741124"
                        ],
                        "name": "L. Valiant",
                        "slug": "L.-Valiant",
                        "structuredName": {
                            "firstName": "Leslie",
                            "lastName": "Valiant",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Valiant"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 153
                            }
                        ],
                        "text": "Why do we hope that a classi er learned from the training corpus will perform well (on the test data) ? Informally, the basic theorem of learning theory (Valiant 1984; Vapnik 1995) guarantees that, if the training data and the test data are sampled from the same distribution11, good performance on the training corpus guarantees good performance on the test corpus."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 50
                            }
                        ],
                        "text": "Informally, the basic theorem of learning theory (Valiant 1984; Vapnik 1995) guarantees that, if the training data and the test data are sampled from the same distribution11,\nonly on its presence."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 59712,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "10ddb646feddc12337b5a755c72e153e37088c02",
            "isKey": false,
            "numCitedBy": 4191,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Humans appear to be able to learn new concepts without needing to be programmed explicitly in any conventional sense. In this paper we regard learning as the phenomenon of knowledge acquisition in the absence of explicit programming. We give a precise methodology for studying this phenomenon from a computational viewpoint. It consists of choosing an appropriate information gathering mechanism, the learning protocol, and exploring the class of concepts that can be learnt using it in a reasonable (polynomial) number of steps. We find that inherent algorithmic complexity appears to set serious limits to the range of concepts that can be so learnt. The methodology and results suggest concrete principles for designing realistic learning systems."
            },
            "slug": "A-theory-of-the-learnable-Valiant",
            "title": {
                "fragments": [],
                "text": "A theory of the learnable"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper regards learning as the phenomenon of knowledge acquisition in the absence of explicit programming, and gives a precise methodology for studying this phenomenon from a computational viewpoint."
            },
            "venue": {
                "fragments": [],
                "text": "STOC '84"
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35229948"
                        ],
                        "name": "S. Katz",
                        "slug": "S.-Katz",
                        "structuredName": {
                            "firstName": "Slava",
                            "lastName": "Katz",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Katz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6555412,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b0130277677e5b915d5cd86b3afafd77fd08eb2e",
            "isKey": false,
            "numCitedBy": 1908,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "The description of a novel type of m-gram language model is given. The model offers, via a nonlinear recursive procedure, a computation and space efficient solution to the problem of estimating probabilities from sparse data. This solution compares favorably to other proposed methods. While the method has been developed for and successfully implemented in the IBM Real Time Speech Recognizers, its generality makes it applicable in other areas where the problem of estimating probabilities from sparse data arises."
            },
            "slug": "Estimation-of-probabilities-from-sparse-data-for-of-Katz",
            "title": {
                "fragments": [],
                "text": "Estimation of probabilities from sparse data for the language model component of a speech recognizer"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "The model offers, via a nonlinear recursive procedure, a computation and space efficient solution to the problem of estimating probabilities from sparse data, and compares favorably to other proposed methods."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Acoust. Speech Signal Process."
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143707112"
                        ],
                        "name": "M. Collins",
                        "slug": "M.-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067634566"
                        ],
                        "name": "James Brooks",
                        "slug": "James-Brooks",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Brooks",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Brooks"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 543,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bc9e5bf851dc95369e26f1869c2637b1d8919e6c",
            "isKey": false,
            "numCitedBy": 224,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent work has considered corpus-based or statistical approaches to the problem of prepositional phrase attachment ambiguity. Typically, ambiguous verb phrases of the form v np1 p np2 are resolved through a model which considers values of the four head words (v, n1, p and n2). This paper shows that the problem is analogous to n-gram language models in speech recognition, and that one of the most common methods for language modeling, the backed-off estimate, is applicable. Results on Wall Street Journal data of 84.5% accuracy are obtained using this method. A surprising result is the importance of low-count events \u2014 ignoring events which occur less than 5 times in training data reduces performance to 81.6%."
            },
            "slug": "Prepositional-Phrase-Attachment-through-a-Model-Collins-Brooks",
            "title": {
                "fragments": [],
                "text": "Prepositional Phrase Attachment through a Backed-off Model"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "This paper shows that the problem of prepositional phrase attachment ambiguity is analogous to n-gram language models in speech recognition, and that one of the most common methods for language modeling, the backed-off estimate, is applicable."
            },
            "venue": {
                "fragments": [],
                "text": "VLC@ACL"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700597"
                        ],
                        "name": "Jyrki Kivinen",
                        "slug": "Jyrki-Kivinen",
                        "structuredName": {
                            "firstName": "Jyrki",
                            "lastName": "Kivinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jyrki Kivinen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794034"
                        ],
                        "name": "Manfred K. Warmuth",
                        "slug": "Manfred-K.-Warmuth",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Warmuth",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manfred K. Warmuth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 297,
                                "start": 275
                            }
                        ],
                        "text": "\u2026no linear-threshold function can make perfect classifications and still maintain its abovementioned dependence\n13Although for the purpose of the experimental study we do not update the network while testing.\non the number of total and relevant attributes (Littlestone 1991; Kivinen & Warmuth 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15718458,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4dc175e8f6e7ca5c40ffd6fb9c6b92323bf7daf2",
            "isKey": false,
            "numCitedBy": 281,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider two algorithms for on-line prediction based on a linear model. The algorithms are the well-known Gradient Descent (GD) algorithm and a new algorithm, which we call EG *. They both maintain a weight vector using simple updates. For the GD algorithm, the weight vector is updated by subtracting from it the gradient of the squared error made on a prediction multiplied by a parameter called the learning rate. The EG* uses the components of the gradient in the exponents of factors that are used in updating the weight vector multiplicatively. We present worst-case on-line loss bounds for EG* and compare them to previously known bounds for the GD algorithm. The bounds suggest that although the on-line losses of the algorithms are in general incomparable, EG * has a much smaller loss if only few of the input variables are relevant for the predictions. Experiments show that the worst-case upper bounds are quite tight already on simple artificial data. Our main methodological idea is using a distance function between weight vectors both in motivating the algorithms and as a potential function in an amortized analysis that leads to worst-case loss bounds. Using squared Euclidean distance leads to the GD algorithm, and using the relative entropy leads to the EG* algorithm."
            },
            "slug": "Additive-versus-exponentiated-gradient-updates-for-Kivinen-Warmuth",
            "title": {
                "fragments": [],
                "text": "Additive versus exponentiated gradient updates for linear prediction"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The main methodological idea is using a distance function between weight vectors both in motivating the algorithms and as a potential function in an amortized analysis that leads to worst-case loss bounds."
            },
            "venue": {
                "fragments": [],
                "text": "STOC '95"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700597"
                        ],
                        "name": "Jyrki Kivinen",
                        "slug": "Jyrki-Kivinen",
                        "structuredName": {
                            "firstName": "Jyrki",
                            "lastName": "Kivinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jyrki Kivinen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794034"
                        ],
                        "name": "Manfred K. Warmuth",
                        "slug": "Manfred-K.-Warmuth",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Warmuth",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manfred K. Warmuth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6130401,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "98eed3f082351c4821d1edb315846207a8fefbe9",
            "isKey": false,
            "numCitedBy": 911,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider two algorithm for on-line prediction based on a linear model. The algorithms are the well-known Gradient Descent (GD) algorithm and a new algorithm, which we call EG(+/-). They both maintain a weight vector using simple updates. For the GD algorithm, the update is based on subtracting the gradient of the squared error made on a prediction. The EG(+/-) algorithm uses the components of the gradient in the exponents of factors that are used in updating the weight vector multiplicatively. We present worst-case loss bounds for EG(+/-) and compare them to previously known bounds for the GD algorithm. The bounds suggest that the losses of the algorithms are in general incomparable, but EG(+/-) has a much smaller loss if only a few components of the input are relevant for the predictions. We have performed experiments, which show that our worst-case upper bounds are quite tight already on simple artificial data."
            },
            "slug": "Exponentiated-Gradient-Versus-Gradient-Descent-for-Kivinen-Warmuth",
            "title": {
                "fragments": [],
                "text": "Exponentiated Gradient Versus Gradient Descent for Linear Predictors"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The bounds suggest that the losses of the algorithms are in general incomparable, but EG(+/-) has a much smaller loss if only a few components of the input are relevant for the predictions, which is quite tight already on simple artificial data."
            },
            "venue": {
                "fragments": [],
                "text": "Inf. Comput."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793475"
                        ],
                        "name": "A. Ratnaparkhi",
                        "slug": "A.-Ratnaparkhi",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ratnaparkhi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ratnaparkhi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2046487"
                        ],
                        "name": "Jeffrey C. Reynar",
                        "slug": "Jeffrey-C.-Reynar",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Reynar",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeffrey C. Reynar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781292"
                        ],
                        "name": "S. Roukos",
                        "slug": "S.-Roukos",
                        "structuredName": {
                            "firstName": "Salim",
                            "lastName": "Roukos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roukos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 129886,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "00f20179b9087fbf24b6656008a9380c590d9ec9",
            "isKey": false,
            "numCitedBy": 272,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "A parser for natural language must often choose between two or more equally grammatical parses for the same sentence. Often the correct parse can be determined from the lexical properties of certain key words or from the context in which the sentence occurs. For example in the sentence."
            },
            "slug": "A-Maximum-Entropy-Model-for-Prepositional-Phrase-Ratnaparkhi-Reynar",
            "title": {
                "fragments": [],
                "text": "A Maximum Entropy Model for Prepositional Phrase Attachment"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A parser for natural language must often choose between two or more equally grammatical parses for the same sentence."
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690967"
                        ],
                        "name": "A. Blum",
                        "slug": "A.-Blum",
                        "structuredName": {
                            "firstName": "Avrim",
                            "lastName": "Blum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Blum"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 195325954,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1f23d52268e53f9ea81cc6b367eac55f38090257",
            "isKey": false,
            "numCitedBy": 97,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Empirical-Support-for-Winnow-and-Weighted-Majority-Blum",
            "title": {
                "fragments": [],
                "text": "Empirical Support for Winnow and Weighted-Majority Based Algorithms: Results on a Calendar Scheduling Domain"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2403128"
                        ],
                        "name": "C. Samuelsson",
                        "slug": "C.-Samuelsson",
                        "structuredName": {
                            "firstName": "Christer",
                            "lastName": "Samuelsson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Samuelsson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3714725,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "91d7c7f4e5d094f45428c31e6122aa1c6dd44e10",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "A general, practical method for handling sparse data that avoids held-out data and iterative reestimation is derived from first principles. It has been tested on a part-of-speech tagging task and out-performed (deleted) interpolation with context-independent weights, even when the latter used a globally optimal parameter setting determined a posteriori."
            },
            "slug": "Handling-Sparse-Data-by-Successive-Abstraction-Samuelsson",
            "title": {
                "fragments": [],
                "text": "Handling Sparse Data by Successive Abstraction"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "A general, practical method for handling sparse data that avoids held-out data and iterative reestimation is derived from first principles and has been tested on a part-of-speech tagging task and out-performed interpolation with context-independent weights."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145022783"
                        ],
                        "name": "E. Brill",
                        "slug": "E.-Brill",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Brill",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Brill"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680292"
                        ],
                        "name": "P. Resnik",
                        "slug": "P.-Resnik",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Resnik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Resnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4683457,
            "fieldsOfStudy": [
                "Computer Science",
                "Linguistics"
            ],
            "id": "ea8f1c73422b087738827a665b2aaf9b93d5c543",
            "isKey": false,
            "numCitedBy": 266,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we describe a new corpus-based approach to prepositional phrase attachment disambiguation, and present results comparing performance of this algorithm with other corpus-based approaches to this problem."
            },
            "slug": "A-Rule-Based-Approach-to-Prepositional-Phrase-Brill-Resnik",
            "title": {
                "fragments": [],
                "text": "A Rule-Based Approach to Prepositional Phrase Attachment Disambiguation"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "A new corpus- based approach to prepositional phrase attachment disambiguation is described, and results comparing performance of this algorithm with other corpus-based approaches to this problem are presented."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145022783"
                        ],
                        "name": "E. Brill",
                        "slug": "E.-Brill",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Brill",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Brill"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 107
                            }
                        ],
                        "text": "Given that, a sentence s can be represented as the set of all active features in it s = (xi1 , xi2 , . . . xim)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 65
                            }
                        ],
                        "text": "The methods compared use context and collocation features as in (Brill 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 40
                            }
                        ],
                        "text": "In the study presented here, following (Brill 1995) and many other studies there are 47 different tags."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 294,
                                "start": 284
                            }
                        ],
                        "text": "\u2026of Bayesian classifiers (Gale, Church, & Yarowsky 1993), decision lists (Yarowsky 1994), Bayesian hybrids (Golding 1995), HMMs (Charniak 1993), inductive logic methods (Zelle & Mooney 1996), memorybased methods (Zavrel, Daelemans, & Veenstra 1997) and transformation-based learning (Brill 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 67
                            }
                        ],
                        "text": "Transformation Based Learning (TBL) Transformation based learning (Brill 1995) is a machine learning approach for rule learning."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In all cases we show that our approach either outperforms other methods tried for these tasks or performs comparably to the best."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 69
                            }
                        ],
                        "text": "In its most general setting, the TBL hypothesis is not a classifier (Brill 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 5
                            }
                        ],
                        "text": "See (Brill 1995) for a survey of much of the work that has been done on POS in the past few years."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "From now on we will therefore treat classifiers as Boolean functions, h : {0, 1}n \u2192 {0, 1}."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 134248,
            "fieldsOfStudy": [
                "Materials Science"
            ],
            "id": "2b2eb4a9bb146e3ffaa0b025fba0ed14240c683f",
            "isKey": true,
            "numCitedBy": 1821,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "A method of injection molding wherein a pair of separable mold plates are initially urged together and fluid plastic is injected into a mold cavity formed between the mold plates to form an article. The injection pressure of the fluid plastic is utilized to generate forces sufficient to overcome the internal forces urging the mold plates apart and thus hold the mold plates together until the material being molded solidifies either by cooling, chemical reaction or phase change."
            },
            "slug": "Transformation-Based-Error-Driven-Learning-and-A-in-Brill",
            "title": {
                "fragments": [],
                "text": "Transformation-Based Error-Driven Learning and Natural Language Processing: A Case Study in Part-of-Speech Tagging"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "Injection molding wherein a pair of separable mold plates are initially urged together and fluid plastic is injected into a mold cavity formed between the mold plates to form an article."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Linguistics"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1643845523"
                        ],
                        "name": "F. ChenStanley",
                        "slug": "F.-ChenStanley",
                        "structuredName": {
                            "firstName": "F",
                            "lastName": "ChenStanley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. ChenStanley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1643789739"
                        ],
                        "name": "GoodmanJoshua",
                        "slug": "GoodmanJoshua",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "GoodmanJoshua",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "GoodmanJoshua"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 215842252,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d4e8bed3b50a035e1eabad614fe4218a34b3b178",
            "isKey": false,
            "numCitedBy": 2861,
            "numCiting": 87,
            "paperAbstract": {
                "fragments": [],
                "text": "We survey the most widely-used algorithms for smoothing models for language n -gram modeling. We then present an extensive empirical comparison of several of these smoothing techniques, including t..."
            },
            "slug": "An-empirical-study-of-smoothing-techniques-for-ChenStanley-GoodmanJoshua",
            "title": {
                "fragments": [],
                "text": "An empirical study of smoothing techniques for language modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "A survey of the most widely-used algorithms for smoothing models for language n -gram modeling and an extensive empirical comparison of several of these smoothing techniques are presented."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056642528"
                        ],
                        "name": "M. Kearns",
                        "slug": "M.-Kearns",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Kearns",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kearns"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46753437"
                        ],
                        "name": "U. Vazirani",
                        "slug": "U.-Vazirani",
                        "structuredName": {
                            "firstName": "Umesh",
                            "lastName": "Vazirani",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Vazirani"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "(See (Vapnik 1995;  Kearns & Vazirani 1992 )) for details)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 44944785,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "97e14147f2e61456bba016f720488410393f9e48",
            "isKey": false,
            "numCitedBy": 1786,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The probably approximately correct learning model Occam's razor the Vapnik-Chervonenkis dimension weak and strong learning learning in the presence of noise inherent unpredictability reducibility in PAC learning learning finite automata by experimentation appendix - some tools for probabilistic analysis."
            },
            "slug": "An-Introduction-to-Computational-Learning-Theory-Kearns-Vazirani",
            "title": {
                "fragments": [],
                "text": "An Introduction to Computational Learning Theory"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "The probably approximately correct learning model Occam's razor the Vapnik-Chervonenkis dimension weak and strong learning learning in the presence of noise inherent unpredictability reducibility in PAC learning learning finite automata is described."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35106875"
                        ],
                        "name": "R. Duda",
                        "slug": "R.-Duda",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Duda",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Duda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3108177"
                        ],
                        "name": "P. Hart",
                        "slug": "P.-Hart",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Hart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hart"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 67
                            }
                        ],
                        "text": "The naive Bayes estimation (NB) The naive Bayes estimation (e.g., (Duda & Hart 1973)) assumes that given the class value c \u2208 C the features values are statistically independent."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12946615,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b07ce649d6f6eb636872527104b0209d3edc8188",
            "isKey": false,
            "numCitedBy": 16927,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Provides a unified, comprehensive and up-to-date treatment of both statistical and descriptive methods for pattern recognition. The topics treated include Bayesian decision theory, supervised and unsupervised learning, nonparametric techniques, discriminant analysis, clustering, preprosessing of pictorial data, spatial filtering, shape description techniques, perspective transformations, projective invariants, linguistic procedures, and artificial intelligence techniques for scene analysis."
            },
            "slug": "Pattern-classification-and-scene-analysis-Duda-Hart",
            "title": {
                "fragments": [],
                "text": "Pattern classification and scene analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "The topics treated include Bayesian decision theory, supervised and unsupervised learning, nonparametric techniques, discriminant analysis, clustering, preprosessing of pictorial data, spatial filtering, shape description techniques, perspective transformations, projective invariants, linguistic procedures, and artificial intelligence techniques for scene analysis."
            },
            "venue": {
                "fragments": [],
                "text": "A Wiley-Interscience publication"
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32266599"
                        ],
                        "name": "J. Brodsky",
                        "slug": "J.-Brodsky",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Brodsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Brodsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 148
                            }
                        ],
                        "text": "We present here only the bottom-line results of an extensive study that appears in companion reports (Golding & Roth 1998; Krymolovsky & Roth 1998; Roth & Zelenko 1998)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 157
                            }
                        ],
                        "text": "In the results presented here, however, we present the performance without the recycling process, so that we maintain the linear function expressivity (see (Roth & Zelenko 1998) for details)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 239,
                                "start": 220
                            }
                        ],
                        "text": "(Collins & Brooks 1995) reports results of 84.4% on a different enhanced set of features, but other systems were not evaluated on these sets.\nchallenge to our approach, as the problem is a multiclass prediction problem (Roth & Zelenko 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 191482121,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "26ad4ccfc5af612c40fe7cfeaac0650b1c069875",
            "isKey": true,
            "numCitedBy": 48,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Winner of the 1987 Nobel Prize for Literature. Joseph Brodsky's second major collection contains poems from the years 1965 to 1978, translated from the Russian. Despite isolation from his native Russian culture and language since becoming an involuntary exile in 1972, Brodsky has inherited the tradition of Mandelstam, Akhmatova, and Pasternak. Restrained, sometimes humorous, sometimes brilliantly epigrammatic, his poems possess the serious, unfrivolous wit of his metaphysical forebears."
            },
            "slug": "A-Part-of-Speech-Brodsky",
            "title": {
                "fragments": [],
                "text": "A Part of Speech"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741124"
                        ],
                        "name": "L. Valiant",
                        "slug": "L.-Valiant",
                        "structuredName": {
                            "firstName": "Leslie",
                            "lastName": "Valiant",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Valiant"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 78
                            }
                        ],
                        "text": "One reason is that the SNOW architecture, influenced by the Neuroidal system (Valiant 1994), is being used in a system developed for the purpose of learning knowledge representations for natural language understanding tasks, and is being evaluated on a variety of tasks for which the node allocation\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "One reason is that the SNOW architecture, influenced by the Neuroidal system (Valiant 1994), is being used in a system"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 34191174,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cc79c2950f47678c6cb2160bfdf1f155276f240b",
            "isKey": false,
            "numCitedBy": 177,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "1: The Approach. 2: Biological Constraints. 2.1: Introduction. 2.2: The Neocortex. 2.3: Pyramidal Neurons. 3: Computational Laws. 3.1: Introduction. 3.2: Three Sources of Complexity. 4: Cognitive Functions. 4.1: Introduction. 4.2: Boolean Functions. 4.3: Learning. 4.4: The Nature of Concepts. 4.5: Experimental Psychology. 5: The Neuroidal Model. 5.1: Programmable Models. 5.2: Neuroids. 5.3: Timing. 6: Knowledge Representations. 6.1: Positive Knowledge Representations. 6.2: Vicinal Algorithms. 6.3: Frontier Properties and Storing New Items. 6.4: Frontier Properties and Associations. 6.5: Hashing. 7: Unsupervised Memorization. 7.1: An Algorithm. 8: Superivsed Memorization. 8.1: Introduction. 8.2: A Simple Algorithm. 8.3: A Second Algorithm. 9: Supervised Inductive Learning. 9.1: Introduction. 9.2: Pac Learning. 9.3: Learning Conjunctions. 9.4: Learning Disjunctions. 9.5: Learning Linear Threshold Functions. 10: Correlational Learning. 10.1: An Algorithm. 10.2: Computing with Numerical Values. 11: Objects and Relational Expressions. 11.1: Multiple Object Scenes. 11.2: Relations. 11.3: Timed Conjunctions. 11.4: Memorizing Expressions Containing Relations. 11.5: Memorizing New Relations. 11.6: Discussion. 12: Systems Questions. 12.1: Introduction. 12.2: General Organizational Principles. 12.3: Compatibility of Mechanisms. 13: Reasoning. 13.1: Introduction. 13.2: Relfex Reasoning. 13.3: Simple Reflex Reasoning. 13.4: Compound Reflex Reasoning. 13.5: Nonmonotonic Phenomena. 14: More Detailed Neural Models. 14.1: Implementing Vicinal Algorithms. 14.2: A Laminar Model. 14.3: A Columnar Model. 14.4: Sparser Random Graphs. 14.5: Another Columnar Model. 15: Afterword. Notes. Exercises. References. Index of Notation. Index"
            },
            "slug": "Circuits-of-the-mind-Valiant",
            "title": {
                "fragments": [],
                "text": "Circuits of the mind"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This chapter discusses the development of Vicinal Algorithms in the context of Neural Models, a model based on the Neocortex, which automates the very labor-intensive and therefore time-heavy and expensive process of learning."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7138354,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8213dbed4db44e113af3ed17d6dad57471a0c048",
            "isKey": false,
            "numCitedBy": 38756,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "Setting of the learning problem consistency of learning processes bounds on the rate of convergence of learning processes controlling the generalization ability of learning processes constructing learning algorithms what is important in learning theory?."
            },
            "slug": "The-Nature-of-Statistical-Learning-Theory-Vapnik",
            "title": {
                "fragments": [],
                "text": "The Nature of Statistical Learning Theory"
            },
            "venue": {
                "fragments": [],
                "text": "Statistics for Engineering and Information Science"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 70
                            }
                        ],
                        "text": "To summarize, we have shown: claim: All the methods discussed \u2013 NB, BO, TBL and p1-DL search for a decision surface which is a linear function in the feature space."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 58
                            }
                        ],
                        "text": "Irrespective of the learning procedure used to derive the TBL representation, we focus here on the final hypothesis used by TBL and how it is evaluated, given an input sentence, to produce a prediction."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 60
                            }
                        ],
                        "text": "However, in many applications and, in particular, in Spell (Mangu & Brill 1997) and PPA (Brill & Resnik 1994) which we discuss later, this is not the case."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 155
                            }
                        ],
                        "text": "Using the terminology introduced above, let (xi1 , ci1), (xi2 , ci2), . . . (xik , cik) be the ordered sequence of rules defining the output hypothesis of TBL. (Notice that it is quite possible, and happens often in practice, for a feature to appear more than once in this sequence, even with different consequents)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 94
                            }
                        ],
                        "text": "There, the conditions do not depend on the labels, and therefore the output hypothesis of the TBL method can be viewed as a classifier."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 18
                            }
                        ],
                        "text": "The hypothesis of TBL is an ordered list of transformations."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 31
                            }
                        ],
                        "text": "Transformation Based Learning (TBL) Transformation based learning (Brill 1995) is a machine learning approach for rule learning."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 19
                            }
                        ],
                        "text": "Alternatively, the TBL hypothesis can be represented as a (positive) 1-Decision-List (p1-DL) (Rivest 1987), over the set F of features9."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 59
                            }
                        ],
                        "text": "The results presented for the TBL14 method are taken from (Mangu & Brill 1997) and represent an average on a subset of 14 of these, all of size 2."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 33
                            }
                        ],
                        "text": "In its most general setting, the TBL hypothesis is not a classifier (Brill 1995)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 48
                            }
                        ],
                        "text": "TBL was also used with an enhanced feature set (Mangu & Brill 1997) with improved results of 93.3% but we have not run the other systems with this set of features.\nthe steering wheel or the verb phrase, as in Buy the car with his money."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 9
                            }
                        ],
                        "text": "SNOW and TBL were trained and tested on the same data."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 29
                            }
                        ],
                        "text": "We compare SNOW against TBL (Mangu & Brill 1997) and a naive-Bayes based system (NB)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 98
                            }
                        ],
                        "text": "This task, however, is known to\n12In practice, when using p1-DL as the hypothesis class (i.e., in TBL) an effort is made to discard many of the features and by that reduce the complexity of the space; however, this process, which is data driven and does not a-priori restrict the function class can be employed by other methods as well (e.g., (Blum 1995)) and is therefore orthogonal to these arguments.\nbe NP-hard (Ho\u0308ffgen & Simon 1992), so we need to resort to heuristics."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 40
                            }
                        ],
                        "text": "An ordered list of transformations (the TBL hypothesis), is evaluated as follows: given a sentence s, an initial label c \u2208 C is assigned to it."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 30
                            }
                        ],
                        "text": "The results presented for the TBL and BO are on the same data set, taken from (Collins & Brooks 1995)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Automatic rule acquisi"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 73
                            }
                        ],
                        "text": "Fact 1 is well known; 2 and 3 can be derived directly from the de nition (Roth 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 156
                            }
                        ],
                        "text": "However, a more important consequence to our discussion here is the fact that not all linearly separable functions can be represented using this predictor (Roth 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 210,
                                "start": 201
                            }
                        ],
                        "text": "Fact 3: The VC dimension of the class of linear separators derived by either NB or BO over n variables is bounded below by n.\nFact 1 is well known; 2 and 3 can be derived directly from the definition (Roth 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 112
                            }
                        ],
                        "text": "We present here only the bottom-line results of an extensive study that appears in companion reports (Golding & Roth 1998; Krymolovsky & Roth 1998; Roth & Zelenko 1998)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 107
                            }
                        ],
                        "text": "Part-of-speech tagging suggests a special\n15SNOW was evaluated with an enhanced feature set (Krymolovsky & Roth 1998) with improved results of 84.8%."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning to resolve natural language ambi"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 228,
                                "start": 210
                            }
                        ],
                        "text": "Consequently, the approach describe in the next section makes use of the Winnow algorithmwhich is known to produce good results when a linear separator exists, as well as under certain more relaxed assumptions (Littlestone 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 228,
                                "start": 212
                            }
                        ],
                        "text": "Consequently, the approach describe in the next section makes use of the Winnow algorithm which is known to produce good results when a linear separator exists, as well as under certain more relaxed assumptions (Littlestone 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 273,
                                "start": 257
                            }
                        ],
                        "text": "\u2026no linear-threshold function can make perfect classifications and still maintain its abovementioned dependence\n13Although for the purpose of the experimental study we do not update the network while testing.\non the number of total and relevant attributes (Littlestone 1991; Kivinen & Warmuth 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 344,
                                "start": 302
                            }
                        ],
                        "text": "Winnow was shown to learn e ciently any linear threshold function and to be robust in the presence of various kinds of noise, and in cases where no linear-threshold function can make perfect classi cations and still maintain its abovementioned dependence on the number of total and relevant attributes (Littlestone 1991; Kivinen & Warmuth 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Redundant noisy attributes, attribute"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 263,
                                "start": 252
                            }
                        ],
                        "text": "However, all learning methods are statistical in the sense that they attempt to make inductive generalization from observed data and use it to make inferences with respect to previously unseen data; as such, the statistical based theories of learning (Vapnik 1995) apply equally to both."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 153
                            }
                        ],
                        "text": "Why do we hope that a classi er learned from the training corpus will perform well (on the test data) ? Informally, the basic theorem of learning theory (Valiant 1984; Vapnik 1995) guarantees that, if the training data and the test data are sampled from the same distribution11, good performance on the training corpus guarantees good performance on the test corpus."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 64
                            }
                        ],
                        "text": "Informally, the basic theorem of learning theory (Valiant 1984; Vapnik 1995) guarantees that, if the training data and the test data are sampled from the same distribution11,\nonly on its presence."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 6
                            }
                        ],
                        "text": "(See (Vapnik 1995; Kearns & Vazirani 1992)) for details)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 59752996,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "5451278e1a11cf3f1be28a05f38d36c8641e68f7",
            "isKey": true,
            "numCitedBy": 4580,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Nature-of-Statistical-Learning-Vapnik",
            "title": {
                "fragments": [],
                "text": "The Nature of Statistical Learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Decision lists for lexlcal ambiguity resolution: application to accent"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In all cases we show that our approach either outperforms other methods tried for these tasks or performs comparably to the best."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Transformation-based errordriven learning and natural language processing: A case study in part of speech tagging"
            },
            "venue": {
                "fragments": [],
                "text": "Computational Linguistics"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 101
                            }
                        ],
                        "text": "This complexity is measured in terms of a combinatorial parameter - the VC-dimension of the class H (Vapnik 1982) - which measures the richness of the function class."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Estimation of Dependences Based on"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 154
                            }
                        ],
                        "text": "Why do we hope that a classifier learned from the training corpus will perform well (on the test data) ? Informally, the basic theorem of learning theory (Valiant 1984; Vapnik 1995) guarantees that, if the training data and the test data are sampled from the same distribution(11),"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 50
                            }
                        ],
                        "text": "Informally, the basic theorem of learning theory (Valiant 1984; Vapnik 1995) guarantees that, if the training data and the test data are sampled from the same distribution11,\nonly on its presence."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A theory of the learnable. Communications of the ACM 27(11):1134\u20131142"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning to resolve natural language ambiguities : a unified approach . Long Version , in Preparation . [ Samuelsson 1996 ]"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Efficient distributionfree learning of probabilistlc concepts"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Computer and System"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning to resolve natural language ambiguities:a uniied approach Long Version, in Preparation. Samuelsson, C. 1996. Handling sparse data by successive abstraction A theory of the learnable"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. of COLING. Valiant, L. G1134{1142. Valiant, L. G. 1994. Circuits of the Mind"
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 95
                            }
                        ],
                        "text": "A partial list consists of Bayesian classi ers (Gale, Church, & Yarowsky 1993), decision lists (Yarowsky 1994), Bayesian hybrids (Golding 1995), HMMs (Charniak 1993), inductive logic methods (Zelle & Mooney 1996), memorybased methods (Zavrel, Daelemans, & Veenstra 1997) and transformation-based learning (Brill 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 97
                            }
                        ],
                        "text": "A partial list consists of Bayesian classifiers (Gale, Church, & Yarowsky 1993), decision lists (Yarowsky 1994), Bayesian hybrids (Golding 1995), HMMs (Charniak 1993), inductive logic methods (Zelle & Mooney 1996), memorybased methods (Zavrel, Daelemans, & Veenstra 1997) and transformation-based\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Decision lists for lexical ambiguity"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 297,
                                "start": 275
                            }
                        ],
                        "text": "\u2026no linear-threshold function can make perfect classifications and still maintain its abovementioned dependence\n13Although for the purpose of the experimental study we do not update the network while testing.\non the number of total and relevant attributes (Littlestone 1991; Kivinen & Warmuth 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Exponentiated gradient versus gradient descent"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning to resolve natural language amhigulties:a unified approach Long Version, in Preparation. Samuelsson, C. 1996. Handling sparse data by successive abstraction A theory of the learnable"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. of COLING. Valiant, L. G Valiant, L. G. 1994. Circuits of the Mind"
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 102
                            }
                        ],
                        "text": "We present here only the bottom-line results of an extensive study that appears in companion reports (Golding & Roth 1998; Krymolovsky & Roth 1998; Roth & Zelenko 1998)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A winnow-based"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Estimation of probabilities from sparse"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Pattern Classification and Scene"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1973
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Pattern Classification and Scene"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1973
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 90
                            }
                        ],
                        "text": "(Usually, these are maximum likelihood estimates evaluated using iterative methods, e.g. (Samuelsson 1996))."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Handling sparse data by successive"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning to resolve natural language ambiguities:a unified approach. Long Version"
            },
            "venue": {
                "fragments": [],
                "text": "Learning to resolve natural language ambiguities:a unified approach. Long Version"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Pattern Classification and Scene"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1973
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 142
                            }
                        ],
                        "text": "A large number of different kinds of ambiguities are to be resolved simultaneously in performing any higher level natural language inference (Cardie 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Embedded Machine Learning Systems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An empirical study of smoothing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 169
                            }
                        ],
                        "text": "\u2026complexity of the space; however, this process, which is data driven and does not a-priori restrict the function class can be employed by other methods as well (e.g., (Blum 1995)) and is therefore orthogonal to these arguments.\nbe NP-hard (Ho\u0308ffgen & Simon 1992), so we need to resort to heuristics."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 2
                            }
                        ],
                        "text": ", (Blum 1995)) and is therefore orthogonal to these arguments."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Empirical support for Winnow"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Prepositional phrase attachment"
            },
            "venue": {
                "fragments": [],
                "text": "Preparation. Littlestone, N. 1988. Learning quickly when"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 17
                            }
                        ],
                        "text": "of the class 7-/ (Vapnik 1982) - which measures the"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 101
                            }
                        ],
                        "text": "This complexity is measured in terms of a combinatorial parameter - the VC-dimension of the class H (Vapnik 1982) - which measures the richness of the function class."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Estimation of Dependences"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Prepositional phrase attachment"
            },
            "venue": {
                "fragments": [],
                "text": "In Preparation"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A winnow-based approach to word correction"
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning. Special issue on Machine Learning and Natural Language;"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Robust trainability"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 62
                            }
                        ],
                        "text": "This argument can be made formal by appealing to a result of (Kearns & Schapire 1994), which shows that even when there is no perfect classifier, the optimal linear separator on a polynomial size set of training examples is optimal (in a precise sense) also on the test data."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Efficient distributionfree learning of probabilistlc concepts"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Computer and System Sciences"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Applying winnow to contextsensltive spelling correction"
            },
            "venue": {
                "fragments": [],
                "text": "Proc . of the International Conference on Machine Learning"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 36
                            }
                        ],
                        "text": "A local learning algorithm, Winnow (Littlestone 1988), is used at each target node to learn its dependence on other nodes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning quickly when irrelevant"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A rule-based approach to prepositional phrase attachment disamhiguation"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. of COLING"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 102
                            }
                        ],
                        "text": "We present here only the bottom-line results of an extensive study that appears in companion reports (Golding & Roth 1998; Krymolovsky & Roth 1998; Roth & Zelenko 1998)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A winnowbased approach to word correction"
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 93
                            }
                        ],
                        "text": "It has been used in many disambiguation tasks and in learning models for speech recognition (Katz 1987; Chen & Goodman 1996; Collins & Brooks 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Estimation of probabilities from sparse"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Prepositional Phrase Attachment The problem is to decide whether the Prepositional Phrase (PP) attaches to the noun phrase, as in Buy the car with References"
            },
            "venue": {
                "fragments": [],
                "text": "Prepositional Phrase Attachment The problem is to decide whether the Prepositional Phrase (PP) attaches to the noun phrase, as in Buy the car with References"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 134
                            }
                        ],
                        "text": "Decision Lists (p1-DL) It is easy to see (details omitted), that the above analysis applies to p1-DL, a method used, for example, in (Yarowsky 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Unsupervised word sense disambigua"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 18,
            "methodology": 18,
            "result": 5
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 70,
        "totalPages": 7
    },
    "page_url": "https://www.semanticscholar.org/paper/Learning-to-Resolve-Natural-Language-Ambiguities:-A-Roth/3ed17a1114e2dc48597ab17cc8d5234006f525c9?sort=total-citations"
}